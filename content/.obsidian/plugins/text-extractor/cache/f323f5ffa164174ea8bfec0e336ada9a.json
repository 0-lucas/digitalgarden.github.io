{"path":"Data Engineering/My Books/Introduction to Probability, Statistics, and Random Processes - Hossein Pishro-Nik.pdf","text":"Preface In tro duc tio n a n d Go a ls For years, I have been joking with my students that I would teach probability with the same level of excitement even if I were woken up in the middle of the night and asked to teach it. Y ears later , as a new father , I started writing this book when it became clear to me that I would not be sleeping at night for the foreseeable future. This book is intended for undergraduate and first-year graduate-level courses in probability , statistics, and random processes. My goal has been to provide a clear and intuitive approach to these topics while maintaining an acceptable level of mathematical accuracy . I have been teaching two courses on this subject for several years at the University of Massachusetts Amherst. While one of these courses is an undergraduate course taken by juniors, the other is a graduate-level course taken by our first-year Masters and PhD students. My goal throughout this process has been to write a textbook that has the flexibility to be used in both courses while sacrificing neither the quality nor the presentational needs of either course. T o achieve such a goal, I have tried to minimize the dependency between dif ferent sections of the book. In particular , when a small part from a dif ferent section of the book is useful elsewhere within the text, I have repeated said part rather than simply referring to it. My reasoning for doing so is twofold. Firstly , this format should make it easier for students to read the book and, secondly , this format should allow instructors the flexibility to select individual sections from the book more easily . Additionally , I wanted the book to be easy to read and accessible as a self-study reference. It was also imperative that the book be available to anyone in the world, and as such the book in its entirety can be found online at www .probabilitycourse.com. The book contains a large number of solved exercises. In addition to the examples found within the text, there is a set of solved problems at the end of each section. Detailed and step-by-step solutions to these problems are provided to help students learn problem-solving techniques. The solutions to the end-of-chapter problems, however , are available only to instructors. Lastly , throughout the book, some examples of applications −such as engineering, finance, everyday life, etc. −are provided to aid in motivating the subject. These examples have been worded to be understandable to all students. As such, some technical issues have been left out. C o vera ge After a brief review of set theory and other required mathematical concepts, the text covers topics as follows: Chapters 1 and 2: basic concepts such as random experiments, probability axioms, conditional probability , law of total probability , Bayes' rule, and counting methods; Chapters 3 through 6: single and multiple random variables (discrete, continuous, and mixed), as well as moment-generating functions, characteristics functions, random vectors, and inequalities; Chapter 7: limit theorems and convergence; Chapters 8 and 9: Bayesian and classical statistics; Chapters 10: Introduction to random processes, processing of random signals; Chapter 1 1: Poisson processes, discrete-time Markov chains, continuous-time Markov chains, and Brownian motion; Chapter 12: basic methods of generating random variables and simulating probabilistic systems (using MA TLAB); Chapter 13: basic methods of generating random variables and simulating probabilistic systems (using R); Chapter 14: recursive methods; All chapters are available at www.probabilitycourse.com . Chapters 12 through 14 are available as PDFs and are downloadable from the textbook website. Chapters 12 and 13 cover the same material. The dif ference is that the codes in chapter 12 are provided in MA TLAB while the codes in Chapter 13 are provided in R. The reason for this again is to give flexibility to instructors and students to choose whichever they prefer . Nevertheless, students who are unfamiliar with MA TLAB and R should still be able to understand the algorithms. Req uired B a c kgro un d The majority of the text does not require any previous knowledge apart from a one- semester course in calculus. The exceptions to this statement are as follows: Sections 5.2 (T wo Continuous Random V ariables) and 6.1 (Methods for More Than T wo Random V ariables) both require a light introduction to double integrals and partial derivatives; Section 6.1.5 (Random V ectors) uses a few concepts from linear algebra; Section 10.2 (Processing of Random Signals) requires familiarity with the Fourier transform. A c kn o wledgemen ts This project grew out of my educational activities regarding my National Science Foundation CAREER award. I am very thankful to the people in charge of the Open Education Initiative at the University of Massachusetts Amherst. In particular , I am indebted to Charlotte Roh and Marilyn Billings at the UMass Amherst library for all of their help and support. I am grateful to my colleagues Dennis Goeckel and Patrick Kelly , who generously provided their lecture notes to me when I first joined UMass. These notes proved to be very useful in developing my course materials and, eventually , in writing this book. I am also thankful to Mario Parente −who used an early version of this book in his course −for very useful discussions. Many people provided comments and suggestions. I would like to especially thank Hamid Saeedi for reading the manuscript in its entirety and providing very valuable comments. I am indebted to Evan Ray and Michael Miller for their helpful comments and suggestions, as well as to Eliza Mitchell and Linnea Duley for their detailed review and comments. I am thankful to Alexandra Saracino for her help regarding the figures and illustrations in this book. I would also like to thank Ali Rakhshan, who coauthored the chapter on simulation and who, along with Ali Eslami, helped me with my LaT eX problems. I am grateful to Sofya V orotnikova, Stephen Donahue, Andrey Smirnov , and Elnaz Jedari Fathi for their help with the website. I am thankful to Atiyeh Sakaei-Far for managing the development of the website and its maintenance. I would also like to thank Elnaz Jedari Fathi for designing the book cover . I am indebted to all of my students in my classes, who not only encouraged me with their positive feedback to continue this project, but who also found many typographical errors in the early versions of this book. I am thankful to all of my teaching assistants who helped in various aspects of both the course and the book. Last −but certainly not least −I would like to thank my family for their patience and support. 1.0 Introduction In this chapter we provide some basic concepts and definitions. W e begin with a brief discussion of what probability is. Then we review some mathematical foundations that are needed for developing probability theory . Next we discuss the concept of random experiments and the axioms of probability . We then introduce discrete and continuous probability models. Finally , we discuss conditional probability . 1.1.0 Introduction: What Is Probability? Randomness and uncertainty exist in our daily lives as well as in every discipline in science, engineering, and technology . Probability theory , the subject of the first part of this book, is a mathematical framework that allows us to describe and analyze random phenomena in the world around us. By random phenomena, we mean events or experiments whose outcomes we can't predict with certainty . Let's consider a couple of specific applications of probability in order to get some intuition. First, let's think more carefully about what we mean by the terms \"randomness\" and \"probability\" in the context of one of the simplest possible random experiments: flipping a fair coin. One way of thinking about \"randomness\" is that it's a way of expressing what we don't know . Perhaps if we knew more about the force I flipped the coin with, the initial orientation of the coin, the impact point between my finger and the coin, the turbulence in the air , the surface smoothness of the table the coin lands on, the material characteristics of the coin and the table, and so on, we would be able to definitively say whether the coin would come up heads or tails. However , in the absence of all that information, we cannot predict the outcome of the coin flip. When we say that something is random, we are saying that our knowledge about the outcome is limited, so we can't be certain what will happen. Since the coin is fair , if we don't know anything about how it was flipped, the probability that it will come up heads is 50%, or . What exactly do we mean by this? There are two common interpretations of the word \"probability .\" One is in terms of relative frequency . In other words, if we flip the coin a very large number of times, it will come up heads about of the time. As the number of coin flips increases, the proportion that come up heads will tend to get closer and closer to . In fact, this intuitive understanding of probability is a special case of the law of large numbers , which we will state and prove formally in later chapters of the book. A second interpretation of probability is that it is a quantification of our degree of subjective personal belief that something will happen. To get a sense of what we mean by this, it may be helpful to consider a second example: predicting the weather . 1 2 1 2 1 2 When we think about the chances that it will rain today , we consider things like whether there are clouds in the sky and the humidity . However , the beliefs that we form based on these factors may vary from person to person - dif ferent people may make dif ferent estimates of the probability that it will rain. Often these two interpretations of probability coincide - for instance, we may base our personal beliefs about the chance that it will rain on an assessment of the relative frequency of rain on days with conditions like today . The beauty of probability theory is that it is applicable regardless of the interpretation of probability that we use (i.e., in terms of long-run frequency or degree of belief). Probability theory provides a solid framework to study random phenomena. It starts by assuming axioms of probability , and then building the entire theory using mathematical arguments. Before delving into studying probability theory , let us briefly look at an example showing how probability theory has been applied in a real life system. 1.1.1 Example: Communication Systems Communication systems play a central role in our lives. Everyday , we use our cell phones, access the internet, use our TV remote controls, and so on. Each of these systems relies on transferring information from one place to another . For example, when you talk on the phone, what you say is converted to a sequence of 0's or 1's called information bits . These information bits are then transmitted by your cell phone antenna to a nearby cell tower as shown in Figure 1.1. Fig.1.1 - Transmission of data from a cell phone to a cell tower . The problem that communication engineers must consider is that the transmission is always af fected by noise . That is, some of the bits received at the cell tower are incorrect. For example, your cell phone may transmit the sequence \" 010010 ⋯ , \" while the sequence \" 010110 ⋯ \" might be received at the cell tower . In this case, the fourth bit is incorrect. Errors like this could af fect the quality of the audio in your phone conversation. The noise in the transmission is a random phenomenon. Before sending the transmission we do not know which bits will be af fected. It is as if someone tosses a (biased) coin for each bit and decides whether or not that bit will be received in error . Probability theory is used extensively in the design of modern communication systems in order to understand the behavior of noise in these systems and take measures to correct the errors. This example shows just one application of probability . Y ou can pick almost any discipline and find many applications in which probability is used as a major tool. Randomness is prevalent everywhere, and probability theory has proven to be a powerful way to understand and manage its ef fects. 1.2 Review of Set Theory Probability theory uses the language of sets. As we will see later , probability is defined and calculated for sets. Thus, here we briefly review some basic concepts from set theory that are used in this book. W e discuss set notations, definitions, and operations (such as intersections and unions). W e then introduce countable and uncountable sets. Finally , we briefly discuss functions. This section may seem somewhat theoretical and thus less interesting than the rest of the book, but it lays the foundation for what is to come. A set is a collection of some items (elements). W e often use capital letters to denote a set. T o define a set we can simply list all the elements in curly brackets, for example to define a set A that consists of the two elements ♣ and ♢, we write A = {♣, ♢}. T o say that ♢ belongs to A, we write ♢ ∈ A, where \"∈\" is pronounced \"belongs to.\" T o say that an element does not belong to a set, we use ∉. For example, we may write ♡ ∉ A. A set is a collection of things (elements). Note that ordering does not matter , so the two sets {♣, ♢} and {♢, ♣} are equal. W e often work with sets of numbers. Some important sets are given the following example. Example 1. 1 The following sets are used in this book: The set of natural numbers, N = {1, 2, 3, ⋯}. The set of integers, Z = {⋯ , −3, −2, −1, 0, 1, 2, 3, ⋯}. The set of rational numbers Q. The set of real numbers R. Closed intervals on the real line. For example, [2, 3] is the set of all real numbers x such that 2 ≤ x ≤ 3. Open intervals on the real line. For example (−1, 3) is the set of all real numbers x such that −1 < x < 3. Similarly , [1, 2) is the set of all real numbers x such that 1 ≤ x < 2. The set of complex numbers C is the set of numbers in the form of a + bi, where a, b ∈ R, and i = √−1. We can also define a set by mathematically stating the properties satisfied by the elements in the set. In particular , we may write A = {x|x satisfies some property} or A = {x : x satisfies some property} The symbols \" | \" and \":\" are pronounced \"such that.\" Example 1. 2 Here are some examples of sets defined by stating the properties satisfied by the elements: If the set C is defined as C = {x|x ∈ Z, −2 ≤ x < 10}, then C = {−2, −1, 0, ⋯ , 9}. If the set D is defined as D = {x 2|x ∈ N}, then D = {1, 4, 9, 16, ⋯}. The set of rational numbers can be defined as Q = { |a, b ∈ Z, b ≠ 0}. For real numbers a and b, where a < b, we can write (a, b] = {x ∈ R ∣ a < x ≤ b}. C = {a + bi ∣ a, b ∈ R, i = √−1}. Set A is a subset of set B if every element of A is also an element of B. W e write A ⊂ B, where \"⊂\" indicates \"subset.\" Equivalently , we say B is a superset of A, or B ⊃ A. Example 1. 3 Here are some examples of sets and their subsets: If E = {1, 4} and C = {1, 4, 9}, then E ⊂ C. N ⊂ Z. Q ⊂ R. a b Two sets are equal if they have the exact same elements. Thus, A = B if and only if A ⊂ B and B ⊂ A. For example, {1, 2, 3} = {3, 2, 1}, and {a, a, b} = {a, b}. The set with no elements, i.e., ∅ = {} is the null set or the empty set . For any set A, ∅ ⊂ A. The universal set is the set of all things that we could possibly consider in the context we are studying. Thus every set A is a subset of the universal set. In this book, we often denote the universal set by S (As we will see, in the language of probability theory , the universal set is called the sample space.) For example, if we are discussing rolling of a die, our universal set may be defined as S = {1, 2, 3, 4, 5, 6}, or if we are discussing tossing of a coin once, our universal set might be S = {H, T} ( H for heads and T for tails). 1.2.1 V enn Diagrams Venn diagrams are very useful in visualizing relation between sets. In a Venn diagram any set is depicted by a closed region. Figure 1.2 shows an example of a V enn diagram. In this figure, the big rectangle shows the universal set S. The shaded area shows another set A. Fig.1.2 - V enn Diagram. Figure 1.3 shows two sets A and B, where B ⊂ A. Fig.1.3 - V enn Diagram for two sets A and B, where B ⊂ A. 1.2.2 Set Operations The union of two sets is a set containing all elements that are in A or in B (possibly both). For example, {1, 2} ∪ {2, 3} = {1, 2, 3}. Thus, we can write x ∈ (A ∪ B) if and only if (x ∈ A) or (x ∈ B). Note that A ∪ B = B ∪ A. In Figure 1.4, the union of sets A and B is shown by the shaded area in the V enn diagram. Fig.1.4 - The shaded area shows the set B ∪ A. Similarly we can define the union of three or more sets. In particular , if A1, A2, A3, ⋯ , An are n sets, their union A1 ∪ A2 ∪ A3 ⋯ ∪ An is a set containing all elements that are in at least one of the sets. W e can write this union more compactly by n ⋃ i=1 Ai. For example, if A1 = {a, b, c}, A2 = {c, h}, A3 = {a, d}, then ⋃i Ai = A1 ∪ A2 ∪ A3 = {a, b, c, h, d}. We can similarly define the union of infinitely many sets A1 ∪ A2 ∪ A3 ∪ ⋯. The intersection of two sets A and B, denoted by A ∩ B, consists of all elements that are both in A and–––– B. For example, {1, 2} ∩ {2, 3} = {2}. In Figure 1.5, the intersection of sets A and B is shown by the shaded area using a V enn diagram. Fig.1.5 - The shaded area shows the set B ∩ A. More generally , for sets A1, A2, A3, ⋯, their intersection ⋂i Ai is defined as the set consisting of the elements that are in all Ai's. Figure 1.6 shows the intersection of three sets. Fig.1.6 - The shaded area shows the set A ∩ B ∩ C. The complement of a set A, denoted by Ac or ¯A, is the set of all elements that are in the universal set S but are not in A. In Figure 1.7, ¯A is shown by the shaded area using a V enn diagram. Fig.1.7 - The shaded area shows the set ¯A = Ac. The difference (subtraction) is defined as follows. The set A − B consists of elements that are in A but not in B. For example if A = {1, 2, 3} and B = {3, 5}, then A − B = {1, 2}. In Figure 1.8, A − B is shown by the shaded area using a V enn diagram. Note that A − B = A ∩ B c. Fig.1.8 - The shaded area shows the set A − B. Two sets A and B are mutually exclusive or disjoint if they do not have any shared elements; i.e., their intersection is the empty set, A ∩ B = ∅. More generally , several sets are called disjoint if they are pairwise disjoint, i.e., no two of them share a common elements. Figure 1.9 shows three disjoint sets. Fig.1.9 - Sets A, B, and C are disjoint. If the earth's surface is our sample space, we might want to partition it to the dif ferent continents. Similarly , a country can be partitioned to dif ferent provinces. In general, a collection of nonempty sets A1, A2, ⋯ is a partition of a set A if they are disjoint and their union is A. In Figure 1.10, the sets A1, A2, A3 and A4 form a partition of the universal set S. Fig.1.10 - The collection of sets A1, A2, A3 and A4 is a partition of S. Here are some rules that are often useful when working with sets. W e will see examples of their usage shortly . Theorem 1. 1 : De Morgan's law For any sets A1, A2, ⋯, An, we have (A1 ∪ A2 ∪ A3 ∪ ⋯ An) c = Ac 1 ∩ Ac 2 ∩ Ac 3 ⋯ ∩ Ac n; (A1 ∩ A2 ∩ A3 ∩ ⋯ An) c = Ac 1 ∪ Ac 2 ∪ Ac 3 ⋯ ∪ Ac n. Theorem 1. 2 : Distributive law For any sets A, B, and C we have A ∩ (B ∪ C) = (A ∩ B) ∪ (A ∩ C); A ∪ (B ∩ C) = (A ∪ B) ∩ (A ∪ C). Example 1. 4 If the universal set is given by S = {1, 2, 3, 4, 5, 6}, and A = {1, 2}, B = {2, 4, 5}, C = {1, 5, 6} are three sets, find the following sets: a . A ∪ B b . A ∩ B c. ¯¯¯¯ A d. ¯¯¯¯ B e. Check De Morgan's law by finding (A ∪ B) c and Ac ∩ B c. f. Check the distributive law by finding A ∩ (B ∪ C) and (A ∩ B) ∪ (A ∩ C). Solution a . A ∪ B = {1, 2, 4, 5}. b . A ∩ B = {2}. c. ¯¯¯¯ A = {3, 4, 5, 6} ( ¯¯¯¯ A consists of elements that are in S but not in A). d. ¯¯¯¯ B = {1, 3, 6}. e. We have (A ∪ B) c = {1, 2, 4, 5}c = {3, 6}, which is the same as Ac ∩ B c = {3, 4, 5, 6} ∩ {1, 3, 6} = {3, 6}. f. We have A ∩ (B ∪ C) = {1, 2} ∩ {1, 2, 4, 5, 6} = {1, 2}, which is the same as (A ∩ B) ∪ (A ∩ C) = {2} ∪ {1} = {1, 2}. A Cartesian product of two sets A and B, written as A × B, is the set containing ordered pairs from A and B. That is, if C = A × B, then each element of C is of the form (x, y), where x ∈ A and y ∈ B: A × B = {(x, y)|x ∈ A and y ∈ B}. For example, if A = {1, 2, 3} and B = {H, T}, then A × B = {(1, H), (1, T), (2, H), (2, T), (3, H), (3, T)}. Note that here the pairs are ordered, so for example, (1, H) ≠ (H, 1). Thus A × B is not the same as B × A. If you have two finite sets A and B, where A has M elements and B has N elements, then A × B has M × N elements. This rule is called the multiplication principle and is very useful in counting the numbers of elements in sets. The number of elements in a set is denoted by |A|, so here we write |A| = M , |B| = N, and |A × B| = M N. In the above example, |A| = 3, |B| = 2, thus |A × B| = 3 × 2 = 6. We can similarly define the Cartesian product of n sets A1, A2, ⋯ , An as A1 × A2 × A3 × ⋯ × An = {(x1, x2, ⋯ , xn)|x1 ∈ A1 and x2 ∈ A2 and  ⋯ xn ∈ An}. The multiplication principle states that for finite sets A1, A2, ⋯ , An, if |A1| = M1, |A2| = M2, ⋯ , |An| = Mn, then ∣A1 × A2 × A3 × ⋯ × An ∣= M1 × M2 × M3 × ⋯ × Mn. An important example of sets obtained using a Cartesian product is Rn, where n is a natural number . For n = 2, we have R2 = R × R = {(x, y)|x ∈ R, y ∈ R}. Thus, R2 is the set consisting of all points in the two-dimensional plane. Similarly , R3 = R × R × R and so on. 1.2.3 Cardinality: Countable and Uncountable Sets Here we need to talk about cardinality of a set, which is basically the size of the set. The cardinality of a set is denoted by |A|. We first discuss cardinality for finite sets and then talk about infinite sets. Finite Sets: Consider a set A. If A has only a finite number of elements, its cardinality is simply the number of elements in A. For example, if A = {2, 4, 6, 8, 10}, then |A| = 5. Before discussing infinite sets, which is the main discussion of this section, we would like to talk about a very useful rule: the inclusion-exclusion principle . For two finite sets A and B, we have |A ∪ B| = |A| + |B| − |A ∩ B|. To see this, note that when we add |A| and |B|, we are counting the elements in |A ∩ B| twice, thus by subtracting it from |A| + |B|, we obtain the number of elements in |A ∪ B|, (you can refer to Figure 1.16 in Problem 2 to see this pictorially). W e can extend the same idea to three or more sets. Inclusion-exclusion principle: 1 . |A ∪ B| = |A| + |B| − |A ∩ B|, 2 . |A ∪ B ∪ C| = |A| + |B| + |C| − |A ∩ B| − |A ∩ C| − |B ∩ C| + |A ∩ B ∩ C|. Generally , for n finite sets A1, A2, A3, ⋯ , An, we can write ∣∣∣ n ⋃ i=1 Ai∣∣∣ = n ∑ i=1 |Ai| − ∑ i<j ∣∣Ai ∩ Aj∣∣ + ∑ i<j<k ∣∣Ai ∩ Aj ∩ Ak∣∣ −   ⋯   + (−1) n+1 |A1 ∩ ⋯ ∩ An| . Example 1. 5 In a party , there are 10 people with white shirts and 8 people with red shirts; 4 people have black shoes and white shirts; 3 people have black shoes and red shirts; the total number of people with white or red shirts or black shoes is 21. How many people have black shoes? Solution Let W, R, and B, be the number of people with white shirts, red shirts, and black shoes respectively . Then, here is the summary of the available information: |W| = 10 |R| = 8 |W ∩ B| = 4 |R ∩ B| = 3 |W ∪ B ∪ R| = 21. Also, it is reasonable to assume that W and R are disjoint, |W ∩ R| = 0. Thus by applying the inclusion-exclusion principle we obtain |W ∪ R ∪ B| = 21 = |W| + |R| + |B| − |W ∩ R| − |W ∩ B| − |R ∩ B| + |W ∩ R ∩ B| = 10 + 8 + |B| − 0 − 4 − 3 + 0. Thus |B| = 10. Note that another way to solve this problem is using a V enn diagram as shown in Figure 1.1 1. Fig.1.1 1 - Inclusion-exclusion V enn diagram. Infinite Sets: What if A is an infinite set? It turns out we need to distinguish between two types of infinite sets, where one type is significantly \"larger\" than the other . In particular , one type is called countable , while the other is called uncountable . Sets such as N and Z are called countable, but \"bigger\" sets such as R are called uncountable. The difference between the two types is that you can list the elements of a countable set A, i.e., you can write A = {a1, a2, ⋯}, but you cannot list the elements in an uncountable set. For example, you can write N = {1, 2, 3, ⋯}, Z = {0, 1, −1, 2, −2, 3, −3, ⋯}. The fact that you can list the elements of a countably infinite set means that the set can be put in one-to-one correspondence with natural numbers N. On the other hand, you cannot list the elements in R, so it is an uncountable set. T o be precise, here is the definition. Definition 1. 1 Set A is called countable if one of the following is true a . if it is a finite set, ∣A ∣< ∞; or b . it can be put in one-to-one correspondence with natural numbers N, in which case the set is said to be countably infinite. A set is called uncountable if it is not countable. Here is a simple guideline for deciding whether a set is countable or not. As far as applied probability is concerned, this guideline should be suf ficient for most cases. N, Z, Q, and any of their subsets are countable. Any set containing an interval on the real line such as [a, b], (a, b], [a, b), or (a, b), where a < b is uncountable. The above rule is usually suf ficient for the purpose of this book. However , to make the argument more concrete, here we provide some useful results that help us prove if a set is countable or not. If you are less interested in proofs, you may decide to skip them. Theorem 1. 3 Any subset of a countable set is countable. Any superset of an uncountable set is uncountable. Proof The intuition behind this theorem is the following: If a set is countable, then any \"smaller\" set should also be countable, so a subset of a countable set should be countable as well. T o provide a proof, we can argue in the following way . Let A be a countable set and B ⊂ A. If A is a finite set, then |B| ≤ |A| < ∞, thus B is countable. If A is countably infinite, then we can list the elements in A, then by removing the elements in the list that are not in B, we can obtain a list for B, thus B is countable. The second part of the theorem can be proved using the first part. Assume B is uncountable. If B ⊂ A and A is countable, by the first part of the theorem B is also a countable set which is a contradiction. Theorem 1. 4 If A1, A2, ⋯ is a list of countable sets, then the set ⋃i Ai = A1 ∪ A2 ∪ A3 ⋯ is also countable. Proof It suffices to create a list of elements in ⋃i Ai. Since each Ai is countable we can list its elements: Ai = {ai1, ai2, ⋯}. Thus, we have A1 = {a11, a12, ⋯}, A2 = {a21, a22, ⋯}, A3 = {a31, a32, ⋯}, ... Now we need to make a list that contains all the above lists. This can be done in different ways. One way to do this is to use the ordering shown in Figure 1.12 to make a list. Here, we can write ⋃ i Ai = {a11, a12, a21, a31, a22, a13, a14, ⋯} (1.1) Fig.1.12 - Ordering to make a list. We have been able to create a list that contains all the elements in ⋃i Ai, so this set is countable. Theorem 1. 5 If A and B are countable, then A × B is also countable. Proof The proof of this theorem is very similar to the previous theorem. Since A and B are countable, we can write A = {a1, a2, a3, ⋯}, B = {b1, b2, b3, ⋯}. Now , we create a list containing all elements in A × B = {(ai, bj)|i, j = 1, 2, 3, ⋯}. The idea is exactly the same as before. Figure 1.13 shows one possible ordering. Fig.1.13 - Ordering to make a list. The above arguments can be repeated for any set C in the form of C = ⋃ i ⋃ j {aij}, where indices i and j belong to some countable sets. Thus, any set in this form is countable. For example, a consequence of this is that the set of rational numbers Q is countable. This is because we can write Q = ⋃ i∈Z ⋃ j∈N { }. The above theorems confirm that sets such as N, Z, Q and their subsets are countable. However , as we mentioned, intervals in R are uncountable. Thus, you can never provide a list in the form of {a1, a2, a3, ⋯} that contains all the elements in, say , [0, 1]. This fact can be proved using a so-called diagonal argument, and we omit the proof here as it is not instrumental for the rest of the book. i j 1.2.4 Functions We often need the concept of functions in probability . A function f is a rule that takes an input from a specific set, called the domain , and produces an output from another set, called co-domain . Thus, a function maps elements from the domain set to elements in the co-domain with the property that each input is mapped to exactly one output. For a function f, if x is an element in the domain, then the function value (the output of the function) is shown by f(x). If A is the domain and B is the co-domain for the function f, we use the following notation: f : A → B. Example 1. 6 Consider the function f : R → R, defined as f(x) = x2. This function takes any real number x and outputs x2. For example, f(2) = 4. Consider the function g : {H, T} → {0, 1}, defined as g(H) = 0 and g(T) = 1. This function can only take two possible inputs H or T, where H is mapped to 0 and T is mapped to 1. The output of a function f : A → B always belongs to the co-domain B. However , not all values in the co-domain are always covered by the function. In the above example, f : R → R, the function value is always a positive number f(x) = x2 ≥ 0. We define the range of a function as the set containing all the possible values of f(x). Thus, the range of a function is always a subset of its co-domain. For the above function f(x) = x2, the range of f is given by Range(f) = R+ = {x ∈ R|x ≥ 0}. Figure 1.14 pictorially shows a function, its domain, co-domain, and range. The figure shows that an element x in the domain is mapped to f(x) in the range. Fig.1.14 - Function f : A → B, the range is always a subset of the co- domain. 1.2.5 Solved Problems: Review of Set Theory Problem 1 Let A, B, C be three sets as shown in the following V enn diagram. For each of the following sets, draw a V enn diagram and shade the area representing the given set. a . A ∪ B ∪ C b . A ∩ B ∩ C c. A ∪ (B ∩ C) d. A − (B ∩ C) e. A ∪ (B ∩ C) c Solution Figure 1.15 shows V enn diagrams for these sets. Fig.1.15 - V enn diagrams for dif ferent sets. Problem 2 Using V enn diagrams, verify the following identities. a . A = (A ∩ B) ∪ (A − B) b . If A and B are finite sets, we have |A ∪ B| = |A| + |B| − |A ∩ B| (1.2) Solution Figure 1.16 pictorially verifies the given identities. Note that in the second identity , we show the number of elements in each set by the corresponding shaded area. Fig.1.16 - V enn diagrams for some identities. Problem 3 Let S = {1, 2, 3}. W rite all the possible partitions of S. Solution Remember that a partition of S is a collection of nonempty sets that are disjoint and their union is S. There are 5 possible partitions for S = {1, 2, 3}: 1 . {1}, {2}, {3}; 2 . {1, 2}, {3}; 3 . {1, 3}, {2}; 4 . {2, 3}, {1}; 5 . {1, 2, 3}. Problem 4 Determine whether each of the following sets is countable or uncountable. a . A = {x ∈ Q| − 100 ≤ x ≤ 100} b . B = {(x, y)|x ∈ N, y ∈ Z} c. C = (0, 0.1] d. D = { |n ∈ N} Solution a . A = {x ∈ Q| − 100 ≤ x ≤ 100} is countable since it is a subset of a countable set, A ⊂ Q. b . B = {(x, y)|x ∈ N, y ∈ Z} is countable because it is the Cartesian product of two countable sets, i.e., B = N × Z. c. C = (0, .1] is uncountable since it is an interval of the form (a, b], where a < b. d. D = { |n ∈ N} is countable since it is in one-to-one correspondence with the set of natural numbers. In particular , you can list all the elements in the set D, D = {1, , , ⋯}. Problem 5 Find the range of the function f : R → R defined as f(x) = sin(x). Solution For any real value x, −1 ≤ sin(x) ≤ 1. Also, all values in [−1, 1] are covered by sin(x). Thus, Range (f) = [−1, 1]. 1 n 1 n 1 2 1 3 1.3.1 Random Experiments Before rolling a die you do not know the result. This is an example of a random experiment . In particular , a random experiment is a process by which we observe something uncertain. After the experiment, the result of the random experiment is known. An outcome is a result of a random experiment. The set of all possible outcomes is called the sample space . Thus in the context of a random experiment, the sample space is our universal set . Here are some examples of random experiments and their sample spaces: Random experiment: toss a coin; sample space: S = {heads, tails} or as we usually write it, {H, T}. Random experiment: roll a die; sample space: S = {1, 2, 3, 4, 5, 6}. Random experiment: observe the number of iPhones sold by an Apple store in Boston in 2015; sample space: S = {0, 1, 2, 3, ⋯}. Random experiment: observe the number of goals in a soccer match; sample space: S = {0, 1, 2, 3, ⋯}. When we repeat a random experiment several times, we call each one of them a trial . Thus, a trial is a particular performance of a random experiment. In the example of tossing a coin, each trial will result in either heads or tails. Note that the sample space is defined based on how you define your random experiment. For example, Example 1. 7 We toss a coin three times and observe the sequence of heads/tails. The sample space here may be defined as S = {(H, H, H), (H, H, T), (H, T, H), (T, H, H), (H, T, T), (T, H, T), (T, T, H), (T, T, T)}. Our goal is to assign probability to certain events . For example, suppose that we would like to know the probability that the outcome of rolling a fair die is an even number . In this case, our event is the set E = {2, 4, 6}. If the result of our random experiment belongs to the set E, we say that the event E has occurred. Thus an event is a collection of possible outcomes. In other words, an event is a subset of the sample space to which we assign a probability . Although we have not yet discussed how to find the probability of an event, you might be able to guess that the probability of {2, 4, 6} is 50 percent which is the same as in the probability theory convention. Outcome: A result of a random experiment. Sample Space: The set of all possible outcomes. Event: A subset of the sample space. Union and Intersection: If A and B are events, then A ∪ B and A ∩ B are also events. By remembering the definition of union and intersection, we observe that A ∪ B occurs if A or B occur . Similarly , A ∩ B occurs if both A and B occur . Similarly , if A1, A2, ⋯ , An are events, then the event A1 ∪ A2 ∪ A3 ⋯ ∪ An occurs if at least one of A1, A2, ⋯ , An occurs. The event A1 ∩ A2 ∩ A3 ⋯ ∩ An occurs if all of A1, A2, ⋯ , An occur . It can be helpful to remember that the key words \"or\" and \"at least\" correspond to unions and the key words \"and\" and \"all of\" correspond to intersections. 1 2 1.3.2 Probability We assign a probability measure P(A) to an event A. This is a value between 0 and 1 that shows how likely the event is. If P(A) is close to 0, it is very unlikely that the event A occurs. On the other hand, if P(A) is close to 1, A is very likely to occur . The main subject of probability theory is to develop tools and techniques to calculate probabilities of dif ferent events. Probability theory is based on some axioms that act as the foundation for the theory , so let us state and explain these axioms. Axioms of Probability: Axiom 1: For any event A, P(A) ≥ 0. Axiom 2: Probability of the sample space S is P(S) = 1. Axiom 3: If A1, A2, A3, ⋯ are disjoint events, then P(A1 ∪ A2 ∪ A3 ⋯) = P(A1) + P(A2) + P(A3) + ⋯ Let us take a few moments and make sure we understand each axiom thoroughly . The first axiom states that probability cannot be negative. The smallest value for P(A) is zero and if P(A) = 0, then the event A will never happen. The second axiom states that the probability of the whole sample space is equal to one, i.e., 100 percent. The reason for this is that the sample space S contains all possible outcomes of our random experiment. Thus, the outcome of each trial always belongs to S, i.e., the event S always occurs and P(S) = 1. In the example of rolling a die, S = {1, 2, 3, 4, 5, 6} , and since the outcome is always among the numbers 1 through 6, P(S) = 1. The third axiom is probably the most interesting one. The basic idea is that if some events are disjoint (i.e., there is no overlap between them), then the probability of their union must be the summations of their probabilities. Another way to think about this is to imagine the probability of a set as the area of that set in the V enn diagram. If several sets are disjoint such as the ones shown Figure 1.9 , then the total area of their union is the sum of individual areas. The following example illustrates the idea behind the third axiom. Example 1. 8 In a presidential election, there are four candidates. Call them A, B, C, and D. Based on our polling analysis, we estimate that A has a 20 percent chance of winning the election, while B has a 40 percent chance of winning. What is the probability that A or B win the election? Solution Notice that the events that {A wins}, {B wins}, {C wins}, and {D wins} are disjoint since more than one of them cannot occur at the same time. For example, if A wins, then B cannot win. From the third axiom of probability , the probability of the union of two disjoint events is the summation of individual probabilities. Therefore, P(A wins or B wins) = P({A wins} ∪ {B wins}) = P({A wins}) + P({B wins}) = 0.2 + 0.4 = 0.6 In summary , if A1 and A2 are disjoint events, then P(A1 ∪ A2) = P(A1) + P(A2). The same argument is true when you have n disjoint events A1, A2, ⋯ , An: P(A1 ∪ A2 ∪ A3 ⋯ ∪ An) = P(A1) + P(A2) + ⋯ + P(An),  if A1, A2, ⋯ , An are disjoint. In fact, the third axiom goes beyond that and states that the same is true even for a countably infinite number of disjoint events. W e will see more examples of how we use the third axiom shortly . As we have seen, when working with events, intersection means \"and\" , and union means \"or\" . The probability of intersection of A and B, P(A ∩ B), is sometimes shown by P(A, B) or P(AB). Notation: P(A ∩ B) = P(A and B) = P(A, B), P(A ∪ B) = P(A or B). 1.3.3 Finding Probabilities Suppose that we are given a random experiment with a sample space S. To find the probability of an event, there are usually two steps: first, we use the specific information that we have about the random experiment. Second, we use the probability axioms. Let's look at an example. Although this is a simple example and you might be tempted to write the answer without following the steps, we encourage you to follow the steps. Example 1. 9 You roll a fair die. What is the probability of E = {1, 5}? Solution Let's first use the specific information that we have about the random experiment. The problem states that the die is fair , which means that all six possible outcomes are equally likely , i.e., P({1}) = P({2}) = ⋯ = P({6}). Now we can use the axioms of probability . In particular , since the events {1}, {2}, ⋯ , {6} are disjoint we can write 1 = P(S) = P({1} ∪ {2} ∪ ⋯ ∪ {6}) = P({1}) + P({2}) + ⋯ + P({6}) = 6P({1}). Thus, P({1}) = P({2}) = ⋯ = P({6}) = . Again since {1} and {5} are disjoint, we have P(E) = P({1, 5}) = P({1}) + P({5}) = = . 1 6 2 6 1 3 It is worth noting that we often write P(1) instead of P({1}) to simplify the notation, but we should emphasize that probability is defined for sets (events) not for individual outcomes. Thus, when we write P(2) = , what we really mean is that P({2}) = . We will see that the two steps explained above can be used to find probabilities for much more complicated events and random experiments. Let us now practice using the axioms by proving some useful facts. Example 1. 10 Using the axioms of probability , prove the following: a . For any event A, P(Ac) = 1 − P(A). b . The probability of the empty set is zero, i.e., P(∅) = 0. c. For any event A, P(A) ≤ 1. d. P(A − B) = P(A) − P(A ∩ B). e. P(A ∪ B) = P(A) + P(B) − P(A ∩ B), (inclusion-exclusion principle for n = 2). f. If A ⊂ B then P(A) ≤ P(B). Solution a . This states that the probability that A does not occur is 1 − P(A). To prove it using the axioms, we can write 1 = P(S) (axiom 2) = P(A ∪ Ac) (definition of compliment) = P(A) + P(Ac) b . Since ∅ = S c, we can use part (a) to see that P(∅) = 1 − P(S) = 0. Note that this makes sense as by definition: an event happens if the outcome of the random experiment belongs to that event. Since the empty set does not have any element, the outcome of the experiment never belongs to the empty set. c. From part (a), P(A) = 1 − P(Ac) and since P(Ac) ≥ 0 (the first axiom), we have P(A) ≤ 1. d. We show that P(A) = P(A ∩ B) + P(A − B). Note that the two sets A ∩ B and A − B are disjoint and their union is A (Figure 1.17). Thus, by the third axiom of 1 6 1 6 probability P(A) = P((A ∩ B) ∪ (A − B)) ( since A = (A ∩ B) ∪ (A − B)) = P(A ∩ B) + P(A − B)  (since A ∩ B and A − B are disjoint). Fig.1.17 - P(A) = P(A ∩ B) + P(A − B). Note that since A − B = A ∩ B c, we have shown P(A) = P(A ∩ B) + P(A ∩ B c). Note also that the two sets B and B c form a partition of the sample space (since they are disjoint and their union is the whole sample space). This is a simple form of law of total probability that we will discuss shortly and is a very useful rule in finding probability of some events. e. Note that A and B − A are disjoint sets and their union is A ∪ B. Thus, P(A ∪ B) = P(A ∪ (B − A)) = P(A) + P(B − A) = P(A) + P(B) − P(A ∩ B) (by part (d)) f. Note that A ⊂ B means that whenever A occurs B occurs, too. Thus intuitively we expect that P(A) ≤ P(B). Again the proof is similar as before. If A ⊂ B, then A ∩ B = A. Thus, P(B) = P(A ∩ B) + P(B − A) (by part (d)) = P(A) + P(B − A) ≥ P(A) (by axiom 1) Example 1. 1 1 Suppose we have the following information: 1 . There is a 60 percent chance that it will rain today . 2 . There is a 50 percent chance that it will rain tomorrow . 3 . There is a 30 percent chance that it does not rain either day . Find the following probabilities: a . The probability that it will rain today or tomorrow . b . The probability that it will rain today and tomorrow . c. The probability that it will rain today but not tomorrow . d. The probability that it either will rain today or tomorrow , but not both. Solution An important step in solving problems like this is to correctly convert them to probability language. This is especially useful when the problems become complex. For this problem, let's define A as the event that it will rain today , and B as the event that it will rain tomorrow . Then, let's summarize the available information: 1 . P(A) = 0.6, 2 . P(B) = 0.5, 3 . P(Ac ∩ B c) = 0.3 Now that we have summarized the information, we should be able to use them alongside probability rules to find the requested probabilities: a . The probability that it will rain today or tomorrow: this is P(A ∪ B). To find this we notice that P(A ∪ B) = 1 − P((A ∪ B) c) by Example 1.10 = 1 − P(Ac ∩ Bc) by De Morgan's Law = 1 − 0.3 = 0.7 b . The probability that it will rain today and tomorrow: this is P(A ∩ B). To find this we note that P(A ∩ B) = P(A) + P(B) − P(A ∪ B) by Example 1.10 = 0.6 + 0.5 − 0.7 = 0.4 c. The probability that it will rain today but not tomorrow: this is P(A ∩ B c). P(A ∩ B c) = P(A − B) = P(A) − P(A ∩ B) by Example 1.10 = 0.6 − 0.4 = 0.2 d. The probability that it either will rain today or tomorrow but not both: this is P(A − B) + P(B − A). We have already found P(A − B) = .2. Similarly , we can find P(B − A): P(B − A) = P(B) − P(B ∩ A) by Example 1.10 = 0.5 − 0.4 = 0.1 Thus, P(A − B) + P(B − A) = 0.2 + 0.1 = 0.3 In this problem, it is stated that there is a 50 percent chance that it will rain tomorrow . You might have heard this information from news on the TV . A more interesting question is how the number 50 is obtained. This is an example of a real-life problem in which tools from probability and statistics are used. As you read more chapters from the book, you will learn many of these tools that are frequently used in practice. Inclusion-Exclusion Principle: The formula P(A ∪ B) = P(A) + P(B) − P(A ∩ B) that we proved in Example 1.10 is a simple form of the inclusion-exclusion principle. W e can extend it to the union of three or more sets. Inclusion-exclusion principle: P(A ∪ B) = P(A) + P(B) − P(A ∩ B), P(A ∪ B ∪ C) = P(A) + P(B) + P(C)− −P(A ∩ B) − P(A ∩ C) − P(B ∩ C) + P(A ∩ B ∩ C) Generally for n events A1, A2, ⋯ , An, we have P(⋃ n i=1 Ai) = ∑n i=1 P(Ai) − ∑i<j P(Ai ∩ Aj) + ∑i<j<k P(Ai ∩ Aj ∩ Ak) −   ⋯   + (−1) n−1 P(⋂ n i=1 Ai) 1.3.4 Discrete Probability Models Here, we will distinguish between two dif ferent types of sample spaces, discrete and continuous. W e will discuss the difference more in detail later on, when we discuss random variables. The basic idea is that in discrete probability models we can compute the probability of events by adding all the corresponding outcomes, while in continuous probability models we need to use integration instead of summation. Consider a sample space S. If S is a countable set, this refers to a discrete probability model. In this case, since S is countable, we can list all the elements in S: S = {s1, s2, s3, ⋯}. If A ⊂ S is an event, then A is also countable, and by the third axiom of probability we can write P(A) = P( ⋃ sj∈A {sj}) = ∑ sj∈A P(sj). Thus in a countable sample space, to find probability of an event, all we need to do is sum the probability of individual elements in that set. Example 1. 12 I play a gambling game in which I will win k − 2 dollars with probability for any k ∈ N , that is, with probability , I lose 1 dollar; with probability , I win 0 dollar; with probability , I win 1 dollar; with probability , I win 2 dollars; with probability , I win 3 dollars; ⋯ What is the probability that I win more than or equal to 1 dollar and less than 4 dollars? What is the probability that I win more than 2 dollars? 1 2k 1 2 1 4 1 8 1 16 1 32 Solution In this problem, the random experiment is the gambling game and the outcomes are the amount in dollars that I win (lose). Thus we may write S = {−1, 0, 1, 2, 3, 4, 5, ⋯}. As we see this is an infinite but countable set. The problem also states that P(k) = P({k}) =  for k ∈ S. First, let's check that this is a valid probability measure. T o do so, we should check if all probabilities add up to one, i.e., P(S) = 1. W e have P(S) = ∑∞ k=−1 P(k) = ∑∞ k=−1 = + + + ⋯ (geometric sum) = 1. Now let's solve the problem. Let's define A as the event that I win more than or equal to 1 dollar and less than 4 dollars, and B as the event that I win more than 2 dollars. Thus, A = {1, 2, 3}, B = {3, 4, 5, ⋯}. Then P(A) = P(1) + P(2) + P(3) = + + = ≈ 0.219 Similarly , P(B) = P(3) + P(4) + P(5) + P(6) + ⋯ = + + + + ⋯ (geometric sum) = = 0.0625 Note that another way to find P(B) is to write 1 2k+2 1 2k+2 1 2 1 4 1 8 1 8 1 16 1 32 7 32 1 32 1 64 1 128 1 256 1 16 P(B) = 1 − P(B c) = 1 − P({−1, 0, 1, 2}) = 1 − (P(−1) + P(0) + P(1) + P(2)) = 1 − ( + + + ) = 1 − = = 0.0625 Note: Here we have used the geometric series sum formula. In particular , for any a, x ∈ R, we have a + ax + ax 2 + ax 3 + ⋯ + ax n−1 = n−1 ∑ k=0 ax k = a (1.3) Moreover , if |x| < 1, then we have a + ax + ax 2 + ax 3 + ⋯ = ∞ ∑ k=0 ax k = a (1.4) Finite Sample Spaces with Equally Likely Outcomes: An important special case of discrete probability models is when we have a finite sample space S, where each outcome is equally likely , i.e., S = {s1, s2, ⋯ , sN },  where P(si) = P(sj) for all i, j ∈ {1, 2, ⋯ , N}. Rolling a fair die is an instance of such a probability model. Since all outcomes are equally likely , we must have P(si) = ,  for all i ∈ {1, 2, ⋯ , N}. In such a model, if A is any event with cardinality |A| = M , we can write P(A) = ∑ sj∈A P(sj) = ∑ sj∈A = = . Thus, finding probability of A reduces to a counting problem in which we need to count how many elements are in A and S. 1 2 1 4 1 8 1 16 15 16 1 16 1 − xn 1 − x 1 1 − x 1 N 1 N M N |A| |S| Example 1. 13 I roll a fair die twice and obtain two numbers: X1 = result of the first roll, and X2 = result of the second roll. Write down the sample space S, and assuming that all outcomes are equally likely (because the die is fair), find the probability of the event A defined as the event that X1 + X2 = 8. Solution The sample space S can be written as S = { (1, 1), (1, 2), (1, 3), (1, 4), (1, 5), (1, 6), (2, 1), (2, 2), (2, 3), (2, 4), (2, 5), (2, 6), (3, 1), (3, 2), (3, 3), (3, 4), (3, 5), (3, 6), (4, 1), (4, 2), (4, 3), (4, 4), (4, 5), (4, 6), (5, 1), (5, 2), (5, 3), (5, 4), (5, 5), (5, 6), (6, 1), (6, 2), (6, 3), (6, 4), (6, 5), (6, 6)}. As we see there are |S| = 36 elements in S. T o find probability of A, all we need to do is find M = |A|. In particular , A is defined as A = {(X1, X2)|X1 + X2 = 8, X1, X2 ∈ {1, 2, ⋯ , 6}} = {(2, 6), (3, 5), (4, 4), (5, 3), (6, 2)}. Thus, |A| = 5, which means that P(A) = = . A very common mistake is not distinguishing between, say (2, 6) and (6, 2). It is important to note that these are two dif ferent outcomes: (2, 6) means that the first roll is a 2 and the second roll is a 6, while (6, 2) means that the first roll is a 6 and the second roll is a 2. Note that it is very common to write P(X1 + X2 = 8) when referring to P(A) as defined above. In fact, X1 and X2 are examples of random variables that will be discussed in detail later on. |A| |S| 5 36 In a finite sample space S, where all outcomes are equally likely , the probability of any event A can be found by P(A) = . The formula P(A) = suggests that it is important to be able to count elements in sets. If sets are small, this is an easy task; however , if the sets are large and defined implicitly , this could be a dif ficult job. That is why we discuss counting methods later on. |A| |S| |A| |S| 1.3.5 Continuous Probability Models Consider a scenario where your sample space S is, for example, [0, 1]. This is an uncountable set; we cannot list the elements in the set. At this time, we have not yet developed the tools needed to deal with continuous probability models, but we can provide some intuition by looking at a simple example. Example 1. 14 Your friend tells you that she will stop by your house sometime after or equal to 1 p.m. and before 2 p.m., but she cannot give you any more information as her schedule is quite hectic. Y our friend is very dependable, so you are sure that she will stop by your house, but other than that we have no information about the arrival time. Thus, we assume that the arrival time is completely random in the 1 p.m. and 2 p.m. interval. (As we will see, in the language of probability theory , we say that the arrival time is \"uniformly\" distributed on the [1, 2) interval). Let T be the arrival time. a . What is the sample space S? b . What is the probability of P(1.5)? Why? c. What is the probability of T ∈ [1, 1.5)? d. For 1 ≤ a ≤ b ≤ 2, what is P(a ≤ T ≤ b) = P([a, b])? Solution a . Since any real number in [1, 2) is a possible outcome, the sample space is indeed S = [1, 2). b . Now , let's look at P(1.5). A reasonable guess would be P(1.5) = 0. But can we provide a reason for that? Let us divide the [1, 2) interval to 2N + 1 equal-length and disjoint intervals, [1, 1 + ), [1 + , 1 + ), ⋯ , [1 + , 1 + ), ⋯ , [1 + , 2). See Figure 1.18. Here, N could be any positive integer . 1 2N+1 1 2N+1 2 2N+1 N 2N+1 N+1 2N+1 2N 2N+1 Fig.1.18 - Dividing the interval [1, 2) to 2N + 1 equal-length intervals. The only information that we have is that the arrival time is \"uniform\" on the [1, 2) interval. Therefore, all of the above intervals should have the same probability , and since their union is S we conclude that P ([1, 1 + )) = P ([1 + , 1 + )) = ⋯ ⋯ = P ([1 + , 1 + )) = ⋯ ⋯ = P ([1 + , 2)) = . In particular , by defining AN = [1 + , 1 + ), we conclude that P(AN ) = P ([1 + , 1 + )) = . Now note that for any positive integer N, 1.5 ∈ AN . Thus, {1.5} ⊂ AN , so P(1.5) ≤ P(AN ) = , for all N ∈ N. Note that as N becomes large, P(AN ) approaches 0. Since P(1.5) cannot be negative, we conclude that P(1.5) = 0. Similarly , we can argue that P(x) = 0 for all x ∈ [1, 2). c. Next, we find P([1, 1.5)). This is the first half of the entire sample space S = [1, 2) and because of uniformity , its probability must be 0.5. In other words, P([1, 1.5)) = P([1.5, 2)) (by uniformity), P([1, 1.5)) + P([1.5, 2)) = P(S) = 1. 1 2N + 1 1 2N + 1 2 2N + 1 N 2N + 1 N + 1 2N + 1 2N 2N + 1 1 2N + 1 N 2N+1 N+1 2N+1 N 2N + 1 N + 1 2N + 1 1 2N + 1 1 2N + 1 Thus P([1, 1.5)) = P([1.5, 2)) = . d. The same uniformity argument suggests that all intervals in [1, 2) with the same length must have the same probability . In particular , the probability of an interval is proportional to its length. For example, since [1, 1.5) = [1, 1.25) ∪ [1.25, 1.5). Thus, we conclude P([1, 1.5)) = P([1, 1.25)) + P([1.25, 1.5)) = 2P([1, 1.25)). And finally , since P([1, 2)) = 1, we conclude P([a, b]) = b − a, for 1 ≤ a ≤ b < 2. The above example was a somewhat simple situation in which we have a continuous sample space. In reality , the probability might not be uniform, so we need to develop tools that help us deal with general distributions of probabilities. These tools will be introduced in the coming chapters. Discussion: You might ask why P(x) = 0 for all x ∈ [1, 2), but at the same time, the outcome of the experiment is always a number in [1, 2)? We can answer this question from dif ferent points of view . From a mathematical point of view , we can explain this issue by using the following analogy: consider a line segment of length one. This line segment consists of points of length zero. Nevertheless, these zero-length points as a whole constitute a line segment of length one. From a practical point of view , we can provide the following explanation: our observed outcome is not all real values in [1, 2). That is, if we are observing time, our measurement might be accurate up to minutes, or seconds, or milliseconds, etc. Our continuous probability model is a limit of a discrete probability model, when the precision becomes infinitely accurate. Thus, in reality we are always interested in the probability of some intervals rather than a specific point x. For example, when we say , \"What is the probability that your friend 1 2 shows up at 1 : 32 p.m.?\", what we may mean is, \"What is the probability that your friend shows up between 1 : 32 : 00 p.m. and 1 : 32 : 59 p.m.?\" This probability is nonzero as it refers to an interval with a one-minute length. Thus, in some sense, a continuous probability model can be looked at as the \"limit\" of a discrete space. Remembering from calculus, we note that integrals are defined as the limits of sums. That is why we use integrals to find probabilities for continuous probability models, as we will see later . 1.3.6 Solved Problems: Random Experiments and Probabilities Problem 1 Consider a sample space S and three events A, B, and C. For each of the following events draw a V enn diagram representation as well as a set expression. a . Among A, B, and C, only A occurs. b . At least one of the events A, B, or C occurs. c. A or C occurs, but not B. d. At most two of the events A, B, or C occur . Solution a . Among A, B, and C, only A occurs: A − B − C = A − (B ∪ C). b . At least one of the events A, B, or C occurs: A ∪ B ∪ C. c. A or C occurs, but not B: (A ∪ C) − B. d. At most two of the events A, B, or C occur: (A ∩ B ∩ C) c = Ac ∪ B c ∪ C c. The Venn diagrams are shown in Figure 1.19. Fig.1.19 - V enn diagrams for solved problem 1. Problem 2 Write the sample space S for the following random experiments. a . We toss a coin until we see two consecutive tails. W e record the total number of coin tosses. b . A bag contains 4 balls: one is red, one is blue, one is white, and one is green. We choose two distinct balls and record their color in order . c. A customer arrives at a bank and waits in the line. W e observe T, which is the total time (in hours) that the customer waits in the line. The bank has a strict policy that no customer waits more than 20 minutes under any circumstances. Solution Remember that the sample space is the set of all possible outcomes. Usually , when you have a random experiment, there are dif ferent ways to define the sample space S depending on what you observe as the outcome. In this problem, for each experiment it is stated what outcomes we observe in order to help you write down the sample space S. a . We toss a coin until we see two consecutive tails. W e record the total number of coin tosses: Here, the total number of coin tosses is a natural number larger than or equal to 2. The sample space is S = {2, 3, 4, ⋯}. b . A bag contains 4 balls: one is red, one is blue, one is white, and one is green. We choose two distinct balls and record their color in order: The sample space can be written as S = {(R, B), (B, R), (R, W), (W, R), (R, G), (G, R), (B, W), (W, B), (B, G), (G, B), (W, G), (G, W)}. c. A customer arrives at a bank and waits in the line. W e observe T...: In theory T can be any real number between 0 and = 20 minutes. Thus, S = [0, ] = {x ∈ R|0 ≤ x ≤ }. Problem 3 Let A, B, and C be three events in the sample space S. Suppose we know A ∪ B ∪ C = S, P(A) = , P(B) = , P(A ∪ B) = . Answer the following questions: a . Find P(A ∩ B). b . Do A, B, and C form a partition of S? c. Find P(C − (A ∪ B)). d. If P(C ∩ (A ∪ B)) = , find P(C). Solution 1 3 1 3 1 3 1 2 2 3 5 6 5 12 As before, it is always useful to draw a V enn diagram; however , here we provide the solution without using a V enn diagram. a . Using the inclusion-exclusion principle, we have P(A ∪ B) = P(A) + P(B) − P(A ∩ B). Thus, P(A ∩ B) = P(A) + P(B) − P(A ∪ B) = + − = . b . No, since A ∩ B ≠ ∅. c. We can write C − (A ∪ B) = (C ∪ (A ∪ B)) − (A ∪ B) = S − (A ∪ B) = (A ∪ B) c. Thus P(C − (A ∪ B)) = P((A ∪ B) c) = 1 − P(A ∪ B) = . d. We have P(C) = P(C ∩ (A ∪ B)) + P(C − (A ∪ B)) = + = . Problem 4 I roll a fair die twice and obtain two numbers X1 = result of the first roll, and X2 = result of the second roll. Find the probability of the following events: a . A defined as \" X1 < X2\"; b . B defined as \"Y ou observe a 6 at least once\". 1 2 2 3 5 6 1 3 1 6 5 12 1 6 7 12 Solution As we saw before, the sample space S has 36 elements. a . We have A = {(1, 2), (1, 3), (1, 4), (1, 5), (1, 6), (2, 3), (2, 4), (2, 5), (2, 6), (3, 4), (3, 5), (3, 6), (4, 5), (4, 6), (5, 6)}. Then, we obtain P(A) = = = . b . We have B = {(6, 1), (6, 2), (6, 3), (6, 4), (6, 5), (6, 6), (1, 6), (2, 6), (3, 6), (4, 6), (5, 6)}. We obtain P(B) = = . Problem 5 You purchase a certain product. The manual states that the lifetime T of the product, defined as the amount of time (in years) the product works properly until it breaks down, satisfies P(T ≥ t) = e −  for all t ≥ 0. For example, the probability that the product lasts more than (or equal to) 2 years is P(T ≥ 2) = e − = 0.6703. a . This is an example of a continuous probability model. W rite down the sample space S. b . Check that the statement in the manual makes sense by finding P(T ≥ 0) and limt→∞ P(T ≥ t). c. Also check that if t1 < t2, then P(T ≥ t1) ≥ P(T ≥ t2). Why does this need to be true? |A| |S| 15 36 5 12 |B| |S| 11 36 t 5 2 5 d. Find the probability that the product breaks down within three years of the purchase time. e. Find the probability that the product breaks down in the second year , i.e., find P(1 ≤ T < 2). Solution a . The sample space S is the set of all possible outcomes. Here, the possible outcomes are the possible values for T which can be any real number larger than or equal to zero. Thus S = [0, ∞). b . We have P(T ≥ 0) = e − = 1, lim t→∞ P(T ≥ t) = e −∞ = 0, which is what we expect. In particular , T is always larger than or equal to zero, thus we expect P(T ≥ 0) = 1. Also, since the product will eventually fail at some point, we expect that P(T ≥ t) approaches zero as t goes to infinity . c. First note that if t1 < t2, then P(T ≥ t1) = e − > e − = P(T ≥ t2) (since f(x) = e(x) is an increasing function). Here we have two events, A is the event that T ≥ t1 and B is the event that T ≥ t2. That is, A = [t1, ∞), B = [t2, ∞). Since B is a subset of A, B ⊂ A, we must have P(B) ≤ P(A), thus P(A) = P(T ≥ t1) ≥ P(T ≥ t2) = P(B). d. The probability that the product breaks down within three years of the purchase time is P(T < 3) = 1 − P(T ≥ 3) = 1 − e − ≈ 0.4512 e. Note that if A ⊂ B, then P(B − A) = P(B) − P(B ∩ A) = P(B) − P(A) (since A ⊂ B). 0 5 t1 5 t2 5 3 5 Choosing A = [1, ∞) and B = [2, ∞), we can write P(1 ≤ T < 2) = P(T ≥ 1) − P(T ≥ 2) = e − − e − = 0.1484 Problem 6 I first saw this question in a math contest many years ago: Y ou get a stick and break it randomly into three pieces. What is the probability that you can make a triangle using the three pieces? Y ou can assume the break points are chosen completely at random, i.e. if the length of the original stick is 1 unit, and x, y, z are the lengths of the three pieces, then (x, y, z) are uniformly chosen from the set {(x, y, z) ∈ R3|x + y + z = 1, x, y, z ≥ 0}. Solution This is again a problem on a continuous probability space. The basic idea is pretty simple. First, we need to identify the sample space S. In this case the sample space is going to be a two-dimensional set. Second, we need to identify the set A that contains the favorable outcomes (the set of (x, y, z) in S that form a triangle). And finally , since the space is uniform, we will divide area of set A by the area of S to obtain P(A). First, we need to find the sets S and A. This is basically a geometry problem. The two sets, S and A, are shown in Figure 1.20. 1 5 2 5 Fig.1.20 - The sample space and set A for Problem 6. Note that in R3, x + y + z = 1 represents a plane that goes through the points (1, 0, 0), (0, 1, 0), (0, 0, 1). To find the sample space S, note that S = {(x, y, z) ∈ R3|x + y + z = 1, x, y, z ≥ 0}, thus S is the part of the plane that is shown in Figure 1.20. To find the set A, note that we need (x, y, z) to satisfy the triangle inequality x + y > z, y + z > x, x + z > y. Note that since x + y + z = 1, we can equivalently write the three equations as x < , y < , z < . Thus, we conclude that the set A is the area shown in Figure 20. In particular , we note that the set S consists of four triangles with equal areas. Therefore, its area is four times the area of A, and we have P(A) = = . 1 2 1 2 1 2 Area of A Area of S 1 4 1.4.0 Conditional Probability In this section, we discuss one of the most fundamental concepts in probability theory . Here is the question: as you obtain additional information, how should you update probabilities of events? For example, suppose that in a certain city , 23 percent of the days are rainy . Thus, if you pick a random day , the probability that it rains that day is 23 percent: P(R) = 0.23, where R is the event that it rains on the randomly chosen day. Now suppose that I pick a random day , but I also tell you that it is cloudy on the chosen day . Now that you have this extra piece of information, how do you update the chance that it rains on that day? In other words, what is the probability that it rains given that it is cloudy? If C is the event that it is cloudy , then we write this as P(R|C), the conditional probability of R given that C has occurred . It is reasonable to assume that in this example, P(R|C) should be larger than the original P(R), which is called the prior probability of R. But what exactly should P(R|C) be? Before providing a general formula, let's look at a simple example. Example 1. 15 I roll a fair die. Let A be the event that the outcome is an odd number , i.e., A = {1, 3, 5} . Also let B be the event that the outcome is less than or equal to 3, i.e., B = {1, 2, 3}. What is the probability of A, P(A)? What is the probability of A given B, P(A|B)? Solution This is a finite sample space, so P(A) = = = . Now , let's find the conditional probability of A given that B occurred. If we know B has occurred, the outcome must be among {1, 2, 3}. For A to also happen the outcome must be in A ∩ B = {1, 3}. Since all die rolls are equally likely , we argue that P(A|B) must be equal to |A| |S| |{1, 3, 5}| 6 1 2 P(A|B) = = . Now let's see how we can generalize the above example. W e can rewrite the calculation by dividing the numerator and denominator by |S| in the following way P(A|B) = = = . Although the above calculation has been done for a finite sample space with equally likely outcomes, it turns out the resulting formula is quite general and can be applied in any setting. Below , we formally provide the formula and then explain the intuition behind it. If A and B are two events in a sample space S, then the conditional probability of A given B is defined as P(A|B) = ,  when P(B) > 0. Here is the intuition behind the formula. When we know that B has occurred, every outcome that is outside B should be discarded. Thus, our sample space is reduced to the set B, Figure 1.21. Now the only way that A can happen is when the outcome belongs to the set A ∩ B. We divide P(A ∩ B) by P(B), so that the conditional probability of the new sample space becomes 1, i.e., P(B|B) = = 1. Note that conditional probability of P(A|B) is undefined when P(B) = 0. That is okay because if P(B) = 0, it means that the event B never occurs so it does not make sense to talk about the probability of A given B. |A ∩ B| |B| 2 3 |A ∩ B| |B| |A∩B| |S| |B| |S| P(A ∩ B) P(B) P(A ∩ B) P(B) P(B∩B) P(B) Fig. 1.21 - V enn diagram for conditional probability , P(A|B). It is important to note that conditional probability itself is a probability measure, so it satisfies probability axioms. In particular , Axiom 1: For any event A, P(A|B) ≥ 0. Axiom 2: Conditional probability of B given B is 1, i.e., P(B|B) = 1. Axiom 3: If A1, A2, A3, ⋯ are disjoint events, then P(A1 ∪ A2 ∪ A3 ⋯ |B) = P(A1|B) + P(A2|B) + P(A3|B) + ⋯ . In fact, all rules that we have learned so far can be extended to conditional probability . For example, the formulas given in Example 1.10 can be rewritten: Example 1. 16 For three events, A, B, and C, with P(C) > 0, we have P(Ac|C) = 1 − P(A|C); P(∅|C) = 0; P(A|C) ≤ 1; P(A − B|C) = P(A|C) − P(A ∩ B|C); P(A ∪ B|C) = P(A|C) + P(B|C) − P(A ∩ B|C); if A ⊂ B then P(A|C) ≤ P(B|C). Let's look at some special cases of conditional probability: When A and B are disjoint: In this case A ∩ B = ∅, so P(A|B) = = = 0. This makes sense. In particular , since A and B are disjoint they cannot both occur at the same time. Thus, given that B has occurred, the probability of A must be zero. When B is a subset of A: If B ⊂ A, then whenever B happens, A also happens. Thus, given that B occurred, we expect that probability of A be one. In this case A ∩ B = B, so P(A|B) = = = 1. When A is a subset of B: In this case A ∩ B = A, so P(A|B) = = . Example 1. 17 I roll a fair die twice and obtain two numbers X1 = result of the first roll and X2 = result of the second roll. Given that I know X1 + X2 = 7, what is the probability that X1 = 4 or X2 = 4? Solution P(A∩B) P(B) P(∅) P(B) P(A∩B) P(B) P(B) P(B) P(A∩B) P(B) P(A) P(B) Let A be the event that X1 = 4 or X2 = 4 and B be the event that X1 + X2 = 7. We are interested in P(A|B), so we can use P(A|B) = We note that A = {(4, 1), (4, 2), (4, 3), (4, 4), (4, 5), (4, 6), (1, 4), (2, 4), (3, 4), (4, 4), (5, 4), (6, 4)}, B = {(6, 1), (5, 2), (4, 3), (3, 4), (2, 5), (1, 6)}, A ∩ B = {(4, 3), (3, 4)}. We conclude P(A|B) = = = . Let's look at a famous probability problem, called the two-child problem. Many versions of this problem have been discussed [1] in the literature and we will review a few of them in this chapter . We suggest that you try to guess the answers before solving the problem using probability formulas. Example 1. 18 Consider a family that has two children. W e are interested in the children's genders. Our sample space is S = {(G, G), (G, B), (B, G), (B, B)}. Also assume that all four possible outcomes are equally likely . a . What is the probability that both children are girls given that the first child is a girl? b . We ask the father: \"Do you have at least one daughter?\" He responds \"Y es!\" Given this extra information, what is the probability that both children are girls? In other words, what is the probability that both children are girls given that we know at least one of them is a girl? P(A ∩ B) P(B) P(A ∩ B) P(B) 2 36 6 36 1 3 Solution Let A be the event that both children are girls, i.e., A = {(G, G)}. Let B be the event that the first child is a girl, i.e., B = {(G, G), (G, B)}. Finally , let C be the event that at least one of the children is a girl, i.e., C = {(G, G), (G, B), (B, G)}. Since the outcomes are equally likely , we can write P(A) = , P(B) = = , P(C) = . a . What is the probability that both children are girls given that the first child is a girl? This is P(A|B), thus we can write P(A|B) = = (since A ⊂ B) = = . b . What is the probability that both children are girls given that we know at least one of them is a girl? This is P(A|C), thus we can write P(A|C) = = (since A ⊂ C) = = . Discussion: Asked to guess the answers in the above example, many people would guess that both P(A|B) and P(A|C) should be 50 percent. However , as we see P(A|B) is 50 percent, while P(A|C) is only 33 percent. This is an example where the answers might seem counterintuitive. T o understand the results of this problem, it is helpful to 1 4 2 4 1 2 3 4 P(A∩B) P(B) P(A) P(B) 1 4 1 2 1 2 P(A∩C) P(C) P(A) P(C) 1 4 3 4 1 3 note that the event B is a subset of the event C. In fact, it is strictly smaller: it does not include the element (B, G), while C has that element. Thus the set C has more outcomes that are not in A than B, which means that P(A|C) should be smaller than P(A|B). It is often useful to think of probability as percentages. For example, to better understand the results of this problem, let us imagine that there are 4000 families that have two children. Since the outcomes (G, G), (G, B), (B, G), and (B, B) are equally likely , we will have roughly 1000 families associated with each outcome as shown in Figure 1.22. T o find probability P(A|C), we are performing the following experiment: we choose a random family from the families with at least one daughter . These are the families shown in the box. From these families, there are 1000 families with two girls and there are 2000 families with exactly one girl. Thus, the probability of choosing a family with two girls is . Fig.1.22 - An example to help the understanding of P(A|C) in Example 1.18. Chain rule for conditional probability: Let us write the formula for conditional probability in the following format P(A ∩ B) = P(A)P(B|A) = P(B)P(A|B) (1.5) This format is particularly useful in situations when we know the conditional probability , but we are interested in the probability of the intersection. W e can interpret this formula using a tree diagram such as the one shown in Figure 1.23. In this figure, we obtain the probability at each point by multiplying probabilities on the branches leading to that point. This type of diagram can be very useful for some problems. 1 3 Fig.1.23 - A tree diagram. Now we can extend this formula to three or more events: P(A ∩ B ∩ C) = P(A ∩ (B ∩ C)) = P(A)P(B ∩ C|A) (1.6) From Equation 1.5 , P(B ∩ C) = P(B)P(C|B). Conditioning both sides on A, we obtain P(B ∩ C|A) = P(B|A)P(C|A, B) (1.7) Combining Equation 1.6 and 1.7 we obtain the following chain rule: P(A ∩ B ∩ C) = P(A)P(B|A)P(C|A, B). The point here is understanding how you can derive these formulas and trying to have intuition about them rather than memorizing them. Y ou can extend the tree in Figure 1.22 to this case. Here the tree will have eight leaves. A general statement of the chain rule for n events is as follows: Chain rule for conditional probability: P(A1 ∩ A2 ∩ ⋯ ∩ An) = P(A1)P(A2|A1)P(A3|A2, A1) ⋯ P(An|An−1An−2 ⋯ A1) Example 1. 19 In a factory there are 100 units of a certain product, 5 of which are defective. W e pick three units from the 100 units at random. What is the probability that none of them are defective? Solution Let us define Ai as the event that the ith chosen unit is not defective, for i = 1, 2, 3. We are interested in P(A1 ∩ A2 ∩ A3). Note that P(A1) = . Given that the first chosen item was good, the second item will be chosen from 94 good units and 5 defective units, thus P(A2|A1) = . Given that the first and second chosen items were okay , the third item will be chosen from 93 good units and 5 defective units, thus P(A3|A2, A1) = . Thus, we have P(A1 ∩ A2 ∩ A3) = P(A1)P(A2|A1)P(A3|A2, A1) = = 0.8560 As we will see later on, another way to solve this problem is to use counting arguments. 95 100 94 99 93 98 95 100 94 99 93 98 1.4.1 Independence Let A be the event that it rains tomorrow , and suppose that P(A) = . Also suppose that I toss a fair coin; let B be the event that it lands heads up. W e have P(B) = . Now I ask you, what is P(A|B)? What is your guess? Y ou probably guessed that P(A|B) = P(A) = . You are right! The result of my coin toss does not have anything to do with tomorrow's weather . Thus, no matter if B happens or not, the probability of A should not change. This is an example of two independent events. T wo events are independent if one does not convey any information about the other . Let us now provide a formal definition of independence. Two events A and B are independent if P(A ∩ B) = P(A)P(B). Now , let's first reconcile this definition with what we mentioned earlier , P(A|B) = P(A). If two events are independent, then P(A ∩ B) = P(A)P(B), so P(A|B) = = = P(A). Thus, if two events A and B are independent and P(B) ≠ 0, then P(A|B) = P(A). T o summarize, we can say \"independence means we can multiply the probabilities of events to obtain the probability of their intersection\", or equivalently , \"independence means that conditional probability of one event given another is the same as the original (prior) probability\". Sometimes the independence of two events is quite clear because the two events seem not to have any physical interaction with each other (such as the two events discussed above). At other times, it is not as clear and we need to check if they satisfy the independence condition. Let's look at an example. 1 3 1 2 1 3 P(A∩B) P(B) P(A)P(B) P(B) Example 1. 20 I pick a random number from {1, 2, 3, ⋯ , 10}, and call it N. Suppose that all outcomes are equally likely . Let A be the event that N is less than 7, and let B be the event that N is an even number . Are A and B independent? Solution We have A = {1, 2, 3, 4, 5, 6}, B = {2, 4, 6, 8, 10}, and A ∩ B = {2, 4, 6}. Then P(A) = 0.6, P(B) = 0.5, P(A ∩ B) = 0.3 Therefore, P(A ∩ B) = P(A)P(B), so A and B are independent. This means that knowing that B has occurred does not change our belief about the probability of A. In this problem the two events are about the same random number , but they are still independent because they satisfy the definition. The definition of independence can be extended to the case of three or more events. Three events A, B, and C are independent if all of the following conditions hold P(A ∩ B) = P(A)P(B), P(A ∩ C) = P(A)P(C), P(B ∩ C) = P(B)P(C), P(A ∩ B ∩ C) = P(A)P(B)P(C). Note that all four of the stated conditions must hold for three events to be independent. In particular , you can find situations in which three of them hold, but the fourth one does not. In general, for n events A1, A2, ⋯ , An to be independent we must have P(Ai ∩ Aj) = P(Ai)P(Aj),  for all distinct i, j ∈ {1, 2, ⋯ , n}; P(Ai ∩ Aj ∩ Ak) = P(Ai)P(Aj)P(Ak),  for all distinct i, j, k ∈ {1, 2, ⋯ , n}; . . . . . . P(A1 ∩ A2 ∩ A3 ⋯ ∩ An) = P(A1)P(A2)P(A3) ⋯ P(An). This might look like a dif ficult definition, but we can usually argue that the events are independent in a much easier way . For example, we might be able to justify independence by looking at the way the random experiment is performed. A simple example of an independent event is when you toss a coin repeatedly . In such an experiment, the results of any subset of the coin tosses do not have any impact on the other ones. Example 1. 21 I toss a coin repeatedly until I observe the first tails at which point I stop. Let X be the total number of coin tosses. Find P(X = 5). Solution Here, the outcome of the random experiment is a number X. The goal is to find P(A) = P(5). But what does X = 5 mean? It means that the first 4 coin tosses result in heads and the fifth one results in tails. Thus the problem is to find the probability of the sequence HHHHT when tossing a coin five times. Note that HHHHT is a shorthand for the event \"(The first coin toss results in heads) and (The second coin toss results in heads) and (The third coin toss results in heads) and (The fourth coin toss results in heads) and (The fifth coin toss results in tails).\" Since all the coin tosses are independent, we can write P(HHHHT) = P(H)P(H)P(H)P(H)P(T) = . . . . = . Discussion: Some people find it more understandable if you look at the problem in the following way . I never stop tossing the coin. So the outcome of this experiment is always an infinite sequence of heads or tails. The value X (which we are interested in) is just a function of the beginning part of the sequence until you observe a tails. If you think about the problem this way , you should not worry about the stopping time. For 1 2 1 2 1 2 1 2 1 2 1 32 this problem it might not make a big dif ference conceptually , but for some similar problems this way of thinking might be beneficial. We have seen that two events A and B are independent if P(A ∩ B) = P(A)P(B). In the next two results, we examine what independence can tell us about other set operations such as compliments and unions. Lemma 1. 1 If A and B are independent then A and B c are independent, Ac and B are independent, Ac and B c are independent. Proof We prove the first one as the others can be concluded from the first one immediately . We have P(A ∩ Bc) = P(A − B) = P(A) − P(A ∩ B) = P(A)(1 − P(B)) = P(A)P(B c). Thus, A and B c are independent. Sometimes we are interested in the probability of the union of several independent events A1, A2, ⋯ , An. For independent events, we know how to find the probability of intersection easily , but not the union. It is helpful in these cases to use De Morgan's Law: A1 ∪ A2 ∪ ⋯ ∪ An = (Ac 1 ∩ Ac 2 ∩ ⋯ ∩ Ac n) c Thus we can write P(A1 ∪ A2 ∪ ⋯ ∪ An) = 1 − P(Ac 1 ∩ Ac 2 ∩ ⋯ ∩ Ac n) = 1 − (1 − P(A1))(1 − P(A2)) ⋯ (1 − P(An)). If A1, A2, ⋯ , An are independent then P(A1 ∪ A2 ∪ ⋯ ∪ An) = 1 − (1 − P(A1))(1 − P(A2)) ⋯ (1 − P(An)). Example 1. 22 Suppose that the probability of being killed in a single flight is pc = based on available statistics. Assume that dif ferent flights are independent. If a businessman takes 20 flights per year , what is the probability that he is killed in a plane crash within the next 20 years? (Let's assume that he will not die because of another reason within the next 20 years.) Solution The total number of flights that he will take during the next 20 years is N = 20 × 20 = 400. Let ps be the probability that he survives a given single flight. Then we have ps = 1 − pc. Since these flights are independent, the probability that he will survive all N = 400 flights is P(Survive N flights) = ps × ps × ⋯ × ps = p N s = (1 − pc) N . Let A be the event that the businessman is killed in a plane crash within the next 20 years. Then P(A) = 1 − (1 − pc) N = 9.9995 × 10−5 ≈ . Warning! One common mistake is to confuse independence and being disjoint . These are completely dif ferent concepts. When two events A and B are disjoint it means that if one of them occurs, the other one cannot occur , i.e., A ∩ B = ∅. Thus, event A usually gives a lot of information about event B which means that they cannot be independent. Let's make it precise. 1 4×106 1 10000 Lemma 1. 2 Consider two events A and B, with P(A) ≠ 0 and P(B) ≠ 0. If A and B are disjoint, then they are not independent. Proof Since A and B are disjoint, we have P(A ∩ B) = 0 ≠ P(A)P(B). Thus, A and B are not independent. □ Table 1.1 summarizes the two concepts of disjointness and independence. Concept Meaning Formulas Disjoint A and B cannot occur at the same time A ∩ B = ∅, P(A ∪ B) = P(A) + P(B) Independent A does not give any information about B P(A|B) = P(A), P(B|A) = P(B) P(A ∩ B) = P(A)P(B) Table 1.1: Dif ferences between disjointness and independence. Example 1. 23 (A similar problem is given in [6] ) Two basketball players play a game in which they alternately shoot a basketball at a hoop. The first one to make a basket wins the game. On each shot, Player 1 (the one who shoots first) has probability p1 of success, while Player 2 has probability p2 of success (assume 0 < p1, p2 < 1). The shots are assumed to be independent. a . Find P(W1), the probability that Player 1 wins the game. b . For what values of p1 and p2 is this a fair game, i.e., each player has a 50 percent chance of winning the game? Solution In this game, the event W1 can happen in many dif ferent ways. W e calculate the probability of each of these ways and then add them up to find the total probability of winning. In particular , Player 1 may win on her first shot, or her second shot, and so on. Define Ai as the event that Player 1 wins on her i'th shot. What is the probability of Ai? Ai happens if Player 1 is unsuccessful at her first i − 1 shots and successful at her ith shot, while Player 2 is unsuccessful at her first i − 1 shots. Since dif ferent shots are independent, we obtain P(A1) = p1, P(A2) = (1 − p1)(1 − p2)p1, P(A3) = (1 − p1)(1 − p2)(1 − p1)(1 − p2)p1, ⋯ P(Ak) = [(1 − p1)(1 − p2)]k−1p1, ⋯ Note that A1, A2, A3, ⋯ are disjoint events, because if one of them occurs the other one cannot occur . The event that Player 1 wins is the union of the Ai's, and since the Ai's are disjoint, we have P(W1) = P(A1 ∪ A2 ∪ A3 ∪ ⋯) = P(A1) + P(A2) + P(A3) + ⋯ = p1 + (1 − p1)(1 − p2)p1 + [(1 − p1)(1 − p2)]2p1 + ⋯ = p1[1 + (1 − p1)(1 − p2) + [(1 − p1)(1 − p2)]2 + ⋯ ]. Note that since 0 < p1, p2 < 1, for x = (1 − p1)(1 − p2) we have 0 < x < 1. Thus, using the geometric sum formula ( ∑∞ k=0 ax k = a for |x| < 1), we obtain P(W1) = = . It is always a good idea to look at limit cases to check our answer . For example, if we plug in p1 = 0, p2 ≠ 0, we obtain P(W1) = 0, which is what we expect. Similarly , if we let p2 = 0, p1 ≠ 0, we obtain P(W1) = 1, which again makes sense. Now , to make this a fair game (in the sense that P(W1) = .5), we have P(W1) = = 0.5 and we obtain 1 1−x p1 1 − (1 − p1)(1 − p2) p1 p1 + p2 − p1p2 p1 p1 + p2 − p1p2 p1 = . Note that this means that p1 < p2, which makes sense intuitively . Since Player 1 has the advantage of starting the game, she should have a smaller success rate so that the whole game is fair . p2 1 + p2 1.4.2 Law of T otal Probability Let us start this section by asking a very simple question: In a certain country there are three provinces, call them B1, B2, and B3 (i.e., the country is partitioned into three disjoint sets B1, B2, and B3). We are interested in the total forest area in the country . Suppose that we know that the forest area in B1, B2, and B3 are 100km 2, 50km 2, and 150km 2, respectively . What is the total forest area in the country? If your answer is 100km 2 + 50km 2 + 150km 2 = 300km 2, you are right. That is, you can simply add forest areas in each province (partition) to obtain the forest area in the whole country . This is the idea behind the law of total probability , in which the area of forest is replaced by probability of an event A. In particular , if you want to find P(A), you can look at a partition of S, and add the amount of probability of A that falls in each partition. W e have already seen the special case where the partition is B and B c: we saw that for any two events A and B, P(A) = P(A ∩ B) + P(A ∩ B c) and using the definition of conditional probability , P(A ∩ B) = P(A|B)P(B), we can write P(A) = P(A|B)P(B) + P(A|B c)P(B c). We can state a more general version of this formula which applies to a general partition of the sample space S. Law of T otal Probability: If B1, B2, B3, ⋯ is a partition of the sample space S, then for any event A we have P(A) = ∑ i P(A ∩ Bi) = ∑ i P(A|Bi)P(Bi). Using a V enn diagram, we can pictorially see the idea behind the law of total probability . In Figure 1.24, we have A1 = A ∩ B1, A2 = A ∩ B2, A3 = A ∩ B3. As it can be seen from the figure, A1, A2, and A3 form a partition of the set A, and thus by the third axiom of probability P(A) = P(A1) + P(A2) + P(A3). Fig.1.24 - Law of total probability . Here is a proof of the law of total probability using probability axioms: Proof Since B1, B2, B3, ⋯ is a partition of the sample space S, we can write S = ⋃i Bi A = A ∩ S = A ∩ (⋃i Bi) = ⋃i(A ∩ Bi) by the distributive law (Theorem 1.2). Now note that the sets A ∩ Bi are disjoint (since the Bi's are disjoint). Thus, by the third probability axiom, P(A) = P( ⋃ i (A ∩ Bi)) = ∑ i P(A ∩ Bi) = ∑ i P(A|Bi)P(Bi). Here is a typical scenario in which we use the law of total probability . We are interested in finding the probability of an event A, but we don't know how to find P(A) directly. Instead, we know the conditional probability of A given some events Bi, where the Bi's form a partition of the sample space. Thus, we will be able to find P(A) using the law of total probability , P(A) = ∑i P(A|Bi)P(Bi). Example 1. 24 I have three bags that each contain 100 marbles: Bag 1 has 75 red and 25 blue marbles; Bag 2 has 60 red and 40 blue marbles; Bag 3 has 45 red and 55 blue marbles. I choose one of the bags at random and then pick a marble from the chosen bag, also at random. What is the probability that the chosen marble is red? Solution Let R be the event that the chosen marble is red. Let Bi be the event that I choose Bag i. We already know that P(R|B1) = 0.75, P(R|B2) = 0.60, P(R|B3) = 0.45 We choose our partition as B1, B2, B3. Note that this is a valid partition because, firstly , the Bi's are disjoint (only one of them can happen), and secondly , because their union is the entire sample space as one the bags will be chosen for sure, i.e., P(B1 ∪ B2 ∪ B3) = 1. Using the law of total probability , we can write P(R) = P(R|B1)P(B1) + P(R|B2)P(B2) + P(R|B3)P(B3) = (0.75) + (0.60) + (0.45) = 0.60 1 3 1 3 1 3 1.4.3 Bayes' Rule Now we are ready to state one of the most useful results in conditional probability: Bayes' rule. Suppose that we know P(A|B), but we are interested in the probability P(B|A). Using the definition of conditional probability , we have P(A|B)P(B) = P(A ∩ B) = P(B|A)P(A). Dividing by P(A), we obtain P(B|A) = , which is the famous Bayes' rule. Often, in order to find P(A) in Bayes' formula we need to use the law of total probability , so sometimes Bayes' rule is stated as P(Bj|A) = , where B1, B2, ⋯ , Bn form a partition of the sample space. Bayes' Rule For any two events A and B, where P(A) ≠ 0, we have P(B|A) = . If B1, B2, B3, ⋯ form a partition of the sample space S, and A is any event with P(A) ≠ 0, we have P(Bj|A) = . Example 1. 25 P(A|B)P(B) P(A) P(A|Bj)P(Bj) ∑i P(A|Bi)P(Bi) P(A|B)P(B) P(A) P(A|Bj)P(Bj) ∑i P(A|Bi)P(Bi) In Example 1.24 , suppose we observe that the chosen marble is red. What is the probability that Bag 1 was chosen? Solution Here we know P(R|Bi) but we are interested in P(B1|R), so this is a scenario in which we can use Bayes' rule. W e have P(B1|R) = = = . P(R) was obtained using the law of total probability in Example 1.24, thus we did not have to recompute it here. Also, note that P(B1|R) = > . This makes sense intuitively because bag 1 is the bag with the highest number of red marbles. Thus if the chosen marble is red, it is more likely that bag 1 was chosen. Example 1. 26 (False positive paradox [5] ) A certain disease af fects about 1 out of 10, 000 people. There is a test to check whether the person has the disease. The test is quite accurate. In particular , we know that the probability that the test result is positive (suggesting the person has the disease), given that the person does not have the disease, is only 2 percent; the probability that the test result is negative (suggesting the person does not have the disease), given that the person has the disease, is only 1 percent. A random person gets tested for the disease and the result comes back positive. What is the probability that the person has the disease? Solution Let D be the event that the person has the disease, and let T be the event that the test result is positive. W e know P(D) = , P(R|B1)P(B1) P(R) 0.75× 1 3 0.6 5 12 5 12 1 3 1 10, 000 P(T|D c) = 0.02, P(T c|D) = 0.01 What we want to compute is P(D|T). Again, we use Bayes' rule: P(D|T) = = = 0.0049 This means that there is less than half a percent chance that the person has the disease. Discussion: This might seem somewhat counterintuitive as we know the test is quite accurate. The point is that the disease is also very rare. Thus, there are two competing forces here, and since the rareness of the disease (1 out of 10,000) is stronger than the accuracy of the test (98 or 99 percent), there is still good chance that the person does not have the disease. Another way to think about this problem is illustrated in the tree diagram in Figure 1.25. Suppose 1 million people get tested for the disease. Out of the one million people, about 100 of them have the disease, while the other 999, 900 do not have the disease. Out of the 100 people who have the disease 100 × .99 = 99 people will have positive test results. However , out of the people who do not have the disease 999, 900 × .02 = 19998 people will have positive test results. Thus in total there are 19998 + 99 people with positive test results, and only 99 of them actually have the disease. Therefore, the probability that a person from the \"positive test result\" group actually have the disease is P(D|T) = = 0.0049 P(T |D)P(D) P(T |D)P(D)+P(T |Dc)P(Dc) (1−0.01)×0.0001 (1−0.01)×0.0001+0.02×(1−0.0001) 99 19998 + 99 Fig.1.25 - T ree diagram for Example 1.26. 1.4.4 Conditional Independence As we mentioned earlier , almost any concept that is defined for probability can also be extended to conditional probability . Remember that two events A and B are independent if P(A ∩ B) = P(A)P(B), or equivalently, P(A|B) = P(A). We can extend this concept to conditionally independent events. In particular , Definition 1. 2 Two events A and B are conditionally independent given an event C with P(C) > 0 if P(A ∩ B|C) = P(A|C)P(B|C) (1.8) Recall that from the definition of conditional probability , P(A|B) = , if P(B) > 0. By conditioning on C, we obtain P(A|B, C) = if P(B|C), P(C) ≠ 0. If A and B are conditionally independent given C, we obtain P(A|B, C) = = = P(A|C). Thus, if A and B are conditionally independent given C, then P(A|B, C) = P(A|C) (1.9) Thus, Equations 1.8 and 1.9 are equivalent statements of the definition of conditional independence. Now let's look at an example. P(A ∩ B) P(B) P(A ∩ B|C) P(B|C) P(A∩B|C) P(B|C) P(A|C)P(B|C) P(B|C) Example 1. 27 A box contains two coins: a regular coin and one fake two-headed coin ( P(H) = 1). I choose a coin at random and toss it twice. Define the following events. A= First coin toss results in an H. B= Second coin toss results in an H. C= Coin 1 (regular) has been selected. Find P(A|C), P(B|C), P(A ∩ B|C), P(A), P(B), and P(A ∩ B). Note that A and B are NOT independent, but they are conditionally independent given C. Solution We have P(A|C) = P(B|C) = . Also, given that Coin 1 is selected, we have P(A ∩ B|C) = . = . To find P(A), P(B), and P(A ∩ B), we use the law of total probability: P(A) = P(A|C)P(C) + P(A|C c)P(C c) = ⋅ + 1 ⋅ = . Similarly , P(B) = . For P(A ∩ B), we have P(A ∩ B) = P(A ∩ B|C)P(C) + P(A ∩ B|C c)P(C c) = P(A|C)P(B|C)P(C) + P(A|C c)P(B|C c)P(C c) = ⋅ ⋅ + 1 ⋅ 1 ⋅ = . As we see, P(A ∩ B) = ≠ P(A)P(B) = , which means that A and B are not independent. W e can also justify this intuitively . For example, if we know A has occurred (i.e., the first coin toss has resulted in heads), we would guess that it is more likely that we have chosen Coin 2 than Coin 1. This in turn increases the conditional probability that B occurs. This suggests that A and B are not independent. On the other hand, given C (Coin 1 is selected), A and B are independent. 1 2 1 2 1 2 1 4 1 2 1 2 1 2 3 4 3 4 1 2 1 2 1 2 1 2 5 8 5 8 9 16 One important lesson here is that, generally speaking, conditional independence neither implies (nor is it implied by) independence. Thus, we can have two events that are conditionally independent but they are not unconditionally independent (such as A and B above). Also, we can have two events that are independent but not conditionally independent, given an event C. Here is a simple example regarding this case. Consider rolling a die and let A = {1, 2}, B = {2, 4, 6}, C = {1, 4}. Then, we have P(A) = , P(B) = ; P(A ∩ B) = = P(A)P(B). Thus, A and B are independent. But we have P(A|C) = , P(B|C) = ; P(A ∩ B|C) = P({2}|C) = 0. Thus P(A ∩ B|C) ≠ P(A|C)P(B|C), which means A and B are not conditionally independent given C. 1 3 1 2 1 6 1 2 1 2 1.4.5 Solved Problems: Conditional Probability In die and coin problems, unless stated otherwise, it is assumed coins and dice are fair and repeated trials are independent. Problem 1 You purchase a certain product. The manual states that the lifetime T of the product, defined as the amount of time (in years) the product works properly until it breaks down, satisfies P(T ≥ t) = e − ,  for all t ≥ 0. For example, the probability that the product lasts more than (or equal to) 2 years is P(T ≥ 2) = e − = 0.6703. I purchase the product and use it for two years without any problems. What is the probability that it breaks down in the third year? Solution Let A be the event that a purchased product breaks down in the third year . Also, let B be the event that a purchased product does not break down in the first two years. W e are interested in P(A|B). We have P(B) = P(T ≥ 2) = e − . We also have P(A) = P(2 ≤ T ≤ 3) = P(T ≥ 2) − P(T ≥ 3) = e − − e − . Finally , since A ⊂ B, we have A ∩ B = A. Therefore, P(A|B) t 5 2 5 2 5 2 5 3 5 = = = = 0.1813 Problem 2 You toss a fair coin three times: a . What is the probability of three heads, HHH? b . What is the probability that you observe exactly one heads? c. Given that you have observed at least one heads, what is the probability that you observe at least two heads? Solution We assume that the coin tosses are independent. a . P(HHH) = P(H) ⋅ P(H) ⋅ P(H) = 0.53 = . b . To find the probability of exactly one heads, we can write P(One heads) = P(HTT ∪ THT ∪ TTH) = P(HTT) + P(THT) + P(TTH) = + + = . c. Given that you have observed at least one heads, what is the probability that you observe at least two heads? Let A1 be the event that you observe at least one heads, and A2 be the event that you observe at least two heads. Then A1 = S − {TTT},  and P(A1) = ; A2 = {HHT, HTH, THH, HHH},  and P(A2) = . Thus, we can write P(A∩B) P(B) P(A) P(B) e − −e − 2 5 3 5 e − 2 5 1 8 1 8 1 8 1 8 3 8 7 8 4 8 P(A2|A1) = = = . = . Problem 3 For three events A, B, and C, we know that A and C are independent, B and C are independent, A and B are disjoint, P(A ∪ C) = , P(B ∪ C) = , P(A ∪ B ∪ C) = Find P(A), P(B), and P(C). Solution We can use the V enn diagram in Figure 1.26 to better visualize the events in this problem. W e assume P(A) = a, P(B) = b, and P(C) = c. Note that the assumptions about independence and disjointness of sets are already included in the figure. Fig.1.26 - V enn diagram for Problem 3. P(A2∩A1) P(A1) P(A2) P(A1) 4 8 8 7 4 7 2 3 3 4 11 12 Now we can write P(A ∪ C) = a + c − ac = ; P(B ∪ C) = b + c − bc = ; P(A ∪ B ∪ C) = a + b + c − ac − bc = . By subtracting the third equation from the sum of the first and second equations, we immediately obtain c = , which then gives a = and b = . Problem 4 Let C1, C2, ⋯ , CM be a partition of the sample space S, and A and B be two events. Suppose we know that A and B are conditionally independent given Ci, for all i ∈ {1, 2, ⋯ , M }; B is independent of all Ci's. Prove that A and B are independent. Solution Since the Ci's form a partition of the sample space, we can apply the law of total probability for A ∩ B: P(A ∩ B) = ∑M i=1 P(A ∩ B|Ci)P(Ci) = ∑M i=1 P(A|Ci)P(B|Ci)P(Ci) = ∑M i=1 P(A|Ci)P(B)P(Ci) = P(B) ∑M i=1 P(A|Ci)P(Ci) = P(B)P(A)  (law of total probability). Problem 5 2 3 3 4 11 12 1 2 1 3 1 2 In my town, it's rainy one third of the days. Given that it is rainy , there will be heavy traf fic with probability , and given that it is not rainy , there will be heavy traf fic with probability . If it's rainy and there is heavy traf fic, I arrive late for work with probability . On the other hand, the probability of being late is reduced to if it is not rainy and there is no heavy traf fic. In other situations (rainy and no traf fic, not rainy and traf fic) the probability of being late is 0.25. You pick a random day . a . What is the probability that it's not raining and there is heavy traf fic and I am not late? b . What is the probability that I am late? c. Given that I arrived late at work, what is the probability that it rained that day? Solution Let R be the event that it's rainy , T be the event that there is heavy traf fic, and L be the event that I am late for work. As it is seen from the problem statement, we are given conditional probabilities in a chain format. Thus, it is useful to draw a tree diagram. Figure 1.27 shows a tree diagram for this problem. In this figure, each leaf in the tree corresponds to a single outcome in the sample space. We can calculate the probabilities of each outcome in the sample space by multiplying the probabilities on the edges of the tree that lead to the corresponding outcome. Fig.1.27 - T ree diagram for Problem 5. a . The probability that it's not raining and there is heavy traf fic and I am not late can be found using the tree diagram which is in fact applying the chain rule: 1 2 1 4 1 2 1 8 P(R c ∩ T ∩ Lc) = P(R c)P(T|R c)P(Lc|R c ∩ T) = ⋅ ⋅ = . b . The probability that I am late can be found from the tree. All we need to do is sum the probabilities of the outcomes that correspond to me being late. In fact, we are using the law of total probability here. P(L) = P(R, T, L) + P(R, T c, L) + P(R c, T, L) + P(R c, T c, L) = + + + = . c. We can find P(R|L) using P(R|L) = . We have already found P(L) = , and we can find P(R ∩ L) similarly by adding the probabilities of the outcomes that belong to R ∩ L. In particular , P(R ∩ L) = P(R, T, L) + P(R, T c, L) = + = . Thus, we obtain P(R|L) = = . = . Problem 6 A box contains three coins: two regular coins and one fake two-headed coin ( P(H) = 1 ), You pick a coin at random and toss it. What is the probability that it lands heads up? You pick a coin at random and toss it, and get heads. What is the probability that it is the two-headed coin? 2 3 1 4 3 4 1 8 1 12 1 24 1 24 1 16 11 48 P(R∩L) P(L) 11 48 1 12 1 24 1 8 P(R∩L) P(L) 1 8 48 11 6 11 Solution This is another typical problem for which the law of total probability is useful. Let C1 be the event that you choose a regular coin, and let C2 be the event that you choose the two-headed coin. Note that C1 and C2 form a partition of the sample space. W e already know that P(H|C1) = 0.5, P(H|C2) = 1. a . Thus, we can use the law of total probability to write P(H) = P(H|C1)P(C1) + P(H|C2)P(C2) = . + 1. = . b . Now , for the second part of the problem, we are interested in P(C2|H). We use Bayes' rule P(C2|H) = = = . Problem 7 Here is another variation of the family-with-two-children problem [1] [7] . A family has two children. W e ask the father , \"Do you have at least one daughter named Lilia?\" He replies, \"Y es!\" What is the probability that both children are girls? In other words, we want to find the probability that both children are girls, given that the family has at least one daughter named Lilia. Here you can assume that if a child is a girl, her name will be Lilia with probability α ≪ 1 independently from other children's names. If the child is a boy , his name will not be Lilia. Compare your result with the second part of Example 1.18 . 1 2 2 3 1 3 2 3 P(H|C2)P(C2) P(H) 1. 1 3 2 3 1 2 Solution Here we have four possibilities, GG = (girl, girl), GB, BG, BB, and P(GG) = P(GB) = P(BG) = P(BB) = . Let also L be the event that the family has at least one child named Lilia. W e have P(L|BB) = 0, P(L|BG) = P(L|GB) = α, P(L|GG) = α(1 − α) + (1 − α)α + α 2 = 2α − α 2. We can use Bayes' rule to find P(GG|L): P(GG|L) = = = = ≈ . Let's compare the result with part (b) of Example 1.18. Amazingly , we notice that the extra information about the name of the child increases the conditional probability of GG from to about . How can we explain this intuitively? Here is one way to look at the problem. In part (b) of Example 1.18, we know that the family has at least one girl. Thus, the sample space reduces to three equally likely outcomes: GG, GB, BG, thus the conditional probability of GG is one third in this case. On the other hand, in this problem, the available information is that the event L has occurred. The conditional sample space here still is GG, GB, BG, but these events are not equally likely anymore. A family with two girls is more likely to name at least one of them Lilia than a family who has only one girl (P(L|BG) = P(L|GB) = α, P(L|GG) = 2α − α 2), thus in this case the conditional probability of GG is higher. We would like to mention here that these problems are confusing and counterintuitive to most people. So, do not be disappointed if they seem confusing to you. We seek several goals by including such problems. First, we would like to emphasize that we should not rely too much on our intuition when solving probability problems. Intuition is useful, but at the end, we must use laws of probability to solve problems. Second, after obtaining counterintuitive results, you 1 4 P(L|GG)P(GG) P(L) P(L|GG)P(GG) P(L|GG)P(GG)+P(L|GB)P(GB)+P(L|BG)P(BG)+P(L|BB)P(BB) (2α−α2) 1 4 (2α−α2) +α +α +0. 1 4 1 4 1 4 1 4 2−α 4−α 1 2 1 3 1 2 are encouraged to think deeply about them to explain your confusion. This thinking process can be very helpful to improve our understanding of probability . Finally , I personally think these paradoxical-looking problems make probability more interesting. Problem 8 If you are not yet confused, let's look at another family-with-two-children problem! I know that a family has two children. I see one of the children in the mall and notice that she is a girl. What is the probability that both children are girls? Again compare your result with the second part of Example 1.18 . Note: Let's agree on what precisely the problem statement means. Here is a more precise statement of the problem: \"A family has two children. W e choose one of them at random and find out that she is a girl. What is the probability that both children are girls?\" Solution Here again, we have four possibilities, GG = (girl, girl), GB, BG, BB, and P(GG) = P(GB) = P(BG) = P(BB) = . Now , let Gr be the event that a randomly chosen child is a girl. Then we have P(Gr|GG) = 1, P(Gr|GB) = P(Gr|BG) = , P(Gr|BB) = 0. We can use Bayes' rule to find P(GG|Gr): P(GG|Gr) = = = = . So the answer again is dif ferent from the second part of Example 1.18. This is surprising to most people. The two problem statements look very similar but the answers are completely dif ferent. This is again similar to the previous problem (please 1 4 1 2 P(Gr|GG)P(GG) P(Gr) P(Gr|GG)P(GG) P(Gr|GG)P(GG)+P(Gr|GB)P(GB)+P(Gr|BG)P(BG)+P(Gr|BB)P(BB) 1. 1 4 1. + + +0. 1 4 1 2 1 4 1 2 1 4 1 4 1 2 read the explanation there). The conditional sample space here still is GG, GB, BG, but the point here is that these are not equally likely as in Example 1.18. The probability that a randomly chosen child from a family with two girls is a girl is one, while this probability for a family who has only one girl is . Thus, intuitively , the conditional probability of the outcome GG in this case is higher than GB and BG, and thus this conditional probability must be larger than one third. Problem 9 Okay , another family-with-two-children problem. Just kidding! This problem has nothing to do with the two previous problems. I toss a coin repeatedly . The coin is unfair and P(H) = p. The game ends the first time that two consecutive heads ( HH) or two consecutive tails ( TT) are observed. I win if HH is observed and lose if TT is observed. For example if the outcome is HTHTT––––, I lose. On the other hand, if the outcome is THTHTHH–––––, I win. Find the probability that I win. Solution Let W be the event that I win. W e can write down the set W by listing all the dif ferent sequences that result in my winning. It is cleaner if we divide W into two parts depending on the result of the first coin toss, W = {HH, HTHH, HTHTHH, ⋯} ∪ {THH, THTHH, THTHTHH, ⋯}. Let q = 1 − p. Then W = P({HH, HTHH, HTHTHH, ⋯}) + P({THH, THTHH, THTHTHH, ⋯}) = p 2 + p 3q + p 4q2 + ⋯ + p 2q + p 3q2 + p 4q3 + ⋯ = p 2(1 + pq + (pq) 2 + (pq) 3 + ⋯) + p 2q(1 + pq + (pq) 2 + (pq) 3 + ⋯) = p 2(1 + q)(1 + pq + (pq) 2 + (pq) 3 + ⋯) = ,  Using the geometric series formula = . 1 2 p2(1+q) 1−pq p2(2−p) 1−p+p2 1.5.0 End of Chapter Problems Problem 1 Suppose that the universal set S is defined as S = {1, 2, ⋯ , 10} and A = {1, 2, 3}, B = {X ∈ S : 2 ≤ X ≤ 7}, and C = {7, 8, 9, 10}. a . Find A ∪ B. b . Find (A ∪ C) − B. c. Find ¯A ∪ (B − C). d. Do A, B, and C form a partition of S? Problem 2 When working with real numbers, our universal set is R. Find each of the following sets. a . [6, 8] ∪ [2, 7) b . [6, 8] ∩ [2, 7) c. [0, 1] c d. [6, 8] − (2, 7) Problem 3 For each of the following V enn diagrams, write the set denoted by the shaded area. a . b . c. d. Problem 4 A coin is tossed twice. Let S be the set of all possible pairs that can be observed, i.e., S = {H, T} × {H, T} = {(H, H), (H, T), (T, H), (T, T)}. Write the following sets by listing their elements. a . A: The first coin toss results in head. b . B: At least one tail is observed. c. C: The two coin tosses result in dif ferent outcomes. Problem 5 * Let A = {1, 2, ⋯ , 100}. For any i ∈ N, Define Ai as the set of numbers in A that are divisible by i. For example: A2 = {2, 4, 6, ⋯ , 100}, A3 = {3, 6, 9, ⋯ , 99}. a . Find |A2|,|A3|,|A4|,|A5|. b . Find |A2 ∪ A3 ∪ A5|. Problem 6 Suppose that A1, A2, A3 form a partition of the universal set S. Let B be an arbitrary set. Assume that we know |B ∩ A1| = 10, |B ∩ A2| = 20, |B ∩ A3| = 15. Find |B|. Problem 7 Determine whether each of the following sets is countable or uncountable. a . A = {1, 2, ⋯ , 1010} b . B = {a + b√2| a, b ∈ Q} c. C = {(X, Y ) ∈ R2| x 2 + y2 ≤ 1}. Problem 8 * Let An = [0, ) = {x ∈ R| 0 ≤ x < }, for n = 2, 3, ⋯. Define A = ∞ ⋃ n=1 An = A1 ∪ A2 ∪ A3 ⋯ Find A. Problem 9 * Let An = [0, ) = {x ∈ R| 0 ≤ x < } for n = 1, 2, ⋯. Define A = ∞ ⋂ n=1 An = A1 ∩ A2 ∩ ⋯ Find A. Problem 10 * In this problem our goal is to show that sets that are not in the form of intervals may also be uncountable. In particular , consider the set A defined as the set of all subsets of N: A = {B : B ⊂ N}. We usually denote this set by A = 2N. a . Show that 2N is in one-to-one correspondence with the set of all (infinite) binary sequences: C = {b1, b2, b3, ⋯ | bi ∈ {0, 1}}. b . Show that C is in one-to-one correspondence with [0, 1]. From (a) and (b) we conclude that the set 2N is uncountable. Problem 1 1 ** Show the set [0, 1) is uncountable. That is you can never provide a list in the form of {a1, a2, a3, ⋯} that contains all the elements in [0, 1). n−1 n n−1 n 1 n 1 n Problem 12 Recall that {H, T}3 = {H, T} × {H, T} × {H, T} = {(H, H, H), (H, H, T), ⋯ , (T, T, T)}. Consider the following function f : {H, T}3 ⟶ N ∪ {0}, defined as f(x) = the number of H's in x. For example, f(HTH) = 2. a . Determine the domain and co-domain for f. b . Find range of f:Range( f). c. If we know f(x) = 2, what can we say about x? Problem 13 Two teams A and B play a soccer match, and we are interested in the winner . The sample space can be defined as S = {a, b, d}, where a shows the outcome that A wins, b shows the outcome that B wins, and d shows the outcome that they draw . Suppose we know that: (1) the probability that A wins is P(a) = P({a}) = 0.5; (2) the probability of a draw is P(d) = P({d}) = 0.25. a . Find the probability that B wins. b . Find the probability that B wins or a draw occurs. Problem 14 * Let A and B be two events such that P(A) = 0.4, P(B) = 0.7, P(A ∪ B) = 0.9 a . Find P(A ∩ B). b . Find P(Ac ∩ B). c. Find P(A − B). d. Find P(Ac − B). e. Find P(Ac ∪ B). f. Find P(A ∩ (B ∪ Ac)). Problem 15 * I roll a fair die twice and obtain two numbers: X1 = result of the first roll, X2 = result of the second roll. a . Find the probability that X2 = 4. b . Find the probability that X1 + X2 = 7. c. Find the probability that X1 ≠ 2 and X2 ≥ 4. Problem 16 Consider a random experiment with a sample space S = {1, 2, 3, ⋯}. Suppose that we know: P(k) = P({k}) = , for k = 1, 2, ⋯ , where c is a constant number . a . Find c. b . Find P({2, 4, 6}). c. Find P({3, 4, 5, ⋯}). Problem 17 Four teams A, B, C, and D compete in a tournament, and exactly one of them will win the tournament. T eams A and B have the same chance of winning the tournament. Team C is twice as likely to win the tournament as team D. The probability that either team A or team C wins the tournament is 0.6. Find the probabilities of each team winning the tournament. c 3k Problem 18 Let T be the time needed to complete a job at a certain factory . By using the historical data, we know that P(T ≤ t) = { t2 for 0 ≤ t ≤ 4 1 for t ≥ 4 a . Find the probability that the job is completed in less than one hour , i.e., find P(T ≤ 1). b . Find the probability that the job needs more than 2 hours. c. Find the probability that 1 ≤ T ≤ 3. Problem 19 * You choose a point (A, B) uniformly at random in the unit square {(x, y) : x, y ∈ [0, 1]}. What is the probability that the equation AX2 + X + B = 0 has real solutions? 1 16 Problem 20 ** (continuity of probability) a . Let A1, A2, A3, ⋯ be a sequence of increasing events, that is A1 ⊂ A2 ⊂ A3 ⊂ ⋯ Show that P ( ∞ ⋃ i=1 Ai) = lim n→∞ P(An) b . Using part(a), show that if A1, A2, ⋯ is a decreasing sequence of events, i.e., A1 ⊃ A2 ⊃ A3 ⊃ ⋯ Then P ( ∞ ⋂ i=1 Ai) = lim n→∞ P(An). Problem 21 ** (continuity of probability) For any sequence of events A1, A2, A3, ⋯, prove P ( ∞ ⋃ i=1 Ai) = lim n→∞ P ( n ⋃ i=1 Ai) , P ( ∞ ⋂ i=1 Ai) = lim n→∞ P ( n ⋂ i=1 Ai) . Problem 22 Suppose that, of all the customers at a cof fee shop, 70% purchase a cup of cof fee; 40% purchase a piece of cake; 20% purchase both a cup of cof fee and a piece of cake. Given that a randomly chosen customer has purchased a piece of cake, what is the probability that he/she has also purchased a cup of cof fee? Problem 23 Let A, B, and C be three events with probabilities given below: a . Find P(A|B). b . Find P(C|B). c. Find P(B|A ∪ C). d. Find P(B|A, C) = P(B|A ∩ C). Problem 24 A real number X is selected uniformly at random in the continuous interval [0, 10]. (For example, X could be 3.87.) a . Find P (2 ≤ X ≤ 5). b . Find P (X ≤ 2|X ≤ 5). c. Find P (3 ≤ X ≤ 8|X ≥ 4). Problem 25 A professor thinks students who live on campus are more likely to get As in the probability course. T o check this theory , the professor combines the data from the past few years: a . 600 students have taken the course, b . 120 students have gotten As, c. 200 students lived on campus, d. 80 students lived of f campus and got As. Does this data suggest that \"getting an A\" and \"living on campus\" are dependent or independent? Problem 26 I roll a die n times, n ∈ N. Find the probability that numbers 1 and 6 are both observed at least once. Problem 27 Consider a communication system. At any given time, the communication channel is in good condition with probability 0.8, and is in bad condition with probability 0.2. An error occurs in a transmission with probability 0.1 if the channel is in good condition, and with probability 0.3 if the channel is in bad condition. Let G be the event that the channel is in good condition and E be the event that there is an error in transmission. a . Complete the following tree diagram: b . Using the tree find P(E). c. Using the tree find P(G|E c). Problem 28 * In a factory there are 100 units of a certain product, 5 of which are defective. We pick three units from the 100 units at random. What is the probability that exactly one of them is defective? Problem 29 Reliability Real-life systems often are composed of several components. For example, a system may consist of two components that are connected in parallel as shown in Figure 1.28. When the system's components are connected in parallel, the system works if at least one of the components is functional. The components might also be connected in series as shown in Figure 1.28. When the system's components are connected in series, the system works if all of the components are functional. Fig.1.28 - In left figure, Components C1 and C2 are connected in parallel. The system is functional if at least one of the C1 and C2 is functional. In right figure, Components C1 and C2 are connected in series. The system is functional only if both C1 and C2 are functional. For each of the following systems, find the probability that the system is functional. Assume that component k is functional with probability Pk independent of other components. a . b . c. d. e. Problem 30 You choose a point (X, Y ) uniformly at random in the unit square S = {(x, y) ∈ R2 : 0 ≤ x ≤ 1, 0 ≤ y ≤ 1}. Let A be the event {(x, y) ∈ S : |x − y| ≤ } and B be the event {(x, y) ∈ S : y ≥ x}. a . Show sets A and B in the x-y plane. b . Find P(A) and P(B). c. Are A and B independent? Problem 31 One way to design a spam filter is to look at the words in an email. In particular , some words are more frequent in spam emails. Suppose that we have the following information: 50% of emails are spam; 1% of spam emails contain the word \"refinance\"; 0.001% of non-spam emails contain the word \"refinance\". Suppose that an email is checked and found to contain the word \"refinance\". What is the probability that the email is spam? Problem 32 * You would like to go from point A to point B in Figure 1.28. There are 5 bridges on different branches of the river as shown in Figure 1.29. 1 2 Fig.1.29 - Problem 32. Bridge i is open with probability Pi, i = 1, 2, 3, 4, 5. Let A be the event that there is a path from A to B and let Bk be the event that k th bridge is open. a . Find P(A). b . Find P(B3|A). Problem 33 * (The Monty Hall Problem) You are in a game show , and the host gives you the choice of three doors. Behind one door is a car and behind the others are goats. Y ou pick a door , say Door 1. The host who knows what is behind the doors opens a dif ferent door and reveals a goat (the host can always open such a door because there is only one door behind which is a car). The host then asks you: \"Do you want to switch?\" The question is, is it to your advantage to switch your choice? Problem 34 I toss a fair die twice, and obtain two numbers X and Y . Let A be the event that X = 2, B be the event that X + Y = 7, and C be the event that Y = 3. a . Are A and B independent? b . Are A and C independent? c. Are B and C independent? d. Are A, B, and C are independent? Problem 35 * You and I play the following game: I toss a coin repeatedly . The coin is unfair and P(H) = p. The game ends the first time that two consecutive heads (HH) or two consecutive tails (TT) are observed. I win if (HH) is observed and you win if (TT) is observed. Given that I won the game, find the probability that the first coin toss resulted in head? Problem 36 * A box contains two coins: a regular coin and one fake two-headed coin (P(H)=1). I choose a coin at random and toss it n times. If the first n coin tosses result in heads, what is the probability that the (n + 1) th coin toss will also result in heads? Problem 37 * A family has n children, n ≥ 2. We ask the father: \"Do you have at least one daughter?\" He responds \"Y es!\" Given this extra information, what is the probability that all n children are girls? In other words, what is the probability that all of their children are girls, given that at least one of them is a girl? Problem 38 * A family has n children, n ≥ 2. We ask from the father , \"Do you have at least one daughter named Lilia?\" He replies, \"Y es!\" What is the probability that all of their children are girls? In other words, we want to find the probability that all n children are girls, given that the family has at least one daughter named Lilia. Here you can assume that if a child is a girl, her name will be Lilia with probability α ≪ 1 independently from other children's names. If the child is a boy , his name will not be Lilia. Problem 39 * A family has n children. W e pick one of them at random and find out that she is a girl. What is the probability that all their children are girls? 2.1 Counting Remember that for a finite sample space S with equally likely outcomes, the probability of an event A is given by P(A) = = Thus, finding probability of A reduces to a counting problem in which we need to count how many elements are in A and S. In this section, we will discuss ways to count the number of elements in a set in an ef ficient manner . Counting is an area of its own and there are books on this subject alone. Here we provide a basic introduction to the material that is usually needed in probability . Almost everything that we need about counting is the result of the multiplication principle . We previously saw the multiplication principle when we were talking about Cartesian products. Here we look at it from a dif ferent perspective. Let us look at a simple example. Example 2. 1 Suppose that I want to purchase a tablet computer . I can choose either a large or a small screen; a 64GB, 128GB, or 256GB storage capacity , and a black or white cover . How many dif ferent options do I have? Solution Here are the options: 1 . L-64-B 2 . L-64-W 3 . L-128-B 4 . L-128-W 5 . L-256-B 6 . L-256-W 7 . S-64-B 8 . S-64-W 9 . S-128-B |A| |S| M N 10 . S-128-W 1 1 . S-256-B 12 . S-256-W Thus, there are 12 possible options. The multiplication principle states that we can simply multiply the number of options in each category (screen size, memory , color) to get the total number of possibilities, i.e., the answer is 2 × 3 × 2 = 12. Here is a formal statement of the multiplication principle. Multiplication Principle Suppose that we perform r experiments such that the kth experiment has nk possible outcomes, for k = 1,2,⋯,r. Then there are a total of n1 × n2 × n3 × ⋯ × nr possible outcomes for the sequence of r experiments. Example 2. 2 I need to choose a password for a computer account. The rule is that the password must consist of two lowercase letters (a to z) followed by one capital letter (A to Z) followed by four digits (0, 1, ⋯ , 9). For example, the following is a valid password ejT3018 Find the total number of possible passwords, N. A hacker has been able to write a program that randomly and independently generates 108 passwords according to the above rule. Note that the same password could be generated more than once. If one of the randomly chosen passwords matches my password, then he can access my account information. What is the probability that he is successful in accessing my account information? Solution To choose a password, I need to first choose a lowercase letter , then another lowercase letter, then one capital letter , and then 4 digits. There are 26 lowercase letters, 26 capital letters, and 10 digits. Thus, by the multiplication principle, the total number of possible valid passwords is N = 26 × 26 × 26 × 10 × 10 × 10 × 10 = 263 × 104 Let Gi denote the event that the hacker's ith guess matches mine, for i = 1, 2, ⋯ , 108. The probability that the ith randomly chosen password matches mine is P(Gi) = Now let phack be the probability that the hacker is successful, that is at least one of the randomly chosen passwords matches mine. Recall that \"at least\" means union: phack = P( ⋃ i Gi) Note that the events Gi are independent since the guesses are independently generated, but they are not disjoint since multiple guesses could be correct if the hacker's program generates the same password. Therefore in this case it is easier to work with intersections than unions, so we will find the probability of the complement event first: P( ⋃i Gi) c = P( ⋂i Gc i ) = ∏ N i=1 P(Gc i ) (by independence) = (1 − ) 108 Therefore, phack = 1 − (1 − ) 108 = 1 − (1 − ) 108 = 0.4339 Example 2. 3 Let A be a set with |A| = n < ∞. How many distinct subsets does A have? 1 N 1 N 1 N 1 263×104 Solution Let's assume A = {a1, a2, a3, ⋯ , an}. We can look at this problem in the following way . To choose a subset B, we perform the following experiment. First we decide whether or not a1 ∈ B (two choices), then we decide whether or not a2 ∈ B (two choices), then we decide whether or not a3 ∈ B (two choices), ..., and finally we decide whether or not an ∈ B (two choices). By the multiplication principle, the total number of subsets is then given by 2 × 2 × 2 × ⋯ × 2 = 2n. To check our answer , let's assume A = {1, 2}. Then our formula states that there are 4 possible subsets. Indeed, the subsets are 1 . {} = ∅ 2 . {1} 3 . {2} 4 . {1, 2} Here, we would like to provide some general terminology for the counting problems that show up in probability to make sure that the language that we use is precise and clear . Sampling : sampling from a set means choosing an element from that set. W e often draw a sample at random from a given set in which each element of the set has equal chance of being chosen. With or without replacement : usually we draw multiple samples from a set. If we put each object back after each draw , we call this sampling with replacement . In this case a single object can be possibly chosen multiple times. For example, if A = {a1, a2, a3, a4} and we pick 3 elements with replacement, a possible choice might be (a3, a1, a3). Thus \"with replacement\" means \"repetition is allowed.\" On the other hand, if repetition is not allowed, we call it sampling without replacement . Ordered or unordered : If ordering matters (i.e.: a1, a2, a3 ≠ a2, a3, a1), this is called ordered sampling . Otherwise, it is called unordered . Thus when we talk about sampling from sets, we can talk about four possibilities. ordered sampling with replacement ordered sampling without replacement unordered sampling without replacement unordered sampling with replacement We will discuss each of these in detail and indeed will provide a formula for each. The formulas will be summarized at the end in Table 2.1 . Nevertheless, the best approach here is to understand how to derive these formulas. You do not actually need to memorize them if you understand the way they are obtained. 2.1.1 Ordered Sampling with Replacement Here we have a set with n elements (e.g.: A = {1, 2, 3, ⋯ . n}), and we want to draw k samples from the set such that ordering matters and repetition is allowed. For example, if A = {1, 2, 3} and k = 2, there are 9 dif ferent possibilities: 1 . (1,1); 2 . (1,2); 3 . (1,3); 4 . (2,1); 5 . (2,2); 6 . (2,3); 7 . (3,1); 8 . (3,2); 9 . (3,3). In general, we can argue that there are k positions in the chosen list: (Position 1, Position 2, ..., Position k). There are n options for each position. Thus, when ordering matters and repetition is allowed, the total number of ways to choose k objects from a set with n elements is n × n×. . . ×n = nk Note that this is a special case of the multiplication principle where there are k \"experiments\" and each experiment has n possible outcomes. Video A il b 2.1.2 Ordered Sampling without Replacement: Permutations Consider the same setting as above, but now repetition is not allowed. For example, if A = {1, 2, 3} and k = 2, there are 6 different possibilities: 1 . (1,2); 2 . (1,3); 3 . (2,1); 4 . (2,3); 5 . (3,1); 6 . (3,2). In general, we can argue that there are k positions in the chosen list: (Position 1, Position 2, ..., Position k). There are n options for the first position, (n − 1) options for the second position (since one element has already been allocated to the first position and cannot be chosen here), (n − 2) options for the third position, ... (n − k + 1) options for the kth position. Thus, when ordering matters and repetition is not allowed, the total number of ways to choose k objects from a set with n elements is n × (n − 1)×. . . ×(n − k + 1). Any of the chosen lists in the above setting (choose k elements, ordered and no repetition) is called a k-permutation of the elements in set A. We use the following notation to show the number of k-permutations of an n-element set: P n k = n × (n − 1)×. . . ×(n − k + 1). Note that if k is larger than n, then P n k = 0. This makes sense, since if k > n there is no way to choose k distinct elements from an n-element set. Let's look at a very famous problem, called the birthday problem, or the birthday paradox. Example 2. 4 If k people are at a party , what is the probability that at least two of them have the same birthday? Suppose that there are n = 365 days in a year and all days are equally likely to be the birthday of a specific person. Solution Let A be the event that at least two people have the same birthday . First note that if k > n, then P(A) = 1; so, let's focus on the more interesting case where k ≤ n. Again, the phrase \"at least\" suggests that it might be easier to find the probability of the complement event, P(Ac). This is the event that no two people have the same birthday , and we have P(A) = 1 − . Thus, to solve the problem it suf fices to find |Ac| and |S|. Let's first find |S|. What is the total number of possible sequences of birthdays of k people? W ell, there are n = 365 choices for the first person, n = 365 choices for the second person,... n = 365 choices for the kth person. Thus there are nk possibilities. This is, in fact, an ordered sampling with replacement problem, and as we have discussed, the answer should be nk (here we draw k samples, birthdays, from the set {1, 2, . . . , n = 365}). Now let's find |Ac|. If no birthdays are the same, this is similar to finding |S| with the difference that repetition is not allowed, so we have |Ac| = P n k = n × (n − 1)×. . . ×(n − k + 1). You can see this directly by noting that there are n = 365 choices for the first person, n − 1 = 364 choices for the second person,..., n − k + 1 choices for the kth person. Thus the probability of A can be found as P(A) = 1 − = 1 − . Discussion: The reason this is called a paradox is that P(A) is numerically dif ferent from what most people expect. For example, if there are k= 23 people in the party , what do you guess is the probability that at least two of them have the same birthday , P(A)? The answer is .5073, which is much higher than what most people guess. The probability crosses 99 percent when the number of peoples reaches 57. But why is the probability higher than what we expect? |Ac| |S| |Ac| |S| P n k nk It is important to note that in the birthday problem, neither of the two people are chosen beforehand. To better answer this question, let us look at a dif ferent problem: I am in a party with k − 1 people. What is the probability that at least one person in the party has the same birthday as mine? W ell, we need to choose the birthdays of k − 1 people, the total number of ways to do this is nk−1. The total number of ways to choose the birthdays so that no one has my birthday is (n − 1) k−1. Thus, the probability that at least one person has the same birthday as mine is P(B) = 1 − ( ) k−1. Now , if k = 23, this probability is only P(B) = 0.0586, which is much smaller than the corresponding P(A) = 0.5073. The reason is that event B is looking only at the case where one person in the party has the same birthday as me. This is a much smaller event than event A which looks at all possible pairs of people. Thus, P(A) is much larger than P(B). We might guess that the value of P(A) is much lower than it actually is, because we might confuse it with P(B). Permutations of n elements : An n-permutation of n elements is just called a permutation of those elements. In this case, k = n and we have P n n = n × (n − 1)×. . . ×(n − n + 1) = n × (n − 1)×. . . ×1, which is denoted by n!, pronounced \" n factorial\". Thus n! is simply the total number of permutations of n elements, i.e., the total number of ways you can order n different objects. To make our formulas consistent, we define 0! = 1. Example 2. 5 Shuf fle a deck of 52 cards. How many outcomes are possible? (In other words, how many dif ferent ways can you order 52 distinct cards? How many dif ferent permutations of 52 distinct cards exist?) The answer is 52!. Now , using the definition of n!, we can rewrite the formula for P n k as P n k = . n − 1 n n! (n − k)! The number of k-permutations of n distinguishable objects is given by P n k = ,  for 0 ≤ k ≤ n. Note: There are several dif ferent common notations that are used to show the number of k-permutations of an n-element set including Pn,k, P(n, k), nPk, etc. In this book, we always use P n k . n! (n − k)! Video A il b 2.1.3 Unordered Sampling without Replacement: Combinations Here we have a set with n elements, e.g., A = {1, 2, 3, . . . . n} and we want to draw k samples from the set such that ordering does not matter and repetition is not allowed. Thus, we basically want to choose a k-element subset of A, which we also call a k- combination of the set A. For example if A = {1, 2, 3} and k = 2, there are 3 different possibilities: 1 . {1,2}; 2 . {1,3}; 3 . {2,3}. We show the number of k-element subsets of A by ( ). This is read \" n choose k.\" A typical scenario here is that we have a group of n people, and we would like to choose k of them to serve on a committee. A simple way to find ( ) is to compare it with P n k . Note that the dif ference between the two is ordering. In fact, for any k-element subset of A = {1, 2, 3, . . . . n}, we can order the elements in k! ways, thus we can write P n k = ( ) × k! Therefore, ( ) = . Note that if k is an integer larger than n, then ( ) = 0. This makes sense, since if k > n there is no way to choose k distinct elements from an n-element set. n k n k n k n k n! k!(n − k)! n k The number of k-combinations of an n-element set is given by ( ) = ,  for 0 ≤ k ≤ n. ( ) is also called the binomial coefficient . This is because the coef ficients in the binomial theorem are given by ( ). In particular , the binomial theorem states that for an integer n ≥ 0, we have (a + b) n = n ∑ k=0 ( )a kbn−k. Note: There are several dif ferent common notations that are used to show the number of k-combinations of an n-element set including Cn,k, C(n, k), C n k , nCk, etc. In this book, we always use ( ). Example 2. 6 I choose 3 cards from the standard deck of cards. What is the probability that these cards contain at least one ace? Solution Again the phrase \"at least\" suggests that it might be easier to first find P(Ac), the probability that there is no ace. Here the sample space contains all possible ways to choose 3 cards from 52 cards, thus |S| = ( ). There are 52 − 4 = 48 non-ace cards, so we have |Ac| = ( ). Thus P(A) = 1 − . n k n! k!(n − k)! n k n k n k n k 52 3 48 3 ( ) 48 3 ( ) 52 3 Example 2. 7 How many distinct sequences can we make using 3 letter \"A\"s and 5 letter \"B\"s? (AAABBBBB, AABABBBB, etc.) Solution You can think of this problem in the following way . You have 3 + 5 = 8 positions to fill with letters A or B. From these 8 positions, you need to choose 3 of them for As. Whatever is left will be filled with Bs. Thus the total number of ways is ( ). Now , you could have equivalently chosen the locations for Bs, so the answer would have been ( ). Thus, we conclude that ( ) = ( ). The same argument can be repeated for general n and k to conclude ( ) = ( ). You can check this identity directly algebraically , but the way we showed it here is interesting in the sense that you do not need any algebra. This is sometimes a very effective way of proving some identities of binomial coef ficients. This is proof by combinatorial interpretation . The basic idea is that you count the same thing twice, each time using a dif ferent method and then conclude that the resulting formulas must be equal. Let us look at some other examples. Example 2. 8 8 3 8 5 8 3 8 5 n k n n − k Show the following identities for non-negative integers k and m and n, using combinatorial interpretation arguments. 1 . We have ∑n k=0 ( ) = 2n. 2 . For 0 ≤ k < n, we have ( ) = ( ) + ( ). 3 . We have ( ) = ∑k i=0 ( )( ) (Vandermonde's identity). Solution 1 . To show this identity , we count the total number of subsets of an n-element set A . We have already seen that this is equal to 2n in Example 2.3 . Another way to count the number of subsets is to first count the subsets with 0 elements, and then add the number of subsets with 1 element, and then add the number of subsets with 2 elements, etc. But we know that the number of k-element subsets of A is ( ), thus we have 2n (2.1) = ∑n k=0 ( ). We can also prove this identity algebraically , using the binomial theorem, (a + b) n = ∑n k=0 ( )a kbn−k. If we let a = b = 1, we obtain 2n = ∑n k=0 ( ). 2 . To show this identity , let's assume that we have an arbitrary set A with n + 1 distinct elements: A = {a1, a2, a3, . . . , an, an+1}. We would like to choose a k + 1-element subset B. We know that we can do this in ( ) ways (the right hand side of the identity). Another way to count the number of k + 1-element subsets B is to divide them into two non-overlapping categories based on whether or not they contain an+1. In particular , if an+1 ∉ B, then we need to choose k + 1 elements from {a1, a2, a3, . . . , an} which we can do in ( ) different ways. If, on the other hand, an+1 ∈ B, then we need to choose another k elements from {a1, a2, a3, . . . , an} to complete B and we can do this in ( ) different ways. Thus, we have shown that the total number of k + 1-element subsets of an n + 1-element set is equal to ( ) + ( ). 3 . Here we assume that we have a set A that has m + n elements: A = {a1, a2, a3, . . . , am, b1, b2, . . . , bn}. n k n+1 k+1 n k+1 n k m+n k m i n k−i n k n k n k n k n+1 k+1 n k+1 n k n k+1 n k We would like to count the number of k-element subsets of A. This is ( ). Another way to do this is first choose i elements from {a1, a2, a3, . . . , am} and then k − i elements from {b1, b2, . . . , bn}. This can be done in ( )( ) number of ways. But i can be any number from 0 to k, so we conclude ( ) = ∑k i=0 ( )( ). Let us now provide another interpretation of ( ). Suppose that we have a group of n people and we would like to divide them two groups A and B such that group A consists of k people and group B consists of n − k people. T o do this, we just simply need to choose k people and put them in group A, and whoever is left will be in group B. Thus, the total number of ways to do this is ( ). The total number of ways to divide n distinct objects into two groups A and B such that group A consists of k objects and group B consists of n − k objects is ( ). Note: For the special case when n = 2k and we do not particularly care about group names A and B, the number of ways to do this division is ( ) to avoid double counting. For example, if 22 players want to play a soccer game and we need to divide them into two groups of 11 players, there will be ( ) ways to do this. The reason for this is that, if we label the players 1 to 22, then the two choices A = {1, 2, 3, . . . , 11} and B = {12, 13, 14, . . . , 22}, A = {12, 13, 14, . . . , 22} and B = {1, 2, 3, . . . , 11} are essentially the same. For example, we can solve Example 2.7 in the following way: W e have 8 blank positions to be filled with letters \"A\" or \"B.\" W e need to divide them into two groups A and B such that group A consists of three blank positions and group B consists of 5 blank spaces. The elements in group A show the positions of \"A\"s and the elements in group B show the positions of \"B\"s. Therefore the total number of possibilities is ( ). Bernoulli T rials and Binomial Distribution: Now , we are ready to discuss an important class of random experiments that appear frequently in practice. First, we define Bernoulli trials and then discuss the binomial distribution. A Bernoulli T rial is a random experiment that has two possible outcomes which we can label as \"success\" and \"failure,\" such as m+n k m i n k−i m+n k m i n k−i n k n k n k 1 2 n k 1 2 22 11 8 3 You toss a coin. The possible outcomes are \"heads\" and \"tails.\" Y ou can define \"heads\" as success and \"tails\" as \"failure\" here. You take a pass-fail test. The possible outcomes are \"pass\" and \"fail.\" We usually denote the probability of success by p and probability of failure by q = 1 − p . If we have an experiment in which we perform n independent Bernoulli trials and count the total number of successes, we call it a binomial experiment. For example, you may toss a coin n times repeatedly and be interested in the total number of heads. Example 2. 9 Suppose that I have a coin for which P(H) = p and P(T) = 1 − p. I toss the coin 5 times. a . What is the probability that the outcome is THHHH? b . What is the probability that the outcome is HTHHH? c. What is the probability that the outcome is HHTHH? d. What is the probability that I will observe exactly four heads and one tails? e. What is the probability that I will observe exactly three heads and two tails? f. If I toss the coin n times, what is the probability that I observe exactly k heads and n − k tails? Solution a . To find the probability of the event A = {THHHH}, we note that A is the intersection of 5 independent events: A ≡ first coin toss is tails, and the next four coin tosses result in heads. Since the individual coin tosses are independent, we obtain P(THHHH) = p(T) × p(H) × p(H) × p(H) × p(H) = (1 − p)p 4. b . Similarly , P(HTHHH) = p(H) × p(T) × p(H) × p(H) × p(H) = (1 − p)p 4. c. Similarly , P(HHTHH) = p(H) × p(H) × p(T) × p(H) × p(H) = (1 − p)p 4. d. Let B be the event that I observe exactly one tails and four heads. Then B = {THHHH, HTHHH, HHTHH, HHHTH, HHHHT}. Thus P(B) = P(THHHH) + P(HTHHH) + P(HHTHH) + P(HHHTH) + P(HHHHT) = (1 − p)p 4 + (1 − p)p 4 + (1 − p)p 4 + (1 − p)p 4 + (1 − p)p 4 = 5p 4(1 − p). e. Let C be the event that I observe exactly three heads and two tails. Then C = {TTHHH, THTHH, THHHTH, . . . , HHHTT}. Thus P(C) = P(TTHHH) + P(THTHH) + P(THHTH)+. . . +P(HHHTT) = (1 − p)2p3 + (1 − p)2p3 + (1 − p)2p3+. . . +(1 − p)2p3 = |C|p3(1 − p)2. But what is |C|? Luckily , we know how to find |C|. This is the total number of distinct sequences that you can create using two tails and three heads. This is exactly the same as Example 2.7 . The idea is that we have 5 positions to fill with letters H or T . From these 5 positions, you need to choose 3 of them for Hs. Whatever is left is going to be filled with T s. Thus the total number of elements in C is ( ), and P(C) = ( )p 3(1 − p) 2. f. Finally , we can repeat the same argument when we toss the coin n times and obtain P(k heads and n − k tails) = ( )p k(1 − p) n−k. Note that here, instead of writing P(k heads and n − k tails ), we can just write P(k heads ). 5 3 5 3 n k Binomial Formula: For n independent Bernoulli trials where each trial has success probability p, the probability of k successes is given by P(k) = ( )p k(1 − p) n−k. Multinomial Coefficients: The interpretation of the binomial coef ficient ( ) as the number of ways to divide n objects into two groups of size k and n − k has the advantage of being generalizable to dividing objects into more than two groups. Example 2. 10 Ten people have a potluck. Five people will be selected to bring a main dish, three people will bring drinks, and two people will bring dessert. How many ways they can be divided into these three groups? Solution We can solve this problem in the following way . First, we can choose 5 people for the main dish. This can be done in ( ) ways. From the remaining 5 people, we then choose 3 people for drinks, and finally the remaining 2 people will bring desert. Thus, by the multiplication principle, the total number of ways is given by ( )( )( ) = ⋅ ⋅ = . This argument can be generalized for the case when we have n people and would like to divide them to r groups. The number of ways in this case is given by the multinomial coef ficients. In particular , if n = n1 + n2+. . . +nr, where all ni ≥ 0 are integers, then the number of ways to divide n distinct objects to r distinct groups of sizes n1, n2, . . . , nr is given by n k n k 10 5 10 5 5 3 2 2 10! 5!5! 5! 3!2! 2! 2!0! 10! 5!3!2! ( ) = . We can also state the general format of the binomial theorem, which is called the multinomial theorem: (x1 + x2 + ⋯ + xr) n = ∑ n1+n2+⋯+nr=n ( )x n1 1 x n2 2 . . . x nr r (2.2) Finally , the binomial formula for Bernoulli trials can also be extended to the case where each trial has more than two possible outcomes. Example 2. 1 1 I roll a die 18 times. What is the probability that each number appears exactly 3 times? Solution First of all, each sequence of outcomes in which each number appears 3 times has probability ( ) 3 × ( ) 3 × ( ) 3 × ( ) 3 × ( ) 3 × ( ) 3 = ( ) 18. How many distinct sequences are there with three 1's, three 2's, ..., and three 6's? Each sequence has 18 positions which we need to fill with the digits. T o obtain a sequence, we need to choose three positions for 1's, three positions for 2's, ..., and three positions for 6's. The number of ways to do this is given by the multinomial coef ficient ( ) = . Thus the total probability is ( ) 18. n n1, n2, . . . , nr n! n1!n2!. . . nr! n n1, n2, … , nr 1 6 1 6 1 6 1 6 1 6 1 6 1 6 18 3, 3, 3, 3, 3, 3 18! 3!3!3!3!3!3! 18! (3!)6 1 6 We now state the general form of the multinomial formula. Suppose that an experiment has r possible outcomes, so the sample space is given by S = {s1, s2, . . . , sr}. Also suppose that P(si) = pi for i = 1, 2, . . . , r. Then for n = n1 + n2+. . . +nr independent trials of this experiment, the probability that each si appears ni times is given by ( )p n1 1 p n2 2 . . . p nr r = p n1 1 p n2 2 . . . p nr r . n n1, n2, . . . , nr n! n1!n2!. . . nr! 2.1.4 Unordered Sampling with Replacement Among the four possibilities we listed for ordered/unordered sampling with/without replacement, unordered sampling with replacement is the most challenging one. Suppose that we want to sample from the set A = {a1, a2, . . . , an} k times such that repetition is allowed and ordering does not matter . For example, if A = {1, 2, 3} and k = 2, then there are 6 different ways of doing this 1,1; 1,2; 1,3; 2,2; 2,3; 3,3; How can we get the number 6 without actually listing all the possibilities? One way to think about this is to note that any of the pairs in the above list can be represented by the number of 1's, 2's and 3's it contains. That is, if x1 is the number of ones, x2 is the number of twos, and x3 is the number of threes, we can equivalently represent each pair by a vector (x1, x2, x3), i.e., 1,1 → (x1, x2, x3) = (2, 0, 0); 1,2 → (x1, x2, x3) = (1, 1, 0); 1,3 → (x1, x2, x3) = (1, 0, 1); 2,2 → (x1, x2, x3) = (0, 2, 0); 2,3 → (x1, x2, x3) = (0, 1, 1); 3,3 → (x1, x2, x3) = (0, 0, 2). Note that here xi ≥ 0 are integers and x1 + x2 + x3 = 2. Thus, we can claim that the number of ways we can sample two elements from the set A = {1, 2, 3} such that ordering does not matter and repetition is allowed is the same as solutions to the following equation x1 + x2 + x3 = 2,  where xi ∈ {0, 1, 2}. This is an interesting observation and in fact using the same argument we can make the following statement for general k and n. Lemma 2. 1 The total number of distinct k samples from an n-element set such that repetition is allowed and ordering does not matter is the same as the number of distinct solutions to the equation x1 + x2+. . . +xn = k,  where xi ∈ {0, 1, 2, 3, . . . }. So far we have seen the number of unordered k-samples from an n element set is the same as the number of solutions to the above equation. But how do we find the number of solutions to that equation? Theorem 2. 1 The number of distinct solutions to the equation x1 + x2+. . . +xn = k,  where xi ∈ {0, 1, 2, 3, . . . } (2.3) is equal to ( ) = ( ). Proof Let us first define the following simple mapping in which we replace an integer xi ≥ 0 with xi vertical lines, i.e., 1 → | 2 → || 3 → ||| . . . Now suppose we have a solution to the Equation 2.3. W e can replace the xi's by their equivalent vertical lines. Thus, for example if we have x1 + x2 + x3 + x4 = 3 + 0 + 2 + 1, we can equivalently write ||| + +|| + |. Thus, we claim that for each solution to the Equation 2.3, we have unique representation using vertical lines (' |') and plus signs (' +'). Indeed, each solution can be represented by k vertical lines (since the xi sum to k) and n − 1 plus signs. Now , this is exactly the same as Example 2.7 : how many distinct sequences you can make using k vertical lines (|) and n − 1 plus signs ( +)? The answer as we have seen is ( ) = ( ). n + k − 1 k n + k − 1 n − 1 n + k − 1 k n + k − 1 n − 1 Example 2. 12 Ten passengers get on an airport shuttle at the airport. The shuttle has a route that includes 5 hotels, and each passenger gets of f the shuttle at his/her hotel. The driver records how many passengers leave the shuttle at each hotel. How many dif ferent possibilities exist? Solution Let xi be the number of passengers that get of f the shuttle at Hotel i. Then we have x1 + x2 + x3 + x4 + x5 = 10, where xi ∈ {0, 1, 2, 3, . . . }. Thus, the number of solutions is ( ) = ( ) = ( ). Let's summarize the formulas for the four categories of sampling. Assuming that we have a set with n elements, and we want to draw k samples from the set, then the total number of ways we can do this is given by the following table. ordered sampling with replacement nk ordered sampling without replacement P n k = unordered sampling without replacement ( ) = unordered sampling with replacement ( ) Table 2.1: Counting results for dif ferent sampling methods. 5 + 10 − 1 10 5 + 10 − 1 5 − 1 14 4 n! (n−k)! n k n! k!(n−k)! n+k−1 k 2.1.5 Solved Problems: Combinatorics Problem 1 Let A and B be two finite sets, with |A| = m and |B| = n. How many distinct functions (mappings) can you define from set A to set B, f : A → B? Solution We can solve this problem using the multiplication principle. Let A = {a1, a2, a3, . . . , am}, B = {b1, b2, b3, . . . , bn}. Note that to define a mapping from A to B, we have n options for f(a1), i.e., f(a1) ∈ B = {b1, b2, b3, . . . , bn}. Similarly we have n options for f(a2), and so on. Thus by the multiplication principle, the total number of distinct functions f : A → B is n ⋅ n ⋅ n ⋯ n = nm. Problem 2 A function is said to be one-to-one if for all x1 ≠ x2, we have f(x1) ≠ f(x2). Equivalently , we can say a function is one-to-one if whenever f(x1) = f(x2), then x1 = x2. Let A and B be two finite sets, with |A| = m and |B| = n. How many distinct one-to-one functions (mappings) can you define from set A to set B, f : A → B? Solution Again let A = {a1, a2, a3, . . . , am}, B = {b1, b2, b3, . . . , bn}. To define a one-to-one mapping from A to B, we have n options for f(a1), i.e., f(a1) ∈ B = {b1, b2, b3, . . . , bn}. Given f(a1), we have n − 1 options for f(a2), and so on. Thus by the multiplication principle, the total number of distinct functions f : A → B, is n ⋅ (n − 1) ⋅ (n − 2) ⋯ (n − m + 1) = P n m. Thus, in other words, choosing a one-to-one function from A to B is equivalent to choosing an m-permutation from the n-element set B (ordered sampling without replacement) and as we have seen there are P n m ways to do that. Problem 3 An urn contains 30 red balls and 70 green balls. What is the probability of getting exactly k red balls in a sample of size 20 if the sampling is done with replacement (repetition allowed)? Assume 0 ≤ k ≤ 20. Solution Here any time we take a sample from the urn we put it back before the next sample (sampling with replacement). Thus in this experiment each time we sample, the probability of choosing a red ball is , and we repeat this in 20 independent trials. This is exactly the binomial experiment. Thus, using the binomial formula we obtain P(k red balls) = ( )(0.3) k(0.7) 20−k. Problem 4 An urn consists of 30 red balls and 70 green balls. What is the probability of getting exactly k red balls in a sample of size 20 if the sampling is done without replacement (repetition not allowed)? Solution Let A be the event (set) of getting exactly k red balls. T o find P(A) = , we need to find |A| and |S|. First, note that |S| = ( ). Next, to find |A|, we need to find out in how 30 100 20 k |A| |S| 100 20 many ways we can choose k red balls and 20 − k green balls. Using the multiplication principle, we have |A| = ( )( ). Thus, we have P(A) = . Problem 5 Assume that there are k people in a room and we know that: k = 5 with probability ; k = 10 with probability ; k = 15 with probability . a . What is the probability that at least two of them have been born in the same month? Assume that all months are equally likely . b . Given that we already know there are at least two people that celebrate their birthday in the same month, what is the probability that k = 10? Solution a . The first part of the problem is very similar to the birthday problem, one difference here is that here n = 12 instead of 365. Let Ak be the event that at least two people out of k people have birthdays in the same month. W e have P(Ak) = 1 − , for k ∈ {2, 3, 4, . . . , 12} Note that P(Ak) = 1 for k > 12. Let A be the event that at least two people in the room were born in the same month. Using the law of total probability , we have P(A) = P(A5) + P(A10) + P(A15) = (1 − ) + (1 − ) + . 30 k 70 20 − k ( )( ) 30 k 70 20−k ( ) 100 20 1 4 1 4 1 2 P 12 k 12k 1 4 1 4 1 2 1 4 P 12 5 125 1 4 P 12 10 1210 1 2 b . The second part of the problem asks for P(k = 10|A). We can use Bayes' rule to write P(k = 10|A) = = = . Problem 6 How many distinct solutions does the following equation have? x1 + x2 + x3 + x4 = 100,  such that  x1 ∈ {1, 2, 3..}, x2 ∈ {2, 3, 4, . . }, x3, x4 ∈ {0, 1, 2, 3, . . . }. Solution We already know that in general the number of solutions to the equation x1 + x2+. . . +xn = k,  where xi ∈ {0, 1, 2, 3, . . . } is equal to ( ) = ( ). We need to convert the restrictions in this problem to match this general form. We are given that x1 ∈ {1, 2, 3..}, so if we define y1 = x1 − 1, then y1 ∈ {0, 1, 2, 3, . . . }. Similarly define y2 = x2 − 2, so y2 ∈ {0, 1, 2, 3, . . . }. Now the question becomes equivalent to finding the number of solutions to the equation y1 + 1 + y2 + 2 + x3 + x4 = 100,  where y1, y2, x3, x4 ∈ {0, 1, 2, 3, . . . }, or equivalently , the number of solutions to the equation y1 + y2 + x3 + x4 = 97,  where y1, y2, x3, x4 ∈ {0, 1, 2, 3, . . . }. As we know , this is equal to P(A|k=10)P(k=10) P(A) P(A10) 4P(A) 1− P 12 10 1210 (1− )+(1− )+2 P 12 5 125 P 12 10 1210 n + k − 1 k n + k − 1 n − 1 ( ) = ( ). Problem 7 (The matching problem) Here is a famous problem: N guests arrive at a party . Each person is wearing a hat. We collect all hats and then randomly redistribute the hats, giving each person one of the N hats randomly . What is the probability that at least one person receives his/her own hat? Hint: Use the inclusion-exclusion principle. Solution Let Ai be the event that i'th person receives his/her own hat. Then we are interested in finding P(E), where E = A1 ∪ A2 ∪ A3∪. . . ∪AN . To find P(E), we use the inclusion- exclusion principle. W e have P(E) = P(⋃ N i=1 Ai) = ∑N i=1 P(Ai) − ∑i,j : i<j P(Ai ∩ Aj) + ∑i,j,k : i<j<k P(Ai ∩ Aj ∩ Ak) −   ⋯   + (−1) N−1 P(⋂ N i=1 Ai). Note that there is complete symmetry here, that is, we can write P(A1) = P(A2) = P(A3) =. . . = P(AN ); P(A1 ∩ A2) = P(A1 ∩ A3) =. . . = P(A2 ∩ A4) =. . . ; P(A1 ∩ A2 ∩ A3) = P(A1 ∩ A2 ∩ A4) =. . . = P(A2 ∩ A4 ∩ A5) =. . . ; . . . Thus, we have N ∑ i=1 P(Ai) = NP(A1); ∑ i,j : i<j P(Ai ∩ Aj) = ( )P(A1 ∩ A2); ∑ i,j,k : i<j<k P(Ai ∩ Aj ∩ Ak) = ( )P(A1 ∩ A2 ∩ A3); . . . 4 + 97 − 1 3 100 3 N 2 N 3 Therefore, we have P(E) = NP(A1) − ( )P(A1 ∩ A2) +( )P(A1 ∩ A2 ∩ A3)−. . . +(−1) N−1P(A1 ∩ A2 ∩ A3. . . ∩AN ) (2.5) Now , we only need to find P(A1), P(A1 ∩ A2), P(A1 ∩ A2 ∩ A3), etc. to finish solving the problem. T o find P(A1), we have P(A1) = . Here, the sample space S consists of all possible permutations of N objects (hats). Thus, we have |S| = N! On the other hand, A1 consists of all possible permutations of N − 1 objects (because the first object is fixed). Thus |A1| = (N − 1)! Therefore, we have P(A1) = = = Similarly , we have |A1 ∩ A2| = (N − 2)! Thus, P(A1 ∩ A2) = = = . Similarly , P(A1 ∩ A2 ∩ A3) = = = ; P(A1 ∩ A2 ∩ A3 ∩ A4) = = = ; . . . Thus, using Equation 2.5 we have N 2 N 3 |A1| |S| |A1| |S| (N − 1)! N! 1 N |A1 ∩ A2| |S| (N − 2)! N! 1 P N N−2 |A1 ∩ A2 ∩ A3| |S| (N − 3)! N! 1 P N N−3 |A1 ∩ A2 ∩ A3 ∩ A4| |S| (N − 4)! N! 1 P N N−4 P(E) = N. − ( ) ⋅ + ( ) ⋅ −. . . +(−1) N−1 (2.6) By simplifying a little bit, we obtain P(E) = 1 − + −. . . . +(−1) N−1 . We are done. It is interesting to note what happens when N becomes large. T o see that, we should remember the T aylor series expansion of e x. In particular , e x = 1 + + + +. . . Letting x = −1, we have e −1 = 1 − + − +. . . Thus, we conclude that as N becomes large, P(E) approaches 1 − . 1 N N 2 1 P N N−2 N 3 1 P N N−3 1 N! 1 2! 1 3! 1 N! x 1! x 2 2! x 3 3! 1 1! 1 2! 1 3! 1 e 2.2.0 End of Chapter Problems Problem 1 A cof fee shop has 4 different types of cof fee. You can order your cof fee in a small, medium, or large cup. Y ou can also choose whether you want to add cream, sugar , or milk (any combination is possible, for example, you can choose to add all three). In how many ways can you order your cof fee? Problem 2 Eight committee members are meeting in a room that has twelve chairs. In how many ways can they sit in the chairs? Problem 3 There are 20 black cell phones and 30 white cell phones in a store. An employee takes 10 phones at random. Find the probability that a . there will be exactly 4 black cell phones among the chosen phones; b . there will be less than 3 black cell phones among the chosen phones. Problem 4 Five cards are dealt from a shuf fled deck. What is the probability that the dealt hand contains a . exactly one ace; b . at least one ace? Problem 5 Five cards are dealt from a shuf fled deck. What is the probability that the dealt hand contains exactly two aces, given that we know it contains at least one ace? Problem 6 The 52 cards in a shuf fled deck are dealt equally among four players, call them A, B, C, and D. If A and B have exactly 7 spades, what is the probability that C has exactly 4 spades? Problem 7 There are 50 students in a class and the professor chooses 15 students at random. What is the probability that you or your friend Joe are among the chosen students? Problem 8 In how many ways can you arrange the letters in the word \"Massachusetts\"? Problem 9 You have a biased coin for which P(H) = p. You toss the coin 20 times. What is the probability that a . you observe 8 heads and 12 tails; b . you observe more than 8 heads and more than 8 tails? Problem 10 A wireless sensor grid consists of 21 × 11 = 231 sensor nodes that are located at points (i, j) in the plane such that i ∈ {0, 1, ⋯ , 20} and j ∈ {0, 1, 2, ⋯ , 10} as shown in Figure 2.1. The sensor node located at point (0, 0) needs to send a message to a node located at (20, 10). The messages are sent to the destination by going from each sensor to a neighboring sensor located above or to the right. That is, we assume that each node located at point (i, j) will only send messages to the nodes located at (i + 1, j) or (i, j + 1). How many dif ferent paths do exist for sending the message from node (0, 0) to node (20, 10)? Fig.2.1 - Figure for Problem 10. Problem 1 1 In Problem 10, assume that all the appropriate paths are equally likely . What is the probability that the sensor located at point (10, 5) receives the message? That is, what is the probability that a randomly chosen path from (0, 0) to (20, 10) goes through the point (10, 5)? Problem 12 * In Problem 10, assume that if a sensor has a choice, it will send the message to the above sensor with probability pa and will send the message to the sensor to the right with probability pr = 1 − pa. What is the probability that the sensor located at point (10, 5) receives the message? Problem 13 There are two coins in a bag. For Coin 1, P(H) = and for Coin 2, P(H) = . Your friend chooses one of the coins at random and tosses it 5 times. a . What is the probability of observing at least 3 heads? b . * You ask your friend: \"Did you observe at least three heads?\". Y our friend replies, \"Y es.\" What is the probability that Coin 2 had been chosen? Problem 14 1 2 1 3 There are 15 people in a party , including Hannah and Sarah. W e divide the 15 people into 3 groups, where each group has 5 people. What is the probability that Hannah and Sarah are in the same group? Problem 15 You roll a die 5 times. What is the probability that at least one value is observed more than once? Problem 16 I have 10 red and 10 blue cards. I shuf fle the cards and then label the cards based on their orders: I write the number one on the first card, the number two on the second card, and so on. What is the probability that a . All red cards are assigned numbers less than or equal to 15? b . Exactly 8 red cards are assigned numbers less than or equal to 15? Problem 17 I have two bags. Bag 1 contains 10 blue marbles, while Bag 2 contains 15 blue marbles. I pick one of the bags at random, and throw 6 red marbles in it. Then I shake the bag and choose 5 marbles (without replacement) at random from the bag. If there are exactly 2 red marbles among the 5 chosen marbles, what is the probability that I have chosen Bag 1? Problem 18 In a communication system, packets are transmitted from a sender to a receiver . Each packet is received with no error with probability p independently from other packets (with probability 1 − p the packet is lost). The receiver can decode the message as soon as it receives k packets with no error . Find the probability that the sender sends exactly n packets until the receiver can decode the message successfully . Problem 19 How many distinct solutions does the following equation have such that all xi ∈ N? x1 + x2 + x3 + x4 + x5 = 100 Problem 20 How many distinct solutions does the following equation have? x1 + x2 + x3 + x4 = 100,  such that  x1 ∈ {0, 1, 2, ⋯ , 10}, x2, x3, x4 ∈ {0, 1, 2, 3, . . . }. Problem 21 * For this problem suppose that the xi's must be non-negative integers, i.e., xi ∈ {0, 1, 2, ⋯} for i = 1, 2, 3. How many distinct solutions does the following equation have such that at least one of the xi's is larger than 40? x1 + x2 + x3 = 100 3.1.1 Random V ariables In general, to analyze random experiments, we usually focus on some numerical aspects of the experiment. For example, in a soccer game we may be interested in the number of goals, shots, shots on goal, corners kicks, fouls, etc. If we consider an entire soccer match as a random experiment, then each of these numerical results gives some information about the outcome of the random experiment. These are examples of random variables . In a nutshell, a random variable is a real-valued variable whose value is determined by an underlying random experiment. Let's look at an example. Example 3. 1 I toss a coin five times. This is a random experiment and the sample space can be written as S = {TTTTT, TTTTH, . . . , HHHHH}. Note that here the sample space S has 25 = 32 elements. Suppose that in this experiment, we are interested in the number of heads. W e can define a random variable X whose value is the number of observed heads. The value of X will be one of 0, 1, 2, 3, 4 or 5 depending on the outcome of the random experiment. In essence, a random variable is a real-valued function that assigns a numerical value to each possible outcome of the random experiment. For example, the random variable X defined above assigns the value 0 to the outcome TTTTT, the value 2 to the outcome THTHT, and so on. Hence, the random variable X is a function from the sample space S={TTTTT,TTTTH, ⋯ ,HHHHH} to the real numbers (for this particular random variable, the values are always integers between 0 and 5). Random V ariables: A random variable X is a function from the sample space to the real numbers. X : S → R We usually show random variables by capital letters such as X, Y , and Z. Since a random variable is a function, we can talk about its range. The range of a random variable X, shown by Range (X) or RX, is the set of possible values for X. In the above example, Range (X) = RX = {0, 1, 2, 3, 4, 5}. The range of a random variable X, shown by Range (X) or RX, is the set of possible values of X. Example 3. 2 Find the range for each of the following random variables. 1 . I toss a coin 100 times. Let X be the number of heads I observe. 2 . I toss a coin until the first heads appears. Let Y be the total number of coin tosses. 3 . The random variable T is defined as the time (in hours) from now until the next earthquake occurs in a certain city . Solution 1 . The random variable X can take any integer from 0 to 100, so RX = {0, 1, 2, . . . , 100}. 2 . The random variable Y can take any positive integer , so RY = {1, 2, 3, . . . } = N. 3 . The random variable T can in theory get any positive real number , so RT = [0, ∞). 3.1.2 Discrete Random V ariables There are two important classes of random variables that we discuss in this book: discrete random variables and continuous random variables . We will discuss discrete random variables in this chapter and continuous random variables in Chapter 4. There will be a third class of random variables that are called mixed random variables . Mixed random variables, as the name suggests, can be thought of as mixture of discrete and continuous random variables. W e will discuss mixed random variables in Chapter 4 as well. Remember that a set A is countable if either A is a finite set such as {1, 2, 3, 4}, or it can be put in one-to-one correspondence with natural numbers (in this case the set is said to be countably infinite) In particular , as we discussed in Chapter 1, sets such as N, Z, Q and their subsets are countable, while sets such as nonempty intervals [a, b] in R are uncountable. A random variable is discrete if its range is a countable set. In Example 3.2 , the random variables X and Y are discrete, while the random variable T is not discrete. X is a discrete random variable, if its range is countable. 3.1.3 Probability Mass Function (PMF) If X is a discrete random variable then its range RX is a countable set, so, we can list the elements in RX. In other words, we can write RX = {x1, x2, x3, . . . }. Note that here x1, x2, x3, . . . are possible values of the random variable X. While random variables are usually denoted by capital letters, to represent the numbers in the range we usually use lowercase letters such as x, x1, y, z, etc. For a discrete random variable X, we are interested in knowing the probabilities of X = xk. Note that here, the event A = {X = xk} is defined as the set of outcomes s in the sample space S for which the corresponding value of X is equal to xk. In particular , A = {s ∈ S|X(s) = xk}. The probabilities of events {X = xk} are formally shown by the probability mass function (pmf) of X. Definition 3. 1 Let X be a discrete random variable with range RX = {x1, x2, x3, . . . } (finite or countably infinite). The function PX(xk) = P(X = xk),  for k = 1, 2, 3, . . . , is called the probability mass function (PMF) of X. Thus, the PMF is a probability measure that gives us probabilities of the possible values for a random variable. While the above notation is the standard notation for the PMF of X, it might look confusing at first. The subscript X here indicates that this is the PMF of the random variable X. Thus, for example, PX(1) shows the probability that X = 1. To better understand all of the above concepts, let's look at some examples. Example 3. 3 I toss a fair coin twice, and let X be defined as the number of heads I observe. Find the range of X, RX, as well as its probability mass function PX. Solution Here, our sample space is given by S = {HH, HT, TH, TT}. The number of heads will be 0, 1 or 2. Thus RX = {0, 1, 2}. Since this is a finite (and thus a countable) set, the random variable X is a discrete random variable. Next, we need to find PMF of X. The PMF is defined as PX(k) = P(X = k) for k = 0, 1, 2. We have PX(0) = P(X = 0) = P(TT) = , PX(1) = P(X = 1) = P({HT, TH}) = + = , PX(2) = P(X = 2) = P(HH) = . Although the PMF is usually defined for values in the range, it is sometimes convenient to extend the PMF of X to all real numbers. If x ∉ RX, we can simply write PX(x) = P(X = x) = 0. Thus, in general we can write PX(x) = { P(X = x) if x is in RX 0 otherwise To better visualize the PMF , we can plot it. Figure 3.1 shows the PMF of the above random variable X. As we see, the random variable can take three possible values 0, 1 and 2. The figure also clearly indicates that the event X = 1 is twice as likely as the other two possible values. The Figure can be interpreted in the following way: If we repeat the random experiment (tossing a coin twice) a large number of times, then about half of the times we observe X = 1, about a quarter of times we observe X = 0, and about a quarter of times we observe X = 2. 1 4 1 4 1 4 1 2 1 4 Fig.3.1 - PMF for random V ariable X in Example 3.3. For discrete random variables, the PMF is also called the probability distribution . Thus, when asked to find the probability distribution of a discrete random variable X, we can do this by finding its PMF . The phrase distribution function is usually reserved exclusively for the cumulative distribution function CDF (as defined later in the book). The word distribution , on the other hand, in this book is used in a broader sense and could refer to PMF , probability density function (PDF), or CDF . Example 3. 4 I have an unfair coin for which P(H) = p, where 0 < p < 1. I toss the coin repeatedly until I observe a heads for the first time. Let Y be the total number of coin tosses. Find the distribution of Y . Solution First, we note that the random variable Y can potentially take any positive integer , so we have RY = N = {1, 2, 3, . . . }. To find the distribution of Y , we need to find PY (k) = P(Y = k) for k = 1, 2, 3, . . .. We have PY (1) = P(Y = 1) = P(H) = p, PY (2) = P(Y = 2) = P(TH) = (1 − p)p, PY (3) = P(Y = 3) = P(TTH) = (1 − p) 2p, . . . . . . . . . . . . PY (k) = P(Y = k) = P(TT. . . TH) = (1 − p) k−1p. Thus, we can write the PMF of Y in the following way PY (y) = { (1 − p) y−1p for y = 1, 2, 3, . . . 0 otherwise Consider a discrete random variable X with Range (X) = RX. Note that by definition the PMF is a probability measure, so it satisfies all properties of a probability measure. In particular , we have 0 ≤ PX(x) ≤ 1 for all x, and ∑x∈RX PX(x) = 1. Also note that for any set A ⊂ RX, we can find the probability that X ∈ A using the PMF P(X ∈ A) = ∑ x∈A PX(x). Properties of PMF: 0 ≤ PX(x) ≤ 1 for all x; ∑x∈RX PX(x) = 1; for any set A ⊂ RX, P(X ∈ A) = ∑x∈A PX(x). Example 3. 5 For the random variable Y in Example 3.4, 1 . Check that ∑x∈RY PY (y) = 1. 2 . If p = , find P (2 ≤ Y < 5). Solution In Example 3.4, we obtained 1 2 PY (k) = P(Y = k) = (1 − p) k−1p,  for k = 1, 2, 3, . . . Thus, 1 . to check that ∑y∈RY PY (y) = 1, we have ∑y∈RY PY (y) = ∑∞ k=1(1 − p)k−1p = p ∑∞ j=0(1 − p)j = p  Geometric sum = 1; 2 . if p = , to find P (2 ≤ Y < 5), we can write P(2 ≤ Y < 5) = ∑4 k=2 PY (k) = ∑4 k=2(1 − p) k−1p = ( + + ) = . 1 1−(1−p) 1 2 1 2 1 2 1 4 1 8 7 16 3.1.4 Independent Random V ariables In real life, we usually need to deal with more than one random variable. For example, if you study physical characteristics of people in a certain area, you might pick a person at random and then look at his/her weight, height, etc. The weight of the randomly chosen person is one random variable, while his/her height is another one. Not only do we need to study each random variable separately , but also we need to consider if there is dependence (i.e., correlation) between them. Is it true that a taller person is more likely to be heavier or not? The issues of dependence between several random variables will be studied in detail later on, but here we would like to talk about a special scenario where two random variables are independent . The concept of independent random variables is very similar to independent events. Remember , two events A and B are independent if we have P(A, B) = P(A)P(B) (remember comma means and , i.e., P(A, B) = P(A and B) = P(A ∩ B)). Similarly , we have the following definition for independent discrete random variables. Definition 3. 2 Consider two discrete random variables X and Y . W e say that X and Y are independent if P(X = x, Y = y) = P(X = x)P(Y = y),  for all x, y. In general, if two random variables are independent, then you can write P(X ∈ A, Y ∈ B) = P(X ∈ A)P(Y ∈ B),  for all sets A and B. Intuitively , two random variables X and Y are independent if knowing the value of one of them does not change the probabilities for the other one. In other words, if X and Y are independent, we can write P(Y = y|X = x) = P(Y = y),  for all x, y. Similar to independent events, it is sometimes easy to argue that two random variables are independent simply because they do not have any physical interactions with each other . Here is a simple example: I toss a coin 2N times. Let X be the number of heads that I observe in the first N coin tosses and let Y be the number of heads that I observe in the second N coin tosses. Since X and Y are the result of independent coin tosses, the two random variables X and Y are independent. On the other hand, in other scenarios, it might be more complicated to show whether two random variables are independent. Example 3. 6 I toss a coin twice and define X to be the number of heads I observe. Then, I toss the coin two more times and define Y to be the number of heads that I observe this time. Find P((X < 2) and (Y > 1)). Solution Since X and Y are the result of dif ferent independent coin tosses, the two random variables X and Y are independent. Also, note that both random variables have the distribution we found in Example 3.3 . We can write P((X < 2) and (Y > 1)) = (PX(0) + PX(1))PY (2) = ( + ) = . We can extend the definition of independence to n random variables. 1 4 1 2 1 4 3 16 Definition 3. 3 Consider n discrete random variables X1, X2, X3, . . . , Xn. W e say that X1, X2, X3, . . . , Xn are independent if P(X1 = x1, X2 = x2, . . . , Xn = xn) = P(X1 = x1)P(X2 = x2). . . P(Xn = xn),  for all x1, x2, . . . , xn. 3.1.5 Special Distributions As it turns out, there are some specific distributions that are used over and over in practice, thus they have been given special names. There is a random experiment behind each of these distributions. Since these random experiments model a lot of real life phenomenon, these special distributions are used frequently in dif ferent applications. That's why they have been given a name and we devote a section to study them. We will provide PMFs for all of these special random variables, but rather than trying to memorize the PMF , you should understand the random experiment behind each of them. If you understand the random experiments, you can simply derive the PMFs when you need them. Although it might seem that there are a lot of formulas in this section, there are in fact very few new concepts. Do not get intimidated by the large number of formulas, look at each distribution as a practice problem on discrete random variables. Bernoulli Distribution What is the simplest discrete random variable (i.e., simplest PMF) that you can imagine? My answer to this question is a PMF that is nonzero at only one point. For example, if you define PX(x) = { 1 for x = 1 0 otherwise then X is a discrete random variable that can only take one value, i.e., X = 1 with a probability of one. But this is not a very interesting distribution because it is not actually random. Then, you might ask what is the next simplest discrete distribution. And my answer to that is the Bernoulli distribution. A Bernoulli random variable is a random variable that can only take two possible values, usually 0 and 1. This random variable models random experiments that have two possible outcomes, sometimes referred to as \"success\" and \"failure.\" Here are some examples: You take a pass-fail exam. Y ou either pass (resulting in X = 1) or fail (resulting in X = 0). You toss a coin. The outcome is ether heads or tails. A child is born. The gender is either male or female. Formally , the Bernoulli distribution is defined as follows: Definition 3. 4 A random variable X is said to be a Bernoulli random variable with parameter p, shown as X ∼ Bernoulli(p), if its PMF is given by PX(x) = ⎧⎪ ⎨ ⎪⎩ p for x = 1 1 − p  for x = 0 0  otherwise where 0 < p < 1. Figure 3.2 shows the PMF of a Bernoulli(p) random variable. Fig.3.2 - PMF of a Bernoulli(p) random variable. A Bernoulli random variable is associated with a certain event A. If event A occurs (for example, if you pass the test), then X = 1; otherwise X = 0. For this reason the Bernoulli random variable, is also called the indicator random variable. In particular , the indicator random variable IA for an event A is defined by IA = { 1  if the event A occurs 0  otherwise The indicator random variable for an event A has Bernoulli distribution with parameter p = P(A), so we can write IA ∼ Bernoulli(P(A)). Geometric Distribution The random experiment behind the geometric distribution is as follows. Suppose that I have a coin with P(H) = p. I toss the coin until I observe the first heads. W e define X as the total number of coin tosses in this experiment. Then X is said to have geometric distribution with parameter p. In other words, you can think of this experiment as repeating independent Bernoulli trials until observing the first success. This is exactly the same distribution that we saw in Example 3.4 . The range of X here is RX = {1, 2, 3, . . . }. In Example 3.4, we obtained PX(k) = P(X = k) = (1 − p) k−1p,  for k = 1, 2, 3, . . . We usually define q = 1 − p, so we can write PX(k) = pqk−1,  for k = 1, 2, 3, . . .. To say that a random variable has geometric distribution with parameter p, we write X ∼ Geometric(p). More formally , we have the following definition: Definition 3. 5 A random variable X is said to be a geometric random variable with parameter p, shown as X ∼ Geometric(p), if its PMF is given by PX(k) = { p(1 − p) k−1 for k = 1, 2, 3, . . . 0  otherwise where 0 < p < 1. Figure 3.3 shows the PMF of a Geometric(0.3) random variable. Fig.3.3 - PMF of a Geometric(0.3) random variable. We should note that some books define geometric random variables slightly dif ferently . They define the geometric random variable X as the total number of failures before observing the first success. By this definition the range of X is RX = {0, 1, 2, . . . } and the PMF is given by PX(k) = { p(1 − p) k for k = 0, 1, 2, 3, . . . 0 otherwise In this book, whenever we write X ∼ Geometric(p), we always mean X as the total number of trials as defined in Definition 3.5. Note that as long as you are consistent in your analysis, it does not matter which definition you use. That is why we emphasize that you should understand how to derive PMFs for these random variables rather than memorizing them. Binomial Distribution The random experiment behind the binomial distribution is as follows. Suppose that I have a coin with P(H) = p. I toss the coin n times and define X to be the total number of heads that I observe. Then X is binomial with parameter n and p, and we write X ∼ Binomial(n, p). The range of X in this case is RX = {0, 1, 2, . . . , n}. As we have seen in Section 2.1.3, the PMF of X in this case is given by binomial formula PX(k) = ( )p k(1 − p) n−k,  for k = 0, 1, 2, . . . , n. We have the following definition: Definition 3. 6 A random variable X is said to be a binomial random variable with parameters n and p, shown as X ∼ Binomial(n, p), if its PMF is given by PX(k) = { ( )p k(1 − p) n−k for k = 0, 1, 2, ⋯ , n 0 otherwise where 0 < p < 1. Figures 3.4 and 3.5 show the Binomial(n, p) PMF for n = 10, p = 0.3 and n = 20, p = 0.6 respectively . n k n k Fig.3.4 - PMF of a Binomial(10, 0.3) random variable. Fig.3.5 - PMF of a Binomial(20, 0.6) random variable. Binomial random variable as a sum of Bernoulli random variables Here is a useful way of thinking about a binomial random variable. Note that a Binomial(n, p) random variable can be obtained by n independent coin tosses. If we think of each coin toss as a Bernoulli(p) random variable, the Binomial(n, p) random variable is a sum of n independent Bernoulli(p) random variables. This is stated more precisely in the following lemma. Lemma 3. 1 If X1, X2, . . . , Xn are independent Bernoulli(p) random variables, then the random variable X defined by X = X1 + X2+. . . +Xn has a Binomial(n, p) distribution. To generate a random variable X ∼ Binomial(n, p), we can toss a coin n times and count the number of heads. Counting the number of heads is exactly the same as finding X1 + X2+. . . +Xn, where each Xi is equal to one if the corresponding coin toss results in heads and zero otherwise. This interpretation of binomial random variables is sometimes very helpful. Let's look at an example. Example 3. 7 Let X ∼ Binomial(n, p) and Y ∼ Binomial(m, p) be two independent random variables. Define a new random variable as Z = X + Y . Find the PMF of Z. Solution Since X ∼ Binomial(n, p), we can think of X as the number of heads in n independent coin tosses, i.e., we can write X = X1 + X2+. . . +Xn, where the Xi's are independent Bernoulli(p) random variables. Similarly , since Y ∼ Binomial(m, p), we can think of Y as the number of heads in m independent coin tosses, i.e., we can write Y = Y1 + Y2+. . . +Ym, where the Yj's are independent Bernoulli(p) random variables. Thus, the random variable Z = X + Y will be the total number of heads in n + m independent coin tosses: Z = X + Y = X1 + X2+. . . +Xn + Y1 + Y2+. . . +Ym, where the Xi's and Yj's are independent Bernoulli(p) random variables. Thus, by Lemma 3.1, Z is a binomial random variable with parameters m + n and p, i.e., Binomial(m + n, p). Therefore, the PMF of Z is PZ(k) = { ( )p k(1 − p) m+n−k for k = 0, 1, 2, 3, . . . , m + n 0 otherwise The above solution is elegant and simple, but we may also want to directly obtain the PMF of Z using probability rules. Here is another method to solve Example 3.7. First, we note that RZ = {0, 1, 2, . . . , m + n}. For k ∈ RZ, we can write PZ(k) = P(Z = k) = P(X + Y = k). We will find P(X + Y = k) by using conditioning and the law of total probability . In particular , we can write PZ(k) = P(X + Y = k) = ∑n i=0 P(X + Y = k|X = i)P(X = i)  (law of total probability) = ∑n i=0 P(Y = k − i|X = i)P(X = i) = ∑n i=0 P(Y = k − i)P(X = i) m+n k = ∑n i=0 ( )p k−i(1 − p) m−k+i( )p i(1 − p) n−i = ∑n i=0 ( )( )p k(1 − p) m+n−k = p k(1 − p) m+n−k ∑n i=0 ( )( ) = ( )p k(1 − p) m+n−k  (by Example 2.8 (part 3)). Thus, we have proved Z ∼ Binomial(m + n, p) by directly finding the PMF of Z. Negative Binomial (Pascal) Distribution The negative binomial or Pascal distribution is a generalization of the geometric distribution. It relates to the random experiment of repeated independent trials until observing m successes. Again, dif ferent authors define the Pascal distribution slightly differently , and as we mentioned before if you understand one of them you can easily derive the other ones. Here is how we define the Pascal distribution in this book. Suppose that I have a coin with P(H) = p. I toss the coin until I observe m heads, where m ∈ N. We define X as the total number of coin tosses in this experiment. Then X is said to have Pascal distribution with parameter m and p. We write X ∼ Pascal(m, p). Note that Pascal(1, p) = Geometric(p). Note that by our definition the range of X is given by RX={m, m + 1, m + 2, m + 3, ⋯}. Let us derive the PMF of a Pascal(m, p) random variable X. Suppose that I toss the coin until I observe m heads, and X is defined as the total number of coin tosses in this experiment. T o find the probability of the event A = {X = k}, we argue as follows. By definition, event A can be written as A = B ∩ C, where B is the event that we observe m − 1 heads (successes) in the first k − 1 trials, and C is the event that we observe a heads in the kth trial. Note that B and C are independent events because they are related to dif ferent independent trials (coin tosses). Thus we can write P(A) = P(B ∩ C) = P(B)P(C). Now , we have P(C) = p. Note also that P(B) is the probability that I observe observe m − 1 heads in the k − 1 coin tosses. This probability is given by the binomial formula, in particular P(B) = ( )p m−1(1 − p) ((k−1)−(m−1)) = ( )p m−1(1 − p) k−m. m k−i n i m k−i n i m k−i n i m+n k k − 1 m − 1 k − 1 m − 1 Thus, we obtain P(A) = P(B ∩ C) = P(B)P(C) = ( )p m(1 − p) k−m. To summarize, we have the following definition for the Pascal random variable Definition 3. 7 A random variable X is said to be a Pascal random variable with parameters m and p, shown as X ∼ Pascal(m, p), if its PMF is given by PX(k) = { ( )pm(1 − p)k−m for k = m, m + 1, m + 2, m + 3, . . . 0 otherwise where 0 < p < 1. Figure 3.6 shows the PMF of a Pascal(m, p) random variable with m = 3 and p = 0.5. Fig.3.6 - PMF of a Pascal(3, 0.5) (negative binomial) random variable. Hypergeometric Distribution Here is the random experiment behind the hypergeometric distribution. Y ou have a bag that contains b blue marbles and r red marbles. Y ou choose k ≤ b + r marbles at random (without replacement). Let X be the number of blue marbles in your sample. By this definition, we have X ≤ min(k, b). Also, the number of red marbles in your sample must be less than or equal to r, so we conclude X ≥ max(0, k − r). Therefore, k − 1 m − 1 k−1 m−1 the range of X is given by RX = {max(0, k − r), max(0, k − r) + 1, max(0, k − r) + 2, . . . , min(k, b)}. To find PX(x), note that the total number of ways to choose k marbles from b + r marbles is ( ). The total number of ways to choose x blue marbles and k − x red marbles is ( )( ). Thus, we have PX(x) = ,  for x ∈ RX. The following definition summarizes the discussion above. Definition 3. 8 A random variable X is said to be a Hypergeometric random variable with parameters b, r and k, shown as X ∼ Hypergeometric(b, r, k), if its range is RX = {max(0, k − r), max(0, k − r) + 1, max(0, k − r) + 2, . . . , min(k, b)}, and its PMF is given by PX(x) = ⎧⎪ ⎨ ⎪⎩ for x ∈ RX 0 otherwise Again, there is no point to memorizing the PMF . All you need to know is how to solve problems that can be formulated as a hypergeometric random variable. Poisson Distribution The Poisson distribution is one of the most widely used probability distributions. It is usually used in scenarios where we are counting the occurrences of certain events in an interval of time or space. In practice, it is often an approximation of a real-life random variable. Here is an example of a scenario where a Poisson random variable might be used. Suppose that we are counting the number of customers who visit a certain store from 1pm to 2pm. Based on data from previous days, we know that on average λ = 15 customers visit the store. Of course, there will be more customers some days and fewer on others. Here, we may model the random variable X showing the number customers as a Poisson random variable with parameter λ = 15. Let us introduce the Poisson PMF first, and then we will talk about more examples and interpretations of this distribution. b+r k b x r k−x ( )( ) b x r k−x ( ) b+r k ( )( ) b x r k−x ( ) b+r k Definition 3. 9 A random variable X is said to be a Poisson random variable with parameter λ, shown as X ∼ Poisson(λ), if its range is RX = {0, 1, 2, 3, . . . }, and its PMF is given by PX(k) = { for k ∈ RX 0  otherwise Before going any further , let's check that this is a valid PMF . First, we note that PX(k) ≥ 0 for all k. Next, we need to check ∑k∈RX PX(k) = 1. To do that, let us first remember the T aylor series for e x, e x = ∑∞ k=0 . Now we can write ∑k∈RX PX(k) = ∑∞ k=0 = e −λ ∑∞ k=0 = e −λe λ = 1. Figures 3.7, 3.8, and 3.9 show the Poisson(λ) PMF for λ = 1, λ = 5, and λ = 10 respectively . e−λλk k! xk k! e−λλk k! λk k! Fig.3.7 - PMF of a Poisson(1) random variable. Fig.3.8 - PMF of a Poisson(5) random variable. Fig.3.9 - PMF of a Poisson(10) random variable. Now let's look at an example. Example 3. 8 The number of emails that I get in a weekday can be modeled by a Poisson distribution with an average of 0.2 emails per minute. 1 . What is the probability that I get no emails in an interval of length 5 minutes? 2 . What is the probability that I get more than 3 emails in an interval of length 10 minutes? Solution 1 . Let X be the number of emails that I get in the 5-minute interval. Then, by the assumption X is a Poisson random variable with parameter λ = 5(0.2) = 1, P(X = 0) = PX(0) = = = ≈ 0.3679 2 . Let Y be the number of emails that I get in the 10-minute interval. Then by the assumption Y is a Poisson random variable with parameter λ = 10(0.2) = 2, P(Y > 3) = 1 − P(Y ≤ 3) = 1 − (PY (0) + PY (1) + PY (2) + PY (3)) = 1 − e −λ − − − = 1 − e −2 − − − = 1 − e−2 (1 + 2 + 2 + ) = 1 − ≈ 0.1429 Poisson as an approximation for binomial The Poisson distribution can be viewed as the limit of binomial distribution. Suppose X ∼ Binomial(n, p) where n is very large and p is very small. In particular , assume that λ = np is a positive constant. W e show that the PMF of X can be approximated by the PMF of a Poisson(λ) random variable. The importance of this is that Poisson PMF is much easier to compute than the binomial. Let us state this as a theorem. Theorem 3. 1 e −λλ 0 0! e−1 ⋅ 1 1 1 e e−λλ 1! e−λλ2 2! e−λλ3 3! 2e−2 1 4e−2 2 8e−2 6 8 6 19 3e2 Let X ∼ Binomial(n, p = ), where λ > 0 is fixed. Then for any k ∈ {0, 1, 2, . . . }, we have lim n→∞ PX(k) = . Proof We have limn→∞ PX(k) = limn→∞ ( )( )k(1 − )n−k = λ k limn→∞ ( ) (1 − )n−k = . limn→∞ ([ ] [(1 − )n] [(1 − )−k]). Note that for a fixed k, we have limn→∞ = 1 limn→∞ (1 − )−k = 1 limn→∞ (1 − )n = e −λ. Thus, we conclude lim n→∞ PX(k) = . λ n e −λλ k k! n k λ n λ n n! k!(n−k)! 1 nk λ n λk k! n(n−1)(n−2)...(n−k+1) nk λ n λ n n(n−1)(n−2)...(n−k+1) nk λ n λ n e −λλ k k! 3.1.6 Solved Problems: Discrete Random V ariables Problem 1 Let X be a discrete random variable with the following PMF PX(x) = ⎧⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪⎩ 0.1 for x = 0.2 0.2 for x = 0.4 0.2 for x = 0.5 0.3 for x = 0.8 0.2 for x = 1 0 otherwise a . Find RX, the range of the random variable X. b . Find P(X ≤ 0.5). c. Find P(0.25 < X < 0.75). d. Find P(X = 0.2|X < 0.6). Solution a . The range of X can be found from the PMF . The range of X consists of possible values for X. Here we have RX = {0.2, 0.4, 0.5, 0.8, 1}. b . The event X ≤ 0.5 can happen only if X is 0.2, 0.4, or 0.5. Thus, P(X ≤ 0.5) = P(X ∈ {0.2, 0.4, 0.5}) = P(X = 0.2) + P(X = 0.4) + P(X = 0.5) = PX(0.2) + PX(0.4) + PX(0.5) = 0.1 + 0.2 + 0.2 = 0.5 c. Similarly , we have P(0.25 < X < 0.75) = P(X ∈ {0.4, 0.5}) = P(X = 0.4) + P(X = 0.5) = PX(0.4) + PX(0.5) = 0.2 + 0.2 = 0.4 d. This is a conditional probability problem, so we can use our famous formula P(A|B) = . We have P(X = 0.2|X < 0.6) = = = = = 0.2 Problem 2 I roll two dice and observe two numbers X and Y . a . Find RX, RY and the PMFs of X and Y . b . Find P(X = 2, Y = 6). c. Find P(X > 3|Y = 2). d. Let Z = X + Y . Find the range and PMF of Z. e. Find P(X = 4|Z = 8). Solution a . We have RX = RY = {1, 2, 3, 4, 5, 6}. Assuming the dice are fair , all values are equally likely so PX(k) = { for k = 1, 2, 3, 4, 5, 6 0 otherwise Similarly for Y , PY (k) = { for k = 1, 2, 3, 4, 5, 6 0 otherwise b . Since X and Y are independent random variables, we can write P(X = 2, Y = 6) = P(X = 2)P(Y = 6) = ⋅ = . P(A∩B) P(B) P((X=0.2) and (X<0.6)) P(X<0.6) P(X=0.2) P(X<0.6) PX(0.2) PX(0.2)+PX(0.4)+PX(0.5) 0.1 0.1+0.2+0.2 1 6 1 6 1 6 1 6 1 36 c. Since X and Y are independent, knowing the value of X does not impact the probabilities for Y , P(X > 3|Y = 2) = P(X > 3) = PX(4) + PX(5) + PX(6) = + + = . d. First, we have RZ = {2, 3, 4, . . . , 12}. Thus, we need to find PZ(k) for k = 2, 3, . . . , 12. We have PZ(2) = P(Z = 2) = P(X = 1, Y = 1) = ⋅ = ; PZ(3) = P(Z = 3) = P(X = 1, Y = 2) + P(X = 2, Y = 1) = P(X = 1)P(Y = 2) + P(X = 2)P(Y = 1) = ⋅ + ⋅ = ; PZ(4) = P(Z = 4) = P(X = 1, Y = 3) + P(X = 2, Y = 2) + P(X = 3, Y = 1) = 3 ⋅ = . We can continue similarly: PZ(5) = = ; PZ(6) = ; PZ(7) = = ; PZ(8) = ; PZ(9) = = ; PZ(10) = = ; PZ(11) = = ; PZ(12) = . It is always a good idea to check our answers by verifying that ∑z∈RZ PZ(z) = 1. Here, we have ∑z∈RZ PZ(z) = + + + + + + + + + + = 1. e. Note that here we cannot argue that X and Z are independent. Indeed, Z seems to completely depend on X, Z = X + Y . To find the conditional probability P(X = 4|Z = 8), we use the formula for conditional probability 1 6 1 6 1 6 1 2 1 6 1 6 1 36 1 6 1 6 1 6 1 6 1 18 1 36 1 12 4 36 1 9 5 36 6 36 1 6 5 36 4 36 1 9 3 36 1 12 2 36 1 18 1 36 1 36 2 36 3 36 4 36 5 36 6 36 5 36 4 36 3 36 2 36 1 36 P(X = 4|Z = 8) = = = . Problem 3 I roll a fair die repeatedly until a number larger than 4 is observed. If N is the total number of times that I roll the die, find P(N = k), for k = 1, 2, 3, . . .. Solution In each trial, I may observe a number larger than 4 with probability = . Thus, you can think of this experiment as repeating a Bernoulli experiment with success probability p = until you observe the first success. Thus, N is a geometric random variable with parameter p = , N ∼ Geometric( ). Hence, we have PN (k) = { ( ) k−1 for k = 1, 2, 3, . . . 0 otherwise Problem 4 You take an exam that contains 20 multiple-choice questions. Each question has 4 possible options. You know the answer to 10 questions, but you have no idea about the other 10 questions so you choose answers randomly . Your score X on the exam is the total number of correct answers. Find the PMF of X. What is P(X > 15)? Solution Let's define the random variable Y as the number of your correct answers to the 10 questions you answer randomly . Then your total score will be X = Y + 10. First, let's find the PMF of Y . For each question your success probability is . Hence, you P(X=4,Z=8) P(Z=8) P(X=4,Y =4) P(Z=8) ⋅ 1 6 1 6 5 361 5 2 6 1 3 1 3 1 3 1 3 1 3 2 3 1 4 perform 10 independent Bernoulli( ) trials and Y is the number of successes. Thus, we conclude Y ∼ Binomial(10, ), so PY (y) = { ( )( ) y( ) 10−y for y = 0, 1, 2, 3, . . . , 10 0 otherwise Now we need to find the PMF of X = Y + 10. First note that RX = {10, 11, 12, . . . , 20}. We can write PX(10) = P(X = 10) = P(Y + 10 = 10) = P(Y = 0) = ( )( )0( )10−0 = ( )10; PX(11) = P(X = 11) = P(Y + 10 = 11) = P(Y = 1) = ( )( )1( )10−1 = 10 ( )9. So, you get the idea. In general for k ∈ RX = {10, 11, 12, . . . , 20}, PX(k) = P(X = k) = P(Y + 10 = k) = P(Y = k − 10) = ( )( )k−10( )20−k. To summarize, PX(k) = { ( )( )k−10( )20−k for k = 10, 11, 12, . . . , 20 0 otherwise In order to calculate P(X > 15), we know we should consider y = 6, 7, 8, 9, 10 PY (y) = { ( )( ) y( ) 10−y for y = 6, 7, 8, 9, 10 0 otherwise PX(k) = { ( )( ) k−10( ) 20−k for k = 16, 17, . . . , 20 0 otherwise P(X > 15) = PX(16) + PX(17) + PX(18) + PX(19) + PX(20) = ( )( ) 6( ) 4 + ( )( ) 7( ) 3 + ( )( ) 8( ) 2 + ( )( ) 9( ) 1 + ( )( ) 10( ) 0. Problem 5 1 4 1 4 10 y 1 4 3 4 10 0 1 4 3 4 3 4 10 1 1 4 3 4 1 4 3 4 10 k−10 1 4 3 4 10 k−10 1 4 3 4 10 y 1 4 3 4 10 k−10 1 4 3 4 10 6 1 4 3 4 10 7 1 4 3 4 10 8 1 4 3 4 10 9 1 4 3 4 10 10 1 4 3 4 Let X ∼ Pascal(m, p) and Y ∼ Pascal(l, p) be two independent random variables. Define a new random variable as Z = X + Y . Find the PMF of Z. Solution This problem is very similar to Example 3.7 , and we can solve it using the same methods. W e will show that Z ∼ Pascal(m + l, p). To see this, consider a sequence of Hs and Ts that is the result of independent coin tosses with P(H) = p, (Figure 3.2). If we define the random variable X as the number of coin tosses until the mth heads is observed, then X ∼ Pascal(m, p). Now , if we look at the rest of the sequence and count the number of heads until we observe l more heads, then the number of coin tosses in this part of the sequence is Y ∼ Pascal(l, p). Looking from the beginning, we have repeatedly tossed the coin until we have observed m + l heads. Thus, we conclude the random variable Z defined as Z = X + Y has a Pascal(m + l, p) distribution. Fig.3.2 - Sum of two Pascal random variables. In particular , remember that Pascal(1, p) = Geometric(p). Thus, we have shown that if X and Y are two independent Geometric(p) random variables, then X + Y is a Pascal(2, p) random variable. More generally , we can say that if X1, X2, X3, . . . , Xm are m independent Geometric(p) random variables, then the random variable X defined by X = X1 + X2+. . . +Xm has a Pascal(m, p) distribution. Problem 6 The number of customers arriving at a grocery store is a Poisson random variable. On average 10 customers arrive per hour . Let X be the number of customers arriving from 10am to 11 : 30am. What is P(10 < X ≤ 15)? Solution We are looking at an interval of length 1.5 hours, so the number of customers in this interval is X ∼ Poisson(λ = 1.5 × 10 = 15). Thus, P(10 < X ≤ 15) = ∑15 k=11 PX(k) = ∑15 k=11 = e−15 [ + + + + ] = 0.4496 Problem 7 Let X ∼ Poisson(α) and Y ∼ Poisson(β) be two independent random variables. Define a new random variable as Z = X + Y . Find the PMF of Z. Solution First note that since RX = {0, 1, 2, . . } and RY = {0, 1, 2, . . }, we can write RZ = {0, 1, 2, . . }. We have PZ(k) = P(X + Y = k) = ∑k i=0 P(X + Y = k|X = i)P(X = i)  (law of total probability) = ∑k i=0 P(Y = k − i|X = i)P(X = i) = ∑k i=0 P(Y = k − i)P(X = i) = ∑k i=0 = e −(α+β) ∑k i=0 = ∑k i=0 α iβk−i = ∑k i=0 ( )α iβk−i = (α + β) k (by the binomial theorem). Thus, we conclude that Z ∼ Poisson(α + β). e−1515k k! 1511 11! 1512 12! 1513 13! 1514 14! 1515 15! e−ββk−i (k−i)! e−ααi i! αiβk−i (k−i)!i! e−(α+β) k! k! (k−i)!i! e−(α+β) k! k i e−(α+β) k! Problem 8 Let X be a discrete random variable with the following PMF PX(k) = ⎧⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪⎩ for k = −2 for k = −1 for k = 0 for k = 1 for k = 2 0 otherwise I define a new random variable Y as Y = (X + 1) 2. a . Find the range of Y . b . Find the PMF of Y . Solution Here, the random variable Y is a function of the random variable X. This means that we perform the random experiment and obtain X = x, and then the value of Y is determined as Y = (x + 1) 2. Since X is a random variable, Y is also a random variable. a . To find RY , we note that RX = {−2, −1, 0, 1, 2}, and RY = {y = (x + 1) 2|x ∈ RX} = {0, 1, 4, 9}. b . Now that we have found RY = {0, 1, 4, 9}, to find the PMF of Y we need to find PY (0), PY (1), PY (4), and PY (9): PY (0) = P(Y = 0) = P((X + 1) 2 = 0) = P(X = −1) = ; PY (1) = P(Y = 1) = P((X + 1) 2 = 1) = P((X = −2) or (X = 0)); PX(−2) + PX(0) = + = ; PY (4) = P(Y = 4) = P((X + 1) 2 = 4) = P(X = 1) = ; PY (9) = P(Y = 9) = P((X + 1) 2 = 9) = P(X = 2) = . 1 4 1 8 1 8 1 4 1 4 1 8 1 4 1 8 3 8 1 4 1 4 Again, it is always a good idea to check that ∑y∈RY PY (y) = 1. We have ∑ y∈RY PY (y) = + + + = 1. 1 8 3 8 1 4 1 4 3.2.1 Cumulative Distribution Function The PMF is one way to describe the distribution of a discrete random variable. As we will see later on, PMF cannot be defined for continuous random variables. The cumulative distribution function (CDF) of a random variable is another method to describe the distribution of random variables. The advantage of the CDF is that it can be defined for any kind of random variable (discrete, continuous, and mixed). Definition 3. 10 The cumulative distribution function (CDF) of random variable X is defined as FX(x) = P(X ≤ x),  for all x ∈ R. Note that the subscript X indicates that this is the CDF of the random variable X. Also, note that the CDF is defined for all x ∈ R. Let us look at an example. Example 3. 9 I toss a coin twice. Let X be the number of observed heads. Find the CDF of X. Solution Note that here X ∼ Binomial(2, ). The range of X is RX = {0, 1, 2} and its PMF is given by PX(0) = P(X = 0) = , PX(1) = P(X = 1) = , PX(2) = P(X = 2) = . To find the CDF , we argue as follows. First, note that if x < 0, then FX(x) = P(X ≤ x) = 0,  for x < 0. 1 2 1 4 1 2 1 4 Next, if x ≥ 2, FX(x) = P(X ≤ x) = 1,  for x ≥ 2. Next, if 0 ≤ x < 1, FX(x) = P(X ≤ x) = P(X = 0) = ,  for 0 ≤ x < 1. Finally , if 1 ≤ x < 2, FX(x) = P(X ≤ x) = P(X = 0) + P(X = 1) = + = ,  for 1 ≤ x < 2. Thus, to summarize, we have FX(x) = ⎧⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪⎩ 0 for x < 0 for 0 ≤ x < 1 for 1 ≤ x < 2 1 for x ≥ 2 Note that when you are asked to find the CDF of a random variable, you need to find the function for the entire real line. Also, for discrete random variables, we must be careful when to use \" <\" or \" ≤\". Figure 3.3 shows the graph of FX(x). Note that the CDF is flat between the points in RX and jumps at each value in the range. The size of the jump at each point is equal to the probability at that point. For , example, at point x = 1, the CDF jumps from to . The size of the jump here is − = which is equal to PX(1). Also, note that the open and closed circles at point x = 1 indicate that FX(1) = and not . Fig.3.3 - CDF for Example 3.9. 1 4 1 4 1 2 3 4 1 4 3 4 1 4 3 4 3 4 1 4 1 2 3 4 1 4 In general, let X be a discrete random variable with range RX = {x1, x2, x3, . . . }, such that x1 < x2 < x3 <. . . Here, for simplicity , we assume that the range RX is bounded from below , i.e., x1 is the smallest value in RX. If this is not the case then FX(x) approaches zero as x → −∞ rather than hitting zero. Figure 3.4 shows the general form of the CDF , FX(x), for such a random variable. W e see that the CDF is in the form of a staircase. In particular , note that the CDF starts at 0; i.e.,FX(−∞) = 0. Then, it jumps at each point in the range. In particular , the CDF stays flat between xk and xk+1, so we can write FX(x) = FX(xk),  for xk ≤ x < xk+1. The CDF jumps at each xk. In particular , we can write FX(xk) − FX(xk − ϵ) = PX(xk),  For ϵ > 0 small enough. Thus, the CDF is always a non-decreasing function, i.e., if y ≥ x then FX(y) ≥ FX(x). Finally , the CDF approaches 1 as x becomes large. W e can write lim x→∞ FX(x) = 1. Fig.3.4 - CDF of a discrete random variable. Note that the CDF completely describes the distribution of a discrete random variable. In particular , we can find the PMF values by looking at the values of the jumps in the CDF function. Also, if we have the PMF, we can find the CDF from it. In particular , if RX = {x1, x2, x3, . . . }, we can write FX(x) = ∑ xk≤x PX(xk). Now , let us prove a useful formula. For all a ≤ b, we have P(a < X ≤ b) = FX(b) − FX(a) (3.1) To see this, note that for a ≤ b we have P(X ≤ b) = P(x ≤ a) + P(a < X ≤ b). Thus, FX(b) = FX(a) + P(a < X ≤ b). Again, pay attention to the use of \" <\" and \" ≤\" as they could make a dif ference in the case of discrete random variables. We will see later that Equation 3.1 is true for all types of random variables (discrete, continuous, and mixed). Note that the CDF gives us P(X ≤ x). To find P(X < x), for a discrete random variable, we can simply write P(X < x) = P(X ≤ x) − P(X = x) = FX(x) − PX(x). Example 3. 10 Let X be a discrete random variable with range RX = {1, 2, 3, . . . }. Suppose the PMF of X is given by PX(k) =  for k = 1, 2, 3, . . . a . Find and plot the CDF of X, FX(x). b . Find P(2 < X ≤ 5). c. Find P(X > 4). Solution First, note that this is a valid PMF . In particular , ∞ ∑ k=1 PX(k) = ∞ ∑ k=1 = 1 (geometric sum) a . To find the CDF , note that For x < 1, FX(x) = 0. For 1 ≤ x < 2, FX(x) = PX(1) = . 1 2k 1 2k 1 2 For 2 ≤ x < 3, FX(x) = PX(1) + PX(2) = + = . In general we have For 0 < k ≤ x < k + 1, FX(x) = PX(1) + PX(2)+. . . +PX(k) = + +. . . + = . Figure 3.5 shows the CDF of X. Fig.3.5 - CDF of random variable given in Example 3.10. b . To find P(2 < X ≤ 5), we can write P(2 < X ≤ 5) = FX(5) − FX(2) = − = . Or equivalently , we can write P(2 < X ≤ 5) = PX(3) + PX(4) + PX(5) = + + = , which gives the same answer . c. To find P(X > 4), we can write P(X > 4) = 1 − P(X ≤ 4) = 1 − FX(4) = 1 − = . 1 2 1 4 3 4 1 2 1 4 1 2k 2k − 1 2k 31 32 3 4 7 32 1 8 1 16 1 32 7 32 15 16 1 16 3.2.2 Expectation If you have a collection of numbers a1, a2, . . . , aN , their average is a single number that describes the whole collection. Now , consider a random variable X. We would like to define its average, or as it is called in probability , its expected value or mean . The expected value is defined as the weighted average of the values in the range. Expected value (= mean=average): Definition 3. 1 1 Let X be a discrete random variable with range RX = {x1, x2, x3, . . . } (finite or countably infinite). The expected value of X, denoted by EX is defined as EX = ∑ xk∈RX xkP(X = xk) = ∑ xk∈RX xkPX(xk). To understand the concept behind EX, consider a discrete random variable with range RX = {x1, x2, x3, . . . }. This random variable is a result of random experiment. Suppose that we repeat this experiment a very large number of times N, and that the trials are independent. Let N1 be the number of times we observe x1, N2 be the number of times we observe x2, ...., Nk be the number of times we observe xk, and so on. Since P(X = xk) = PX(xk), we expect that PX(x1) ≈ , PX(x2) ≈ , . . . PX(xk) ≈ , . . . In other words, we have Nk ≈ NPX(xk). Now , if we take the average of the observed values of X, we obtain Average  = ≈ N1 N N2 N Nk N N1x1+N2x2+N3x3+... N x1NPX(x1)+x2NPX(x2)+x3NPX(x3)+... N = x1PX(x1) + x2PX(x2) + x3PX(x3)+. . . = EX. Thus, the intuition behind EX is that if you repeat the random experiment independently N times and take the average of the observed data, the average gets closer and closer to EX as N gets larger and larger . We sometimes denote EX by μX. Dif ferent notations for expected value of X: EX = E[X] = E(X) = μX. Let's compute the expected values of some well-known distributions. Example 3. 1 1 Let X ∼ Bernoulli(p). Find EX. Solution For the Bernoulli distribution, the range of X is RX = {0, 1}, and PX(1) = p and PX(0) = 1 − p. Thus, EX = 0 ⋅ PX(0) + 1 ⋅ PX(1) = 0 ⋅ (1 − p) + 1 ⋅ p = p. For a Bernoulli random variable, finding the expectation EX was easy . However , for some random variables, to find the expectation sum, you might need a little algebra. Let's look at another example. Example 3. 12 Let X ∼ Geometric(p). Find EX. Solution For the geometric distribution, the range is RX = {1, 2, 3, . . . } and the PMF is given by PX(k) = qk−1p,  for k = 1, 2, . . . where, 0 < p < 1 and q = p − 1. Thus, we can write EX = ∑xk∈RX xkPX(xk) = ∑∞ k=1 kqk−1p = p ∑∞ k=1 kqk−1. Now , we already know the geometric sum formula ∞ ∑ k=0 x k = ,  for |x| < 1. But we need to find a sum ∑∞ k=1 kqk−1. Luckily , we can convert the geometric sum to the form we want by taking derivative with respect to x, i.e., ∞ ∑ k=0 x k = ,  for |x| < 1. Thus, we have ∞ ∑ k=0 kx k−1 = ,  for |x| < 1. To finish finding the expectation, we can write EX = p ∑∞ k=1 kqk−1 = p = p = . So, for X ∼ Geometric(p), EX = . Note that this makes sense intuitively . The random experiment behind the geometric distribution was that we tossed a coin until we observed the first heads, where P(H) = p. Here, we found out that on average you need to toss the coin times in this experiment. In particular , if p is small (heads are unlikely), then is large, so you need to toss the coin a large number of times before you observe a heads. Conversely , for large p a few coin tosses usually suf fices. Example 3. 13 Let X ∼ Poisson(λ). Find EX. 1 1 − x d dx d dx 1 1 − x 1 (1 − x)2 1 (1−q)2 1 p2 1 p 1 p 1 p 1 p Solution Before doing the math, we suggest that you try to guess what the expected value would be. It might be a good idea to think about the examples where the Poisson distribution is used. For the Poisson distribution, the range is RX = {0, 1, 2, ⋯} and the PMF is given by PX(k) = ,  for k = 0, 1, 2, . . . Thus, we can write EX = ∑xk∈RX xkPX(xk) = ∑∞ k=0 k = e −λ ∑∞ k=1 = e −λ ∑∞ j=0 ( by letting j = k − 1) = λe −λ ∑∞ j=0 = λe −λe λ ( Taylor series for e λ) = λ. So the expected value is λ. Remember , when we first talked about the Poisson distribution, we introduced its parameter λ as the average number of events. So it is not surprising that the expected value is EX = λ. Before looking at more examples, we would like to talk about an important property of expectation, which is linearity . Note that if X is a random variable, any function of X is also a random variable, so we can talk about its expected value. For example, if Y = aX + b, we can talk about EY = E[aX + b]. Or if you define Y = X1 + X2 + ⋯ + Xn, where Xi's are random variables, we can talk about EY = E[X1 + X2 + ⋯ + Xn]. The following theorem states that expectation is linear , which makes it easier to calculate the expected value of linear functions of random variables. e −λλ k k! e−λλk k! λk (k−1)! λ(j+1) j! λj j! Expectation is linear: Theorem 3. 2 We have E[aX + b] = aEX + b, for all a, b ∈ R; E[X1 + X2 + ⋯ + Xn] = EX1 + EX2 + ⋯ + EXn, for any set of random variables X1, X2, ⋯ , Xn. We will prove this theorem later on in Chapter 5, but here we would like to emphasize its importance with an example. Example 3. 14 Let X ∼ Binomial(n, p). Find EX. Solution We provide two ways to solve this problem. One way is as before: we do the math and calculate EX = ∑xk∈RX xkPX(xk) which will be a little tedious. A much faster way would be to use linearity of expectation. In particular , remember that if X1, X2, . . . , Xn are independent Bernoulli(p) random variables, then the random variable X defined by X = X1 + X2+. . . +Xn has a Binomial(n, p) distribution. Thus, we can write EX = E[X1 + X2 + ⋯ + Xn] = EX1 + EX2 + ⋯ + EXn by linearity of expectation = p + p + ⋯ + p = np. We will provide the direct calculation of EX = ∑xk∈RX xkPX(xk) in the Solved Problems section and as you will see it needs a lot more algebra than above. The bottom line is that linearity of expectation can sometimes make our calculations much easier . Let's look at another example. Example 3. 15 Let X ∼ Pascal(m, p). Find EX. (Hint: Try to write X = X1 + X2 + ⋯ + Xm, such that you already know EXi.) Solution We claim that if the Xi's are independent and Xi ∼ Geometric(p), for i = 1, 2, ⋯, m, then the random variable X defined by X = X1 + X2 + ⋯ + Xm has Pascal(m, p). To see this, you can look at Problem 5 in Section 3.1.6 and the discussion there. Now , since we already know EXi = , we conclude EX = E[X1 + X2 + ⋯ + Xm] = EX1 + EX2 + ⋯ + EXm by linearity of expectation = + + ⋯ + = . Again, you can try to find EX directly and as you will see, you need much more algebra compared to using the linearity of expectation. 1 p 1 p 1 p 1 p m p 3.2.3 Functions of Random V ariables If X is a random variable and Y = g(X), then Y itself is a random variable. Thus, we can talk about its PMF , CDF , and expected value. First, note that the range of Y can be written as RY = {g(x)|x ∈ RX}. If we already know the PMF of X, to find the PMF of Y = g(X), we can write PY (y) = P(Y = y) = P(g(X) = y) = ∑ x:g(x)=y PX(x) Let's look at an example. Example 3. 16 Let X be a discrete random variable with PX(k) = for k = −1, 0, 1, 2, 3. Let Y = 2|X|. Find the range and PMF of Y . Solution First, note that the range of Y is RY = {2|x| where x ∈ RX} = {0, 2, 4, 6}. To find PY (y), we need to find P(Y = y) for y = 0, 2, 4, 6. W e have PY (0) = P(Y = 0) = P(2|X| = 0) = P(X = 0) = ; PY (2) = P(Y = 2) = P(2|X| = 2) = P((X = −1) or (X = 1)) = PX(−1) + PX(1) = + = ; 1 5 1 5 1 5 1 5 2 5 PY (4) = P(Y = 4) = P(2|X| = 4) = P(X = 2) + P(X = −2) = ; PY (6) = P(Y = 6) = P(2|X| = 6) = P(X = 3) + P(X = −3) = . So, to summarize, PY (k) = ⎧⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪⎩ for k = 0, 4, 6 for k = 2 0 otherwise Expected V alue of a Function of a Random V ariable (LOTUS) Let X be a discrete random variable with PMF PX(x), and let Y = g(X). Suppose that we are interested in finding EY . One way to find EY is to first find the PMF of Y and then use the expectation formula EY = E[g(X)] = ∑y∈RY yPY (y). But there is another way which is usually easier . It is called the law of the unconscious statistician (LOTUS). Law of the unconscious statistician (LOTUS) for discrete random variables: E[g(X)] = ∑ xk∈RX g(xk)PX(xk) (3.2) You can prove this by writing EY = E[g(X)] = ∑y∈RY yPY (y) in terms of PX(x). In practice it is usually easier to use LOTUS than direct definition when we need E[g(X)]. Example 3. 17 Let X be a discrete random variable with range RX = {0, , , , π}, such that PX(0) = PX( ) = PX( ) = PX( ) = PX(π) = . Find E[sin(X)]. Solution Using LOTUS, we have 1 5 1 5 1 5 2 5 π 4 π 2 3π 4 π 4 π 2 3π 4 1 5 E[g(X)] = ∑xk∈RX g(xk)PX(xk) = sin(0) ⋅ + sin( ) ⋅ + sin( ) ⋅ + sin( ) ⋅ + sin(π) ⋅ = 0 ⋅ + ⋅ + 1 ⋅ + ⋅ + 0 ⋅ = . Example 3. 18 Prove E[aX + b] = aEX + b (linearity of expectation). Solution Here g(X) = aX + b, so using LOTUS we have E[aX + b] = ∑xk∈RX (axk + b)PX(xk) = ∑xk∈RX axkPX(xk) + ∑xk∈RX bPX(xk) = a ∑xk∈RX xkPX(xk) + b ∑xk∈RX PX(xk) = aEX + b. 1 5 π 4 1 5 π 2 1 5 3π 4 1 5 1 5 1 5 √2 2 1 5 1 5 √2 2 1 5 1 5 √2+1 5 3.2.4 V ariance Consider two random variables X and Y with the following PMFs. PX(x) = ⎧⎪ ⎨ ⎪⎩ 0.5 for x = −100 0.5 for x = 100 0 otherwise (3.3) PY (y) = { 1 for y = 0 0 otherwise (3.4) Note that EX = EY = 0. Although both random variables have the same mean value, their distribution is completely dif ferent. Y is always equal to its mean of 0, while X is either 100 or −100, quite far from its mean value. The variance is a measure of how spread out the distribution of a random variable is. Here, the variance of Y is quite small since its distribution is concentrated at a single value, while the variance of X will be larger since its distribution is more spread out. The variance of a random variable X, with mean EX = μX, is defined as Var(X) = E[(X − μX) 2]. By definition, the variance of X is the average value of (X − μX)2. Since (X − μX)2 ≥ 0 , the variance is always larger than or equal to zero. A large value of the variance means that (X − μX)2 is often large, so X often takes values far from its mean. This means that the distribution is very spread out. On the other hand, a low variance means that the distribution is concentrated around its average. Note that if we did not square the dif ference between X and its mean, the result would be 0. That is E[X − μX] = EX − E[μX] = μX − μX = 0. X is sometimes below its average and sometimes above its average. Thus, X − μX is sometimes negative and sometimes positive, but on average it is zero. To compute V ar(X) = E[(X − μX) 2], note that we need to find the expected value of g(X) = (X − μX) 2, so we can use LOTUS. In particular , we can write Var(X) = E[(X − μX) 2] = ∑ xk∈RX (xk − μX) 2PX(xk). For example, for X and Y defined in Equations 3.3 and 3.4, we have Var(X) = (−100 − 0) 2(0.5) + (100 − 0) 2(0.5) = 10, 000 Var(Y ) = (0 − 0) 2(1) = 0. As we expect, X has a very large variance while V ar (Y ) = 0. Note that V ar (X) has a dif ferent unit than X. For example, if X is measured in meters then Var (X) is in meters2. T o solve this issue, we define another measure, called the standard deviation , usually shown as σX, which is simply the square root of variance. The standard deviation of a random variable X is defined as SD(X) = σX = √Var(X). The standard deviation of X has the same unit as X. For X and Y defined in Equations 3.3 and 3.4, we have σX = √10, 000 = 100 σY = √0 = 0. Here is a useful formula for computing the variance. Computational formula for the variance: Var(X) = E[X2] − [EX]2 (3.5) To prove it note that Var(X) = E[(X − μX) 2] = E[X2 − 2μXX + μ2 X] = E[X2] − 2E[μXX] + E[μ2 X]  by linearity of expectation. Note that for a given random variable X, μX is just a constant real number . Thus, E[μXX] = μXE[X] = μ2 X, and E[μ2 X] = μ2 X, so we have Var(X) = E[X2] − 2μ2 X + μ2 X = E[X2] − μ2 X. quation 3.5 is usually easier to work with compared to Var(X) = E[(X − μX) 2]. To use this equation, we can find E[X2] = EX2 using LOTUS EX2 = ∑ xk∈RX x 2 kPX(xk), and then subtract μ2 X to obtain the variance. Example 3. 19 I roll a fair die and let X be the resulting number . Find EX, V ar (X), and σX. Solution We have RX = {1, 2, 3, 4, 5, 6} and PX(k) = for k = 1, 2, . . . , 6. Thus, we have EX = 1 ⋅ + 2 ⋅ + 3 ⋅ + 4 ⋅ + 5 ⋅ + 6 ⋅ = ; EX2 = 1 ⋅ + 4 ⋅ + 9 ⋅ + 16 ⋅ + 25 ⋅ + 36 ⋅ = . Thus Var(X) = E[X2] − (EX) 2 = − ( ) 2 = − ≈ 2.92, σX = √Var(X) ≈ √2.92 ≈ 1.71 1 6 1 6 1 6 1 6 1 6 1 6 1 6 7 2 1 6 1 6 1 6 1 6 1 6 1 6 91 6 91 6 7 2 91 6 49 4 Note that variance is not a linear operator . In particular , we have the following theorem. Theorem 3. 3 For a random variable X and real numbers a and b, Var(aX + b) = a 2Var(X) (3.6) Proof If Y = aX + b, EY = aEX + b. Thus, Var(Y ) = E[(Y − EY ) 2] = E[(aX + b − aEX − b) 2] = E[a 2(X − μX) 2] = a 2E[(X − μX) 2] = a 2Var(X) From Equation 3.6, we conclude that, for standard deviation, SD(aX + b) = |a|SD(X). We mentioned that variance is NOT a linear operation. But there is a very important case, in which variance behaves like a linear operation and that is when we look at sum of independent random variables. Theorem 3. 4 If X1, X2, ⋯ , Xn are independent random variables and X = X1 + X2 + ⋯ + Xn, then Var(X) = Var(X1) + Var(X2) + ⋯ + Var(Xn) (3.7) We will prove this theorem in Chapter 6, but for now we can look at an example to see how we can use it. Example 3. 20 If X ∼ Binomial(n, p) find V ar (X). Solution We know that we can write a Binomial(n, p) random variable as the sum of n independent Bernoulli(p) random variables, i.e., X = X1 + X2 + ⋯ + Xn. Thus, we conclude Var(X) = Var(X1) + Var(X2) + ⋯ + Var(Xn). If Xi ∼ Bernoulli(p), then its variance is Var(Xi) = E[X2 i ] − (EXi) 2 = 12 ⋅ p + 02 ⋅ (1 − p) − p 2 = p(1 − p). Thus, Var(X) = p(1 − p) + p(1 − p) + ⋯ + p(1 − p) = np(1 − p). 3.2.5 Solved Problems: More about Discrete Random V ariables Problem 1 Let X be a discrete random variable with the following PMF PX(x) = ⎧⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪⎩ 0.3 for x = 3 0.2 for x = 5 0.3 for x = 8 0.2 for x = 10 0 otherwise Find and plot the CDF of X. Solution The CDF is defined by FX(x) = P(X ≤ x). W e have FX(x) = ⎧⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪⎩ 0 for x < 3 PX(3) = 0.3 for 3 ≤ x < 5 PX(3) + PX(5) = 0.5 for 5 ≤ x < 8 PX(3) + PX(5) + PX(8) = 0.8 for 8 ≤ x < 10 1 for x ≥ 10 Problem 2 Let X be a discrete random variable with the following PMF PX(k) = ⎧⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪⎩ 0.1 for k = 0 0.4 for k = 1 0.3 for k = 2 0.2 for k = 3 0 otherwise a . Find EX. b . Find V ar (X). c. If Y = (X − 2) 2, find EY . Solution a . EX = ∑xk∈RX xkPX(xk) = 0(0.1) + 1(0.4) + 2(0.3) + 3(0.2) = 1.6 b . We can use V ar (X) = EX2 − (EX) 2 = EX2 − (1.6) 2. Thus we need to find EX2. Using LOTUS, we have EX2 = 02(0.1) + 12(0.4) + 22(0.3) + 32(0.2) = 3.4 Thus, we have Var(X) = (3.4) − (1.6) 2 = 0.84 c. Again, using LOTUS, we have E(X − 2) 2 = (0 − 2) 2(0.1) + (1 − 2) 2(0.4) + (2 − 2) 2(0.3) + (3 − 2) 2(0.2) = 1. Problem 3 Let X be a discrete random variable with PMF PX(k) = ⎧⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪⎩ 0.2 for k = 0 0.2 for k = 1 0.3 for k = 2 0.3 for k = 3 0 otherwise Define Y = X(X − 1)(X − 2). Find the PMF of Y . Solution First, note that RY = {x(x − 1)(x − 2)|x ∈ {0, 1, 2, 3}} = {0, 6}. Thus, PY (0) = P(Y = 0) = P((X = 0) or (X = 1) or (X = 2)) = PX(0) + PX(1) + PX(2) = 0.7; PY (6) = P(X = 3) = 0.3 Thus, PY (k) = ⎧⎪ ⎨ ⎪⎩ 0.7 for k = 0 0.3 for k = 6 0 otherwise Problem 4 Let X ∼ Geometric(p). Find E [ ]. Solution The PMF of X is given by PX(k) = { pqk−1 for k = 1, 2, 3, . . . 0 otherwise where q = 1 − p. Thus, E [ ] = ∑∞ k=1 PX(k) = ∑∞ k=1 qk−1p = ∑∞ k=1 ( )k−1 = = . Problem 5 If X ∼ Hypergeometric(b, r, k), find EX. Solution The PMF of X is given by 1 2X 1 2X 1 2k 1 2k p 2 q 2 p 2 1 1− q 2 p 1+p PX(x) = ⎧⎪ ⎨ ⎪⎩ for x ∈ RX 0 otherwise where RX = {max(0, k − r), max(0, k − r) + 1, max(0, k − r) + 2, . . . , min(k, b)}. Finding EX directly seems to be very complicated. So let's try to see if we can find an easier way to find EX. In particular , a powerful tool that we have is linearity of expectation. Can we write X as the sum of simpler random variables Xi? To do so, let's remember the random experiment behind the hypergeometric distribution. Y ou have a bag that contains b blue marbles and r red marbles. Y ou choose k ≤ b + r marbles at random (without replacement) and let X be the number of blue marbles in your sample. In particular , let's define the indicator random variables Xi as follows: Xi = { 1 if the ith chosen marble is blue 0 otherwise Then, we can write X = X1 + X2 + ⋯ + Xk. Thus, EX = EX1 + EX2 + ⋯ + EXk. To find P(Xi = 1), we note that for any particular Xi all marbles are equally likely to be chosen. This is because of symmetry: no marble is more likely to be chosen than the i th marble as any other marbles. Therefore, P(Xi = 1) =  for all i ∈ {1, 2, ⋯ , k}. We conclude EXi = 0 ⋅ p(Xi = 0) + 1 ⋅ P(Xi = 1) = . Thus, we have EX = . Problem 6 In Example 3.14 we showed that if X ∼ Binomial(n, p), then EX = np. W e found this by writing X as the sum of n Bernoulli(p) random variables. Now , find EX directly ( )( ) b x r k−x ( ) b+r k b b + r b b+r kb b + r using EX = ∑xk∈RX xkPX(xk). Hint: Use k( ) = n( ). Solution First note that we can prove k( ) = n( ) by the following combinatorial interpretation: Suppose that from a group of n students we would like to choose a committee of k students, one of whom is chosen to be the committee chair . W e can do this 1 . by choosing k people first (in ( ) ways), and then choosing one of them to be the chair ( k ways), or 2 . by choosing the chair first ( n possibilities and then choosing k − 1 students from the remaining n − 1 students (in ( ) ways)). Thus, we conclude k( ) = n( ). Now , let's find EX for X ∼ Binomial(n, p). EX = ∑n k=0 k( )p kqn−k = ∑n k=1 k( )p kqn−k = ∑n k=1 n( )p kqn−k = np ∑n k=1 ( )p k−1qn−k = np ∑n−1 l=0 ( )p lq(n−1)−l = np. Note that the last line is true because the ∑n−1 l=0 ( )p lq(n−1)−l is equal to ∑n−1 l=0 PY (l) for a random variable Y that has Binomial(n − 1, p) distribution, hence it is equal to 1. Problem 7 Let X be a discrete random variable with RX ⊂ {0, 1, 2, . . . }. Prove EX = ∞ ∑ k=0 P(X > k). Solution Note that P(X > 0) = PX(1) + PX(2) + PX(3) + PX(4) + ⋯, n k n−1 k−1 n k n−1 k−1 n k n−1 k−1 n k n − 1 k − 1 n k n k n−1 k−1 n−1 k−1 n−1 l n−1 l P(X > 1) = PX(2) + PX(3) + PX(4) + ⋯, P(X > 2) = PX(3) + PX(4) + PX(5) + ⋯. Thus ∑∞ k=0 P(X > k) = P(X > 0) + P(X > 1) + P(X > 2)+. . . = PX(1) + 2PX(2) + 3PX(3) + 4PX(4)+. . . = EX. Problem 8 If X ∼ Poisson(λ), find V ar (X). Solution We already know EX = λ, thus V ar (X) = EX2 − λ2. Y ou can find EX2 directly using LOTUS; however , it is a little easier to find E[X(X − 1)] first. In particular , using LOTUS we have E[X(X − 1)] = ∑∞ k=0 k(k − 1)PX(k) = ∑∞ k=0 k(k − 1)e −λ = e −λ ∑∞ k=2 = e −λλ 2 ∑∞ k=2 = e −λλ 2e λ = λ2. So, we have λ2 = E[X(X − 1)] = EX2 − EX = EX2 − λ. Thus, EX2 = λ2 + λ and we conclude Var(X) = EX2 − (EX)2 = λ2 + λ − λ2 = λ. Problem 9 λk k! λk (k−2)! λk−2 (k−2)! Let X and Y be two independent random variables. Suppose that we know V ar (2X − Y ) = 6 and V ar (X + 2Y ) = 9. Find V ar (X) and V ar (Y ). Solution Let's first make sure we understand what V ar (2X − Y ) and V ar (X + 2Y ) mean. They are V ar (Z) and V ar (W), where the random variables Z and W are defined as Z = 2X − Y and W = X + 2Y . Since X and Y are independent random variables, then 2X and −Y are independent random variables. Also, X and 2Y are independent random variables. Thus, by using Equation 3.7 , we can write Var(2X − Y ) = Var(2X) + Var(-Y) = 4Var(X) + Var(Y) = 6, Var(X + 2Y ) = Var(X) + Var(2Y) = Var(X) + 4Var(Y) = 9. By solving for Var (X) and V ar (Y ), we obtain Var (X) = 1 and V ar (Y ) = 2. 3.3 End of Chapter Problems Problem 1 Let X be a discrete random variable with the following PMF: PX(x) = ⎧⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪⎩ for x = 0 for x = 1 for x = 2 0 otherwise a . Find RX, the range of the random variable X. b . Find P(X ≥ 1.5). c. Find P(0 < X < 2). d. Find P(X = 0|X < 2) Problem 2 Let X be the number of the cars being repaired at a repair shop. W e have the following information: At any time, there are at most 3 cars being repaired. The probability of having 2 cars at the shop is the same as the probability of having one car . The probability of having no car at the shop is the same as the probability of having 3 cars. The probability of having 1 or 2 cars is half of the probability of having 0 or 3 cars. Find the PMF of X. Problem 3 I roll two dice and observe two numbers X and Y . If Z = X − Y , find the range and PMF of Z. Problem 4 1 2 1 3 1 6 Let X and Y be two independent discrete random variables with the following PMFs: PX(k) = ⎧⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪⎩ for k = 1 for k = 2 for k = 3 for k = 4 0 otherwise and PY (k) = ⎧⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪⎩ for k = 1 for k = 2 for k = 3 for k = 4 0 otherwise a . Find P(X ≤ 2 and Y ≤ 2). b . Find P(X > 2 or Y > 2). c. Find P(X > 2|Y > 2). d. Find P(X < Y ). Problem 5 50 students live in a dormitory . The parking lot has the capacity for 30 cars. If each student has a car with probability (independently from other students), what is the probability that there won't be enough parking spaces for all the cars? Problem 6 (The Matching Problem) N guests arrive at a party . Each person is wearing a hat. W e collect all the hats and then randomly redistribute the hats, giving each person one of the N hats randomly . Let XN be the number of people who receive their own hats. Find the PMF of XN . Hint: We previously found that ( Problem 7 in Section 2.1.5) P(XN = 0) = − + − ⋯ (−1) N , for N = 1, 2, ⋯. Using this, find P(XN = k) for all k ∈ {0, 1, ⋯ N}. 1 4 1 8 1 8 1 2 1 6 1 6 1 3 1 3 1 2 1 2! 1 3! 1 4! 1 N! Problem 7 For each of the following random variables, find P(X > 5), P(2 < X ≤ 6) and P(X > 5|X < 8). a . X ∼ Geometric( ) b . X ∼ Binomial(10, ) c. X ∼ Pascal(3, ) d. X ∼ Hypergeometric(10, 10, 12) e. X ∼ Poisson(5) Problem 8 Suppose you take a pass-fail test repeatedly . Let Sk be the event that you are successful in your k th try , and Fk be the event that you fail the test in your k th try . On your first try , you have a 50 percent chance of passing the test. P(S1) = 1 − P(F1) = . Assume that as you take the test more often, your chance of failing the test goes down. In particular , P(Fk) = ⋅ P(Fk−1),  for k = 2, 3, 4, ⋯ However , the result of dif ferent exams are independent. Suppose you take the test repeatedly until you pass the test for the first time. Let X be the total number of tests you take, so Range(X) = {1, 2, 3, ⋯}. a . Find P(X = 1), P(X = 2), P(X = 3). b . Find a general formula for P(X = k) for k = 1, 2, ⋯. c. Find the probability that you take the test more than 2 times. d. Given that you take the test more than once, find the probability that you take the test exactly twice. Problem 9 In this problem, we would like to show that the geometric random variable is memoryless . Let X ∼ Geometric(p). Show that P(X > m + l|X > m) = P(X > l),  for m, l ∈ {1, 2, 3, ⋯}. 1 5 1 3 1 2 1 2 1 2 We can interpret this in the following way: Remember that a geometric random variable can be obtained by tossing a coin repeatedly until observing the first heads. If we toss the coin several times, and do not observe a heads, from now on it is like we start all over again. In other words, the failed coin tosses do not impact the distribution of waiting time from this point forward. The reason for this is that the coin tosses are independent. Problem 10 An urn consists of 20 red balls and 30 green balls. W e choose 10 balls at random from the urn. The sampling is done without replacement (repetition not allowed). a . What is the probability that there will be exactly 4 red balls among the chosen balls? b . Given that there are at least 3 red balls among the chosen balls, what is the probability that there are exactly 4 red balls? Problem 1 1 The number of emails that I get in a weekday (Monday through Friday) can be modeled by a Poisson distribution with an average of emails per minute. The number of emails that I receive on weekends (Saturday and Sunday) can be modeled by a Poisson distribution with an average of emails per minute. a . What is the probability that I get no emails in an interval of length 4 hours on a Sunday? b . A random day is chosen (all days of the week are equally likely to be selected), and a random interval of length one hour is selected on the chosen day . It is observed that I did not receive any emails in that interval. What is the probability that the chosen day is a weekday? Problem 12 Let X be a discrete random variable with the following PMF: 1 6 1 30 PX(x) = ⎧⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪⎩ 0.2 for x = −2 0.3 for x = −1 0.2 for x = 0 0.2 for x = 1 0.1 for x = 2 0 otherwise Find and plot the CDF of X. Problem 13 Let X be a discrete random variable with the following CDF: FX(x) = ⎧⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪⎩ 0 for x < 0 for 0 ≤ x < 1 for 1 ≤ x < 2 for 2 ≤ x < 3 1 for x ≥ 3 Find the range and PMF of X. Problem 14 Let X be a discrete random variable with the following PMF PX(k) = ⎧⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪⎩ 0.5 for k = 1 0.3 for k = 2 0.2 for k = 3 0 otherwise a . Find EX. b . Find V ar (X) and SD(X). c. If Y = , find EY . Problem 15 Let X ∼ Geometric( ), and let Y = |X − 5|. Find the range and PMF of Y . Problem 16 Let X be a discrete random variable with the following PMF 1 6 1 2 3 4 2 X 1 3 PX(k) = ⎧⎪ ⎪ ⎨ ⎪ ⎪⎩ for k ∈ {−10, −9, ⋯ , −1, 0, 1, ⋯ , 9, 10} 0 otherwise The random variable Y = g(X) is defined as Y = g(X) = ⎧⎪ ⎨ ⎪⎩ 0 if X ≤ 0 X if 0 < X ≤ 5 5 otherwise Find the PMF of Y . Problem 17 Let X ∼ Geometric(p). Find V ar(X). Problem 18 Let X ∼ Pascal(m, p). Find V ar(X). Problem 19 Suppose that Y = −2X + 3. If we know EY = 1 and EY 2 = 9, find EX and V ar(X). Problem 20 There are 1000 households in a town. Specifically , there are 100 households with one member , 200 households with 2 members, 300 households with 3 members, 200 households with 4 members, 100 households with 5 members, and 100 households with 6 members. Thus, the total number of people living in the town is N = 100 ⋅ 1 + 200 ⋅ 2 + 300 ⋅ 3 + 200 ⋅ 4 + 100 ⋅ 5 + 100 ⋅ 6 = 3300. a . We pick a household at random, and define the random variable X as the number of people in the chosen household. Find the PMF and the expected value of X. b . We pick a person in the town at random, and define the random variable Y as the number of people in the household where the chosen person lives. Find the PMF and the expected value of Y . 1 21 Problem 21 (Coupon collector's problem [8] ) Suppose that there are N dif ferent types of coupons. Each time you get a coupon, it is equally likely to be any of the N possible types. Let X be the number of coupons you will need to get before having observed each coupon at least once. a . Show that you can write X = X0 + X1 + ⋯ + XN−1 , where Xi ∼ Geometric( ). b . Find EX. Problem 22 (St. Petersburg Paradox [9]) Here is a famous problem called the St. Petersburg Paradox. Wikipedia states the problem as follows: \"A casino of fers a game of chance for a single player in which a fair coin is tossed at each stage. The pot starts at 1 dollar and is doubled every time a head appears. The first time a tail appears, the game ends and the player wins whatever is in the pot. Thus the player wins 1 dollar if a tail appears on the first toss, 2 dollars if a head appears on the first toss and a tail on the second, 4 dollars if a head appears on the first two tosses and a tail on the third, 8 dollars if a head appears on the first three tosses and a tail on the fourth, and so on. In short, the player wins 2k−1 dollars if the coin is tossed k times until the first tail appears. What would be a fair price to pay the casino for entering the game?\" a . Let X be the amount of money (in dollars) that the player wins. Find EX. b . What is the probability that the player wins more than 65 dollars? c. Now suppose that the casino only has a finite amount of money . Specifically , suppose that the maximum amount of the money that the casion will pay you is 230 dollars (around 1.07 billion dollars). That is, if you win more than 230 dollars, the casino is going to pay you only 230 dollars. Let Y be the money that the player wins in this case. Find EY . Problem 23 Let X be a random variable with mean EX = μ. Define the function f(α) as f(α) = E[(X − α) 2]. Find the value of α that minimizes f. N−i N Problem 24 You are of fered to play the following game. Y ou roll a fair die once and observe the result which is shown by the random variable X. At this point, you can stop the game and win X dollars. You can also choose to roll the die for the second time to observe the value Y . In this case, you will win Y dollars. Let W be the value that you win in this game. What strategy do you use to maximize EW? What is the maximum EW you can achieve using your strategy? Problem 25 The median of a random variable X is defined as any number m that satisfies both of the following conditions: P(X ≥ m) ≥  and  P(X ≤ m) ≥ Note that the median of X is not necessarily unique. Find the median of X if a . The PMF of X is given by PX(k) = ⎧⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪⎩ 0.4 for k = 1 0.3 for k = 2 0.3 for k = 3 0 otherwise b . X is the result of a rolling of a fair die. c. X ∼ Geometric(p), where 0 < p < 1. 1 2 1 2 4.0.0 Introduction Remember that discrete random variables can take only a countable number of possible values. On the other hand, a continuous random variable X has a range in the form of an interval or a union of non-overlapping intervals on the real line (possibly the whole real line). Also, for any x ∈ R, P(X = x) = 0. Thus, we need to develop new tools to deal with continuous random variables. The good news is that the theory of continuous random variables is completely analogous to the theory of discrete random variables. Indeed, if we want to oversimplify things, we might say the following: take any formula about discrete random variables, and then replace sums with integrals , and replace PMFs with probability density functions ( PDFs), and you will get the corresponding formula for continuous random variables. Of course, there is a little bit more to the story and that's why we need a chapter to discuss it. In this chapter , we will also introduce mixed random variables that are mixtures of discrete and continuous random variables. 4.1.0 Continuous Random V ariables and their Distributions We have in fact already seen examples of continuous random variables before, e.g., Example 1.14 . Let us look at the same example with just a little bit dif ferent wording. Example 4. 1 I choose a real number uniformly at random in the interval [a, b], and call it X. By uniformly at random, we mean all intervals in [a, b] that have the same length must have the same probability . Find the CDF of X. Solution As we mentioned, this is almost exactly the same problem as Example 1.14, with the difference being, in that problem, we considered the interval from 1 to 2. In that example, we saw that all individual points have probability 0, i.e., P(X = x) = 0 for all x . Also, the uniformity implies that the probability of an interval of length l in [a, b] must be proportional to its length: P(X ∈ [x1, x2]) ∝ (x2 − x1), where a ≤ x1 ≤ x2 ≤ b. Since P(X ∈ [a, b]) = 1, we conclude P(X ∈ [x1, x2]) = , where a ≤ x1 ≤ x2 ≤ b. Now , let us find the CDF . By definition FX(x) = P(X ≤ x), thus we immediately have FX(x) = 0, for x < a, FX(x) = 1, for x ≥ b. For a ≤ x ≤ b, we have FX(x) = P(X ≤ x) = P(X ∈ [a, x]) = . x2 − x1 b − a x−a b−a Thus, to summarize FX(x) = ⎧⎪ ⎪ ⎨ ⎪ ⎪⎩ 0 for x < a for a ≤ x ≤ b 1 for x > b (4.1) Note that here it does not matter if we use \" <\" or \" ≤\", as each individual point has probability zero, so for example P(X < 2) = P(X ≤ 2). Figure 4.1 shows the CDF of X . As we expect the CDF starts at zero and ends at 1. Fig.4.1 - CDF for a continuous random variable uniformly distributed over [a, b]. One big dif ference that we notice here as opposed to discrete random variables is that the CDF is a continuous function, i.e., it does not have any jumps. Remember that jumps in the CDF correspond to points x for which P(X = x) > 0. Thus, the fact that the CDF does not have jumps is consistent with the fact that P(X = x) = 0 for all x. Indeed, we have the following definition for continuous random variables. x−a b−a Definition 4. 1 A random variable X with CDF FX(x) is said to be continuous if FX(x) is a continuous function for all x ∈ R. We will also assume that the CDF of a continuous random variable is dif ferentiable almost everywhere in R. 4.1.1 Probability Density Function (PDF) To determine the distribution of a discrete random variable we can either provide its PMF or CDF . For continuous random variables, the CDF is well-defined so we can provide the CDF . However , the PMF does not work for continuous random variables, because for a continuous random variable P(X = x) = 0 for all x ∈ R. Instead, we can usually define the probability density function (PDF) . The PDF is the density of probability rather than the probability mass. The concept is very similar to mass density in physics: its unit is probability per unit length. T o get a feeling for PDF , consider a continuous random variable X and define the function fX(x) as follows (wherever the limit exists): fX(x) = lim Δ→0+ . The function fX(x) gives us the probability density at point x. It is the limit of the probability of the interval (x, x + Δ] divided by the length of the interval as the length of the interval goes to 0. Remember that P(x < X ≤ x + Δ) = FX(x + Δ) − FX(x). So, we conclude that fX(x) = lim Δ→0 = = F ′ X(x), if FX(x) is differentiable at x. Thus, we have the following definition for the PDF of continuous random variables: P(x < X ≤ x + Δ) Δ FX(x + Δ) − FX(x) Δ dFX(x) dx Definition 4. 2 Consider a continuous random variable X with an absolutely continuous CDF FX(x). The function fX(x) defined by fX(x) = = F ′ X(x), if FX(x) is differentiable at x is called the probability density function (PDF) of X. Let us find the PDF of the uniform random variable X discussed in Example 4.1 . This random variable is said to have Uniform(a, b) distribution. The CDF of X is given in Equation 4.1. By taking the derivative, we obtain fX(x) = { a < x < b 0 x < a or x > b Note that the CDF is not dif ferentiable at points a and b. Nevertheless, as we will discuss later on, this is not important. Figure 4.2 shows the PDF of X. As we see, the value of the PDF is constant in the interval from a to b. That is why we say X is uniformly distributed over [a, b]. Fig.4.2 - PDF for a continuous random variable uniformly distributed over [a, b]. dFX(x) dx 1 b−a The uniform distribution is the simplest continuous random variable you can imagine. For other types of continuous random variables the PDF is non-uniform. Note that for small values of δ we can write P(x < X ≤ x + δ) ≈ fX(x)δ. Thus, if fX(x1) > fX(x2), we can say P(x1 < X ≤ x1 + δ) > P(x2 < X ≤ x2 + δ), i.e., the value of X is more likely to be around x1 than x2. This is another way of interpreting the PDF . Since the PDF is the derivative of the CDF , the CDF can be obtained from PDF by integration (assuming absolute continuity): FX(x) = ∫ x −∞ fX(u)du. Also, we have P(a < X ≤ b) = FX(b) − FX(a) = ∫ b a fX(u)du. In particular , if we integrate over the entire real line, we must get 1, i.e., ∫ ∞ −∞ fX(u)du = 1. That is, the area under the PDF curve must be equal to one. W e can see that this holds for the uniform distribution since the area under the curve in Figure 4.2 is one. Note that fX(x) is density of probability , so it must be larger than or equal to zero, but it can be larger than 1. Let us summarize the properties of the PDF . Consider a continuous random variable X with PDF fX(x). We have 1 . fX(x) ≥ 0 for all x ∈ R. 2 . ∫ ∞ −∞ fX(u)du = 1. 3 . P(a < X ≤ b) = FX(b) − FX(a) = ∫ b a fX(u)du. 4 . More generally , for a set A, P(X ∈ A) = ∫A fX(u)du. In the last item above, the set A must satisfy some mild conditions which are almost always satisfied in practice. An example of set A could be a union of some disjoint intervals. For example, if you want to find P(X ∈ [0, 1] ∪ [3, 4]), you can write P(X ∈ [0, 1] ∪ [3, 4]) = ∫ 1 0 fX(u)du + ∫ 4 3 fX(u)du. Let us look at an example to practice the above concepts. Example 4. 2 Let X be a continuous random variable with the following PDF fX(x) = { ce−x x ≥ 0 0 otherwise where c is a positive constant. a . Find c. b . Find the CDF of X, FX(x). c. Find P(1 < X < 3). Solution a . To find c, we can use Property 2 above, in particular 1 = ∫ ∞ −∞ fX(u)du = ∫ ∞ 0 ce −udu = c[ − e −x] ∞ 0 = c. Thus, we must have c = 1. b . To find the CDF of X, we use FX(x) = ∫ x −∞ fX(u)du, so for x < 0, we obtain FX(x) = 0. For x ≥ 0, we have FX(x) = ∫ x 0 e −udu = 1 − e −x. Thus, FX(x) = { 1 − e −x x ≥ 0 0 otherwise c. We can find P(1 < X < 3) using either the CDF or the PDF . If we use the CDF , we have P(1 < X < 3) = FX(3) − FX(1) = [1 − e −3] − [1 − e −1] = e −1 − e −3. Equivalently , we can use the PDF . We have P(1 < X < 3) = ∫ 3 1 fX(t)dt = ∫ 3 1 e−tdt = e −1 − e −3. Range The range of a random variable X is the set of possible values of the random variable. If X is a continuous random variable, we can define the range of X as the set of real numbers x for which the PDF is larger than zero, i.e, RX = {x|fX(x) > 0}. The set RX defined here might not exactly show all possible values of X, but the difference is practically unimportant. 4.1.2 Expected V alue and V ariance As we mentioned earlier , the theory of continuous random variables is very similar to the theory of discrete random variables. In particular , usually summations are replaced by integrals and PMFs are replaced by PDFs. The proofs and ideas are very analogous to the discrete case, so sometimes we state the results without mathematical derivations for the purpose of brevity . Remember that the expected value of a discrete random variable can be obtained as EX = ∑ xk∈RX xkPX(xk). Now , by replacing the sum by an integral and PMF by PDF , we can write the definition of expected value of a continuous random variable as EX = ∫ ∞ −∞ xfX(x)dx Example 4. 3 Let X ∼ Uniform(a, b). Find EX. Solution As we saw , the PDF of X is given by fX(x) = { a < x < b 0 x < a or x > b so to find its expected value, we can write EX = ∫ ∞ −∞ xfX(x)dx = ∫ b a x( )dx 1 b−a 1 b−a = [ x2] b adx = . This result is intuitively reasonable: since X is uniformly distributed over the interval [a, b], we expect its mean to be the middle point, i.e., EX = . Example 4. 4 Let X be a continuous random variable with PDF fX(x) = { 2x 0 ≤ x ≤ 1 0 otherwise Find the expected value of X. Solution We have EX = ∫ ∞ −∞ xfX(x)dx = ∫ 1 0 x(2x)dx = ∫ 1 0 2x 2dx = . Expected V alue of a Function of a Continuous Random V ariable Remember the law of the unconscious statistician (LOTUS) for discrete random variables: E[g(X)] = ∑ xk∈RX g(xk)PX(xk) (4.2) Now , by changing the sum to integral and changing the PMF to PDF we will obtain the similar formula for continuous random variables. 1 b−a 1 2 a+b 2 a+b 2 2 3 Law of the unconscious statistician (LOTUS) for continuous random variables: E[g(X)] = ∫ ∞ −∞ g(x)fX(x)dx (4.3) As we have seen before, expectation is a linear operation, thus we always have E[aX + b] = aEX + b, for all a, b ∈ R, and E[X1 + X2+. . . +Xn] = EX1 + EX2+. . . +EXn, for any set of random variables X1, X2, . . . , Xn. Example 4. 5 Let X be a continuous random variable with PDF fX(x) = { x + 0 ≤ x ≤ 1 0 otherwise Find E(Xn), where n ∈ N. Solution Using LOTUS we have E[Xn] = ∫ ∞ −∞ x nfX(x)dx = ∫ 1 0 x n(x + )dx = [ x n+2 + x n+1] 1 0 = . V ariance Remember that the variance of any random variable is defined as Var(X) = E[(X − μX) 2] = EX2 − (EX) 2. So for a continuous random variable, we can write 1 2 1 2 1 n+2 1 2(n+1) 3n+4 2(n+1)(n+2) Var(X) = E[(X − μX) 2] = ∫ ∞ −∞(x − μX) 2fX(x)dx = EX2 − (EX) 2 = ∫ ∞ −∞ x 2fX(x)dx − μ2 X Also remember that for a, b ∈ R, we always have Var(aX + b) = a 2Var(X). (4.4) Example 4. 6 Let X be a continuous random variable with PDF fX(x) = { x ≥ 1 0 otherwise Find the mean and variance of X. Solution E[X] = ∫ ∞ −∞ xfX(x)dx = ∫ ∞ 1 dx = [ − x −2] ∞ 1 = . Next, we find EX2 using LOTUS, E[X2] = ∫ ∞ −∞ x 2fX(x)dx = ∫ ∞ 1 dx = [ − 3x −1] ∞ 1 = 3. Thus, we have 3 x4 3 x3 3 2 3 2 3 x2 Var(X) = EX2 − (EX) 2 = 3 − = . 9 4 3 4 4.1.3 Functions of Continuous Random V ariables If X is a continuous random variable and Y = g(X) is a function of X, then Y itself is a random variable. Thus, we should be able to find the CDF and PDF of Y . It is usually more straightforward to start from the CDF and then to find the PDF by taking the derivative of the CDF . Note that before differentiating the CDF , we should check that the CDF is continuous. As we will see later , the function of a continuous random variable might be a non-continuous random variable. Let's look at an example. Example 4. 7 Let X be a Uniform(0, 1) random variable, and let Y = e X. a . Find the CDF of Y . b . Find the PDF of Y . c. Find EY . Solution First, note that we already know the CDF and PDF of X. In particular , FX(x) = ⎧⎪ ⎨ ⎪⎩ 0 for x < 0 x for 0 ≤ x ≤ 1 1 for x > 1 It is a good idea to think about the range of Y before finding the distribution. Since e x is an increasing function of x and RX = [0, 1], we conclude that RY = [1, e]. So we immediately know that FY (y) = P(Y ≤ y) = 0, for y < 1, FY (y) = P(Y ≤ y) = 1, for y ≥ e. a . To find FY (y) for y ∈ [1, e], we can write FY (y) = P(Y ≤ y) = P(e X ≤ y) = P(X ≤ ln y) = FX(ln y) = ln y To summarize FY (y) = ⎧⎪ ⎨ ⎪⎩ 0 for y < 1 ln y for 1 ≤ y < e 1 for y ≥ e b . The above CDF is a continuous function, so we can obtain the PDF of Y by taking its derivative. W e have fY (y) = F ′ Y (y) = { for 1 ≤ y ≤ e 0 otherwise Note that the CDF is not technically dif ferentiable at points 1 and e, but as we mentioned earlier we do not worry about this since this is a continuous random variable and changing the PDF at a finite number of points does not change probabilities. c. To find the EY , we can directly apply LOTUS, E[Y ] = E[e X] = ∫ ∞ −∞ e xfX(x)dx = ∫ 1 0 exdx = e − 1. For this problem, we could also find EY using the PDF of Y , E[Y ] = ∫ ∞ −∞ yfY (y)dy = ∫ e 1 y dy = e − 1. Note that since we have already found the PDF of Y it did not matter which method we used to find E[Y ]. However , if the problem only asked for E[Y ] without asking for the PDF of Y , then using LOTUS would be much easier . Example 4. 8 Let X ∼ Uniform(−1, 1) and Y = X2. Find the CDF and PDF of Y . Solution 1 y 1 y First, we note that RY = [0, 1]. As usual, we start with the CDF . For y ∈ [0, 1], we have FY (y) = P(Y ≤ y) = P(X2 ≤ y) = P(−√y ≤ X ≤ √y) = since X ∼ Uniform(−1, 1) = √y. Thus, the CDF of Y is given by FY (y) = ⎧⎪ ⎨ ⎪⎩ 0 for y < 0 √y for 0 ≤ y ≤ 1 1 for y > 1 Note that the CDF is a continuous function of Y , so Y is a continuous random variable. Thus, we can find the PDF of Y by dif ferentiating FY (y), fY (y) = F ′ Y (y) = { for 0 ≤ y ≤ 1 0 otherwise The Method of T ransformations So far , we have discussed how we can find the distribution of a function of a continuous random variable starting from finding the CDF . If we are interested in finding the PDF of Y = g(X), and the function g satisfies some properties, it might be easier to use a method called the method of transformations. Let's start with the case where g is a function satisfying the following properties: g(x) is differentiable; g(x) is a strictly increasing function, that is, if x1 < x2, then g(x1) < g(x2). Now , let X be a continuous random variable and Y = g(X). We will show that you can directly find the PDF of Y using the following formula. fY (y) = ⎧ ⎨ ⎩ = fX(x1). where g(x1) = y 0 if g(x) = y does not have a solution Note that since g is strictly increasing, its inverse function g −1 is well defined. That is, for each y ∈ RY , there exists a unique x1 such that g(x1) = y. We can write x1 = g −1(y). FY (y) = P(Y ≤ y) √y−(−√y) 1−(−1) 1 2√y fX(x1) g′(x1) dx1 dy = P(g(X) ≤ y) = FX(g −1(y)). To find the PDF of Y , we differentiate fY (y) = FX(x1)  where g(x1) = y = ⋅ F ′ X(x1) = fX(x1) = since  = . We can repeat the same argument for the case where g is strictly decreasing. In that case, g ′(x1) will be negative, so we need to use |g ′(x1)|. Thus, we can state the following theorem for a strictly monotonic function. (A function g : R → R is called strictly monotonic if it is strictly increasing or strictly decreasing.) Theorem 4. 1 Suppose that X is a continuous random variable and g : R → R is a strictly monotonic dif ferentiable function. Let Y = g(X). Then the PDF of Y is given by fY (y) = ⎧ ⎨ ⎩ = fX(x1). | | where g(x1) = y 0 if g(x) = y does not have a solution (4.5) To see how to use the formula, let's look at an example. Example 4. 9 Let X be a continuous random variable with PDF fX(x) = { 4x 3 0 < x ≤ 1 0 otherwise and let Y = . Find fY (y). Solution d dy dx1 dy dx1 dy fX(x1) g′(x1) dx dy 1 dy dx fX(x1) |g′(x1)| dx1 dy 1 X First note that RY = [1, ∞). Also, note that g(x) is a strictly decreasing and differentiable function on (0, 1], so we may use Equation 4.5. W e have g ′(x) = − . For any y ∈ [1, ∞), x1 = g −1(y) = . So, for y ∈ [1, ∞) fY (y) = = = 4x 5 1 = . Thus, we conclude fY (y) = { y ≥ 1 0 otherwise Theorem 4.1 can be extended to a more general case. In particular , if g is not monotonic, we can usually divide it into a finite number of monotonic dif ferentiable functions. Figure 4.3 shows a function g that has been divided into monotonic parts. We may state a more general form of Theorem 4.1. Fig.4.3 - Partitioning a function to monotone parts. 1 x2 1 y fX(x1) |g′(x1)| 4x 3 1 |− | 1 x2 1 4 y 5 4 y 5 Theorem 4. 2 Consider a continuous random variable X with domain RX, and let Y = g(X). Suppose that we can partition RX into a finite number of intervals such that g(x) is strictly monotone and dif ferentiable on each partition. Then the PDF of Y is given by fY (y) = n ∑ i=1 = n ∑ i=1 fX(xi). ∣ ∣ ∣ ∣ ∣ ∣ (4.6) where x1, x2, . . . , xn are real solutions to g(x) = y. Let us look at an example to see how we can use Theorem 4.2. Example 4. 10 Let X be a continuous random variable with PDF fX(x) = e − , for all x ∈ R and let Y = X2. Find fY (y). Solution We note that the function g(x) = x2 is strictly decreasing on the interval (−∞, 0), strictly increasing on the interval (0, ∞), and dif ferentiable on both intervals, g ′(x) = 2x. Thus, we can use Equation 4.6. First, note that RY = (0, ∞). Next, for any y ∈ (0, ∞) we have two solutions for y = g(x), in particular , x1 = √y, x2 = −√y Note that although 0 ∈ RX it has not been included in our partition of RX. This is not a problem, since P(X = 0) = 0. Indeed, in the statement of Theorem 4.2, we could replace RX by RX − A, where A is any set for which P(X ∈ A) = 0. In particular , this is convenient when we exclude the endpoints of the intervals. Thus, we have fY (y) = + = + fX(xi) |g ′(xi)| dxi dy 1 √2π x2 2 fX(x1) |g′(x1)| fX(x2) |g′(x2)| fX(√y) |2√y| fX(−√y) |−2√y| = e − + e − = e − ,  for y ∈ (0, ∞). 1 2√2πy y 2 1 2√2πy y 2 1 √2πy y 2 4.1.4 Solved Problems: Continuous Random V ariables Problem 1 Let X be a random variable with PDF given by fX(x) = { cx 2 |x| ≤ 1 0 otherwise a . Find the constant c. b . Find EX and V ar (X). c. Find P(X ≥ ). Solution a . To find c, we can use ∫ ∞ −∞ fX(u)du = 1: 1 = ∫ ∞ −∞ fX(u)du = ∫ 1 −1 cu 2du = c. Thus, we must have c = . b . To find EX, we can write EX = ∫ 1 −1 ufX(u)du = ∫ 1 −1 u3du = 0. In fact, we could have guessed EX = 0 because the PDF is symmetric around x = 0. To find V ar (X), we have Var(X) = EX2 − (EX) 2 = EX2 = ∫ 1 −1 u2fX(u)du = ∫ 1 −1 u4du = . c. To find P(X ≥ ), we can write 1 1 2 2 3 3 2 3 2 3 2 3 5 1 2 P(X ≥ ) = ∫ 1 x 2dx = . Problem 2 Let X be a continuous random variable with PDF given by fX(x) = e −|x|, for all x ∈ R. If Y = X2, find the CDF of Y . Solution First, we note that RY = [0, ∞). For y ∈ [0, ∞), we have FY (y) = P(Y ≤ y) = P(X2 ≤ y) = P(−√y ≤ X ≤ √y) = ∫ √y −√y e −|x|dx = ∫ √y 0 e −xdx = 1 − e −√y. Thus, FY (y) = { 1 − e −√y y ≥ 0 0 otherwise Problem 3 Let X be a continuous random variable with PDF fX(x) = { 4x 3 0 < x ≤ 1 0 otherwise Find P(X ≤ |X > ). Solution 1 2 3 2 1 2 7 16 1 2 1 2 2 3 1 3 We have P(X ≤ |X > ) = = = . Problem 4 Let X be a continuous random variable with PDF fX(x) = { x2 (2x + ) 0 < x ≤ 1 0 otherwise If Y = + 3, find V ar (Y ). Solution First, note that Var(Y ) = Var ( + 3) = 4Var ( ) , using Equation 4.4 Thus, it suf fices to find V ar ( ) = E[ ] − (E[ ]) 2. Using LOTUS, we have E [ ] = ∫ 1 0 x (2x + ) dx = E [ ] = ∫ 1 0 (2x + ) dx = . Thus, V ar ( ) = E[ ] − (E[ ]) 2 = . So, we obtain Var(Y ) = 4Var ( ) = . 2 3 1 3 P( <X≤ ) 1 3 2 3 P(X> ) 1 3 ∫ 4x3dx 2 3 1 3 ∫ 1 4x3dx1 3 3 16 3 2 2 X 2 X 1 X 1 X 1 X2 1 X 1 X 3 2 17 12 1 X2 3 2 5 2 1 X 1 X2 1 X 71 144 1 X 71 36 Problem 5 Let X be a positive continuous random variable. Prove that EX = ∫ ∞ 0 P(X ≥ x)dx. Solution We have P(X ≥ x) = ∫ ∞ x fX(t)dt. Thus, we need to show that ∫ ∞ 0 ∫ ∞ x fX(t)dtdx = EX. The left hand side is a double integral. In particular , it is the integral of fX(t) over the shaded region in Figure 4.4. Fig.4.4 - The shaded area shows the region of the double integral of Problem 5. We can take the integral with respect to x or t. Thus, we can write ∫ ∞ 0 ∫ ∞ x fX(t)dtdx = ∫ ∞ 0 ∫ t 0 fX(t)dxdt = ∫ ∞ 0 fX(t) (∫ t 0 1dx) dt Problem 6 Let X ∼ Uniform(− , π) and Y = sin(X). Find fY (y). Solution Here Y = g(X), where g is a dif ferentiable function. Although g is not monotone, it can be divided to a finite number of regions in which it is monotone. Thus, we can use Equation 4.6. W e note that since RX = [− , π], RY = [−1, 1]. By looking at the plot of g(x) = sin(x) over [− , π], we notice that for y ∈ (0, 1) there are two solutions to y = g(x), while for y ∈ (−1, 0), there is only one solution. In particular , if y ∈ (0, 1), we have two solutions: x1 = arcsin(y), and x2 = π − arcsin(y). If y ∈ (−1, 0) we have one solution, x1 = arcsin(y). Thus, for y ∈ (−1, 0), we have fY (y) = = = . For y ∈ (0, 1), we have fY (y) = + = + = + = . To summarize, we can write fY (y) = ⎧⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪⎩ −1 < y < 0 0 < y < 1 0 otherwise π 2 π 2 π 2 fX(x1) |g′(x1)| fX(arcsin(y)) | cos(arcsin(y))| 2 3π √1−y 2 fX(x1) |g′(x1)| fX(x2) |g′(x2)| fX(arcsin(y)) | cos(arcsin(y))| fX(π−arcsin(y)) | cos(π−arcsin(y))| 2 3π √1−y 2 2 3π √1−y 2 4 3π√1−y 2 2 3π√1−y 2 4 3π√1−y 2 4.2.1 Uniform Distribution We have already seen the uniform distribution. In particular , we have the following definition: A continuous random variable X is said to have a Uniform distribution over the interval [a, b], shown as X ∼ Uniform(a, b), if its PDF is given by fX(x) = { a < x < b 0 x < a or x > b We have already found the CDF and the expected value of the uniform distribution. In particular , we know that if X ∼ Uniform(a, b), then its CDF is given by equation 4.1 in example 4.1 , and its mean is given by EX = To find the variance, we can find EX2 using LOTUS: EX2 = ∫ ∞ −∞ x 2fX(x)dx = ∫ b a x 2 ( ) dx = . Therefore, Var(X) = EX2 − (EX) 2 = . 1 b−a a + b 2 1 b−a a2+ab+b2 3 (b−a)2 12 4.2.2 Exponential Distribution The exponential distribution is one of the widely used continuous distributions. It is often used to model the time elapsed between events. W e will now mathematically define the exponential distribution, and derive its mean and expected value. Then we will develop the intuition for the distribution and discuss several interesting properties that it has. A continuous random variable X is said to have an exponential distribution with parameter λ > 0, shown as X ∼ Exponential(λ), if its PDF is given by fX(x) = { λe −λx x > 0 0 otherwise Figure 4.5 shows the PDF of exponential distribution for several values of λ. Fig.4.5 - PDF of the exponential random variable. It is convenient to use the unit step function defined as u(x) = { 1 x ≥ 0 0 otherwise so we can write the PDF of an Exponential(λ) random variable as fX(x) = λe −λxu(x). Let us find its CDF , mean and variance. For x > 0, we have FX(x) = ∫ x 0 λe −λtdt = 1 − e −λx. So we can express the CDF as FX(x) = (1 − e −λx)u(x). Let X ∼ Exponential(λ). We can find its expected value as follows, using integration by parts: EX = ∫ ∞ 0 xλe −λxdx = ∫ ∞ 0 ye −ydy = [ − e −y − ye −y] ∞ 0 = . Now let's find V ar (X). We have EX2 = ∫ ∞ 0 x 2λe −λxdx = ∫ ∞ 0 y2e −ydy = [ − 2e −y − 2ye −y − y2e −y] ∞ 0 = . Thus, we obtain Var(X) = EX2 − (EX) 2 = − = . If X ∼ Exponential(λ), then EX = and V ar (X) = . 1 λ 1 λ 1 λ 1 λ2 1 λ2 2 λ2 2 λ2 1 λ2 1 λ2 1 λ 1 λ2 An interesting property of the exponential distribution is that it can be viewed as a continuous analogue of the geometric distribution. T o see this, recall the random experiment behind the geometric distribution: you toss a coin (repeat a Bernoulli experiment) until you observe the first heads (success). Now , suppose that the coin tosses are Δ seconds apart and in each toss the probability of success is p = Δλ. Also suppose that Δ is very small, so the coin tosses are very close together in time and the probability of success in each trial is very low . Let X be the time you observe the first success. We will show in the Solved Problems section that the distribution of X converges to Exponential(λ) as Δ approaches zero. To get some intuition for this interpretation of the exponential distribution, suppose you are waiting for an event to happen. For example, you are at a store and are waiting for the next customer . In each millisecond, the probability that a new customer enters the store is very small. Y ou can imagine that, in each millisecond, a coin (with a very small P(H)) is tossed, and if it lands heads a new customers enters. If you toss a coin every millisecond, the time until a new customer arrives approximately follows an exponential distribution. The above interpretation of the exponential is useful in better understanding the properties of the exponential distribution. The most important of these properties is that the exponential distribution is memoryless . To see this, think of an exponential random variable in the sense of tossing a lot of coins until observing the first heads. If we toss the coin several times and do not observe a heads, from now on it is like we start all over again. In other words, the failed coin tosses do not impact the distribution of waiting time from now on. The reason for this is that the coin tosses are independent. We can state this formally as follows: P(X > x + a|X > a) = P(X > x). If X is exponential with parameter λ > 0, then X is a memoryless random variable, that is P(X > x + a | X > a) = P(X > x),  for a, x ≥ 0. From the point of view of waiting time until arrival of a customer , the memoryless property means that it does not matter how long you have waited so far . If you have not observed a customer until time a, the distribution of waiting time (from time a) until the next customer is the same as when you started at time zero. Let us prove the memoryless property of the exponential distribution. P(X > x + a|X > a) = = = = = e −λx = P(X > x). P(X > x + a, X > a) P(X > a) P(X > x + a) P(X > a) 1 − FX(x + a) 1 − FX(a) e −λ(x+a) e−λa 4.2.3 Normal (Gaussian) Distribution The normal distribution is by far the most important probability distribution. One of the main reasons for that is the Central Limit Theorem (CL T) that we will discuss later in the book. T o give you an idea, the CL T states that if you add a large number of random variables, the distribution of the sum will be approximately normal under certain conditions. The importance of this result comes from the fact that many random variables in real life can be expressed as the sum of a large number of random variables and, by the CL T, we can argue that distribution of the sum should be normal. The CL T is one of the most important results in probability and we will discuss it later on. Here, we will introduce normal random variables. We first define the standard normal random variable . We will then see that we can obtain other normal random variables by scaling and shifting a standard normal random variable. A continuous random variable Z is said to be a standard normal (standard Gaussian) random variable, shown as Z ∼ N(0, 1), if its PDF is given by fZ(z) = exp{− }, for all z ∈ R. The is there to make sure that the area under the PDF is equal to one. W e will verify that this holds in the solved problems section. Figure 4.6 shows the PDF of the standard normal random variable. 1 √2π z2 2 1 √2π Fig.4.6 - PDF of the standard normal random variable. Let us find the mean and variance of the standard normal distribution. T o do that, we will use a simple useful fact. Consider a function g(u) : R → R. If g(u) is an odd function, i.e., g(−u) = −g(u), and | ∫ ∞ 0 g(u)du| < ∞, then ∫ ∞ −∞ g(u)du = 0. For our purpose, let g(u) = u 2k+1 exp{− }, where k = 0, 1, 2, . . .. Then g(u) is an odd function. Also | ∫ ∞ 0 g(u)du| < ∞. One way to see this is to note that g(u) decays faster than the function exp{−u} and since | ∫ ∞ 0 exp{−u}du| < ∞, we conclude that | ∫ ∞ 0 g(u)du| < ∞. Now , let Z be a standard normal random variable. Then, we have EZ 2k+1 = ∫ ∞ −∞ u 2k+1 exp{− }du = 0, for all k ∈ {0, 1, 2, . . , }. Thus, we have shown that for a standard normal random variable Z, we have EZ = EZ 3 = EZ 5 =. . . . = 0. In particular , the standard normal distribution has zero mean. This is not surprising as we can see from Figure 4.6 that the PDF is symmetric around the origin, so we expect that EZ = 0. Next, let's find EZ 2. EZ 2 = ∫ ∞ −∞ u 2 exp{− }du u 2 2 1 √2π u 2 2 1 √2π u2 2 = [ − u exp{− }] ∞ −∞+ + ∫ ∞ −∞ exp{− }du (integration by parts) = ∫ ∞ −∞ exp{− }du = 1. The last equality holds because we are integrating the standard normal PDF from −∞ to ∞. Thus, we conclude that for a standard normal random variable Z, we have Var(Z) = 1. So far we have shown the following: If Z ∼ N(0, 1), then EZ = 0 and V ar (Z) = 1. CDF of the standard normal To find the CDF of the standard normal distribution, we need to integrate the PDF function. In particular , we have FZ(z) = ∫ z −∞ exp{− }du. This integral does not have a closed form solution. Nevertheless, because of the importance of the normal distribution, the values of FZ(z) have been tabulated and many calculators and software packages have this function. W e usually denote the standard normal CDF by Φ. The CDF of the standard normal distribution is denoted by the Φ function: Φ(x) = P(Z ≤ x) = ∫ x −∞ exp{− }du. As we will see in a moment, the CDF of any normal random variable can be written in terms of the Φ function, so the Φ function is widely used in probability . Figure 4.7 shows the Φ function. 1 √2π u2 2 1 √2π u2 2 1 √2π u2 2 1 √2π u 2 2 1 √2π u 2 2 Fig.4.7 - The Φ function (CDF of standard normal). Here are some properties of the Φ function that can be shown from its definition. 1 . lim x→∞ Φ(x) = 1, lim x→−∞ Φ(x) = 0; 2 . Φ(0) = ; 3 . Φ(−x) = 1 − Φ(x), for all x ∈ R. Also, since the Φ function does not have a closed form, it is sometimes useful to use upper or lower bounds. In particular we can state the following bounds (see Problem 7 in the Solved Problems section). For all x ≥ 0, exp{− } ≤ 1 − Φ(x) ≤ exp{− } (4.7) As we mentioned earlier , because of the importance of the normal distribution, the values of the Φ function have been tabulated and many calculators and software packages have this function. For example, you can use the normcdf command in MA TLAB to compute Φ(x) for a given number x. More specifically , normcdf(x) returns Φ(x). Also, the function norminv returns Φ−1(x). That is, if you run x = norminv(y), then x will be the real number for which Φ(x) = y. Normal random variables Now that we have seen the standard normal random variable, we can obtain any normal random variable by shifting and scaling a standard normal random variable. In particular , define X = σZ + μ, where σ > 0. Then EX = σEZ + μ = μ, 2 2 1 2 1 √2π x x2 + 1 x2 2 1 √2π 1 x x2 2 Var(X) = σ2Var(Z) = σ2. We say that X is a normal random variable with mean μ and variance σ2. We write X ∼ N(μ, σ2). If Z is a standard normal random variable and X = σZ + μ, then X is a normal random variable with mean μ and variance σ2, i.e, X ∼ N(μ, σ2). Conversely , if X ∼ N(μ, σ2), the random variable defined by Z = is a standard normal random variable, i.e., Z ∼ N(0, 1). To find the CDF of X ∼ N(μ, σ2), we can write FX(x) = P(X ≤ x) = P(σZ + μ ≤ x) (where Z ∼ N(0, 1)) = P (Z ≤ ) = Φ ( ) . To find the PDF , we can take the derivative of FX, fX(x) = FX(x) = Φ ( ) = Φ′ ( ) (chain rule for derivative) = fZ ( ) = exp{− }. X−μ σ x−μ σ x−μ σ d dx d dx x−μ σ 1 σ x−μ σ 1 σ x−μ σ 1 σ√2π (x−μ)2 2σ2 If X is a normal random variable with mean μ and variance σ2, i.e, X ∼ N(μ, σ2), then fX(x) = exp{− }, FX(x) = P(X ≤ x) = Φ ( ) , P(a < X ≤ b) = Φ ( ) − Φ ( ) . Figure 4.8 shows the PDF of the normal distribution for several values of μ and σ. Fig.4.8 - PDF for normal distribution. Example 4. 1 1 Let X ∼ N(−5, 4). a . Find P(X < 0). b . Find P(−7 < X < −3). c. Find P(X > −3|X > −5). Solution X is a normal random variable with μ = −5 and σ = √4 = 2, thus we have 1 σ√2π (x − μ) 2 2σ2 x − μ σ b − μ σ a − μ σ a . Find P(X < 0): P(X < 0) = FX(0) = Φ( ) = Φ(2.5) ≈ 0.99 b . Find P(−7 < X < −3): P(−7 < X < −3) = FX(−3) − FX(−7) = Φ( ) − Φ( ) = Φ(1) − Φ(−1) = 2Φ(1) − 1 (since Φ(−x) = 1 − Φ(x)) ≈ 0.68 c. Find P(X > −3|X > −5): P(X > −3|X > −5) = = = = ≈ ≈ 0.32 An important and useful property of the normal distribution is that a linear transformation of a normal random variable is itself a normal random variable. In particular , we have the following theorem: Theorem 4. 3 If X ∼ N(μX, σ2 X), and Y = aX + b, where a, b ∈ R, then Y ∼ N(μY , σ2 Y ) where μY = aμX + b, σ2 Y = a 2σ2 X. Proof 0−(−5) 2 (−3)−(−5) 2 (−7)−(−5) 2 P(X>−3,X>−5) P(X>−5) P(X>−3) P(X>−5) 1−Φ( ) (−3)−(−5) 2 1−Φ( ) (−5)−(−5) 2 1−Φ(1) 1−Φ(0) 0.1587 0.5 We can write X = σXZ + μX where Z ∼ N(0, 1). Thus, Y = aX + b = a(σXZ + μX) + b = (aσX)Z + (aμX + b). Therefore, Y ∼ N(aμX + b, a 2σ2 X). 4.2.4 Gamma Distribution The gamma distribution is another widely used distribution. Its importance is largely due to its relation to exponential and normal distributions. Here, we will provide an introduction to the gamma distribution. In Chapters 6 and 1 1 , we will discuss more properties of the gamma random variables. Before introducing the gamma random variable, we need to introduce the gamma function. Gamma function: The gamma function [ 10 ], shown by Γ(x), is an extension of the factorial function to real (and complex) numbers. Specifically , if n ∈ {1, 2, 3, . . . }, then Γ(n) = (n − 1)! More generally , for any positive real number α, Γ(α) is defined as Γ(α) = ∫ ∞ 0 x α−1e −xdx, for α > 0. Figure 4.9 shows the gamma function for positive real values. Figure 4.9: The Gamma function for some real values of α. Note that for α = 1, we can write Γ(1) = ∫ ∞ 0 e −xdx = 1. Using the change of variable x = λy, we can show the following equation that is often useful when working with the gamma distribution: Γ(α) = λ α ∫ ∞ 0 yα−1e −λydy for α, λ > 0. Also, using integration by parts it can be shown that Γ(α + 1) = αΓ(α), for α > 0. Note that if α = n, where n is a positive integer , the above equation reduces to n! = n ⋅ (n − 1)! P ro p erties o f the ga mma fun ctio n For any positive real number α: 1 . Γ(α) = ∫ ∞ 0 xα−1e−xdx; 2 . ∫ ∞ 0 x α−1e −λxdx = , for λ > 0; 3 . Γ(α + 1) = αΓ(α); 4 . Γ(n) = (n − 1)!,  for n = 1, 2, 3, ⋯ ; 5 . Γ( ) = √π. Example 4. 12 Answer the following questions: 1 . Find Γ( ). 2 . Find the value of the following integral: I = ∫ ∞ 0 x 6e −5xdx. Solution 1 . To find Γ( ), we can write Γ(α) λα 1 2 7 2 7 2 Γ( ) = ⋅ Γ( ) (using Property 3) = ⋅ ⋅ Γ( ) (using Property 3) = ⋅ ⋅ ⋅ Γ( )(using Property 3) = ⋅ ⋅ ⋅ √π (using Property 5) = √π. 2 . Using Property 2 with α = 7 and λ = 5, we obtain I = ∫ ∞ 0 x 6e −5xdx = = (using Property 4) ≈ 0.0092 Gamma Distribution: We now define the gamma distribution by providing its PDF: A continuous random variable X is said to have a gamma distribution with parameters α > 0 and λ > 0, shown as X ∼ Gamma(α, λ), if its PDF is given by fX(x) = { x > 0 0 otherwise If we let α = 1, we obtain fX(x) = { λe −λx x > 0 0 otherwise Thus, we conclude Gamma(1, λ) = Exponential(λ). More generally , if you sum n independent Exponential(λ) random variables, then you will get a Gamma(n, λ) random variable. W e will prove this later on using the moment generating function. The gamma distribution is also related to the normal distribution as will be discussed later . Figure 4.10 shows the PDF of the gamma distribution for several values of α. 7 2 5 2 5 2 5 2 3 2 3 2 5 2 3 2 1 2 1 2 5 2 3 2 1 2 15 8 Γ(7) 57 6! 57 λαxα−1e−λx Γ(α) Figure 4.10: PDF of the gamma distribution for some values of α and λ. Example 4. 13 Using the properties of the gamma function, show that the gamma PDF integrates to 1, i.e., show that for α, λ > 0, we have ∫ ∞ 0 dx = 1. Solution We can write ∫ ∞ 0 dx = ∫ ∞ 0 x α−1e −λxdx = ⋅ (using Property 2 of the gamma function) = 1. In the Solved Problems section, we calculate the mean and variance for the gamma distribution. In particular , we find out that if X ∼ Gamma(α, λ), then λ αx α−1e −λx Γ(α) λ αx α−1e −λx Γ(α) λ α Γ(α) λ α Γ(α) Γ(α) λα EX = , V ar(X) = . α λ α λ2 4.2.5 Other Distributions In addition to the special distributions that we discussed previously , there are many other continuous random variables that are used in practice. Depending on the applications you are interested in you might need to deal with some of them. W e have provided a list of important distributions in the appendix. In the next chapters, we will discuss some of them in more detail. There are also some problems at the end of this chapter that discuss some of these distributions. There is no need to try to memorize these distributions. When you understand the general theory behind random variables, you can essentially work with any distribution. 4.2.6 Solved Problems: Special Continuous Distributions Problem 1 Suppose the number of customers arriving at a store obeys a Poisson distribution with an average of λ customers per unit time. That is, if Y is the number of customers arriving in an interval of length t, then Y ∼ Poisson(λt). Suppose that the store opens at time t = 0. Let X be the arrival time of the first customer . Show that X ∼ Exponential(λ). Solution We first find P(X > t): P(X > t) = P(No arrival in [0, t]) = e −λt = e −λt. Thus, the CDF of X for x > 0 is given by FX(x) = 1 − P(X > x) = 1 − e −λx, which is the CDF of Exponential(λ). Note that by the same argument, the time between the first and second customer also has Exponential(λ) distribution. In general, the time between the k'th and k + 1'th customer is Exponential(λ). Problem 2 (Exponential as the limit of Geometric) Let Y ∼ Geometric(p), where p = λΔ. Define X = Y Δ, where λ, Δ > 0. Prove that for any x ∈ (0, ∞), we have lim Δ→0 FX(x) = 1 − e −λx. Solution (λt)0 0! If Y ∼ Geometric(p) and q = 1 − p, then P(Y ≤ n) = ∑n k=1 pqk−1 = p. = 1 − (1 − p)n. Then for any y ∈ (0, ∞), we can write P(Y ≤ y) = 1 − (1 − p) ⌊y⌋, where ⌊y⌋ is the largest integer less than or equal to y. Now , since X = Y Δ, we have FX(x) = P(X ≤ x) = P (Y ≤ ) = 1 − (1 − p) ⌊ ⌋ = 1 − (1 − λΔ) ⌊ ⌋. Now , we have limΔ→0 FX(x) = limΔ→0 1 − (1 − λΔ) ⌊ ⌋ = 1 − limΔ→0(1 − λΔ) ⌊ ⌋ = 1 − e −λx. The last equality holds because − 1 ≤ ⌊ ⌋ ≤ , and we know lim Δ→0+(1 − λΔ) = e −λ. Problem 3 Let U ∼ Uniform(0, 1) and X = − ln(1 − U). Show that X ∼ Exponential(1). Solution First note that since RU = (0, 1), RX = (0, ∞). W e will find the CDF of X. For x ∈ (0, ∞) , we have FX(x) = P(X ≤ x) = P(− ln(1 − U) ≤ x) = P ( ≤ e x) = P(U ≤ 1 − e −x) = 1 − e −x, 1−qn 1−q x Δ x Δ x Δ x Δ x Δ x Δ x Δ x Δ 1 Δ 1 1−U which is the CDF of an Exponential(1) random variable. Problem 4 Let X ∼ N(2, 4) and Y = 3 − 2X. a . Find P(X > 1). b . Find P(−2 < Y < 1). c. Find P(X > 2|Y < 1). Solution a . Find P(X > 1): W e have μX = 2 and σX = 2. Thus, P(X > 1) = 1 − Φ ( ) = 1 − Φ(−0.5) = Φ(0.5) = 0.6915 b . Find P(−2 < Y < 1): Since Y = 3 − 2X, using Theorem 4.3, we have Y ∼ N(−1, 16). Therefore, P(−2 < Y < 1) = Φ ( ) − Φ ( ) = Φ(0.5) − Φ(−0.25) = 0.29 c. Find P(X > 2|Y < 1):P(X > 2|Y < 1) = P(X > 2|3 − 2X < 1) = P(X > 2|X > 1) = = = = ≈ 0.72 1−2 2 1−(−1) 4 (−2)−(−1) 4 P(X>2,X>1) P(X>1) P(X>2) P(X>1) 1−Φ( ) 2−2 2 1−Φ( ) 1−2 2 1−Φ(0) 1−Φ(−0.5) Problem 5 Let X ∼ N(0, σ2). Find E|X| Solution We can write X = σZ, where Z ∼ N(0, 1). Thus, E|X| = σE|Z|. W e have E|Z| = ∫ ∞ −∞ |t|e − dt = ∫ ∞ 0 |t|e − dt (integral of an even function) = √ ∫ ∞ 0 te − dt = √ [ − e − ] ∞ 0 = √ Thus, we conclude E|X| = σE|Z| = σ√ . Problem 6 Show that the constant in the normal distribution must be . That is, show that I = ∫ ∞ −∞ e − dx = √2π. Hint: Write I 2 as a double integral in polar coordinates. Solution Let I = ∫ ∞ −∞ e − dx. W e show that I 2 = 2π. T o see this, note I 2 = ∫ ∞ −∞ e − dx ∫ ∞ −∞ e − dy = ∫ ∞ −∞ ∫ ∞ −∞ e − dxdy. To evaluate this double integral we can switch to polar coordinates. This can be done by change of variables x = r cos θ, y = r sin θ, and dxdy = rdrdθ. In particular , we have I 2 1 √2π t2 2 2 √2π t2 2 2 π t2 2 2 π t2 2 2 π 2 π 1 √2π x2 2 x2 2 x2 2 y2 2 x2+y2 2 = ∫ ∞ −∞ ∫ ∞ −∞ e − dxdy = ∫ ∞ 0 ∫ 2π 0 e − rdθdr = 2π ∫ ∞ 0 re − dr = 2π[ − e − ] ∞ 0 = 2π. Problem 7 Let Z ∼ N(0, 1). Prove for all x ≥ 0, e − ≤ P(Z ≥ x) ≤ e − . Solution To show the upper bound, we can write P(Z ≥ x) = ∫ ∞ x e − du = [−e − ] ∞ x = e − . To show the lower bound, let Q(x) = P(Z ≥ x), and h(x) = Q(x) − e − ,  for all x ≥ 0. It suffices to show that h(x) ≥ 0,  for all x ≥ 0. To see this, note that the function h has the following properties 1 . h(0) = ; 2 . lim x→∞ h(x) = 0; x2+y2 2 r2 2 r2 2 r2 2 1 √2π x x2 + 1 x2 2 1 √2π 1 x x2 2 1 √2π u2 2 1 √2π 1 x u2 2 1 √2π 1 x x2 2 1 √2π x x2 + 1 x2 2 1 2 3 . h ′(x) = − ⎛ ⎝ ⎞ ⎠ < 0, for all x ≥ 0. Therefore, h(x) is a strictly decreasing function that starts at h(0) = and decreases as x increases. It approaches 0 as x goes to infinity . W e conclude that h(x) ≥ 0, for all x ≥ 0. Problem 8 Let X ∼ Gamma(α, λ), where α, λ > 0. Find EX, and V ar(X). Solution To find EX we can write EX = ∫ ∞ 0 xfX(x)dx = ∫ ∞ 0 x ⋅ x α−1e −λxdx = ∫ ∞ 0 x ⋅ x α−1e −λxdx = ∫ ∞ 0 x αe −λxdx = (using Property 2 of the gamma function) = (using Property 3 of the gamma function) = . Similarly , we can find EX2: 2 √2π e − x2 2 (x2+1) 1 2 λα Γα λα Γ(α) λα Γ(α) λ α Γ(α) Γ(α + 1) λα+1 αΓ(α) λΓ(α) α λ EX2 = ∫ ∞ 0 x 2dx = ∫ ∞ 0 x 2 ⋅ x α−1e −λxdx = ∫ ∞ 0 x 2 ⋅ x α−1e −λxdx = ∫ ∞ 0 x α+1e −λxdx = (using Property 2 of the gamma function) = (using Property 3 of the gamma function) = (using Property 3 of the gamma function) = . So, we conclude V ar(X) = EX2 − (EX) 2 = − = . λ α Γ(α) λ α Γ(α) λ α Γ(α) λα Γ(α) Γ(α + 2) λα+2 (α + 1)Γ(α + 1) λ2Γ(α) (α + 1)αΓ(α) λ2Γ(α) α(α + 1) λ2 α(α + 1) λ2 α 2 λ2 α λ2 4.3.1 Mixed Random V ariables Here, we will discuss mixed random variables. These are random variables that are neither discrete nor continuous, but are a mixture of both. In particular , a mixed random variable has a continuous part and a discrete part. Thus, we can use our tools from previous chapters to analyze them. In this section, we will provide some examples on how we can do this. Then in section 4.3.2, we will revisit the concept of mixed random variables using the delta \"function .\" Example 4. 14 Let X be a continuous random variable with the following PDF: fX(x) = { 2x 0 ≤ x ≤ 1 0 otherwise Let also Y = g(X) = { X 0 ≤ X ≤ X > Find the CDF of Y . Solution First we note that RX = [0, 1]. For x ∈ [0, 1], 0 ≤ g(x) ≤ . Thus, RY = [0, ], and therefore FY (y) = 0,  for y < 0, FY (y) = 1,  for y > . Now note that P (Y = ) = P (X > ) = ∫ 1 2xdx = . 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 3 4 Also, for 0 < y < , FY (y) = P(Y ≤ y) = P(X ≤ y) = ∫ y 0 2xdx = y2. Thus, the CDF of Y is given by FY (y) = ⎧⎪ ⎪ ⎨ ⎪ ⎪⎩ 1 y ≥ y2 0 ≤ y < 0 otherwise Figure 4.9 shows the CDF of Y . We note that the CDF is not continuous, so Y is not a continuous random variable. On the other hand, the CDF is not in the staircase form, so it is not a discrete random variable either . It is indeed a mixed random variable. There is a jump at y = , and the amount of jump is 1 − = , which is the probability that Y = . The CDF is continuous at other points. Fig.4.9 - CDF of a Mixed random variable, Example 4.12. The CDF of Y has a continuous part and a discrete part. In particular , we can write FY (y) = C(y) + D(y), where C(y) is the continuous part of FY (y), i.e., 1 2 1 2 1 2 1 2 1 4 3 4 1 2 1 1 C(y) = ⎧⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪⎩ y ≥ y2 0 ≤ y < 0 y < 0 The discrete part of FY (y) is D(y), given by D(y) = { y ≥ 0 y < In general, the CDF of a mixed random variable Y can be written as the sum of a continuous function and a staircase function: FY (y) = C(y) + D(y). We dif ferentiate the continuous part of the CDF . In particular , let's define c(y) = ,  wherever C(y) is differentiable. Note that this is not a valid PDF as it does not integrate to one. Also, let {y1, y2, y3, . . . } be the set of jump points of D(y), i.e., the points for which P(Y = yk) > 0. We then have ∫ ∞ −∞ c(y)dy + ∑ yk P(Y = yk) = 1. The expected value of Y can be obtained as EY = ∫ ∞ −∞ yc(y)dy + ∑ yk ykP(Y = yk). Example 4. 15 Let Y be the mixed random variable defined in Example 4.14. a . Find P( ≤ Y ≤ ). b . Find P(Y ≥ ). c. Find EY . Solution 1 4 1 2 1 2 3 4 1 2 1 2 dC(y) dy 1 4 3 8 1 4 Since we have the CDF of Y , we can find the probability that Y is in any given interval. We should pay special attention if the interval includes any jump points. a . Find P( ≤ Y ≤ ): We can write P ( ≤ Y ≤ ) = FY ( ) − FY ( ) + P (Y = ) = ( )2 − ( )2 + 0 = . b . Find P(Y ≥ ): We have P (Y ≥ ) = 1 − FY ( ) + P (Y = ) = 1 − ( )2 = . c. Find EY : Here, we can dif ferentiate the continuous part of the CDF to obtain c(y) = = { 2y 0 ≤ y ≤ 0 otherwise So, we can find EY as EY = ∫0 y(2y)dy + P (Y = ) = + = . 1 4 3 8 1 4 3 8 3 8 1 4 1 4 3 8 1 4 5 64 1 4 1 4 1 4 1 4 1 4 15 16 dC(y) dy 1 2 1 2 1 2 1 2 1 12 3 8 11 24 4.3.2 Using the Delta Function In this section, we will use the Dirac delta function to analyze mixed random variables. Technically speaking, the Dirac delta function is not actually a function. It is what we may call a generalized function. Nevertheless, its definition is intuitive and it simplifies dealing with probability distributions. Remember that any random variable has a CDF . Thus, we can use the CDF to answer questions regarding discrete, continuous, and mixed random variables. On the other hand, the PDF is defined only for continuous random variables, while the PMF is defined only for discrete random variables. Using delta functions will allow us to define the PDF for discrete and mixed random variables. Thus, it allows us to unify the theory of discrete, continuous, and mixed random variables. Dirac Delta Function Remember , we cannot define the PDF for a discrete random variable because its CDF has jumps. If we could somehow dif ferentiate the CDF at jump points, we would be able to define the PDF for discrete random variables as well. This is the idea behind our ef fort in this section. Here, we will introduce the Dirac delta function and discuss its application to probability distributions. If you are less interested in the derivations, you may directly jump to Definition 4.3 and continue from there. Consider the unit step function u(x) defined by u(x) = { 1 x ≥ 0 0 otherwise (4.8) This function has a jump at x = 0. Let us remove the jump and define, for any α > 0, the function uα as uα(x) = ⎧⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪⎩ 1 x > (x + ) − ≤ x ≤ 0 x < − The good thing about uα(x) is that it is a continuous function. Now let us define the function δα(x) as the derivative of uα(x) wherever it exists. δα(x) = = { |x| < 0 |x| > α 2 1 α α 2 α 2 α 2 α 2 duα(x) dx 1 α α 2 α 2 Figure 4.10 shows these functions. Fig.4.10 - Functions u(x), uα(x), and δα(x). We notice the following relations: δα(x) = uα(x), u(x) = lim α→0 uα(x) (4.9) Now , we would like to define the delta \"function\" , δ(x), as δ(x) = lim α→0 δα(x) (4.10) Note that as α becomes smaller and smaller , the height of δα(x) becomes larger and larger and its width becomes smaller and smaller . Taking the limit, we obtain δ(x) = { ∞ x = 0 0 otherwise Combining Equations 4.9 and 4.10, we would like to symbolically write δ(x) = u(x). Intuitively , when we are using the delta function, we have in mind δα(x) with extremely small α. In particular , we would like to have the following definitions. Let g : R ↦ R be a continuous function. W e define d dx d dx ∫ ∞ −∞ g(x)δ(x − x0)dx = lim α→0 [ ∫ ∞ −∞ g(x)δα(x − x0)dx] (4.11) Then, we have the following lemma, which in fact is the most useful property of the delta function. Lemma 4. 1 Let g : R ↦ R be a continuous function. W e have ∫ ∞ −∞ g(x)δ(x − x0)dx = g(x0). Proof Let I be the value of the above integral. Then, we have I = limα→0 [ ∫ ∞ −∞ g(x)δα(x − x0)dx] = limα→0 [ ∫ x0+ x0− dx]. By the mean value theorem in calculus, for any α > 0, we have ∫ x0+ x0− dx = α = g(xα), for some xα ∈ (x0 − , x0 + ). Thus, we have I = lim α→0 g(xα) = g(x0). The last equality holds because g(x) is a continuous function and limα→0 xα = x0. For example, if we let g(x) = 1 for all x ∈ R, we obtain ∫ ∞ −∞ δ(x)dx = 1. It is worth noting that the Dirac δ function is not strictly speaking a valid function. The reason is that there is no function that can satisfy both of the conditions δ(x) = 0( for x ≠ 0) and ∫ ∞ −∞ δ(x)dx = 1. We can think of the delta function as a convenient notation for the integration condition 4.1 1. The delta function can also be developed formally as a generalized function. Now , let us summarize properties of the delta function. α 2 α 2 g(x) α α 2 α 2 g(x) α g(xα) α α 2 α 2 Definition 4. 3 : Properties of the delta function We define the delta function δ(x) as an object with the following properties: 1 . δ(x) = { ∞ x = 0 0 otherwise 2 . δ(x) = u(x), where u(x) is the unit step function (Equation 4.8); 3 . ∫ ϵ −ϵ δ(x)dx = 1, for any ϵ > 0; 4 . For any ϵ > 0 and any function g(x) that is continuous over (x0 − ϵ, x0 + ϵ), we have ∫ ∞ −∞ g(x)δ(x − x0)dx = ∫ x0+ϵ x0−ϵ g(x)δ(x − x0)dx = g(x0). Figure 4.1 1 shows how we represent the delta function. The delta function, δ(x), is shown by an arrow at x = 0. The height of the arrow is equal to 1. If we want to represent 2δ(x), the height would be equal to 2. In the figure, we also show the function δ(x − x0), which is the shifted version of δ(x). Fig.4.1 1 - Graphical representation of delta function. Using the Delta Function in PDFs of Discrete and Mixed Random V ariables In this section, we will use the delta function to extend the definition of the PDF to discrete and mixed random variables. Consider a discrete random variable X with range RX = {x1, x2, x3, . . . } and PMF PX(xk). Note that the CDF for X can be written as FX(x) = ∑ xk∈RX PX(xk)u(x − xk). Now that we have symbolically defined the derivative of the step function as the delta function, we can write a PDF for X by \"dif ferentiating\" the CDF: d dx fX(x) = = ∑xk∈RX PX(xk) u(x − xk) = ∑xk∈RX PX(xk)δ(x − xk). We call this the generalized PDF . For a discrete random variable X with range RX = {x1, x2, x3, . . . } and PMF PX(xk), we define the (generalized) probability density function (PDF) as fX(x) = ∑ xk∈RX PX(xk)δ(x − xk). Note that for any xk ∈ RX, the probability of X = xk is given by the coef ficient of the corresponding δ function, δ(x − xk). It is useful to use the generalized PDF because all random variables have a generalized PDF , so we can use the same formulas for discrete, continuous, and mixed random variables. If the (generalized) PDF of a random variable can be written as the sum of delta functions, then X is a discrete random variable. If the PDF does not include any delta functions, then X is a continuous random variable. Finally , if the PDF has both delta functions and non-delta functions, then X is a mixed random variable. Nevertheless, the formulas for probabilities, expectation and variance are the same for all kinds of random variables. To see how this works, we will consider the calculation of the expected value of a discrete random variable. Remember that the expected value of a continuous random variable is given by EX = ∫ ∞ −∞ xfX(x)dx. Now suppose that I have a discrete random variable X. We can write EX = ∫ ∞ −∞ xfX(x)dx = ∫ ∞ −∞ x ∑xk∈RX PX(xk)δ(x − xk)dx = ∑xk∈RX PX(xk) ∫ ∞ −∞ xδ(x − xk)dx = ∑xk∈RX xkPX(xk) by the 4th property in Definition 4.3, which is the same as our original definition of expected value for discrete random dFX(x) dx d dx variables. Let us practice these concepts by looking at an example. Example 4. 16 Let X be a random variable with the following CDF: FX(x) = ⎧⎪ ⎪ ⎨ ⎪ ⎪⎩ + (1 − e−x) x ≥ 1 + (1 − e−x) 0 ≤ x < 1 0 x < 0 a . What kind of random variable is X (discrete, continuous, or mixed)? b . Find the (generalized) PDF of X. c. Find P(X > 0.5), both using the CDF and using the PDF . d. Find EX and V ar (X). Solution a . Let us plot FX(x) to better understand the problem. Figure 4.12 shows FX(x). We see that the CDF has two jumps, at x = 0 and x = 1. The CDF increases continuously from x = 0 to x = 1 and also after x = 1. Since the CDF is neither in the form of a staircase function, nor is it continuous, we conclude that X is a mixed random variable. Fig.4.1 1 - The CDF of X in Example 4.16. b . To find the PDF , we need to dif ferentiate the CDF . We must be careful about the points of discontinuity . In particular , we have two jumps: one at x = 0 and one at x = 1. The size of the jump for both points is equal to . Thus, the CDF has two delta functions: δ(x) + δ(x − 1). The continuous part of the CDF can be written as (1 − e −x), for x > 0. Thus, we conclude fX(x) = δ(x) + δ(x − 1) + e −xu(x). 1 2 1 2 1 4 1 2 1 4 1 4 1 4 1 2 1 4 1 4 1 2 c. Using the CDF , we haveP(X > 0.5) = 1 − FX(0.5) = 1 − [ + (1 − e−x)] = + e −0.5 = 0.5533 Using The PDF , we can write P(X > 0.5) = ∫ ∞ 0.5 fX(x)dx = ∫ ∞ 0.5 ( δ(x) + δ(x − 1) + e −xu(x))dx = 0 + + ∫ ∞ 0.5 e −xdx (using Property 3 in Definition 4.3) = + e −0.5 = 0.5533 d. We have EX = ∫ ∞ −∞ xfX(x)dx = ∫ ∞ −∞ ( xδ(x) + xδ(x − 1) + xe −xu(x))dx = × 0 + × 1 + ∫ ∞ 0 xe −xdx (using Property 4 in Definition 4.3) = + × 1 = . Note that here ∫ ∞ 0 xe −xdx is just the expected value of an Exponential(1) random variable, which we know is equal to 1. EX2 = ∫ ∞ −∞ x 2fX(x)dx = ∫ ∞ −∞ ( x 2δ(x) + x 2δ(x − 1) + x 2e −xu(x))dx = × 0 + × 1 + ∫ ∞ 0 x 2e −xdx (using Property 4 in Definition 4.3) = + × 2 = . Again, note that ∫ ∞ 0 x 2e −xdx is just EX2 for an Exponential(1) random variable, which we know is equal to 2. Thus, Var(X) = EX2 − (EX) 2 = − ( )2 = . 1 4 1 2 1 4 1 2 1 4 1 4 1 2 1 4 1 2 1 4 1 2 1 4 1 4 1 2 1 4 1 4 1 2 1 4 1 2 3 4 1 4 1 4 1 2 1 4 1 4 1 2 1 4 1 2 5 4 5 4 3 4 11 16 In general, we can make the following statement: The (generalized) PDF of a mixed random variable can be written in the form fX(x) = ∑ k akδ(x − xk) + g(x), where ak = P(X = xk), and g(x) ≥ 0 does not contain any delta functions. Furthermore, we have ∫ ∞ −∞ fX(x)dx = ∑ k ak + ∫ ∞ −∞ g(x)dx = 1. 4.3.3 Solved Problems: Mixed Random V ariables Problem 1 Here is one way to think about a mixed random variable. Suppose that we have a discrete random variable Xd with (generalized) PDF and CDF fd(x) and Fd(x), and a continuous random variable Xc with PDF and CDF fc(x) and Fc(x). Now we create a new random variable X in the following way . W e have a coin with P(H) = p. W e toss the coin once. If it lands heads, then the value of X is determined according to the probability distribution of Xd. If the coin lands tails, the value of X is determined according to the probability distribution of Xc. a . Find the CDF of X, FX(x). b . Find the PDF of X, fX(x). c. Find EX. d. Find V ar (X). Solution a . Find the CDF of X, FX(x): We can write FX(x) = P(X ≤ x) = P(X ≤ x|H)P(H) + P(X ≤ x|T)P(T)(law of total probability) = pP(Xd ≤ x) + (1 − p)P(Xc ≤ x) = pFd(x) + (1 − p)Fc(x). b . Find the PDF of X, fX(x): By dif ferentiating FX(x), we obtain fX(x) = = pfd(x) + (1 − p)fc(x). c. Find EX: We have EX = ∫ ∞ −∞ xfX(x)dx = p ∫ ∞ −∞ xfd(x)dx + (1 − p) ∫ ∞ −∞ xfc(x)dx = pEXd + (1 − p)EXc. dFX(x) dx d. Find V ar (X): EX2 = ∫ ∞ −∞ x 2fX(x)dx = p ∫ ∞ −∞ x 2fd(x)dx + (1 − p) ∫ ∞ −∞ x 2fc(x)dx = pEX2 d + (1 − p)EX2 c . Thus, Var(X) = EX2 − (EX) 2 = pEX2 d + (1 − p)EX2 c − (pEXd + (1 − p)EXc)2 = pEX2 d + (1 − p)EX2 c − p2(EXd)2 − (1 − p)2(EXc)2 − 2p(1 − p)EXdEXc = p(EX2 d − (EXd)2) + (1 − p)(EX2 c − (EXc)2) + p(1 − p)(EXd − EXc)2 = pVar(Xd) + (1 − p)Var(Xc) + p(1 − p)(EXd − EXc) 2. Problem 2 Let X be a random variable with CDF FX(x) = ⎧⎪ ⎪ ⎨ ⎪ ⎪⎩ 1 x ≥ 1 + 0 ≤ x < 1 0 x < 0 a . What kind of random variable is X: discrete, continuous, or mixed? b . Find the PDF of X, fX(x). c. Find E(e X). d. Find P(X = 0|X ≤ 0.5). Solution a . What kind of random variable is X: discrete, continuous, or mixed? W e note that the CDF has a discontinuity at x = 0, and it is continuous at other points. Since FX(x) is not flat in other locations, we conclude X is a mixed random variable. Indeed, we can write FX(x) = u(x) + FY (x), where Y is a Uniform(0, 1) random variable. If we use the interpretation of Problem 1, we can say the following. W e toss a fair coin. If it lands heads then X = 0, otherwise X is obtained according the a Uniform(0, 1) distribution. 1 2 x 2 1 2 1 2 b . Find the PDF of X, fX(x): By dif ferentiating the CDF , we obtain fX(x) = δ(x) + fY (x), where fY (x) is the PDF of Uniform(0, 1), i.e., fY (x) = { 1 0 < x < 1 0 otherwise c. Find E(e X): We can use LOTUS to write E(e X) = ∫ ∞ −∞ e xfX(x)dx = ∫ ∞ −∞ e xδ(x)dx + ∫ ∞ −∞ e xfY (x)dx = e 0 + ∫ 1 0 e xdx = + (e − 1) = e. Here is another way to think about this part: similar to part (c) of Problem 1, we can write E(e X) = × e 0 + E[e Y ] = + ∫ 1 0 e ydy = e. d. Find P(X = 0|X ≤ 0.5): We have P(X = 0|X ≤ 0.5) = = = = = . Problem 3 Let X be a Uniform(−2, 2) continuous random variable. W e define Y = g(X), where the function g(x) is defined as 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 P(X=0,X≤0.5) P(X≤0.5) P(X=0) P(X≤0.5) 0.5 ∫ 0.5 0 fX(x)dx 0.5 0.75 2 3 g(x) = ⎧⎪ ⎨ ⎪⎩ 1 x > 1 x 0 ≤ x ≤ 1 0 otherwise Find the CDF and PDF of Y . Solution Note that RY = [0, 1]. Therefore, FY (y) = 0, for y < 0, FY (y) = 1, for y ≥ 1. We also note that P(Y = 0) = P(X < 0) = , P(Y = 1) = P(X > 1) = . Also for 0 < y < 1, FY (y) = P(Y ≤ y) = P(X ≤ y) = FX(y) = . Thus, the CDF of Y is given by FY (y) = ⎧⎪ ⎪ ⎨ ⎪ ⎪⎩ 1 y ≥ 1 0 ≤ y < 1 0 otherwise In particular , we note that there are two jumps in the CDF , one at y = 0 and another at y = 1. W e can find the generalized PDF of Y by dif ferentiating FY (y): fY (y) = δ(y) + δ(y − 1) + (u(y) − u(y − 1)). 1 2 1 4 y + 2 4 y+2 4 1 2 1 4 1 4 4.4 End of Chapter Problems Problem 1 Choose a real number uniformly at random in the interval [2, 6] and call it X. a . Find the CDF of X, FX(x). b . Find EX. Problem 2 Let X be a continuous random variable with the following PDF fX(x) = { ce−4x x ≥ 0 0 otherwise where c is a positive constant. a . Find c. b . Find the CDF of X, FX(x). c. Find P(2 < X < 5). d. Find EX. Problem 3 Let X be a continuous random variable with PDF fX(x) = { x 2 + 0 ≤ x ≤ 1 0 otherwise a . Find E(Xn), for n = 1, 2, 3, ⋯. b . Find the variance of X. Problem 4 Let X be a uniform(0, 1) random variable, and let Y = e −X. a . Find the CDF of Y . 2 3 b . Find the PDF of Y . c. Find EY . Problem 5 Let X be a continuous random variable with PDF fX(x) = { x 4 0 < x ≤ 2 0 otherwise and let Y = X2. a . Find the CDF of Y . b . Find the PDF of Y . c. Find EY . Problem 6 Let X ∼ Exponential(λ), and Y = aX, where a is a positive real number . Show that Y ∼ Exponential ( ) . Problem 7 Let X ∼ Exponential(λ). Show that a . EXn = EXn−1, for n = 1, 2, 3, ⋯; b . EXn = , for n = 1, 2, 3, ⋯. Problem 8 Let X ∼ N(3, 9). a . Find P(X > 0). b . Find P(−3 < X < 8). c. Find P(X > 5|X > 3). Problem 9 5 32 λ a n λ n! λn Let X ∼ N(3, 9) and Y = 5 − X. a . Find P(X > 2). b . Find P(−1 < Y < 3). c. Find P(X > 4|Y < 2). Problem 10 Let X be a continuous random variable with PDF fX(x) = e − for all x ∈ R. and let Y = √|X|. Find fY (y). Problem 1 1 Let X ∼ Exponential(2) and Y = 2 + 3X. a . Find P(X > 2). b . Find EY and V ar(Y ). c. Find P(X > 2|Y < 11). Problem 12 The median of a continuous random variable X can be defined as the unique real number m that satisfies P(X ≥ m) = P(X < m) = . Find the median of the following random variables a . X ∼ Uniform(a, b). b . Y ∼ Exponential(λ). c. W ∼ N(μ, σ2). Problem 13 Let X be a random variable with the following CDF 1 √2π x2 2 1 2 FX(x) = ⎧⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪⎩ 0 for x < 0 x for 0 ≤ x < x + for ≤ x < 1 for x ≥ a . Plot FX(x) and explain why X is a mixed random variable. b . Find P(X ≤ ). c. Find P(X ≥ ). d. Write CDF of X in the form of FX(x) = C(x) + D(x), where C(x) is a continuous function and D(x) is in the form of a staircase function, i.e., D(x) = ∑ k aku(x − xk). e. Find c(x) = C(x). f. Find EX using EX = ∫ ∞ −∞ xc(x)dx + ∑k xkak Problem 14 Let X be a random variable with the following CDF FX(x) = ⎧⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪⎩ 0 for x < 0 x for 0 ≤ x < x + for ≤ x < 1 for x ≥ a . Find the generalized PDF of X, fX(x). b . Find EX using fX(x). c. Find V ar(X) using fX(x). Problem 15 Let X be a mixed random variable with the following generalized PDF fX(x) = δ(x + 2) + δ(x − 1) + e − . 1 4 1 2 1 4 1 2 1 2 1 3 1 4 d dx 1 4 1 2 1 4 1 2 1 2 1 3 1 6 1 2 1 √2π x2 2 a . Find P(X = 1) and P(X = −2). b . Find P(X ≥ 1). c. Find P(X = 1|X ≥ 1). d. Find EX and V ar(X). Problem 16 A company makes a certain device. W e are interested in the lifetime of the device. It is estimated that around 2% of the devices are defective from the start so they have a lifetime of 0 years. If a device is not defective, then the lifetime of the device is exponentially distributed with a parameter λ = 2 years. Let X be the lifetime of a randomly chosen device. a . Find the generalized PDF of X. b . Find P(X ≥ 1). c. Find P(X > 2|X ≥ 1). d. Find EX and V ar(X). Problem 17 A continuous random variable is said to have a Laplace(μ, b) distribution [ 14 ] if its PDF is given by fX(x) = exp(− ) = ⎧⎪ ⎨ ⎪⎩ exp( ) if  x < μ exp(− ) if  x ≥ μ where μ ∈ R and b > 0. a . If X ∼ Laplace(0, 1), find EX and V ar(X). b . If X ∼ Laplace(0, 1) and Y = bX + μ, show that Y ∼ Laplace(μ, b). c. Let Y ∼ Laplace(μ, b), where μ ∈ R and b > 0. Find EY and V ar(Y ). Problem 18 Let X ∼ Laplace(0, b), i.e., 1 2b |x − μ| b 1 2b x−μ b 1 2b x−μ b fX(x) = exp(− ), where b > 0. Define Y = |X|. Show that Y ∼ Exponential ( ). Problem 19 A continuous random variable is said to have the standard Cauchy distribution if its PDF is given by fX(x) = . If X has a standard Cauchy distribution, show that EX is not well-defined. Also, show EX2 = ∞. Problem 20 A continuous random variable is said to have a Rayleigh distribution with parameter σ if its PDF is given by fX(x) = e −x2/2σ2 u(x) = { e −x2/2σ2 if x ≥ 0 0 if x < 0 where σ > 0. a . If X ∼ Rayleigh(σ), find EX. b . If X ∼ Rayleigh(σ), find the CDF of X, FX(x). c. If X ∼ Exponential(1) and Y = √2σ2X, show that Y ∼ Rayleigh(σ). Problem 21 A continuous random variable is said to have a Pareto(xm, α) distribution [ 15 ] if its PDF is given by fX(x) = { α for x ≥ xm 0 for x < xm where xm, α > 0. Let X ∼ Pareto(xm, α). 1 2b |x| b 1 b 1 π(1 + x2) x σ2 x σ2 x α m xα+1 a . Find the CDF of X, FX(x). b . Find P(X > 3xm|X > 2xm). c. If α > 2, find EX and V ar(X). Problem 22 Let Z ∼ N(0, 1). If we define X = e σZ+μ, then we say that X has a log-normal distribution with parameters μ and σ, and we write X ∼ LogNormal(μ, σ). a . If X ∼ LogNormal(μ, σ), find the CDF of X in terms of the Φ function. b . Find EX and V ar(X). Problem 23 Let X1, X2, ⋯, Xn be independent random variables with Xi ∼ Exponential(λ). Define Y = X1 + X2 + ⋯ + Xn. As we will see later , Y has a Gamma distribution with parameters n and λ, i.e., Y ∼ Gamma(n, λ). Using this, show that if Y ∼ Gamma(n, λ), then EY = and V ar(Y ) = . n λ n λ2 5.1.0 Joint Distributions: T wo Random V ariables In real life, we are often interested in several random variables that are related to each other . For example, suppose that we choose a random family , and we would like to study the number of people in the family , the household income, the ages of the family members, etc. Each of these is a random variable, and we suspect that they are dependent. In this chapter , we develop tools to study joint distributions of random variables. The concepts are similar to what we have seen so far . The only dif ference is that instead of one random variable, we consider two or more. In this chapter , we will focus on two random variables, but once you understand the theory for two random variables, the extension to n random variables is straightforward. W e will first discuss joint distributions of discrete random variables and then extend the results to continuous random variables. 5.1.1 Joint Probability Mass Function (PMF) Remember that for a discrete random variable X, we define the PMF as PX(x) = P(X = x). Now , if we have two random variables X and Y , and we would like to study them jointly , we define the joint probability mass function as follows: The joint probability mass function of two discrete random variables X and Y is defined as PXY (x, y) = P(X = x, Y = y). Note that as usual, the comma means \"and,\" so we can write PXY (x, y) = P(X = x, Y = y) = P((X = x) and (Y = y)). We can define the joint range for X and Y as RXY = {(x, y)|PXY (x, y) > 0}. In particular , if RX = {x1, x2, . . . } and RY = {y1, y2, . . . }, then we can always write RXY ⊂ RX × RY = {(xi, yj)|xi ∈ RX, yj ∈ RY }. In fact, sometimes we define RXY = RX × RY to simplify the analysis. In this case, for some pairs (xi, yj) in RX × RY , PXY (xi, yj) might be zero. For two discrete random variables X and Y , we have ∑ (xi,yj)∈RXY PXY (xi, yj) = 1 We can use the joint PMF to find P((X, Y ) ∈ A) for any set A ⊂ R2. Specifically , we have P((X, Y ) ∈ A) = ∑ (xi,yj)∈(A∩RXY ) PXY (xi, yj) Note that the event X = x can be written as {(xi, yj) : xi = x, yj ∈ RY }. Also, the event Y = y can be written as {(xi, yj) : xi ∈ RX, yj = y}. Thus, we can write PXY (x, y) = P(X = x, Y = y) = P((X = x) ∩ (Y = y)). Marginal PMFs The joint PMF contains all the information regarding the distributions of X and Y . This means that, for example, we can obtain PMF of X from its joint PMF with Y . Indeed, we can write PX(x) = P(X = x) = ∑ yj∈RY P(X = x, Y = yj) law of total probablity = ∑ yj∈RY PXY (x, yj). Here, we call PX(x) the marginal PMF of X. Similarly , we can find the marginal PMF of Y as PY (Y ) = ∑ xi∈RX PXY (xi, y). Marginal PMFs of X and Y : PX(x) = ∑ yj∈RY PXY (x, yj),  for any x ∈ RX PY (y) = ∑ xi∈RX PXY (xi, y),  for any y ∈ RY (5.1) Let's practice these concepts by looking at an example. Example 5. 1 Consider two random variables X and Y with joint PMF given in T able 5.1. Table 5.1 Joint PMF of X and Y in Example 5.1   Y = 0 Y = 1 Y = 2 X = 0 X = 1 Figure 5.1 shows PXY (x, y). Figure 5.1: Joint PMF of X and Y (Example 5.1). 1 6 1 4 1 8 1 8 1 6 1 6 a . Find P(X = 0, Y ≤ 1). b . Find the marginal PMFs of X and Y . c. Find P(Y = 1|X = 0). d. Are X and Y independent? Solution a . To find P(X = 0, Y ≤ 1), we can write P(X = 0, Y ≤ 1) = PXY (0, 0) + PXY (0, 1) = + = . b . Note that from the table, RX = {0, 1}  and  RY = {0, 1, 2}. Now we can use Equation 5.1 to find the marginal PMFs. For example, to find PX(0), we can write PX(0) = PXY (0, 0) + PXY (0, 1) + PXY (0, 2) = + + = . We obtain PX(x) = ⎧⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪⎩ x = 0 x = 1 0 otherwise PY (y) = ⎧⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪⎩ y = 0 y = 1 y = 2 0 otherwise c. Find P(Y = 1|X = 0): Using the formula for conditional probability , we have 1 6 1 4 5 12 1 6 1 4 1 8 13 24 13 24 11 24 7 24 5 12 7 24 P(Y = 1|X = 0) = = = = . d. Are X and Y independent? X and Y are not independent, because as we just found out P(Y = 1|X = 0) = ≠ P(Y = 1) = . Caution : If we want to show that X and Y are independent, we need to check that P(X = xi, Y = yj) = P(X = xi)P(Y = yj), for all xi ∈ RX and all yj ∈ RY . Thus, even if in the above calculation we had found P(Y = 1|X = 0) = P(Y = 1), we would not yet have been able to conclude that X and Y are independent. For that, we would need to check the independence condition for all xi ∈ RX and all yj ∈ RY . P(X = 0, Y = 1) P(X = 0) PXY (0, 1) PX(0) 1 4 13 24 6 13 6 13 5 12 5.1.2 Joint Cumulative Distributive Function (CDF) Remember that, for a random variable X, we define the CDF as FX(x) = P(X ≤ x). Now , if we have two random variables X and Y and we would like to study them jointly , we can define the joint cumulative function as follows: The joint cumulative distribution function of two random variables X and Y is defined as FXY (x, y) = P(X ≤ x, Y ≤ y). As usual, comma means \"and,\" so we can write FXY (x, y) = P(X ≤ x, Y ≤ y) = P((X ≤ x) and (Y ≤ y)) = P((X ≤ x) ∩ (Y ≤ y)). Figure 5.2 shows the region associated with FXY (x, y) in the two-dimensional plane. Note that the above definition of joint CDF is a general definition and is applicable to discrete, continuous, and mixed random variables. Since the joint CDF refers to the probability of an event, we must have 0 ≤ FXY (x, y) ≤ 1. Figure 5.2: FXY (x, y) is the probability that (X, Y ) belongs to the shaded region. The dots are the pairs (xi, yj) in RXY . If we know the joint CDF of X and Y , we can find the marginal CDFs, FX(x) and FY (y) . Specifically , for any x ∈ R, we have FXY (x, ∞) = P(X ≤ x, Y ≤ ∞) = P(X ≤ x) = FX(x). Here, by FXY (x, ∞), we mean lim y→∞ FXY (x, y). Similarly , for any y ∈ R, we have FY (y) = FXY (∞, y). Marginal CDFs of X and Y : FX(x) = FXY (x, ∞) = lim y→∞ FXY (x, y),  for any x, FY (y) = FXY (∞, y) = lim x→∞ FXY (x, y),  for any y (5.2) Also, note that we must have FXY (∞, ∞) = 1, FXY (−∞, y) = 0,  for any y, FXY (x, −∞) = 0,  for any x. Example 5. 2 Let X ∼ Bernoulli(p) and Y ∼ Bernoulli(q) be independent, where 0 < p, q < 1. Find the joint PMF and joint CDF for X and Y . Solution First note that the joint range of X and Y is given by RXY = {(0, 0), (0, 1), (1, 0), (1, 1)}. Since X and Y are independent, we have PXY (i, j) = PX(i)PY (j), for i, j = 0, 1. Thus, we conclude PXY (0, 0) = PX(0)PY (0) = (1 − p)(1 − q), PXY (0, 1) = PX(0)PY (1) = (1 − p)q, PXY (1, 0) = PX(1)PY (0) = p(1 − q), PXY (1, 1) = PX(1)PY (1) = pq. Now that we have the joint PMF , we can find the joint CDF FXY (x, y) = P(X ≤ x, Y ≤ y). Specifically , since 0 ≤ X, Y ≤ 1, we conclude FXY (x, y) = 0, if x < 0, FXY (x, y) = 0, if y < 0, FXY (x, y) = 1, if x ≥ 1 and y ≥ 1. Now , for 0 ≤ x < 1 and y ≥ 1, we have FXY (x, y) = P(X ≤ x, Y ≤ y) = P(X = 0, y ≤ 1) = P(X = 0) = 1 − p. Similarly , for 0 ≤ y < 1 and x ≥ 1, we have FXY (x, y) = P(X ≤ x, Y ≤ y) = P(X ≤ 1, y = 0) = P(Y = 0) = 1 − q. Finally , for 0 ≤ x < 1 and 0 ≤ y < 1, we have FXY (x, y) = P(X ≤ x, Y ≤ y) = P(X = 0, y = 0) = P(X = 0)P(Y = 0) = (1 − p)(1 − q). Figure 5.3 shows the values of FXY (x, y) in dif ferent regions of the two-dimensional plane. Note that, in general, we actually need a three-dimensional graph to show a joint CDF of two random variables, i.e., we need three axes: x, y, and z = FXY (x, y). However , because the random variables of this example are simple, and can take only two values, a two-dimensional figure suf fices. Figure 5.3 Joint CDF for X and Y in Example 5.2 Here is a useful lemma: Lemma 5. 1 For two random variables X and Y , and real numbers x1 ≤ x2, y1 ≤ y2, we have P(x1 < X ≤ x2, y1 < Y ≤ y2) = FXY (x2, y2) − FXY (x1, y2) − FXY (x2, y1) + FXY (x1, y1). To see why the above formula is true, you can look at the region associated with FXY (x, y) (as shown in Figure 5.2) for each of the pairs (x2, y2), (x1, y2), (x2, y1), (x1, y1). You can see, as we subtract and add regions, the part that is left is the region {x1 < X ≤ x2, y1 < Y ≤ y2}. 5.1.3 Conditioning and Independence We have discussed conditional probability before, and you have already seen some problems regarding random variables and conditional probability . Here, we will discuss conditioning for random variables more in detail and introduce the conditional PMF , conditional CDF , and conditional expectation. W e would like to emphasize that there is only one main formula regarding conditional probability which is P(A|B) = ,  when P(B) > 0. Any other formula regarding conditional probability can be derived from the above formula. Specifically , if you have two random variables X and Y , you can write P(X ∈ C|Y ∈ D) = ,  where C, D ⊂ R. Conditional PMF and CDF: Remember that the PMF is by definition a probability measure, i.e., it is P(X = xk). Thus, we can talk about the conditional PMF . Specifically , the conditional PMF of X given event A, is defined as PX|A(xi) = P(X = xi|A) = . Example 5. 3 I roll a fair die. Let X be the observed number . Find the conditional PMF of X given that we know the observed number was less than 5. Solution Here, we condition on the event A = {X < 5}, where P(A) = . Thus, P(A ∩ B) P(B) P(X ∈ C, Y ∈ D) P(Y ∈ D) P(X = xi and A) P(A) 4 6 PX|A(1) = P(X = 1|X < 5) = = = . Similarly , we have PX|A(2) = PX|A(3) = PX|A(4) = . Also, PX|A(5) = PX|A(6) = 0. For a discrete random variable X and event A, the conditional PMF of X given A is defined as PX|A(xi) = P(X = xi|A) = , for any xi ∈ RX. Similarly , we define the conditional CDF of X given A as FX|A(x) = P(X ≤ x|A). Conditional PMF of X Given Y : In some problems, we have observed the value of a random variable Y , and we need to update the PMF of another random variable X whose value has not yet been observed. In these problems, we use the conditional PMF of X given Y . The conditional PMF of X given Y is defined as P(X = 1 and X < 5) P(X < 5) P(X = 1) P(X < 5) 1 4 1 4 P(X = xi and A) P(A) PX|Y (xi|yj) = P(X = xi|Y = yj) = = . Similarly , we can define the conditional probability of Y given X: PY |X(yj|xi) = P(Y = yj|X = xi) = . For discrete random variables X and Y , the conditional PMFs of X given Y and vice versa are defined as PX|Y (xi|yj) = , PY |X(yj|xi) = for any xi ∈ RX and yj ∈ RY . Independent Random V ariables: We have defined independent random variables previously . Now that we have seen joint PMFs and CDFs, we can restate the independence definition. Two discrete random variables X and Y are independent if PXY (x, y) = PX(x)PY (y),  for all x, y. Equivalently , X and Y are independent if FXY (x, y) = FX(x)FY (y),  for all x, y. P(X = xi, Y = yj) P(Y = yj) PXY (xi, yj) PY (yj) PXY (xi, yj) PX(xi) PXY (xi, yj) PY (yj) PXY (xi, yj) PX(xi) Figure 5.4: Grid for example 5.4 So, if X and Y are independent, we have PX|Y (xi|yj) = P(X = xi|Y = yj) = = = PX(xi). As we expect, for independent random variables, the conditional PMF is equal to the marginal PMF . In other words, knowing the value of Y does not provide any information about X. Example 5. 4 Consider the set of points in the grid shown in Figure 5.4. These are the points in set G defined as G = {(x, y)|x, y ∈ Z, |x| + |y| ≤ 2}. Suppose that we pick a point (X, Y ) from this grid completely at random. Thus, each point has a probability of of being chosen. PXY (xi, yj) PY (yj) PX(xi)PY (yj) PY (yj) 1 13 a . Find the joint and marginal PMFs of X and Y . b . Find the conditional PMF of X given Y = 1. c. Are X and Y independent? Solution a . Here, note that RXY = G = {(x, y)|x, y ∈ Z, |x| + |y| ≤ 2}. Thus, the joint PMF is given by PXY (x, y) = { (x, y) ∈ G 0 otherwise To find the marginal PMF of X, PX(i), we use Equation 5.1. Thus, PX(−2) = PXY (−2, 0) = , PX(−1) = PXY (−1, −1) + PXY (−1, 0) + PXY (−1, 1) = , PX(0) = PXY (0, −2) + PXY (0, −1) + PXY (0, 0) + PXY (0, 1) + PXY (0, 2) = , PX(1) = PXY (1, −1) + PXY (1, 0) + PXY (1, −1) = , PX(2) = PXY (2, 0) = . Similarly , we can find PY (j) = ⎧⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪⎩ for j = 2, −2 for j = −1, 1 for j = 0 0 otherwise We can write this in a more compact form as PX(k) = PY (k) = ,  for k = −2, −1, 0, 1, 2. b . For i = −1, 0, 1, we can write 1 13 1 13 3 13 5 13 3 13 1 13 1 13 3 13 5 13 5 − 2|k| 13 PX|Y (i|1) = = = ,  for i = −1, 0, 1. Thus, we conclude PX|Y (i|1) = {  for i = −1, 0, 1 0 otherwise By looking at the above conditional PMF , we conclude that, given Y = 1, X is uniformly distributed over the set {−1, 0, 1}. c. X and Y are not independent. W e can see this as the conditional PMF of X given Y = 1 (calculated above) is not the same as marginal PMF of X, PX(x). Conditional Expectation: Given that we know event A has occurred, we can compute the conditional expectation of a random variable X, E[X|A]. Conditional expectation is similar to ordinary expectation. The only dif ference is that we replace the PMF by the conditional PMF. Specifically , we have E[X|A] = ∑ xi∈RX xiPX|A(xi). Similarly , given that we have observed the value of random variable Y , we can compute the conditional expectation of X. Specifically , the conditional expectation of X given that Y = y is E[X|Y = y] = ∑ xi∈RX xiPX|Y (xi|y). PXY (i, 1) PY (1) 1 13 3 13 1 3 1 3 Conditional Expectation of X: E[X|A] = ∑ xi∈RX xiPX|A(xi), E[X|Y = yj] = ∑ xi∈RX xiPX|Y (xi|yj) Example 5. 5 Let X and Y be the same as in Example 5.4. a . Find E[X|Y = 1|. b . Find E[X| − 1 < Y < 2]. c. Find E[|X|| − 1 < Y < 2]. Solution a . To find E[X|Y = 1], we have E[X|Y = 1] = ∑ xi∈RX xiPX|Y (xi|1). We found in Example 5.4 that given Y = 1, X is uniformly distributed over the set {−1, 0, 1}. Thus, we conclude that E[X|Y = 1] = (−1 + 0 + 1) = 0. b . To find E[X| − 1 < Y < 2], let A be the event that −1 < Y < 2, i.e., Y ∈ {0, 1}. T o find E[X|A], we need to find the conditional PMF , PX|A(k), for k = −2, 1, 0, 1, 2. First, note that P(A) = PY (0) + PY (1) = + = . Thus, for k = −2, 1, 0, 1, 2, we have PX|A(k) = P(X = k, A). So, we can write 1 3 5 13 3 13 8 13 13 8 PX|A(−2) = P(X = −2, A) = PXY (−2, 0) = , PX|A(−1) = P(X = −1, A) = [PXY (−1, 0) + PXY (−1, 1)] = = , PX|A( 0 ) = P(X = 0, A) = [PXY (0, 0) + PXY (0, 1)] = = , PX|A( 1 ) = P(X = 1, A) = [PXY (1, 0) + PXY (1, 1)] = = , PX|A( 2 ) = P(X = 2, A) = PXY (2, 0) = . Thus, we have E[X|A] = ∑ xi∈RX xiPX|A(xi) = (−2) + (−1) + (0) + (1) + (2) = 0. c. To find E[|X|| − 1 < Y < 2], we use the conditional PMF and LOTUS. W e have E[|X||A] = ∑ xi∈RX |xi|PX|A(xi) = | − 2| ⋅ + | − 1| ⋅ + 0 ⋅ + 1 ⋅ + 2 ⋅ = 1. Conditional expectation has some interesting properties that are used commonly in practice. Thus, we will revisit conditional expectation in Section 5.1.5 , where we discuss properties of conditional expectation, conditional variance, and their applications. Law of T otal Probability: Remember the law of total probability: If B1, B2, B3, . . . is a partition of the sample space S, then for any event A we have 13 8 13 8 1 8 13 8 13 8 2 8 1 4 13 8 13 8 2 8 1 4 13 8 13 8 2 8 1 4 13 8 13 8 1 8 1 8 1 4 1 4 1 4 1 8 1 8 1 4 1 4 1 4 1 8 P(A) = ∑ i P(A ∩ Bi) = ∑ i P(A|Bi)P(Bi). If Y is a discrete random variable with range RY = {y1, y2, . . . }, then the events {Y = y1}, {Y = y2}, {Y = y3}, ⋯ form a partition of the sample space. Thus, we can use the law of total probability . In fact we have already used the law of total probability to find the marginal PMFs: PX(x) = ∑ yj∈RY PXY (x, yj) = ∑ yj∈RY PX|Y (x|yj)PY (yj). We can write this more generally as P(X ∈ A) = ∑ yj∈RY P(X ∈ A|Y = yj)PY (yj), for any set A. We can write a similar formula for expectation as well. Indeed, if B1, B2, B3, . . . is a partition of the sample space S, then EX = ∑ i E[X|Bi]P(Bi). To see this, just write the definition of E[X|Bi] and apply the law of total probability . The above equation is sometimes called the law of total expectation [ 2 ]. Law of T otal Probability: P(X ∈ A) = ∑ yj∈RY P(X ∈ A|Y = yj)PY (yj), for any set A. Law of T otal Expectation: 1 . If B1, B2, B3, . . . is a partition of the sample space S, EX = ∑ i E[X|Bi]P(Bi) (5.3) 2 . For a random variable X and a discrete random variable Y , EX = ∑ yj∈RY E[X|Y = yj]PY (yj) (5.4) Example 5. 6 Let X ∼ Geometric(p). Find EX by conditioning on the result of the first \"coin toss.\" Solution Remember that the random experiment behind Geometric(p) is that we have a coin with P(H) = p. We toss the coin repeatedly until we observe the first heads. X is the total number of coin tosses. Now , there are two possible outcomes for the first coin toss: H or T. Thus, we can use the law of total expectation (Equation 5.3): EX = E[X|H]P(H) + E[X|T]P(T) = pE[X|H] + (1 − p)E[X|T] = p ⋅ 1 + (1 − p)(EX + 1). In this equation, E[X|T] = 1 + EX, because the tosses are independent, so if the first toss is tails, it is like starting over on the second toss. Solving for EX, we obtain EX = . Example 5. 7 Suppose that the number of customers visiting a fast food restaurant in a given day is N ∼ Poisson(λ). Assume that each customer purchases a drink with probability p, independently from other customers and independently from the value of N. Let X be the number of customers who purchase drinks. Find EX. Solution By the above information, we conclude that given N = n, then X is a sum of n independent Bernoulli(p) random variables. Thus, given N = n, X has a binomial distribution with parameters n and p. W e write X|N = n ∼ Binomial(n, p). That is, PX|N (k|n) = ( )p k(1 − p) n−k. Thus, we conclude 1 p n k E[X|N = n] = np. Thus, using the law of total probability , we have E[X] = ∞ ∑ n=0 E[X|N = n]PN (n) = ∞ ∑ n=0 npPN (n) = p ∞ ∑ n=0 nPN (n) = pE[N] = pλ. 5.1.4 Functions of T wo Random V ariables Analysis of a function of two random variables is pretty much the same as for a function of a single random variable. Suppose that you have two discrete random variables X and Y , and suppose that Z = g(X, Y ), where g : R2 ↦ R. Then, if we are interested in the PMF of Z, we can write PZ(z) = P(g(X, Y ) = z) = ∑ (xi,yj)∈Az PXY (xi, yj),  where Az = {(xi, yj) ∈ RXY : g(xi, yj) = z}. Note that if we are only interested in E[g(X, Y )], we can directly use LOTUS, without finding PZ(z): Law of the unconscious statistician (LOTUS) for two discrete random variables: E[g(X, Y )] = ∑ (xi,yj)∈RXY g(xi, yj)PXY (xi, yj) (5.5) Example 5. 8 Linearity of Expectation: For two discrete random variables X and Y , show that E[X + Y ] = EX + EY . Solution Let g(X, Y ) = X + Y . Using LOTUS, we have E[X + Y ] = ∑ (xi,yj)∈RXY (xi + yj)PXY (xi, yj) = ∑ (xi,yj)∈RXY xiPXY (xi, yj) + ∑ (xi,yj)∈RXY yjPXY (xi, yj) = ∑ xi∈RX ∑ yj∈RY xiPXY (xi, yj) + ∑ xi∈RX ∑ yj∈RY yjPXY (xi, yj) = ∑ xi∈RX xi ∑ yj∈RY PXY (xi, yj) + ∑ yj∈RY yj ∑ xi∈RX PXY (xi, yj) = ∑ xi∈RX xiPX(xi) + ∑ yj∈RY yjPY (yj) (marginal PMF (Equation 5.1)) = EX + EY . Example 5. 9 Let X and Y be two independent Geometric(p) random variables. Also let Z = X − Y . Find the PMF of Z. Solution First note that since RX = RY = N = {1, 2, 3, . . . }, we have RZ = Z = {. . . , −3, −2, −1, 0, 1, 2, 3, . . . }. Since X, Y ∼ Geometric(p), we have PX(k) = PY (k) = pqk−1,  for k = 1, 2, 3, . . . , where q = 1 − p. W e can write for any k ∈ Z PZ(k) = P(Z = k) = P(X − Y = k) = P(X = Y + k) = ∞ ∑ j=1 P(X = Y + k|Y = j)P(Y = j) (law of total probability) = ∞ ∑ j=1 P(X = j + k|Y = j)P(Y = j) = ∞ ∑ j=1 P(X = j + k)P(Y = j) (since X, Y are independent) = ∞ ∑ j=1 PX(j + k)PY (j). Now , consider two cases: k ≥ 0 and k < 0. If k ≥ 0, then PZ(k) = ∞ ∑ j=1 PX(j + k)PY (j) = ∞ ∑ j=1 pqj+k−1pqj−1 = p 2qk ∞ ∑ j=1 q2(j−1) = p 2qk (geometric sum (Equation 1.4 )) = . For k < 0, we have PZ(k) = ∞ ∑ j=1 PX(j + k)PY (j) = ∞ ∑ j=−k+1 pqj+k−1pqj−1 (since PX(j + k) = 0 for j < −k + 1) = p 2 ∞ ∑ j=−k+1 qk+2(j−1) = p 2[q−k + q−k+2 + q−k+4+. . . ] = p 2q−k[1 + q2 + q4+. . . ] = (geometric sum (Equation 1.4 )). To summarize, we conclude PZ(k) = ⎧ ⎨⎩ k ∈ Z 0 otherwise 1 1 − q2 p(1 − p) k 2 − p p (1 − p)k(2 − p) p(1−p)|k| 2−p 5.1.5 Conditional Expectation (Revisited) and Conditional V ariance In Section 5.1.3 , we briefly discussed conditional expectation. Here, we will discuss the properties of conditional expectation in more detail as they are quite useful in practice. W e will also discuss conditional variance. An important concept here is that we interpret the conditional expectation as a random variable. Conditional Expectation as a Function of a Random V ariable: Remember that the conditional expectation of X given that Y = y is given by E[X|Y = y] = ∑ xi∈RX xiPX|Y (xi|y). Note that E[X|Y = y] depends on the value of y. In other words, by changing y, E[X|Y = y] can also change. Thus, we can say E[X|Y = y] is a function of y, so let's write g(y) = E[X|Y = y]. Thus, we can think of g(y) = E[X|Y = y] as a function of the value of random variable Y . W e then write g(Y ) = E[X|Y ]. We use this notation to indicate that E[X|Y ] is a random variable whose value equals g(y) = E[X|Y = y] when Y = y. Thus, if Y is a random variable with range RY = {y1, y2, ⋯}, then E[X|Y ] is also a random variable with E[X|Y ] = ⎧⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪⎩ E[X|Y = y1] with probability P(Y = y1) E[X|Y = y2] with probability P(Y = y2) . . . . . . Let's look at an example. Example 5. 10 Let X = aY + b. Then E[X|Y = y] = E[aY + b|Y = y] = ay + b. Here, we have g(y) = ay + b, and therefore, E[X|Y ] = aY + b, which is a function of the random variable Y . Since E[X|Y ] is a random variable, we can find its PMF , CDF , variance, etc. Let's look at an example to better understand E[X|Y ]. Example 5. 1 1 Consider two random variables X and Y with joint PMF given in T able 5.2. Let Z = E[X|Y ]. a . Find the Marginal PMFs of X and Y . b . Find the conditional PMF of X given Y = 0 and Y = 1, i.e., find PX|Y (x|0) and PX|Y (x|1). c. Find the PM F of Z. d. Find EZ, and check that EZ = EX. e. Find V ar (Z). Table 5.2: Joint PMF of X and Y in example 5.1 1   Y = 0 Y = 1 X = 0 X = 1 0 Solution a . Using the table we find out 1 5 2 5 2 5 PX(0) = + = , PX(1) = + 0 = , PY (0) = + = , PY (1) = + 0 = . Thus, the marginal distributions of X and Y are both Bernoulli( ). However , note that X and Y are not independent. b . We have PX|Y (0|0) = = = . Thus, PX|Y (1|0) = 1 − = . We conclude X|Y = 0 ∼ Bernoulli ( ) . Similarly , we find PX|Y (0|1) = 1, PX|Y (1|1) = 0. Thus, given Y = 1, we have always X = 0. c. We note that the random variable Y can take two values: 0 and 1. Thus, the random variable Z = E[X|Y ] can take two values as it is a function of Y . Specifically , Z = E[X|Y ] = ⎧⎪ ⎨ ⎪⎩ E[X|Y = 0] if Y = 0 E[X|Y = 1] if Y = 1 Now , using the previous part, we have E[X|Y = 0] = , E[X|Y = 1] = 0, 1 5 2 5 3 5 2 5 2 5 1 5 2 5 3 5 2 5 2 5 2 5 PXY (0, 0) PY (0) 1 5 3 5 1 3 1 3 2 3 2 3 2 3 and since P(y = 0) = , and P(y = 1) = , we conclude that Z = E[X|Y ] = ⎧⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪⎩ with probability  0 with probability  So we can write PZ(z) = ⎧⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪⎩ if z = if z = 0 0 otherwise d. Now that we have found the PMF of Z, we can find its mean and variance. Specifically , E[Z] = ⋅ + 0 ⋅ = . We also note that EX = . Thus, here we have E[X] = E[Z] = E[E[X|Y ]]. In fact, as we will prove shortly , the above equality always holds. It is called the law of iterated expectations. e. To find V ar (Z), we write Var(Z) = E[Z 2] − (EZ) 2 = E[Z 2] − , where E[Z 2] = ⋅ + 0 ⋅ = . Thus, Var(Z) = − = . 3 5 2 5 2 3 3 5 2 5 3 5 2 3 2 5 2 3 3 5 2 5 2 5 2 5 4 25 4 9 3 5 2 5 4 15 4 15 4 25 8 75 Example 5. 12 Let X and Y be two random variables and g and h be two functions. Show that E[g(X)h(Y )|X] = g(X)E[h(Y )|X]. Solution Note that E[g(X)h(Y )|X] is a random variable that is a function of X. In particular , if X = x, then E[g(X)h(Y )|X] = E[g(X)h(Y )|X = x]. Now , we can write E[g(X)h(Y )|X = x] = E[g(x)h(Y )|X = x] = g(x)E[h(Y )|X = x] (since g(x) is a constant). Thinking of this as a function of the random variable X, it can be rewritten as E[g(X)h(Y )|X] = g(X)E[h(Y )|X]. This rule is sometimes called \"taking out what is known.\" The idea is that, given X, g(X) is a known quantity , so it can be taken out of the conditional expectation. E[g(X)h(Y )|X] = g(X)E[h(Y )|X] (5.6) Iterated Expectations: Let us look again at the law of total probability for expectation. Assuming g(Y ) = E[X|Y ], we have E[X] = ∑ yj∈RY E[X|Y = yj]PY (yj) = ∑ yj∈RY g(yj)PY (yj) = E[g(Y )] by LOTUS (Equation 5.2) = E[E[X|Y ]]. Thus, we conclude E[X] = E[E[X|Y ]]. (5.7) This equation might look a little confusing at first, but it is just another way of writing the law of total expectation (Equation 5.4). T o better understand it, let's solve Example 5.7 using this terminology . In that example, we want to find EX. W e can write E[X] = E[E[X|N]] = E[Np] (since X|N ∼ Binomial(N, p)) = pE[N] = pλ. Equation 5.7 is called the law of iterated expectations . Since it is basically the same as Equation 5.4, it is also called the law of total expectation [ 3 ]. Law of Iterated Expectations: E[X] = E[E[X|Y ]] Expectation for Independent Random V ariables: Note that if two random variables X and Y are independent, then the conditional PMF of X given Y will be the same as the marginal PMF of X, i.e., for any x ∈ RX, we have PX|Y (x|y) = PX(x). Thus, for independent random variables, we have E[X|Y = y] = ∑ x∈RX xPX|Y (x|y) = ∑ x∈RX xPX(x) = E[X]. Again, thinking of this as a random variable depending on Y , we obtain E[X|Y ] = E[X],  when X and Y  are independent. More generally , if X and Y are independent then any function of X, say g(X), and Y are independent, thus E[g(X)|Y ] = E[g(X)]. Remember that for independent random variables, PXY (x, y) = PX(x)PY (y). From this, we can show that E[XY ] = EXEY . Lemma 5. 2 If X and Y are independent, then E[XY ] = EXEY . Using LOTUS, we have E[XY ] = ∑ x∈Rx ∑ y∈Ry xyPXY (x, y) = ∑ x∈Rx ∑ y∈Ry xyPX(x)PY (y) = ( ∑ x∈Rx xPX(x))( ∑ y∈Ry yPY (y)) = EXEY . Note that the converse is not true. That is, if the only thing that we know about X and Y is that E[XY ] = EXEY , then X and Y may or may not be independent. Using essentially the same proof as above, we can show if X and Y are independent, then E[g(X)h(Y )] = E[g(X)]E[h(Y )] for any functions g : R ↦ R and h : R ↦ R. If X and Y are independent random variables, then 1 . E[X|Y ] = EX; 2 . E[g(X)|Y ] = E[g(X)]; 3 . E[XY ] = EXEY ; 4 . E[g(X)h(Y )] = E[g(X)]E[h(Y )]. Conditional V ariance: Similar to the conditional expectation, we can define the conditional variance of X, V ar (X|Y = y), which is the variance of X in the conditional space where we know Y = y. If we let μX|Y (y) = E[X|Y = y], then Var(X|Y = y) = E[(X − μX|Y (y)) 2|Y = y] = ∑ xi∈RX (xi − μX|Y (y)) 2PX|Y (xi) = E[X2|Y = y] − μX|Y (y) 2. Note that V ar (X|Y = y) is a function of y. Similar to our discussion on E[X|Y = y] and E[X|Y ], we define V ar (X|Y ) as a function of the random variable Y . That is, V ar (X|Y ) is a random variable whose value equals V ar (X|Y = y) whenever Y = y. Let us look at an example. Example 5. 13 Let X, Y , and Z = E[X|Y ] be as in Example 5.1 1. Let also V =Var (X|Y ). a . Find the PMF of V . b . Find EV . c. Check that V ar (X) = E(V )+Var (Z). Solution In Example 5.1 1, we found out that X, Y ∼ Bernoulli( ). W e also obtained X|Y = 0 ∼ Bernoulli ( ) , P(X = 0|Y = 1) = 1, Var(Z) = . a . To find the PMF of V , we note that V is a function of Y . Specifically , V = Var(X|Y ) = ⎧⎪ ⎨ ⎪⎩ Var(X|Y = 0) if Y = 0 Var(X|Y = 1) if Y = 1 Therefore, V = Var(X|Y ) = ⎧⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪⎩ Var(X|Y = 0) with probability  Var(X|Y = 1) with probability  Now , since X|Y = 0 ∼ Bernoulli ( ), we have Var(X|Y = 0) = ⋅ = , and since given Y = 1, X = 0, we have Var(X|Y = 1) = 0. Thus, V = Var(X|Y ) = ⎧⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪⎩ with probability  0 with probability  2 5 2 3 8 75 3 5 2 5 2 3 2 3 1 3 2 9 2 9 3 5 2 5 So we can write PV (v) = ⎧⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪⎩ if v = if v = 0 0 otherwise b . To find EV , we write EV = ⋅ + 0 ⋅ = . c. To check that V ar (X) = E(V )+Var (Z), we just note that Var(X) = ⋅ = , EV = , Var(Z) = . In the above example, we checked that V ar (X) = E(V )+Var (Z), which says Var(X) = E(Var(X|Y )) + Var(E[X|Y ]). It turns out this is true in general and it is called the law of total variance , or variance decomposition formula [ 3 ]. Let us first prove the law of total variance, and then we explain it intuitively . Note that if V =Var (X|Y ), and Z = E[X|Y ], then V = E[X2|Y ] − (E[X|Y ]) 2 = E[X2|Y ] − Z 2. Thus, EV = E[E[X2|Y ]] − E[Z 2] = E[X2] − E[Z 2] (law of iterated expectations(Equation 5.7)) (5.8) Next, we have Var(Z) = E[Z 2] − (EZ) 2 = E[Z 2] − (EX) 2 (law of iterated expectations) (5.9) Combining Equations 5.8 and 5.9, we obtain the law of total variance. 3 5 2 9 2 5 2 9 3 5 2 5 2 15 2 5 3 5 6 25 2 15 8 75 Law of T otal V ariance: Var(X) = E[Var(X|Y )] + Var(E[X|Y ]) (5.10) There are several ways that we can look at the law of total variance to get some intuition. Let us first note that all the terms in Equation 5.10 are positive (since variance is always positive). Thus, we conclude Var(X) ≥ E(Var(X|Y )) (5.11) This states that when we condition on Y , the variance of X reduces on average. T o describe this intuitively , we can say that variance of a random variable is a measure of our uncertainty about that random variable. For example, if V ar (X) = 0, we do not have any uncertainty about X. Now , the above inequality simply states that if we obtain some extra information, i.e., we know the value of Y , our uncertainty about the value of the random variable X reduces on average. So, the above inequality makes sense. Now , how do we explain the whole law of total variance? To describe the law of total variance intuitively , it is often useful to look at a population divided into several groups. In particular , suppose that we have this random experiment: W e pick a person in the world at random and look at his/her height. Let's call the resulting value X. Define another random variable Y whose value depends on the country of the chosen person, where Y = 1, 2, 3, . . . , n, and n is the number of countries in the world. Then, let's look at the two terms in the law of total variance. Var(X) = E(Var(X|Y )) + Var(E[X|Y ]). Note that Var(X|Y = i) is the variance of X in country i. Thus, E(Var(X|Y )) is the average of variances in each country . On the other hand, E[X|Y = i] is the average height in country i. Thus, Var(E[X|Y ]) is the variance between countries. So, we can interpret the law of total variance in the following way . V ariance of X can be decomposed into two parts: the first is the average of variances in each individual country , while the second is the variance between height averages in each country . Example 5. 14 Let N be the number of customers that visit a certain store in a given day . Suppose that we know E[N] and V ar (N). Let Xi be the amount that the ith customer spends on average. W e assume Xi's are independent of each other and also independent of N. We further assume they have the same mean and variance EXi = EX, Var(Xi) = Var(X). Let Y be the store's total sales, i.e., Y = N ∑ i=1 Xi. Find EY and V ar (Y ). Solution To find EY , we cannot directly use the linearity of expectation because N is random. But, conditioned on N = n, we can use linearity and find E[Y |N = n]; so, we use the law of iterated expectations: EY = E[E[Y |N]] (law of iterated expectations) = E [E[ N ∑ i=1 Xi|N]] = E [ N ∑ i=1 E[Xi|N]] (linearity of expectation) = E [ N ∑ i=1 E[Xi]] (Xi's and N are indpendent) = E[NE[X]] (since EXi = EXs) = E[X]E[N] (since EX is not random). To find V ar (Y ), we use the law of total variance: Var(Y ) = E(Var(Y |N)) + Var(E[Y |N]) = E(Var(Y |N)) + Var(NEX) (as above) = E(Var(Y |N)) + (EX) 2Var(N) (5.12) To find E(Var(Y |N)), note that, given N = n, Y is a sum n independent random variables. As we discussed before, for n independent random variables, the variance of the sum is equal to sum of the variances. This fact is of ficially proved in Section 5.3 and also in Chapter 6, but we have occasionally used it as it simplifies the analysis. Thus, we can write Var(Y |N) = N ∑ i=1 Var(Xi|N) = N ∑ i=1 Var(Xi) (since Xi's are independent of N) = NV ar(X). Thus, we have E(Var(Y |N)) = ENV ar(X) (5.13) Combining Equations 5.12 and 5.13, we obtain Var(Y ) = ENV ar(X) + (EX) 2V ar(N). 5.1.6 Solved Problems Problem 1 Consider two random variables X and Y with joint PMF given in T able 5.3. a . Find P(X ≤ 2, Y ≤ 4). b . Find the marginal PMFs of X and Y . c. Find P(Y = 2|X = 1). d. Are X and Y independent?   Y = 2 Y = 4 Y = 5 X = 1 X = 2 X = 3 Solution a . To find P(X ≤ 2, Y ≤ 4), we can write P(X ≤ 2, Y ≤ 4) = PXY (1, 2) + PXY (1, 4) + PXY (2, 2) + PXY (2, 4) = + + + = . b . Note from the table that RX = {1, 2, 3} and RY = {2, 4, 5}. Now we can use Equation 5.1 to find the marginal PMFs: 1 12 1 24 1 24 1 6 1 12 1 8 1 4 1 8 1 12 1 12 1 24 1 6 1 12 3 8 PX(x) = ⎧⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪⎩ x = 1 x = 2 x = 3 0 otherwise PY (y) = ⎧⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪⎩ y = 2 y = 4 y = 5 0 otherwise c. Using the formula for conditional probability , we have P(Y = 2|X = 1) = = = = . d. Are X and Y independent? T o check whether X and Y are independent, we need to check that P(X = xi, Y = yj) = P(X = xi)P(Y = yj), for all xi ∈ RX and all yj ∈ RY . Looking at the table and the results from previous parts, we find P(X = 2, Y = 2) = ≠ P(X = 2)P(Y = 2) = . Thus, we conclude that X and Y are not independent. Problem 2 I have a bag containing 40 blue marbles and 60 red marbles. I choose 10 marbles (without replacement) at random. Let X be the number of blue marbles and y be the number of red marbles. Find the joint PMF of X and Y . 1 6 3 8 11 24 1 2 1 4 1 4 P(X = 1, Y = 2) P(X = 1) PXY (1, 2) PX(1) 1 12 1 6 1 2 1 6 3 16 Solution This is, in fact, a hypergeometric distribution. First, note that we must have X + Y = 10 , so RXY = {(i, j)|i + j = 10, i, j ∈ Z, i, j ≥ 0} = {(0, 10), (1, 9), (2, 8), . . . , (10, 0)}. Then, we can write PXY (i, j) = ⎧⎪ ⎨ ⎪⎩ i + j = 10, i, j ∈ Z, i, j ≥ 0 0 otherwise Problem 3 Let X and Y be two independent discrete random variables with the same CDFs FX and FY . Define Z = max(X, Y ), W = min(X, Y ). Find the CDFs of Z and W. Solution To find the CDF of Z, we can write FZ(z) = P(Z ≤ z) = P(max(X, Y ) ≤ z) = P((X ≤ z) and (Y ≤ z)) = P(X ≤ z)P(Y ≤ z) ( since X and Y  are independent) = FX(z)FY (z). To find the CDF of W, we can write ( )( ) 40 i 60 j ( ) 100 10 FW (w) = P(W ≤ w) = P(min(X, Y ) ≤ w) = 1 − P(min(X, Y ) > w) = 1 − P((X > w) and (Y > w)) = 1 − P(X > w)P(Y > w) ( since X and Y  are independent) = 1 − (1 − FX(w))(1 − FY (w)) = FX(w) + FY (w) − FX(w)FY (w). Problem 4 Let X and Y be two discrete random variables, with range RXY = {(i, j) ∈ Z2|i, j ≥ 0, |i − j| ≤ 1}, and joint PMF given by PXY (i, j) = ,  for (i, j) ∈ RXY . a . Pictorially show RXY in the x − y plane. b . Find the marginal PMFs PX(i), PY (j). c. Find P(X = Y |X < 2). d. Find P(1 ≤ X2 + Y 2 ≤ 5). e. Find P(X = Y ). f. Find E[X|Y = 2]. g. Find V ar (X|Y = 2). Solution a . Figure 5.5 shows the RXY in the x − y plane. 1 6 ⋅ 2min(i,j) Figure 5.5: Figure shows RXY for X and Y in problem 4. b . First, by symmetry we note that X and Y have the same PMF . Next, we can write PX(0) = PXY (0, 0) + PXY (0, 1) = + = , PX(1) = PXY (1, 0) + PXY (1, 1) + PXY (1, 2) = (1 + + ) = , PX(2) = PXY (2, 1) + PXY (2, 2) + PXY (2, 3) = ( + + ) = , PX(3) = PXY (3, 2) + PXY (3, 3) + PXY (3, 4) = ( + + ) = . In general, we obtain PX(k) = PY (k) = ⎧⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪⎩ k = 0 k = 1, 2, 3, . . . 0 otherwise c. Find P(X = Y |X < 2): W e have 1 6 1 6 1 3 1 6 1 2 1 2 1 3 1 6 1 2 1 4 1 4 1 6 1 6 1 4 1 8 1 8 1 12 1 3 1 3⋅2k−1 P(X = Y |X < 2) = = = = . d. Find P(1 ≤ X2 + Y 2 ≤ 5): W e have P(1 ≤ X2 + Y 2 ≤ 5) = PXY (0, 1) + PXY (1, 0) + PXY (1, 1) + PXY (1, 2) + PXY (2, 1) = + + + + = . e. By symmetry , we can argue that P(X = Y ) = . The reason is that RXY consists of three lines with points with the same probabilities. W e can also find P(X = Y ) by P(X = Y ) = ∞ ∑ i=0 PXY (i, i) = ∞ ∑ i=0 = . f. To find E[X|Y = 2], we first need the conditional PMF of X given Y = 2. W e have PX|Y (k|2) = = 6PXY (k, 2), so we obtain PX|Y (k|2) = ⎧⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪⎩ k = 1 k = 2, 3 0 otherwise Thus, P(X = Y , X < 2) P(X < 2) PXY (0, 0) + PXY (1, 1) PX(0) + PX(1) + 1 6 1 12 + 1 3 1 3 3 8 1 6 1 6 1 12 1 12 1 12 7 12 1 3 1 6.2i 1 3 PXY (k, 2) P(Y = 2) 1 2 1 4 E[X|Y = 2] = 1 ⋅ + 2 ⋅ + 3 ⋅ = . g. Find V ar (X|Y = 2): we have E[X2|Y = 2] = 1 ⋅ + 4 ⋅ + 9 ⋅ = . Thus, Var(X) = E[X2|Y = 2] − (E[X|Y = 2]) 2 = − = . Problem 5 Suppose that the number of customers visiting a fast food restaurant in a given day is N ∼ Poisson(λ). Assume that each customer purchases a drink with probability p, independently from other customers, and independently from the value of N. Let X be the number of customers who purchase drinks. Let Y be the number of customers that do not purchase drinks; so X + Y = N. a . Find the marginal PMFs of X and Y . b . Find the joint PMF of X and Y . c. Are X and Y independent? d. Find E[X2Y 2]. Solution a . First note that RX = RY = {0, 1, 2, . . . }. Also, given N = n, X is a sum of n independent Bernoulli(p) random variables. Thus, given N = n, X has a binomial distribution with parameters n and p, so X|N = n ∼ Binomial(n, p), Y |N = n ∼ Binomial(n, q = 1 − p). We have 1 2 1 4 1 4 7 4 1 2 1 4 1 4 15 4 15 4 49 16 11 16 PX(k) = ∞ ∑ n=0 P(X = k|N = n)PN (n) (law of total probability) = ∞ ∑ n=k ( )p kqn−kexp(−λ) = ∞ ∑ n=k = ∞ ∑ n=k = exp(λq) (Taylor series for e x) = ,  for k = 0, 1, 2, . . . Thus, we conclude that X ∼ Poisson(λp), Y ∼ Poisson(λq). b . To find the joint PMF of X and Y , we can also use the law of total probability: PXY (i, j) = ∞ ∑ n=0 P(X = i, Y = j|N = n)PN (n) (law of total probability). But note that P(X = i, Y = j|N = n) = 0 if N ≠ i + j, thus PXY (i, j) = P(X = i, Y = j|N = i + j)PN (i + j) = P(X = i|N = i + j)PN (i + j) = ( )p iqjexp(−λ) = = . = PX(i)PY (j). c. X and Y are independent, since as we saw above PXY (i, j) = PX(i)PY (j). d. Since X and Y are independent, we have E[X2Y 2] = E[X2]E[Y 2]. Also, note that for a Poisson random variable W with parameter λ, n k λ n n! p kqn−kexp(−λ)λ n k!(n − k)! exp(−λ)(λp) k k! (λq) n−k (n − k)! exp(−λ)(λp) k k! exp(−λp)(λp) k k! i + j i λ i+j (i + j)! exp(−λ)(λp) i(λq) j i!j! exp(−λp)(λp) i i! exp(−λq)(λq) j j! E[W 2] = Var(W) + (EW) 2 = λ + λ 2. Thus, E[X2Y 2] = E[X2]E[Y 2] = (λp + λ 2p 2)(λq + λ 2q2) = λ 2pq(λ 2pq + λ + 1). Problem 6 I have a coin with P(H) = p. I toss the coin repeatedly until I observe two consecutive heads. Let X be the total number of coin tosses. Find EX. Solution We solve this problem using a similar approach as in Example 5.6. Let μ = EX. W e first condition on the result of the first coin toss. Specifically , μ = EX = E[X|H]P(H) + E[X|T]P(T) = E[X|H]p + (1 + μ)(1 − p). In this equation, E[X|T] = 1 + EX, because the tosses are independent, so if the first toss is tails, it is like starting over on the second toss. Thus, pμ = pE[X|H] + (1 − p) (5.14) We still need to find E[X|H] so we condition on the second coin toss E[X|H] = E[X|HH]P + E[X|HT](1 − p) = 2p + (2 + μ)(1 − p) = 2 + (1 − p)μ. Here, E[X|HT] = 2 + EX because, if the first two tosses are HT, we have wasted two coin tosses and we start over at the third toss. By letting E[X|H] = 2 + (1 − p)μ in Equation 5.14, we obtain μ = EX = . Problem 7 Let X, Y ∼ Geometric(p) be independent, and let Z = . 1 + p p2 X Y a . Find the range of Z. b . Find the PMF of Z. c. Find EZ. Solution a . The range of Z is given by RZ = { |m, n ∈ N} , which is the set of all positive rational numbers. b . To find PMF of Z, let m, n ∈ N such that (m, n) = 1, where (m, n) is the largest divisor of m and n. Then PZ ( ) = ∞ ∑ k=1 P(X = mk, Y = nk) = ∞ ∑ k=1 P(X = mk)P(Y = nk) (since X and Y are independent) = ∞ ∑ k=1 pqmk−1pqnk−1 (where q = 1 − p) = p 2q−2 ∞ ∑ k=1 q(m+n)k = = . c. Find EZ: W e can use LOTUS to find EZ. Let us first remember the following useful identities: ∞ ∑ k=1 kx k−1 = ,  for |x| < 1, − ln(1 − x) = ∞ ∑ k=1 ,  for |x| < 1. The first one is obtained by taking derivative of the geometric sum formula, and the second one is a T aylor series. Now , let's apply LOTUS. m n m n p 2qm+n−2 1 − qm+n p 2(1 − p) m+n−2 1 − (1 − p)m+n 1 (1 − x)2 x k k E[ ] = ∞ ∑ n=1 ∞ ∑ m=1 P(X = m, Y = n) = ∞ ∑ n=1 ∞ ∑ m=1 p 2qm−1qn−1 = ∞ ∑ n=1 p 2qn−1 ∞ ∑ m=1 mqm−1 = ∞ ∑ n=1 p 2qn−1 = ∞ ∑ n=1 qn−1 = ∞ ∑ n=1 = ln . X Y m n m n 1 n 1 n 1 (1 − q)2 1 n 1 q qn n 1 1 − p 1 p 5.2.0 T wo Continuous Random V ariables In Chapter 4, we introduced continuous random variables. As a simplified view of things, we mentioned that when we move from discrete random variables to continuous random variables, two things happen: sums become integrals, and PMFs become PDFs. The same statement can be repeated when we talk about joint distributions: (double) sums become (double) integrals, and joint PMFs become joint PDFs. Note that the CDF has the same definition for all kinds of random variables. Experience shows that students usually can learn the concepts behind joint continuous random variables without much dif ficulty; however , they sometimes run into issues when dealing with double integrals. That is, in the discussion of joint continuous distributions, students' problems often relate to multivariate calculus rather than their lack of understanding of probability concepts. The good news is that in practice we do not often need to evaluate multiple integrals anyway . Nevertheless, since this part of the book relies on familiarity with multivariate calculus, we recommend a quick review of double integrals and partial derivatives in case you have not dealt with them recently . W e will only need the calculus concepts very lightly and our goal here is to focus on probability . In this section, we will discuss joint continuous distributions. Since the ideas behind the theory is very analogous to joint discrete random variables, we will provide a quick introduction to main concepts and then focus on examples. 5.2.1 Joint Probability Density Function (PDF) Here, we will define jointly continuous random variables. Basically , two random variables are jointly continuous if they have a joint probability density function as defined below . Definition 5. 1 Two random variables X and Y are jointly continuous if there exists a nonnegative function fXY : R2 → R, such that, for any set A ∈ R2, we have P((X, Y ) ∈ A) = ∬ A fXY (x, y)dxdy (5.15) The function fXY (x, y) is called the joint probability density function (PDF) of X and Y . In the above definition, the domain of fXY (x, y) is the entire R2. W e may define the range of (X, Y ) as RXY = {(x, y)|fX,Y (x, y) > 0}. The above double integral (Equation 5.15) exists for all sets A of practical interest. If we choose A = R2, then the probability of (X, Y ) ∈ A must be one, so we must have ∫ ∞ −∞ ∫ ∞ −∞ fXY (x, y)dxdy = 1 The intuition behind the joint density fXY (x, y) is similar to that of the PDF of a single random variable. In particular , remember that for a random variable X and small positive δ, we have P(x < X ≤ x + δ) ≈ fX(x)δ. Similarly , for small positive δx and δy, we can write P(x < X ≤ x + δx, y ≤ Y ≤ y + δy) ≈ fXY (x, y)δxδy. Example 5. 15 Let X and Y be two jointly continuous random variables with joint PDF fXY (x, y) = ⎧⎪ ⎨ ⎪⎩ x + cy2 0 ≤ x ≤ 1, 0 ≤ y ≤ 1 0 otherwise a . Find the constant c. b . Find P(0 ≤ X ≤ , 0 ≤ Y ≤ ). Solution a . To find c, we use ∫ ∞ −∞ ∫ ∞ −∞ fXY (x, y)dxdy = 1. Thus, we have 1 = ∫ ∞ −∞ ∫ ∞ −∞ fXY (x, y)dxdy = ∫ 1 0 ∫ 1 0 x + cy2 dxdy = ∫ 1 0 [ x 2 + cy2x] x=1 x=0 dy = ∫ 1 0 + cy2 dy = [ y + cy3] y=1 y=0 = + c. 1 2 1 2 1 2 1 2 1 2 1 3 1 2 1 3 Therefore, we obtain c = . b . To find P(0 ≤ X ≤ , 0 ≤ Y ≤ ), we can write P((X, Y ) ∈ A) = ∬ A fXY (x, y)dxdy, for A = {(x, y)|0 ≤ x, y ≤ 1}. Thus, P(0 ≤ X ≤ , 0 ≤ Y ≤ ) = ∫0 ∫0 (x + y2) dxdy = ∫0 [ x 2 + y2x] 0 dy = ∫0 ( + y2) dy = . We can find marginal PDFs of X and Y from their joint PDF . This is exactly analogous to what we saw in the discrete case. In particular , by integrating over all y's, we obtain fX(x). W e have Marginal PDFs fX(x) = ∫ ∞ −∞ fXY (x, y)dy,  for all x, fY (y) = ∫ ∞ −∞ fXY (x, y)dx,  for all y. Example 5. 16 In Example 5.15 find the marginal PDFs fX(x) and fY (y). Solution For 0 ≤ x ≤ 1, we have 3 2 1 2 1 2 1 2 1 2 1 2 1 2 3 2 1 2 1 2 3 2 1 2 1 2 1 8 3 4 3 32 fX(x) = ∫ ∞ −∞ fXY (x, y)dy = ∫ 1 0 (x + y2) dy = [xy + y3] 1 0 = x + . Thus, fX(x) = ⎧⎪ ⎪ ⎨ ⎪ ⎪⎩ x + 0 ≤ x ≤ 1 0 otherwise Similarly , for 0 ≤ y ≤ 1, we have fY (y) = ∫ ∞ −∞ fXY (x, y)dx = ∫ 1 0 (x + y2) dx = [ x 2 + y2x] 1 0 = y2 + . Thus, fY (y) = ⎧⎪ ⎪ ⎨ ⎪ ⎪⎩ y2 + 0 ≤ y ≤ 1 0 otherwise Example 5. 17 Let X and Y be two jointly continuous random variables with joint PDF fXY (x, y) = ⎧⎪ ⎨ ⎪⎩ cx 2y 0 ≤ y ≤ x ≤ 1 0 otherwise a . Find RXY and show it in the x − y plane. b . Find the constant c. c. Find marginal PDFs, fX(x) and fY (y). 3 2 1 2 1 2 1 2 3 2 1 2 3 2 3 2 1 2 3 2 1 2 d. Find P(Y ≤ ). e. Find P(Y ≤ |Y ≤ ). Solution a . From the joint PDF , we find that RXY = {(x, y) ∈ R2|0 ≤ y ≤ x ≤ 1}. Figure 5.6 shows RXY in the x − y plane. Figure 5.6: Figure shows RXY as well as integration region for finding P(Y ≤ ). b . To find the constant c, we can write 1 = ∫ ∞ −∞ ∫ ∞ −∞ fXY (x, y)dxdy = ∫ 1 0 ∫ x 0 cx 2y dydx = ∫ 1 0 x 4dx = . Thus, c = 10. c. To find the marginal PDFs, first note that RX = RY = [0, 1]. For 0 ≤ x ≤ 1, we can write fX(x) = ∫ ∞ −∞ fXY (x, y)dy = ∫ x 0 10x 2ydy = 5x 4. Thus, X 2 X 4 X 2 X 2 c 2 c 10 fX(x) = ⎧⎪ ⎨ ⎪⎩ 5x 4 0 ≤ x ≤ 1 0 otherwise For 0 ≤ y ≤ 1, we can write fY (y) = ∫ ∞ −∞ fXY (x, y)dx = ∫ 1 y 10x 2ydx = y(1 − y3). Thus, fY (y) = ⎧⎪ ⎪ ⎨ ⎪ ⎪⎩ y(1 − y3) 0 ≤ y ≤ 1 0 otherwise d. To find P(Y ≤ ), we need to integrate fXY (x, y) over region A shown in Figure 5.6. In particular , we have P (Y ≤ ) = ∫ ∞ −∞ ∫0 fXY (x, y)dydx = ∫ 1 0 ∫0 10x 2y dydx = ∫ 1 0 x 4dx = . e. To find P(Y ≤ |Y ≤ ), we have P (Y ≤ |Y ≤ ) = = 4P (Y ≤ ) = 4 ∫ 1 0 ∫0 10x 2y dydx = 4 ∫ 1 0 x 4dx = . 10 3 10 3 X 2 X 2 x 2 x 2 5 4 1 4 X 4 X 2 X 4 X 2 P (Y ≤ , Y ≤ ) X 4 X 2 P (Y ≤ ) X 2 X 4 x 4 5 16 1 4 5.2.2 Joint Cumulative Distribution Function (CDF) We have already seen the joint CDF for discrete random variables. The joint CDF has the same definition for continuous random variables. It also satisfies the same properties. The joint cumulative function of two random variables X and Y is defined as FXY (x, y) = P(X ≤ x, Y ≤ y). The joint CDF satisfies the following properties: 1 . FX(x) = FXY (x, ∞), for any x (marginal CDF of X); 2 . FY (y) = FXY (∞, y), for any y (marginal CDF of Y ); 3 . FXY (∞, ∞) = 1; 4 . FXY (−∞, y) = FXY (x, −∞) = 0; 5 . P(x1 < X ≤ x2, y1 < Y ≤ y2) = FXY (x2, y2) − FXY (x1, y2) − FXY (x2, y1) + FXY (x1, y1) ; 6 . if X and Y are independent, then FXY (x, y) = FX(x)FY (y). Example 5. 18 Let X and Y be two independent Uniform(0, 1) random variables. Find FXY (x, y). Solution Since X, Y ∼ Uniform(0, 1), we have FX(x) = ⎧⎪ ⎨ ⎪⎩ 0 for x < 0 x for 0 ≤ x ≤ 1 1 for x > 1 FY (y) = ⎧⎪ ⎨ ⎪⎩ 0 for y < 0 y for 0 ≤ y ≤ 1 1 for y > 1 Since X and Y are independent, we obtain FXY (x, y) = FX(x)FY (y) = ⎧⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪⎩ 0 for y < 0 or x < 0 xy for 0 ≤ x ≤ 1, 0 ≤ y ≤ 1 y for x > 1, 0 ≤ y ≤ 1 x for y > 1, 0 ≤ x ≤ 1 1 for x > 1, y > 1 Figure 5.7 shows the values of FXY (x, y) in the x − y plane. Note that FXY (x, y) is a continuous function in both arguments. This is always true for jointly continuous random variables. This fact sometimes simplifies finding FXY (x, y). The next example (Example 5.19) shows how we can use this fact. Figure 5.7: The joint CDF of two independent Uniform(0, 1) random variables X and Y . Remember that, for a single random variable, we have the following relationship between the PDF and CDF: FX(x) = ∫ x −∞ fX(u)du, fX(x) = . Similar formulas hold for jointly continuous random variables. In particular , we have the following: FXY (x, y) = ∫ y −∞ ∫ x −∞ fXY (u, v)dudv, fXY (x, y) = FXY (x, y) Example 5. 19 Find the joint CDF for X and Y in Example 5.15 Solution In Example 5.15, we found fXY (x, y) = ⎧⎪ ⎪ ⎨ ⎪ ⎪⎩ x + y2 0 ≤ x, y ≤ 1 0 otherwise First, note that since RXY = {(x, y)|0 ≤ x, y ≤ 1}, we find that FXY (x, y) = 0,  for x < 0 or y < 0, FXY (x, y) = 1,  for x ≥ 1 and y ≥ 1. To find the joint CDF for x > 0 and y > 0, we need to integrate the joint PDF: dFX(x) dx ∂ 2 ∂x∂y 3 2 FXY (x, y) = ∫ y −∞ ∫ x −∞ fXY (u, v)dudv = ∫ y 0 ∫ x 0 fXY (u, v)dudv = ∫ min(y,1) 0 ∫ min(x,1) 0 (u + v2) dudv. For 0 ≤ x, y ≤ 1, we obtain FXY (x, y) = ∫ y 0 ∫ x 0 (u + v2) dudv = ∫ y 0 [ u 2 + v2u] x 0dv = ∫ y 0 ( x 2 + xv2) dv = x 2y + xy3. For 0 ≤ x ≤ 1 and y ≥ 1, we use the fact that FXY is continuous to obtain FXY (x, y) = FXY (x, 1) = x 2 + x. Similarly , for 0 ≤ y ≤ 1 and x ≥ 1, we obtain FXY (x, y) = FXY (1, y) = y + y3. 3 2 3 2 1 2 3 2 1 2 3 2 1 2 1 2 1 2 1 2 1 2 1 2 5.2.3 Conditioning and Independence Here, we will discuss conditioning for continuous random variables. In particular , we will discuss the conditional PDF , conditional CDF , and conditional expectation. W e have discussed conditional probability for discrete random variables before. The ideas behind conditional probability for continuous random variables are very similar to the discrete case. The dif ference lies in the fact that we need to work with probability density in the case of continuous random variables. Nevertheless, we would like to emphasize again that there is only one main formula regarding conditional probability which is P(A|B) = ,  when P(B) > 0. Any other formula regarding conditional probability can be derived from the above formula. In fact, for some problems we only need to apply the above formula. Y ou have already used this in Example 5.17. As another example, if you have two random variables X and Y , you can write P(X ∈ C|Y ∈ D) = ,  where C, D ⊂ R. However , sometimes we need to use the concepts of conditional PDFs and CDFs. The formulas for conditional PDFs and CDFs of continuous random variables are very similar to those of discrete random variables. Since there are no new fundamental ideas in this section, we usually provide the main formulas and guidelines, and then work on examples. Specifically , we do not spend much time deriving formulas. Nevertheless, to give you the basic idea of how to derive these formulas, we start by deriving a formula for the conditional CDF and PDF of a random variable X given that X ∈ I = [a, b]. Consider a continuous random variable X. Suppose that we know that the event X ∈ I = [a, b] has occurred. Call this event A. The conditional CDF of X given A, denoted by FX|A(x) or FX|a≤X≤b(x), is FX|A(x) = P(X ≤ x|A) = P(X ≤ x|a ≤ X ≤ b) = . P(A ∩ B) P(B) P(X ∈ C, Y ∈ D) P(Y ∈ D) P(X ≤ x, a ≤ X ≤ b) P(A) Now if x < a, then FX|A(x) = 0. On the other hand, if a ≤ x ≤ b, we have FX|A(x) = = = . Finally , if x > b, then FX|A(x) = 1. Thus, we obtain FX|A(x) = ⎧⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪⎩ 1 x > b a ≤ x < b 0 otherwise Note that since X is a continuous random variable, we do not need to be careful about end points, i.e., changing x > b to x ≥ b does not make a dif ference in the above formula. T o obtain the conditional PDF of X, denoted by fX|A(x), we can dif ferentiate FX|A(x). W e obtain fX|A(x) = ⎧⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪⎩ a ≤ x < b 0 otherwise It is insightful if we derive the above formula for fX|A(x) directly from the definition of the PDF for continuous random variables. Recall that the PDF of X can be defined as fX(x) = lim Δ→0+ . Now , the conditional PDF of X given A, denoted by fX|A(x), is fX|A(x) = lim Δ→0+ = lim Δ→0+ = lim Δ→0+ . Now consider two cases. If a ≤ x < b, then P(X ≤ x, a ≤ X ≤ b) P(A) P(a ≤ X ≤ x) P(A) FX(x) − FX(a) FX(b) − FX(a) FX(x)−FX(a) FX(b)−FX(a) fX(x) P(A) P(x < X ≤ x + Δ) Δ P(x < X ≤ x + Δ|A) Δ P(x < X ≤ x + Δ, A) ΔP(A) P(x < X ≤ x + Δ, a ≤ X ≤ b) ΔP(A) fX|A(x) = lim Δ→0+ = lim Δ→0+ = . On the other hand, if x < a or x ≥ b, then fX|A(x) = lim Δ→0+ = 0. If X is a continuous random variable, and A is the event that a < X < b (where possibly b = ∞ or a = −∞), then FX|A(x) = ⎧⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪⎩ 1 x > b a ≤ x < b 0 x < a fX|A(x) = ⎧⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪⎩ a ≤ x < b 0 otherwise The conditional expectation and variance are defined by replacing the PDF by conditional PDF in the definitions of expectation and variance. In general, for a random variable X and an event A, we have the following: P(x < X ≤ x + Δ, a ≤ X ≤ b) ΔP(A) 1 P(A) P(x < X ≤ x + Δ) Δ fX(x) P(A) P(x < X ≤ x + Δ, a ≤ X ≤ b) ΔP(A) FX(x)−FX(a) FX(b)−FX(a) fX(x) P(A) E[X|A] = ∫ ∞ −∞ xfX|A(x)dx, E[g(X)|A] = ∫ ∞ −∞ g(x)fX|A(x)dx, Var(X|A) = E[X2|A] − (E[X|A]) 2 Example 5. 20 Let X ∼ Exponential(1). a . Find the conditional PDF and CDF of X given X > 1. b . Find E[X|X > 1]. c. Find V ar (X|X > 1). Solution a . Let A be the event that X > 1. Then P(A) = ∫ ∞ 1 e −xdx = . Thus, fX|X>1(x) = ⎧⎪ ⎨ ⎪⎩ e −x+1 x > 1 0 otherwise For x > 1, we have FX|A(A) = = 1 − e −x+1. Thus, FX|A(x) = ⎧⎪ ⎨ ⎪⎩ 1 − e −x+1 x > 1 0 otherwise 1 e FX(x) − FX(1) P(A) b . We have E[X|X > 1] = ∫ ∞ 1 xfX|X>1(x)dx = ∫ ∞ 1 xe −x+1dx = e ∫ ∞ 1 xe −xdx = e[ − e −x − xe −x] ∞ 1 = e = 2. c. We have E[X2|X > 1] = ∫ ∞ 1 x 2fX|X>1(x)dx = ∫ ∞ 1 x 2e −x+1dx = e ∫ ∞ 1 x 2e −xdx = e[ − 2e −x − 2xe −x − x 2e −x] ∞ 1 = e = 5. Thus, Var(X|X > 1) = E[X2|X > 1] − (E[X|X > 1]) 2 = 5 − 4 = 1. Conditioning by Another Random V ariable: If X and Y are two jointly continuous random variables, and we obtain some information regarding Y , we should update the PDF and CDF of X based on the new information. In particular , if we get to observe the value of the random variable Y , then how do we need to update the PDF and CDF of X? Remember for the discrete case, the conditional PMF of X given Y = y is given by 2 e 5 e PX|Y (xi|yj) = . Now , if X and Y are jointly continuous, the conditional PDF of X given Y is given by fX|Y (x|y) = . This means that if we get to observe Y = y, then we need to use the above conditional density for the random variable X. T o get an intuition about the formula, note that by definition, for small Δx and Δy we should have fX|Y (x|y) ≈ (definition of PDF) = ≈ = . Similarly , we can write the conditional PDF of Y , given X = x, as fY |X(y|x) = . PXY (xi, yj) PY (yj) fXY (x, y) fY (y) P(x ≤ X ≤ x + Δx|y ≤ Y ≤ y + Δy) Δx P(x ≤ X ≤ x + Δx, y ≤ Y ≤ y + Δy) P(y ≤ Y ≤ y + Δy)Δx fXY (x, y)ΔxΔy fY (y)ΔyΔx fXY (x, y) fY (y) fXY (x, y) fX(x) For two jointly continuous random variables X and Y , we can define the following conditional concepts: 1 . The conditional PDF of X given Y = y: fX|Y (x|y) = 2 . The conditional probability that X ∈ A given Y = y: P(X ∈ A|Y = y) = ∫A fX|Y (x|y)dx 3 . The conditional CDF of X given Y = y: FX|Y (x|y) = P(X ≤ x|Y = y) = ∫ x −∞ fX|Y (x|y)dx Example 5. 21 Let X and Y be two jointly continuous random variables with joint PDF fXY (x, y) = ⎧⎪ ⎪ ⎨ ⎪ ⎪⎩ + + 0 ≤ x ≤ 1, 0 ≤ y ≤ 2 0 otherwise For 0 ≤ y ≤ 2, find a . the conditional PDF of X given Y = y; b . P(X < |Y = y). Solution a . Let us first find the marginal PDF of Y . W e have fY (y) = ∫ 1 0 + + dx = , for 0 ≤ y ≤ 2. Thus, for 0 ≤ y ≤ 2, we obtain fXY (x, y) fY (y) x2 4 y 2 4 xy 6 1 2 x 2 4 y2 4 xy 6 3y2 + y + 1 12 fX|Y (x|y) = = , for 0 ≤ x ≤ 1. Thus, for 0 ≤ y ≤ 2, we have fX|Y (x|y) = ⎧⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪⎩ 0 ≤ x ≤ 1 0 otherwise b . We have P (X < |Y = y) = ∫0 dx = [x 3 + yx 2 + 3y2x] 0 = . Note that, as we expect, P (X < |Y = y) depends on y. Conditional expectation and variance are similarly defined. Given Y = y, we need to replace fX(x) by fX|Y (x|y) in the formulas for expectation: fXY (x, y) fY (y) 3x 2 + 3y2 + 2xy 3y2 + y + 1 3x 2+3y 2+2xy 3y 2+y+1 1 2 1 2 3x2 + 3y2 + 2xy 3y2 + y + 1 1 3y2 + y + 1 1 2 y2 + + 3 2 y 4 1 8 3y2 + y + 1 1 2 For two jointly continuous random variables X and Y , we have: 1 . Expected value of X given Y = y: E[X|Y = y] = ∫ ∞ −∞ xfX|Y (x|y)dx 2 . Conditional LOTUS: E[g(X)|Y = y] = ∫ ∞ −∞ g(x)fX|Y (x|y)dx 3 . Conditional variance of X given Y = y: V ar(X|Y = y) = E[X2|Y = y] − (E[X|Y = y]) 2 Example 5. 22 Let X and Y be as in Example 5.21. Find E[X|Y = 1] and V ar (X|Y = 1). Solution E[X|Y = 1] = ∫ ∞ −∞ xfX|Y (x|1)dx = ∫ 1 0 x |y=1 dx = ∫ 1 0 x dx (y = 1) = ∫ 1 0 3x 3 + 2x 2 + 3x dx = , E[X2|Y = 1] = ∫ ∞ −∞ x 2fX|Y (x|1)dx = ∫ 1 0 3x 4 + 2x 3 + 3x 2 dx = . So we have 3x 2 + 3y2 + 2xy 3y2 + y + 1 3x 2 + 3 + 2x 3 + 1 + 1 1 5 7 12 1 5 21 50 Var(X|Y = 1) = E[X2|Y = 1] − (E[X|Y = 1]) 2 = − ( ) 2 = . Independent Random V ariables: When two jointly continuous random variables are independent, we must have fX|Y (x|y) = fX(x). That is, knowing the value of Y does not change the PDF of X. Since fX|Y (x|y) = , we conclude that for two independent continuous random variables we must have fXY (x, y) = fX(x)fY (y). Two continuous random variables X and Y are independent if fXY (x, y) = fX(x)fY (y),  for all x, y. Equivalently , X and Y are independent if FXY (x, y) = FX(x)FY (y),  for all x, y. If X and Y are independent, we have E[XY ] = EXEY , E[g(X)h(Y )] = E[g(X)]E[h(Y )]. Suppose that we are given the joint PDF fXY (x, y) of two random variables X and Y . If we can write fXY (x, y) = f1(x)f2(y), then X and Y are independent. 21 50 7 12 287 3600 fXY (x,y) fY (y) Example 5. 23 Determine whether X and Y are independent: a . fXY (x, y) = ⎧⎪ ⎨ ⎪⎩ 2e −x−2y x, y > 0 0 otherwise b . fXY (x, y) = ⎧⎪ ⎨ ⎪⎩ 8xy 0 < x < y < 1 0 otherwise Solution a . We can write fXY (x, y) = [e −xu(x)][2e −2yu(y)], where u(x) is the unit step function: u(x) = { 1 x ≥ 1 0 otherwise Thus, we conclude that X and Y are independent. b . For this case, it does not seem that we can write fXY (x, y) as a product of some f1(x) and f2(y). Note that the given region 0 < x < y < 1 enforces that x < y. That is, we always have X < Y . Thus, we conclude that X and Y are not independent. T o show this, we can obtain the marginal PDFs of X and Y and show that fXY (x, y) ≠ fX(x)fY (y),  for some x, y. W e have, for 0 ≤ x ≤ 1, fX(x) = ∫ 1 x 8xydy = 4x(1 − x 2). Thus, fX(x) = ⎧⎪ ⎨ ⎪⎩ 4x(1 − x 2) 0 < x < 1 0 otherwise Similarly , we obtain fY (y) = ⎧⎪ ⎨ ⎪⎩ 4y3 0 < y < 1 0 otherwise As we see, fXY (x, y) ≠ fX(x)fY (y), thus X and Y are NOT independent. Example 5. 24 Consider the unit disc D = {(x, y)|x 2 + y2 ≤ 1}. Suppose that we choose a point (X, Y ) uniformly at random in D. That is, the joint PDF of X and Y is given by fXY (x, y) = ⎧⎪ ⎨ ⎪⎩ c (x, y) ∈ D 0 otherwise a . Find the constant c. b . Find the marginal PDFs fX(x) and fY (y). c. Find the conditional PDF of X given Y = y, where −1 ≤ y ≤ 1. d. Are X and Y independent? Solution a . We have 1 = ∫ ∞ −∞ ∫ ∞ −∞ fXY (x, y)dxdy = ∬ D c dxdy = c(area of D) = c(π). Thus, c = . b . For −1 ≤ x ≤ 1, we have fX(x) = ∫ ∞ −∞ fXY (x, y)dy = ∫ √1−x2 −√1−x2 dy = √1 − x 2. 1 π 1 π 2 π Thus, fX(x) = ⎧⎪ ⎨ ⎪⎩ √1 − x2 −1 ≤ x ≤ 1 0 otherwise Similarly , fY (y) = ⎧⎪ ⎨ ⎪⎩ √1 − y2 −1 ≤ y ≤ 1 0 otherwise c. We have fX|Y (x|y) = = ⎧⎪ ⎪ ⎨ ⎪ ⎪⎩ −√1 − y2 ≤ x ≤ √1 − y2 0 otherwise Note that the above equation indicates that, given Y = y, X is uniformly distributed on [−√1 − y2, √1 − y2]. W e write X|Y = y ∼ Uniform(−√1 − y2, √1 − y2). d. Are X and Y independent? No, because fXY (x, y) ≠ fX(x)fY (y). Law of T otal Probability: Now , we'll discuss the law of total probability for continuous random variables. This is completely analogous to the discrete case. In particular , the law of total probability , the law of total expectation (law of iterated expectations), and the law of total variance can be stated as follows: 2 π 2 π fXY (x, y) fY (y) 1 2√1−y 2 Law of T otal Probability: P(A) = ∫ ∞ −∞ P(A|X = x)fX(x) dx (5.16) Law of T otal Expectation: E[Y ] = ∫ ∞ −∞ E[Y |X = x]fX(x) dx (5.17) = E[E[Y |X]] Law of T otal V ariance: Var(Y ) = E[Var(Y |X)] + Var(E[Y |X]) (5.18) Let's look at some examples. Example 5. 25 Let X and Y be two independent Uniform(0, 1) random variables. Find P(X3 + Y > 1) . Solution Using the law of total probability (Equation 5.16), we can write P(X3 + Y > 1) = ∫ ∞ −∞ P(X3 + Y > 1|X = x)fX(x) dx = ∫ 1 0 P(x 3 + Y > 1|X = x) dx = ∫ 1 0 P(Y > 1 − x 3) dx (since X and Y  are independent) = ∫ 1 0 x 3 dx (since Y ∼ Uniform(0, 1))  = . Example 5. 26 Suppose X ∼ Uniform(1, 2) and given X = x, Y is an exponential random variable with parameter λ = x, so we can write Y |X = x ∼ Exponential(x). We sometimes write this as Y |X ∼ Exponential(X). a . Find EY . b . Find V ar(Y ). Solution a . We use the law of total expectation (Equation 5.17) to find EY . Remember that if Y ∼ Exponential(λ), then EY = . Thus we conclude E[Y |X = x] = . Using the law of total expectation, we have EY = ∫ ∞ −∞ E[Y |X = x]fX(x)dx = ∫ 2 1 E[Y |X = x] ⋅ 1dx = ∫ 2 1 dx = ln 2. 1 4 1 λ 1 x 1 x Another way to write the above calculation is EY = E[E[Y |X]] (law of total expectation) = E [ ] (since E[Y |X] = ) = ∫ 2 1 dx = ln 2. b . To find V ar(Y ), we can write V ar(Y ) = E[Y 2] − (E[Y ]) 2 = E[Y 2] − (ln 2) 2 = E[E[Y 2|X]] − (ln 2) 2 (law of total expectation) = E [ ] − (ln 2) 2 (since Y |X ∼ Exponential(X)) = ∫ 2 1 dx − (ln 2) 2 = 1 − (ln 2) 2. Another way to find V ar(Y ) is to apply the law of total variance: Var(Y ) = E[Var(Y |X)] + Var(E[Y |X]). Since Y |X ∼ Exponential(X), we conclude E[Y |X] = , V ar(Y |X) = . Therefore Var(Y ) = E [ ] + Var ( ) = E [ ] + E [ ] − (E [ ]) 2 = E [ ] − (ln 2) 2 = 1 − (ln 2) 2. 1 X 1 X 1 x 2 X2 2 x2 1 X 1 X2 1 X2 1 X 1 X2 1 X2 1 X 2 X2 5.2.4 Functions of T wo Continuous Random V ariables So far , we have seen several examples involving functions of random variables. When we have two continuous random variables g(X, Y ), the ideas are still the same. First, if we are just interested in E[g(X, Y )], we can use LOTUS: LOTUS for two continuous random variables: E[g(X, Y )] = ∫ ∞ −∞ ∫ ∞ −∞ g(x, y)fXY (x, y) dxdy (5.19) Example 5. 27 Let X and Y be two jointly continuous random variables with joint PDF fXY (x, y) = ⎧⎪ ⎨ ⎪⎩ x + y 0 ≤ x, y ≤ 1 0 otherwise Find E[XY 2]. Solution We have E[XY 2] = ∫ ∞ −∞ ∫ ∞ −∞(xy2)fXY (x, y) dxdy = ∫ 1 0 ∫ 1 0 xy2(x + y) dxdy = ∫ 1 0 ∫ 1 0 x 2y2 + xy3 dxdy = ∫ 1 0 ( y2 + y3) dy = . 1 3 1 2 17 72 If Z = g(X, Y ) and we are interested in its distribution, we can start by writing FZ(z) = P(Z ≤ z) = P(g(X, Y ) ≤ z) = ∬ D fXY (x, y) dxdy, where D = {(x, y)|g(x, y) < z}. To find the PDF of Z, we differentiate FZ(z). Example 5. 28 Let X and Y be two independent Uniform(0, 1) random variables, and Z = XY . Find the CDF and PDF of Z. Solution First note that RZ = [0, 1]. Thus, FZ(z) = 0,  for z ≤ 0, FZ(z) = 1,  for z ≥ 1. For 0 < z < 1, we have FZ(z) = P(Z ≤ z) = P(XY ≤ z) = P (X ≤ ) . Just to get some practice, we will show you two ways to calculate P(X ≤ ) for 0 < z < 1. The first way is just integrating fXY (x, y) in the region x ≤ . We have P (X ≤ ) = ∫ 1 0 ∫0 fXY (x, y) dxdy = ∫ 1 0 ∫ min(1, ) 0 1 dxdy = ∫ 1 0 min (1, ) dy. Note that if we let g(y) = min (1, ), then z Y z Y z y z Y z y z y z y z y g(y) = ⎧⎪ ⎨ ⎪⎩ 1 for 0 < y < z for z ≤ y ≤ 1 Therefore, P (X ≤ ) = ∫ 1 0 g(y) dy = ∫ z 0 1 dy + ∫ 1 z dy = z − z ln z. The second way to find P(X ≤ ) is to use the law of total probability . We have P(X ≤ ) = ∫ 1 0 P(X ≤ |Y = y)fY (y) dy = ∫ 1 0 P (X ≤ ) fY (y) dy  (since X and Y  are independent) (5.20) Note that P (X ≤ ) = ⎧⎪ ⎨ ⎪⎩ 1 for 0 < y < z for z ≤ y ≤ 1 Therefore, P (X ≤ ) = ∫ 1 0 P (X ≤ ) fY (y) dy = ∫ z 0 1 dy + ∫ 1 z dy = z − z ln z. Thus, in the end we obtain FZ(z) = ⎧⎪ ⎨ ⎪⎩ 0 z ≤ 0 z − z ln z 0 < z < 1 1 z ≥ 1 You can check that FZ(z) is a continuous function. T o find the PDF , we differentiate the CDF. We have fZ(z) = { − ln z 0 < z < 1 0 otherwise z y z Y z y z Y z Y z Y z y z y z y z Y z y z y The Method of T ransformations: When we have functions of two or more jointly continuous random variables, we may be able to use a method similar to Theorems 4.1 and 4.2 to find the resulting PDFs. In particular , we can state the following theorem. While the statement of the theorem might look a little confusing, its application is quite straightforward and we will see a few examples to illustrate the methodology . Theorem 5. 1 Let X and Y be two jointly continuous random variables. Let (Z, W) = g(X, Y ) = (g1(X, Y ), g2(X, Y )), where g : R2 ↦ R2 is a continuous one-to-one (invertible) function with continuous partial derivatives. Let h = g −1, i.e., (X, Y ) = h(Z, W) = (h1(Z, W), h2(Z, W)). Then Z and W are jointly continuous and their joint PDF , fZW (z, w), for (z, w) ∈ RZW is given by fZW (z, w) = fXY (h1(z, w), h2(z, w))|J|, where J is the Jacobian of h defined by J = det ⎡ ⎢ ⎢ ⎢ ⎣ ⎤ ⎥ ⎥ ⎥ ⎦ = . − . The following examples show how to apply the above theorem. Example 5. 29 Let X and Y be two independent standard normal random variables. Let also { Z = 2X − Y W = −X + Y Find fZW (z, w). ∂h1 ∂z ∂h1 ∂w ∂h2 ∂z ∂h2 ∂w ∂h1 ∂z ∂h2 ∂w ∂h2 ∂z ∂h1 ∂w Solution X and Y are jointly continuous and their joint PDF is given by fXY (x, y) = fX(x)fY (y) = exp{− },  for all x, y ∈ R. Here, the function g is defined by (z, w) = g(x, y) = (g1(x, y), g2(x, y)) = (2x − y, −x + y) . Solving for x and y, we obtain the inverse function h: { x = z + w = h1(z, w) y = z + 2w = h2(z, w) We have fZW (z, w) = fXY (h1(z, w), h2(z, w))|J| = fXY (z + w, z + 2w)|J|, where J = det ⎡ ⎢ ⎢ ⎢ ⎣ ⎤ ⎥ ⎥ ⎥ ⎦ = det ⎡ ⎢ ⎣ 1 1 1 2 ⎤ ⎥ ⎦ = 1. Thus, we conclude that fZW (z, w) = fXY (z + w, z + 2w)|J| = exp{− } = exp{− }. Example 5. 30 Let X and Y be two random variables with joint PDF fXY (x, y). Let Z = X + Y . Find fZ(z). Solution 1 2π x 2 + y2 2 ∂h1 ∂z ∂h1 ∂w ∂h2 ∂z ∂h2 ∂w 1 2π (z + w) 2 + (z + 2w) 2 2 1 2π 2z2 + 5w2 + 6zw 2 To apply Theorem 5.1, we need two random variables Z and W. We can simply define W = X. Thus, the function g is given by { z = x + y w = x Then, we can find the inverse transform: { x = w y = z − w Then, we have |J| = | det ⎡ ⎢ ⎣ 0 1 1 −1 ⎤ ⎥ ⎦ | = | − 1| = 1. Thus, fZW (z, w) = fXY (w, z − w). But since we are interested in the marginal PDF , fZ(z), we have fZ(z) = ∫ ∞ −∞ fXY (w, z − w)dw. Note that, if X and Y are independent, then fXY (x, y) = fX(x)fY (y) and we conclude that fZ(z) = ∫ ∞ −∞ fX(w)fY (z − w)dw. The above integral is called the convolution of fX and fY , and we write fZ(z) = fX(z) ∗ fY (z) = ∫ ∞ −∞ fX(w)fY (z − w)dw = ∫ ∞ −∞ fY (w)fX(z − w)dw. If X and Y are two jointly continuous random variables and Z = X + Y , then fZ(z) = ∫ ∞ −∞ fXY (w, z − w)dw = ∫ ∞ −∞ fXY (z − w, w)dw. If X and Y are also independent, then fZ(z) = fX(z) ∗ fY (z) = ∫ ∞ −∞ fX(w)fY (z − w)dw = ∫ ∞ −∞ fY (w)fX(z − w)dw. Example 5. 31 Let X and Y be two independent standard normal random variables, and let Z = X + Y . Find the PDF of Z. Solution We have fZ(z) = fX(z) ∗ fY (z) = ∫ ∞ −∞ fX(w)fY (z − w)dw = ∫ ∞ −∞ e − e − dw = e ∫ ∞ −∞ e −(w− )2 dw = e , where ∫ ∞ −∞ e −(w− )2 dw = 1 because it is the integral of the PDF of a normal random variable with mean and variance . Thus, we conclude that Z ∼ N(0, 2). In fact, this is one of the interesting properties of the normal distribution: the sum of two independent normal random variables is also normal. In particular , similar to our calculation above, we can show the following: 1 2π w2 2 (z−w)2 2 1 √4π −z2 4 1 √π z 2 1 √4π −z2 4 1 √π z 2 z 2 1 2 Theorem 5. 2 If X ∼ N(μX, σ2 X) and Y ∼ N(μY , σ2 Y ) are independent, then X + Y ∼ N(μX + μY , σ2 X + σ2 Y ). We will see an easier proof of Theorem 5.2 when we discuss moment generating functions . 5.2.5 Solved Problems Problem 1 Let X and Y be jointly continuous random variables with joint PDF fX,Y (x, y) = ⎧⎪ ⎨ ⎪⎩ cx + 1 x, y ≥ 0, x + y < 1 0 otherwise 1 . Show the range of (X, Y ), RXY , in the x − y plane. 2 . Find the constant c. 3 . Find the marginal PDFs fX(x) and fY (y). 4 . Find P(Y < 2X2). Solution 1 . Figure 5.8(a) shows RXY in the x − y plane. The figure shows (a) RXY as well as (b) the integration region for finding P(Y < 2X2) for Solved Problem 1. 2 . To find the constant c, we write 1 = ∫ ∞ −∞ ∫ ∞ −∞ fXY (x, y)dxdy = ∫ 1 0 ∫ 1−x 0 cx + 1 dydx = ∫ 1 0 (cx + 1)(1 − x) dx = + c. 1 2 1 6 Thus, we conclude c = 3. 3 . We first note that RX = RY = [0, 1]. fX(x) = ∫ ∞ −∞ fXY (x, y)dy = ∫ 1−x 0 3x + 1 dy = (3x + 1)(1 − x),  for x ∈ [0, 1]. Thus, we have fX(x) = ⎧⎪ ⎨ ⎪⎩ (3x + 1)(1 − x) 0 ≤ x ≤ 1 0 otherwise Similarly , we obtain fY (y) = ∫ ∞ −∞ fXY (x, y)dx = ∫ 1−y 0 3x + 1 dx = (1 − y)(5 − 3y),  for y ∈ [0, 1]. Thus, we have fY (y) = ⎧⎪ ⎪ ⎨ ⎪ ⎪⎩ (1 − y)(5 − 3y) 0 ≤ y ≤ 1 0 otherwise 4 . To find P(Y < 2X2), we need to integrate fXY (x, y) over the region shown in Figure 5.8(b). W e have P(Y < 2X2) = ∫ ∞ −∞ ∫ 2x2 −∞ fXY (x, y)dydx = ∫ 1 0 ∫ min(2x2,1−x) 0 3x + 1 dydx = ∫ 1 0 (3x + 1) min(2x 2, 1 − x) dx = ∫0 2x 2(3x + 1) dx + ∫ 1(3x + 1)(1 − x) dx = . 1 2 1 2 1 2 1 2 53 96 Problem 2 Let X and Y be jointly continuous random variables with joint PDF fX,Y (x, y) = ⎧⎪ ⎨ ⎪⎩ 6e−(2x+3y) x, y ≥ 0 0 otherwise 1 . Are X and Y independent? 2 . Find E[Y |X > 2]. 3 . Find P(X > Y ). Solution 1 . We can write fX,Y (x, y) = fX(x)fY (y), where fX(x) = 2e −2xu(x), fY (y) = 3e −3yu(y). Thus, X and Y are independent. 2 . Since X and Y are independent, we have E[Y |X > 2] = E[Y ]. Note that Y ∼ Exponential(3), thus EY = . 3 . We have P(X > Y ) = ∫ ∞ 0 ∫ ∞ y 6e −(2x+3y)dxdy = ∫ ∞ 0 3e −5ydy = . Problem 3 Let X be a continuous random variable with PDF fX(x) = ⎧⎪ ⎨ ⎪⎩ 2x 0 ≤ x ≤ 1 0 otherwise 1 3 3 5 We know that given X = x, the random variable Y is uniformly distributed on [−x, x]. 1 . Find the joint PDF fXY (x, y). 2 . Find fY (y). 3 . Find P(|Y | < X3). Solution 1 . First note that, by the assumption fY |X(y|x) = ⎧⎪ ⎪ ⎨ ⎪ ⎪⎩ −x ≤ y ≤ x 0 otherwise Thus, we have fXY (x, y) = fY |X(y|x)fX(x) = ⎧⎪ ⎨ ⎪⎩ 1 0 ≤ x ≤ 1, −x ≤ y ≤ x 0 otherwise Thus, fXY (x, y) = ⎧⎪ ⎨ ⎪⎩ 1 |y| ≤ x ≤ 1 0 otherwise 2 . First, note that RY = [−1, 1]. To find fY (y), we can write fY (y) = ∫ ∞ −∞ fXY (x, y)dx = ∫ 1 |y| 1dx = 1 − |y|. Thus, fY (y) = ⎧⎪ ⎨ ⎪⎩ 1 − |y| |y| ≤ 1 0 otherwise 3 . To find P(|Y | < X3), we can use the law of total probability (Equation 5.16): 1 1 2x P(|Y | < X3) = ∫ 1 0 P(|Y | < X3|X = x)fX(x)dx = ∫ 1 0 P(|Y | < x 3|X = x)2xdx = ∫ 1 0 ( ) 2xdx since Y |X = x ∼ Uniform(−x, x) = . Problem 4 Let X and Y be two jointly continuous random variables with joint PDF fX,Y (x, y) = ⎧⎪ ⎨ ⎪⎩ 6xy 0 ≤ x ≤ 1, 0 ≤ y ≤ √x 0 otherwise 1 . Show RXY in the x − y plane. 2 . Find fX(x) and fY (y). 3 . Are X and Y independent? 4 . Find the conditional PDF of X given Y = y, fX|Y (x|y). 5 . Find E[X|Y = y], for 0 ≤ y ≤ 1. 6 . Find Var(X|Y = y), for 0 ≤ y ≤ 1. Solution 1 . Figure 5.9 shows RXY in the x − y plane. 2x 3 2x 1 2 Figure 5.9: The figure shows RXY for Solved Problem 4. 2 . First, note that RX = RY = [0, 1]. To find fX(x) for 0 ≤ x ≤ 1, we can write fX(x) = ∫ ∞ −∞ fXY (x, y) dy = ∫ √x 0 6xy dy = 3x 2. Thus, fX(x) = ⎧⎪ ⎨ ⎪⎩ 3x 2 0 ≤ x ≤ 1 0 otherwise To find fY (y) for 0 ≤ y ≤ 1, we can write fY (y) = ∫ ∞ −∞ fXY (x, y) dx = ∫ 1 y 2 6xy dx = 3y(1 − y4). fY (y) = ⎧⎪ ⎨ ⎪⎩ 3y(1 − y4) 0 ≤ y ≤ 1 0 otherwise 3 . X and Y are not independent, since fXY (x, y) ≠ fx(x)fY (y). 4 . We have fX|Y (x|y) = = ⎧⎪ ⎪ ⎨ ⎪ ⎪⎩ y2 ≤ x ≤ 1 0 otherwise 5 . We have E[X|Y = y] = ∫ ∞ −∞ xfX|Y (x|y) dx = ∫ 1 y 2 x dx = . 6 . We have E[X2|Y = y] = ∫ ∞ −∞ x 2fX|Y (x|y) dx = ∫ 1 y 2 x 2 dx = . Thus, Var(X|Y = y) = E[X2|Y = y] − (E[X|Y = y]) 2 = − ( ) 2. Problem 5 Consider the unit disc D = {(x, y)|x 2 + y2 ≤ 1}. Suppose that we choose a point (X, Y ) uniformly at random in D. That is, the joint PDF of X and Y is given by fXY (x, y) fY (y) 2x 1−y 4 2x 1 − y4 2(1 − y6) 3(1 − y4) 2x 1 − y4 1 − y8 2(1 − y4) 1 − y8 2(1 − y4) 2(1 − y6) 3(1 − y4) 1 fXY (x, y) = ⎧⎪ ⎨ ⎪⎩ (x, y) ∈ D 0 otherwise Let (R, Θ) be the corresponding polar coordinates as shown in Figure 5.10. The inverse transformation is given by { X = R cos Θ Y = R sin Θ where R ≥ 0 and −π < Θ ≤ π. Find the joint PDF of R and Θ. Figure 5.10: Polar Coordinates Solution Here (X, Y ) are jointly continuous and are related to (R, Θ) by a one-to-one relationship. W e use the method of transformations (Theorem 5.1). The function h(r, θ) is given by { x = h1(r, θ) = r cos θ y = h2(r, θ) = r sin θ Thus, we have fRΘ(r, θ) = fXY (h1(r, θ), h2(r, θ))|J| = fXY (r cos θ, r sin θ)|J|. 1 πwhere J = det ⎡ ⎢ ⎢ ⎢ ⎣ ⎤ ⎥ ⎥ ⎥ ⎦ = det ⎡ ⎢ ⎣ cos θ −r sin θ sin θ r cos θ ⎤ ⎥ ⎦ = r cos 2 θ + r sin2 θ = r. We conclude that fRΘ(r, θ) = fXY (r cos θ, r sin θ)|J| = ⎧⎪ ⎨ ⎪⎩ r ∈ [0, 1], θ ∈ (−π, π] 0 otherwise Note that from above we can write fRΘ(r, θ) = fR(r)fΘ(θ), where fR(r) = ⎧⎪ ⎨ ⎪⎩ 2r r ∈ [0, 1] 0 otherwise fΘ(θ) = ⎧⎪ ⎪ ⎨ ⎪ ⎪⎩ θ ∈ (−π, π] 0 otherwise Thus, we conclude that R and Θ are independent. ∂h1 ∂r ∂h1 ∂θ ∂h2 ∂r ∂h2 ∂θ r π 1 2π 5.3.1 Covariance and Correlation Consider two random variables X and Y . Here, we define the covariance between X and Y , written Cov(X, Y ). The covariance gives some information about how X and Y are statistically related. Let us provide the definition, then discuss the properties and applications of covariance. The covariance between X and Y is defined as Cov(X, Y ) = E[(X − EX)(Y − EY )] = E[XY ] − (EX)(EY ). Note that E[(X − EX)(Y − EY )] = E[XY − X(EY ) − (EX)Y + (EX)(EY )] = E[XY ] − (EX)(EY ) − (EX)(EY ) + (EX)(EY ) = E[XY ] − (EX)(EY ). Intuitively , the covariance between X and Y indicates how the values of X and Y move relative to each other . If large values of X tend to happen with large values of Y , then (X − EX)(Y − EY ) is positive on average. In this case, the covariance is positive and we say X and Y are positively correlated. On the other hand, if X tends to be small when Y is large, then (X − EX)(Y − EY ) is negative on average. In this case, the covariance is negative and we say X and Y are negatively correlated. Example 5. 32 Suppose X ∼ Uniform(1, 2), and given X = x, Y is exponential with parameter λ = x. Find Cov(X, Y ). Solution We can use Cov (X, Y ) = EXY − EXEY . We have EX = and 3 2 EY = E[E[Y |X]] (law of iterated expectations (Equation 5.17)) = E [ ] (since Y |X ∼ Exponential(X)) = ∫ 2 1 dx = ln 2. We also have EXY = E[E[XY |X]] (law of iterated expectations) EXY = E[XE[Y |X]] (sinceE[X|X = x] = x) = E [X ] (since Y |X ∼ Exponential(X)) = 1. Thus, Cov(X, Y ) = E[XY ] − (EX)(EY ) = 1 − ln 2. Now we discuss the properties of covariance. Lemma 5. 3 The covariance has the following properties: 1 . Cov(X, X) = Var(X); 2 . if X and Y are independent then Cov(X, Y ) = 0; 3 . Cov(X, Y ) = Cov(Y , X); 4 . Cov(aX, Y ) = aCov(X, Y ); 5 . Cov(X + c, Y ) = Cov(X, Y ); 6 . Cov(X + Y , Z) = Cov(X, Z) + Cov(Y , Z); 7 . more generally , Cov ( m ∑ i=1 aiXi, n ∑ j=1 bjYj) = m ∑ i=1 n ∑ j=1 aibjCov(Xi, Yj). 1 X 1 x 1 X 3 2 All of the above results can be proven directly from the definition of covariance. For example, if X and Y are independent, then as we have seen before E[XY ] = EXEY , so Cov(X, Y ) = E[XY ] − EXEY = 0. Note that the converse is not necessarily true. That is, if Cov(X, Y ) = 0, X and Y may or may not be independent. Let us prove Item 6 in Lemma 5.3, Cov(X + Y , Z) = Cov(X, Z) + Cov(Y , Z). We have Cov(X + Y , Z) = E[(X + Y )Z] − E(X + Y )EZ = E[XZ + Y Z] − (EX + EY )EZ = EXZ − EXEZ + EY Z − EY EZ = Cov(X, Z) + Cov(Y , Z). You can prove the rest of the items in Lemma 5.3 similarly . Example 5. 33 Let X and Y be two independent N(0, 1) random variables and Z = 1 + X + XY 2, W = 1 + X. Find Cov (Z, W). Solution Cov(Z, W) = Cov(1 + X + XY 2, 1 + X) = Cov(X + XY 2, X) (by part 5 of Lemma 5.3) = Cov(X, X) + Cov(XY 2, X) (by part 6 of Lemma 5.3) = Var(X) + E[X2Y 2] − E[XY 2]EX (by part 1 of Lemma 5.3 & definition of Cov) = 1 + E[X2]E[Y 2] − E[X] 2E[Y 2] (since X and Y  are independent) = 1 + 1 − 0 = 2. V ariance of a sum: One of the applications of covariance is finding the variance of a sum of several random variables. In particular , if Z = X + Y , then Var(Z) = Cov(Z, Z) = Cov(X + Y , X + Y ) = Cov(X, X) + Cov(X, Y ) + Cov(Y , X) + Cov(Y , Y ) = Var(X) + Var(Y ) + 2Cov(X, Y ). More generally , for a, b ∈ R, we conclude: Var(aX + bY ) = a 2Var(X) + b2Var(Y ) + 2abCov(X, Y ) (5.21) Correlation Coefficient: The correlation coefficient , denoted by ρXY or ρ(X, Y ), is obtained by normalizing the covariance. In particular , we define the correlation coef ficient of two random variables X and Y as the covariance of the standardized versions of X and Y . Define the standardized versions of X and Y as U = , V = (5.22) Then, ρXY = Cov(U, V ) = Cov ( , ) = Cov ( , ) (by Item 5 of Lemma 5.3) = . ρXY = ρ(X, Y ) = = X − EX σX Y − EY σY X − EX σX Y − EY σY X σX Y σY Cov(X, Y ) σXσY Cov(X, Y ) √Var(X) Var(Y) Cov(X, Y ) σXσY A nice thing about the correlation coef ficient is that it is always between −1 and 1. This is an immediate result of Cauchy-Schwarz inequality that is discussed in Section 6.2.4. One way to prove that −1 ≤ ρ ≤ 1 is to use the following inequality: αβ ≤ , for α, β ∈ R. This is because (α − β) 2 ≥ 0. The equality holds only if α = β. From this, we can conclude that for any two random variables U and V , E[UV ] ≤ , with equality only if U = V with probability one. Now , let U and V be the standardized versions of X and Y as defined in Equation 5.22. Then, by definition ρXY = Cov(U, V ) = EUV . But since EU 2 = EV 2 = 1, we conclude ρXY = E[UV ] ≤ = 1, with equality only if U = V . That is, = , which implies Y = X + (EY − EX) = aX + b, where a and b are constants. Replacing X by −X, we conclude that ρ(−X, Y ) ≤ 1. But ρ(−X, Y ) = −ρ(X, Y ), thus we conclude ρ(X, Y ) ≥ −1. Thus, we can summarize some properties of the correlation coef ficient as follows. α 2 + β2 2 EU 2 + EV 2 2 EU 2 + EV 2 2 Y − EY σY X − EX σX σY σX σY σX Properties of the correlation coef ficient: 1 . −1 ≤ ρ(X, Y ) ≤ 1; 2 . if ρ(X, Y ) = 1, then Y = aX + b, where a > 0; 3 . if ρ(X, Y ) = −1, then Y = aX + b, where a < 0; 4 . ρ(aX + b, cY + d) = ρ(X, Y ) for a, c > 0. Definition 5. 2 Consider two random variables X and Y : - If ρ(X, Y ) = 0, we say that X and Y are uncorrelated . - If ρ(X, Y ) > 0, we say that X and Y are positively correlated. - If ρ(X, Y ) < 0, we say that X and Y are negatively correlated. Note that as we discussed previously , two independent random variables are always uncorrelated, but the converse is not necessarily true. That is, if X and Y are uncorrelated, then X and Y may or may not be independent. Also, note that if X and Y are uncorrelated from Equation 5.21, we conclude that Var(X + Y ) = Var(X) + Var(Y ) . If X and Y are uncorrelated, then Var(X + Y ) = Var(X) + Var(Y ). More generally , if X1, X2, . . . , Xn are pairwise uncorrelated, i.e., ρ(Xi, Xj) = 0 when i ≠ j, then Var(X1 + X2+. . . +Xn) = Var(X1) + Var(X2)+. . . +Var(Xn). Note that if X and Y are independent, then they are uncorrelated, and so Var(X + Y ) = Var(X) + Var(Y ). This is a fact that we stated previously in Chapter 3 , and now we could easily prove using covariance. Example 5. 34 Let X and Y be as in Example 5.24 in Section 5.2.3, i.e., suppose that we choose a point (X, Y ) uniformly at random in the unit disc D = {(x, y)|x 2 + y2 ≤ 1}. Are X and Y uncorrelated? Solution We need to check whether Cov(X, Y ) = 0. First note that, in Example 5.24 of Section 5.2.3 , we found out that X and Y are not independent and in fact, we found that X|Y ∼ Uniform(−√1 − Y 2, √1 − Y 2). Now let's find Cov(X, Y ) = EXY − EXEY . We have EX = E[E[X|Y ]] (law of iterated expectations (Equation 5.17)) = E[0] = 0 (since X|Y ∼ Uniform(−√1 − Y 2, √1 − Y 2)). Also, we have E[XY ] = E[E[XY |Y ]] (law of iterated expectations (Equation 5.17)) = E[Y E[X|Y ]] (Equation 5.6) = E[Y ⋅ 0] = 0. Thus, Cov(X, Y ) = E[XY ] − EXEY = 0. Thus, X and Y are uncorrelated. 5.3.2 Bivariate Normal Distribution Remember that the normal distribution is very important in probability theory and it shows up in many dif ferent applications. W e have discussed a single normal random variable previously; we will now talk about two or more normal random variables. W e recently saw in Theorem 5.2 that the sum of two independent normal random variables is also normal. However , if the two normal random variables are not independent, then their sum is not necessarily normal. Here is a simple counterexample: Example 5. 35 Let X ∼ N(0, 1) and W ∼ Bernoulli ( ) be independent random variables. Define the random variable Y as a function of X and W: Y = h(X, W) = ⎧⎪ ⎨ ⎪⎩ X if W = 0 −X if W = 1 Find the PDF of Y and X + Y . Solution Note that by symmetry of N(0, 1) around zero, −X is also N(0, 1). In particular , we can write FY (y) = P(Y ≤ y) = P(Y ≤ y|W = 0)P(W = 0) + P(Y ≤ y|W = 1)P(W = 1) = P(X ≤ y|W = 0) + P(−X ≤ y|W = 1) = P(X ≤ y) + P(−X ≤ y) (since X and W are independent) = Φ(y) + Φ(y) (since X and −X are N(0, 1)) = Φ(y). Thus, Y ∼ N(0, 1). Now , note that Z = X + Y = ⎧⎪ ⎪ ⎨ ⎪ ⎪⎩ 2X with probability  0 with probability  Thus, Z is a mixed random variable and its PDF is given by fZ(z) = δ(z) + (PDF of 2X at z), fZ(z) = δ(z) + (PDF of a N(0, 4) at z) = δ(z) + e − . In particular , note that X and Y are both normal but their sum is not. Now , we are ready to define bivariate normal or jointly normal random variables. 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 4√2π z2 8 Definition 5. 3 Two random variables X and Y are said to be bivariate normal , or jointly normal , if aX + bY has a normal distribution for all a, b ∈ R. In the above definition, if we let a = b = 0, then aX + bY = 0. We agree that the constant zero is a normal random variable with mean and variance 0. From the above definition, we can immediately conclude the following facts: - If X and Y are bivariate normal, then by letting a = 1, b = 0, we conclude X must be normal. - If X and Y are bivariate normal, then by letting a = 0, b = 1, we conclude Y must be normal. - If X ∼ N(μX, σ2 X) and Y ∼ N(μY , σ2 Y ) are independent, then they are jointly normal (Theorem 5.2). - If X ∼ N(μX, σ2 X) and Y ∼ N(μY , σ2 Y ) are jointly normal, then X + Y ∼ N(μX + μY , σ2 X + σ2 Y + 2ρ(X, Y )σXσY ) (Equation 5.21). But how can we obtain the joint normal PDF in general? Can we provide a simple way to generate jointly normal random variables? The basic idea is that we can start from several independent random variables and by considering their linear combinations, we can obtain bivariate normal random variables. Similar to our discussion on normal random variables, we start by introducing the standard bivariate normal distribution and then obtain the general case from the standard one. The following example gives the idea. Example 5. 36 Let Z1 and Z2 be two independent N(0, 1) random variables. Define X = Z1, Y = ρZ1 + √1 − ρ 2Z2, where ρ is a real number in (−1, 1). a . Show that X and Y are bivariate normal. b . Find the joint PDF of X and Y . c. Find ρ(X, Y ). Solution First, note that since Z1 and Z2 are normal and independent, they are jointly normal, with the joint PDF fZ1Z2 (z1, z2) = fZ1 (z1)fZ2 (z2) = exp { − [z2 1 + z2 2 ]}. a . We need to show aX + bY is normal for all a, b ∈ R. We have 1 2π 1 2 aX + bY = aZ1 + b(ρZ1 + √1 − ρ 2Z2) = (a + bρ)Z1 + b√1 − ρ 2Z2, which is a linear combination of Z1 and Z2 and thus it is normal. b . We can use the method of transformations (Theorem 5.1) to find the joint PDF of X and Y . The inverse transformation is given by Z1 = X = h1(X, Y ), Z2 = − X + Y = h2(X, Y ). We have fXY (z1, z2) = fZ1Z2 (h1(x, y), h2(x, y))|J| = fZ1Z2 (x, − x + y)|J|, where J = det ⎡ ⎢ ⎢ ⎢ ⎢ ⎣ ⎤ ⎥ ⎥ ⎥ ⎥ ⎦ = det ⎡ ⎢ ⎢ ⎣ 1 0 − ⎤ ⎥ ⎥ ⎦ = . Thus, we conclude that fXY (x, y) = fZ1Z2 (x, − x + y)|J| = exp{− [x 2 + (−ρx + y) 2]} ⋅ = exp { − [x 2 − 2ρxy + y2]}. c. To find ρ(X, Y ), first note V ar(X) = V ar(Z1) = 1, V ar(Y ) = ρ 2V ar(Z1) + (1 − ρ 2)V ar(Z2) = 1. Therefore, ρ(X, Y ) = Cov(X, Y ) = Cov(Z1, ρZ1 + √1 − ρ 2Z2) = ρCov(Z1, Z1) + √1 − ρ 2Cov(Z1, Z2) = ρ ⋅ 1 + √1 − ρ 2 ⋅ 0 = ρ. We call the above joint distribution for X and Y the standard bivariate normal distribution with correlation coefficient ρ. It is the distribution for two jointly normal random variables when their variances are equal to one and their correlation coef ficient is ρ. ρ √1 − ρ2 1 √1 − ρ2 ρ √1 − ρ2 1 √1 − ρ2 ∂h1 ∂x ∂h1 ∂y ∂h2 ∂x ∂h2 ∂y ρ √1−ρ2 1 √1−ρ2 1 √1 − ρ2 ρ √1 − ρ2 1 √1 − ρ2 1 2π 1 2 1 1 − ρ2 1 √1 − ρ2 1 2π√1 − ρ2 1 2(1 − ρ2) Two random variables X and Y are said to have the standard bivariate normal distribution with correlation coefficient ρ if their joint PDF is given by fXY (x, y) = exp{− [x 2 − 2ρxy + y2]}, where ρ ∈ (−1, 1). If ρ = 0, then we just say X and Y have the standard bivariate normal distribution. Now , if you want two jointly normal random variables X and Y such that X ∼ N(μX, σ2 X), Y ∼ N(μY , σ2 Y ), and ρ(X, Y ) = ρ, you can start with two independent N(0, 1) random variables, Z1 and Z2, and define { X = σXZ1 + μX Y = σY (ρZ1 + √1 − ρ2Z2) + μY (5.23) We can find the joint PDF of X and Y as above. While the joint PDF has a big formula, we usually do not need to use the formula itself. Instead, we usually work with properties of jointly normal random variables such as their mean, variance, and covariance. Definitions 5.3 and 5.4 are equivalent in the sense that, if X and Y are jointly normal based on one definition, they are jointly normal based on the other definition, too. The proof of their equivalence can be concluded from Problem 10 in Section 6.1.6. In that problem, we show that the two definitions result in the same moment generating functions. Definition 5. 4 Two random variables X and Y are said to have a bivariate normal distribution with parameters μX, σ2 X, μY , σ2 Y , and ρ, if their joint PDF is given by fXY (x, y) = ⋅ exp{− [( ) 2 + ( ) 2 − 2ρ ]}(5.24) where μX, μY ∈ R, σX, σY > 0 and ρ ∈ (−1, 1) are all constants. In the above discussion, we introduced bivariate normal distributions by starting from independent normal random variables, Z1 and Z2. Another approach would have been to define the bivariate normal distribution using the joint PDF . The two definitions are equivalent mathematically . In particular , we can state the following theorem. Theorem 5. 3 1 2π√1 − ρ2 1 2(1 − ρ2) 1 2πσXσY √1 − ρ2 1 2(1 − ρ2) x − μX σX y − μY σY (x − μX)(y − μY ) σXσY Let X and Y be two bivariate normal random variables, i.e., their joint PDF is given by Equation 5.24. Then there exist independent standard normal random variables Z1 and Z2 such that { X = σXZ1 + μX Y = σY (ρZ1 + √1 − ρ2Z2) + μY Proof. (Sketch) To prove the theorem, define ⎧⎪ ⎨ ⎪⎩ Z1 = Z2 = − + Now find the joint PDF of Z1 and Z2 using the method of transformations (Theorem 5.1), similar to what we did above. Y ou will find out that Z1 and Z2 are independent and standard normal and by definition satisfy the equations of Theorem 5.3. The reason we started our discussion on bivariate normal random variables from Z1 and Z2 is three fold. First, it is more convenient and insightful than the joint PDF formula. Second, sometimes the construction using Z1 and Z2 can be used to solve problems regarding bivariate normal distributions. Third, this method gives us a way to generate samples from the bivariate normal distribution using a computer program. Since most computing packages have a built-in command for independent normal random variable generation, we can simply use this command to generate bivariate normal variables using Equation 5.23. Example 5. 37 Let X and Y be jointly normal random variables with parameters μX, σ2 X, μY , σ2 Y , and ρ. Find the conditional distribution of Y given X = x. Solution One way to solve this problem is by using the joint PDF formula (Equation 5.24). In particular , since X ∼ N(μX, σ2 X), we can use fY |X(y|x) = . Another way to solve this problem is to use Theorem 5.3. W e can write { X = σXZ1 + μX Y = σY (ρZ1 + √1 − ρ2Z2) + μY Thus, given X = x, we have Z1 = , and Y = σY ρ + σY √1 − ρ 2Z2 + μY . Since Z1 and Z2 are independent, knowing Z1 does not provide any information on Z2. We have shown that given X = x, Y is a linear function of Z2, thus it is normal. In particular X−μX σX ρ √1−ρ2 X−μX σX 1 √1−ρ2 Y −μY σY fXY (x, y) fX(x) x − μX σX x − μX σX E[Y |X = x] = σY ρ + σY √1 − ρ 2E[Z2] + μY = μY + ρσY , V ar(Y |X = x) = σ2 Y (1 − ρ 2)V ar(Z2) = (1 − ρ 2)σ2 Y . We conclude that given X = x, Y is normally distributed with mean μY + ρσY and variance (1 − ρ 2)σ2 Y . Theorem 5. 4 Suppose X and Y are jointly normal random variables with parameters μX, σ2 X, μY , σ2 Y , and ρ. Then, given X = x, Y is normally distributed with E[Y |X = x] = μY + ρσY , V ar(Y |X = x) = (1 − ρ 2)σ2 Y . Example 5. 38 Let X and Y be jointly normal random variables with parameters μX = 1, σ2 X = 1, μY = 0, σ2 Y = 4, and ρ = . a . Find P(2X + Y ≤ 3). b . Find Cov(X + Y , 2X − Y ). c. Find P(Y > 1|X = 2). Solution a . Since X and Y are jointly normal, the random variable V = 2X + Y is normal. W e have EV = 2EX + EY = 2, V ar(V ) = 4V ar(X) + V ar(Y ) + 4Cov(X, Y ) = 4 + 4 + 4σXσY ρ(X, Y ) = 8 + 4 × 1 × 2 × = 12. Thus, V ∼ N(2, 12). Therefore, P(V ≤ 3) = Φ ( ) = Φ ( ) = 0.6136 b . Note that Cov(X, Y ) = σXσY ρ(X, Y ) = 1. We have Cov(X + Y , 2X − Y ) = 2Cov(X, X) − Cov(X, Y ) + 2Cov(Y , X) − Cov(Y , Y ) = 2 − 1 + 2 − 4 = −1. x − μX σX x − μX σX x−μX σX x − μX σX 1 2 1 2 3 − 2 √12 1 √12 c. Using Theorem 5.4, we conclude that given X = 2, Y is normally distributed with E[Y |X = 2] = μY + ρσY = 1 V ar(Y |X = x) = (1 − ρ 2)σ2 Y = 3. Thus P(Y > 1|X = 2) = 1 − Φ ( ) = . Remember that if two random variables X and Y are independent, then they are uncorrelated, i.e., Cov(X, Y ) = 0. However , the converse is not true in general. In the case of jointly normal random variables, the converse is true. Thus, for jointly normal random variables, being independent and being uncorrelated are equivalent. Theorem 5. 5 If X and Y are bivariate normal and uncorrelated, then they are independent. Proof. Since X and Y are uncorrelated, we have ρ(X, Y ) = 0. By Theorem 5.4, given X = x, Y is normally distributed with E[Y |X = x] = μY + ρσY = μY , V ar(Y |X = x) = (1 − ρ 2)σ2 Y = σ2 Y . Thus, fY |X(y|x) = fY (y) for all x, y ∈ R. Thus X and Y are independent. Another way to prove the theorem is to let ρ = 0 in Equation 5.24 and observe that fXY (x, y) = fX(x)fY (y). 2 − μX σX 1 − 1 √3 1 2 x − μX σX 5.3.3 Solved Problems Problem 1 Let X and Y be two jointly continuous random variables with joint PDF fXY (x, y) = ⎧⎪ ⎨ ⎪⎩ 2 y + x ≤ 1, x > 0, y > 0 0 otherwise Find Cov(X, Y ) and ρ(X, Y ). Solution For 0 ≤ x ≤ 1, we have fX(x) = ∫ ∞ −∞ fXY (x, y)dy = ∫ 1−x 0 2dy = 2(1 − x). Thus, fX(x) = ⎧⎪ ⎨ ⎪⎩ 2(1 − x) 0 ≤ x ≤ 1 0 otherwise Similarly , we obtain fY (y) = ⎧⎪ ⎨ ⎪⎩ 2(1 − y) 0 ≤ y ≤ 1 0 otherwise Thus, we have EX = ∫ 1 0 2x(1 − x)dx = = EY , 1 1 3 EX2 = ∫ 1 0 2x 2(1 − x)dx = = EY 2. Thus, V ar(X) = V ar(Y ) = . We also have EXY = ∫ 1 0 ∫ 1−x 0 2xydydx = ∫ 1 0 x(1 − x) 2dx = . Now , we can find Cov(X, Y ) and ρ(X, Y ): Cov(X, Y ) = EXY − EXEY = − ( ) 2 = − , ρ(X, Y ) = = − . Problem 2 I roll a fair die n times. Let X be the number of 1's that I observe and let Y be the number of 2's that I observe. Find Cov(X, Y ) and ρ(X, Y ). Hint: One way to solve this problem is to look at V ar(X + Y ). Solution Note that you can look at this as a binomial experiment. In particular , we can say that X and Y are Binomial(n, ). Also, X + Y is Binomial(n, ). Remember the variance of a Binomial(n, p) random variable is np(1 − p). Thus, we can write 1 6 1 18 1 12 1 12 1 3 1 36 Cov(X, Y ) √V ar(X)V ar(Y ) 1 2 1 6 2 6 n . = V ar(X + Y ) = V ar(X) + V ar(Y ) + 2Cov(X, Y ) = n . + n . + 2Cov(X, Y ). Thus, Cov(X, Y ) = − . And, ρ(X, Y ) = = − . Problem 3 In this problem, you will provide another proof for the fact that |ρ(X, Y )| ≤ 1. By definition ρXY = Cov(U, V ), where U and V are the normalized versions of X and Y as defined in Equation 5.22: U = , V = . Use the fact that Var(U + V ) ≥ 0 to show that |ρ(X, Y )| ≤ 1. Solution We have Var(U + V ) = Var(U) + Var(V ) + 2Cov(U, V ) = 1 + 1 + 2ρXY . Since Var(U + V ) ≥ 0, we conclude ρ(X, Y ) ≥ −1. Also, from this we conclude that ρ(−X, Y ) ≥ −1. But ρ(−X, Y ) = −ρ(X, Y ), so we conclude ρ(X, Y ) ≤ 1. 2 6 4 6 1 6 5 6 1 6 5 6 n 36 Cov(X, Y ) √V ar(X)V ar(Y ) 1 5 X − EX σX Y − EY σY Problem 4 Let X and Y be two independent Uniform(0, 1) random variables. Let also Z = max(X, Y ) and W = min(X, Y ). Find Cov(Z, W). Solution It is useful to find the distributions of Z and W. T o find the CDF of Z, we can write FZ(z) = P(Z ≤ z) = P(max(X, Y ) ≤ z) = P((X ≤ z) and (Y ≤ z)) = P(X ≤ z)P(Y ≤ z) (since X and Y  are independent) = FX(z)FY (z). Thus, we conclude FZ(z) = ⎧⎪ ⎨ ⎪⎩ 0 z < 0 z2 0 ≤ z ≤ 1 1 z > 1 Therefore, fZ(z) = { 2z 0 ≤ z ≤ 1 0 otherwise From this we obtain EZ = . Note that we can find EW as follows 1 = E[X + Y ] = E[Z + W] = EZ + EW = + EW. Thus, EW = . Nevertheless, it is a good exercise to find the CDF and PDF of W, too. To find the CDF of W, we can write FW (w) = P(W ≤ w) = P(min(X, Y ) ≤ w) = 1 − P(min(X, Y ) > w) = 1 − P((X > w) and (Y > w)) = 1 − P(X > w)P(Y > w) (since X and Y  are independent) = 1 − (1 − FX(w))(1 − FY (w)) = FX(w) + FY (w) − FX(w)FY (w). 2 3 2 3 1 3 Thus, FW (w) = ⎧⎪ ⎨ ⎪⎩ 0 w < 0 2w − w 2 0 ≤ w ≤ 1 1 w > 1 Therefore, fW (w) = { 2 − 2w 0 ≤ w ≤ 1 0 otherwise From the above PDF we can verify that EW = . Now , to find Cov(Z, W), we can write Cov(Z, W) = E[ZW] − EZEW = E[XY ] − EZEW = E[X]E[Y ] − E[Z]E[W] (since X and Y  are independent) = . − . = . Note that Cov(Z, W) > 0 as we expect intuitively . Problem 5 Let X and Y be jointly (bivariate) normal, with V ar(X) = V ar(Y ). Show that the two random variables X + Y and X − Y are independent. Solution Note that since X and Y are jointly normal, we conclude that the random variables X + Y and X − Y are also jointly normal. W e have Cov(X + Y , X − Y ) = Cov(X, X) − Cov(X, Y ) + Cov(Y , X) − Cov(Y , Y ) = V ar(X) − V ar(Y ) = 0. Since X + Y and X − Y are jointly normal and uncorrelated, they are independent. 1 3 1 2 1 2 2 3 1 3 1 36 Problem 6 Let X and Y be jointly normal random variables with parameters μX = 0, σ2 X = 1, μY = −1, σ2 Y = 4, and ρ = − . 1 . Find P(X + Y > 0). 2 . Find the constant a if we know aX + Y and X + 2Y are independent. 3 . Find P(X + Y > 0|2X − Y = 0). Solution 1 . Since X and Y are jointly normal, the random variable U = X + Y is normal. W e have EU = EX + EY = −1, V ar(U) = V ar(X) + V ar(Y ) + 2Cov(X, Y ) = 1 + 4 + 2σXσY ρ(X, Y ) = 5 − 2 × 1 × 2 × = 3. Thus, U ∼ N(−1, 3). Therefore, P(U > 0) = 1 − Φ ( ) = 1 − Φ ( ) = 0.2819 2 . Note that aX + Y and X + 2Y are jointly normal. Thus, for them, independence is equivalent to having Cov(aX + Y , X + 2Y ) = 0. Also, note that Cov(X, Y ) = σXσY ρ(X, Y ) = −1. W e have Cov(aX + Y , X + 2Y ) = aCov(X, X) + 2aCov(X, Y ) + Cov(Y , X) + 2Cov(Y , Y ) = a − (2a + 1) + 8 = −a + 7. Thus, a = 7. 3 . If we define U = X + Y and V = 2X − Y , then note that U and V are jointly normal. W e have EU = −1, V ar(U) = 3, EV = 1, V ar(V ) = 12, and 1 2 1 2 0 − (−1) √3 1 √3 Cov(U, V ) = Cov(X + Y , 2X − Y ) = 2Cov(X, X) − Cov(X, Y ) + 2Cov(Y , X) − Cov(Y , Y ) = 2V ar(X) + Cov(X, Y ) − V ar(Y ) = 2 − 1 − 4 = −3. Thus, ρ(U, V ) = = − . Using Theorem 5.4, we conclude that given V = 0, U is normally distributed with E[U|V = 0] = μU + ρ(U, V )σU = − , V ar(U|V = 0) = (1 − ρ 2 U V )σ2 U = . Thus P(X + Y > 0|2X − Y = 0) = P(U > 0|V = 0) = 1 − Φ ⎛ ⎝ ⎞ ⎠ = 1 − Φ ( ) = 0.3085. Cov(U, V ) √V ar(U)V ar(V ) 1 2 0 − μV σV 3 4 9 4 0 − (− ) 3 4 3 2 1 2 5.4.0 End of Chapter Problems Problem 1 Consider two random variables X and Y with joint PMF given in T able 5.4 Joint PMF of X and Y in Problem 1   Y = 1 Y = 2 X = 1 X = 2 0 X = 4 a . Find P(X ≤ 2, Y > 1). b . Find the marginal PMFs of X and Y . c. Find P(Y = 2|X = 1). d. Are X and Y independent? Problem 2 Let X and Y be as defined in Problem 1. I define a new random variable Z = X − 2Y . a . Find the PMF of Z. b . Find P(X = 2|Z = 0). Problem 3 A box contains two coins: a regular coin and a biased coin with P(H) = . I choose a coin at random and toss it once. I define the random variable X as a Bernoulli random 1 3 1 12 1 6 1 12 1 3 2 3 variable associated with this coin toss, i.e., X = 1 if the result of the coin toss is heads and X = 0 otherwise. Then I take the remaining coin in the box and toss it once. I define the random variable Y as a Bernoulli random variable associated with the second coin toss. Find the joint PMF of X and Y . Are X and Y independent? Problem 4 Consider two random variables X and Y with joint PMF given by PXY (k, l) = , for k, l = 1, 2, 3, . . . a . Show that X and Y are independent and find the marginal PMFs of X and Y . b . Find P(X2 + Y 2 ≤ 10). Problem 5 Let X and Y be as defined in Problem 1. Also, suppose that we are given that Y = 1. a . Find the conditional PMF of X given Y = 1. That is, find PX|Y (x|1). b . Find E[X|Y = 1]. c. Find V ar(X|Y = 1). Problem 6 The number of customers visiting a store in one hour has a Poisson distribution with mean λ = 10. Each customer is a female with probability p = independent of other customers. Let X be the total number of customers in a one-hour interval and Y be the total number of female customers in the same interval. Find the joint PMF of X and Y . Problem 7 Let X ∼ Geometric(p). Find V ar(X) as follows: Find EX and EX2 by conditioning on the result of the first \"coin toss\", and use V ar(X) = EX2 − (EX)2. Problem 8 Let X and Y be two independent Geometric(p) random variables. Find E [ ]. Problem 9 Consider the set of points in the set C: C = {(x, y)|x, y ∈ Z, x 2 + |y| ≤ 2}. 1 2k+l 3 4 X2+Y 2 XY Suppose that we pick a point (X, Y ) from this set completely at random. Thus, each point has a probability of of being chosen. a . Find the joint and marginal PMFs of X and Y . b . Find the conditional PMF of X given Y = 1. c. Are X and Y independent? d. Find E[XY 2]. Problem 10 Consider the set of points in the set C: C = {(x, y)|x, y ∈ Z, x 2 + |y| ≤ 2}. Suppose that we pick a point (X, Y ) from this set completely at random. Thus, each point has a probability of of being chosen. a . Find E[X|Y = 1]. b . Find V ar(X|Y = 1). c. Find E[X||Y | ≤ 1]. d. Find E[X2||Y | ≤ 1]. Problem 1 1 The number of cars being repaired at a small repair shop has the following PMF: PN (n) = ⎧⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪⎩ for n = 0 for n = 1 for n = 2 for n = 3 0 otherwise Each car that is being repaired is a four-door car with probability and a two-door car with probability , independently from other cars and independently from the number of cars being repaired. Let X be the number of four-door cars and Y be the number of two-door cars currently being repaired. a . Find the marginal PMFs of X and Y . b . Find the joint PMF of X and Y . c. Are X and Y independent? 1 11 1 11 1 8 1 8 1 4 1 2 3 4 1 4 Problem 12 Let X and Y be two independent random variables with PMFs PX(k) = PY (k) = { for x = 1, 2, 3, 4, 5 0 otherwise Define Z = X − Y . Find the PMF of Z. Problem 13 Consider two random variables X and Y with joint PMF given in T able 5.5 Table 5.5: Joint PMF of X and Y in Problem 13   Y = 0 Y = 1 Y = 2 X = 0 X = 1 Define the random variable Z as Z = E[X|Y ]. a . Find the Marginal PMFs of X and Y . b . Find the conditional PMF of X, given Y = 0 and Y = 1, i.e., find PX|Y (x|0) and PX|Y (x|1). c. Find the PMF of Z. d. Find EZ, and check that EZ = EX. e. Find V ar (Z). Problem 14 Let X, Y , and Z = E[X|Y ] be as in Problem 13. Define the random variable V as V = V ar(X|Y ). a . Find the PMF of V . b . Find EV . c. Check that V ar(X) = EV + V ar(Z). 1 5 1 6 1 6 1 8 1 8 1 6 1 4 Problem 15 Let N be the number of phone calls made by the customers of a phone company in a given hour . Suppose that N ∼ Poisson(β), where β > 0 is known. Let Xi be the length of the i'th phone call, for i = 1, 2, . . . , N . We assume Xi's are independent of each other and also independent of N. We further assume Xi ∼ Exponential(λ), where λ > 0 is known. Let Y be the sum of the lengths of the phone calls, i.e., Y = N ∑ i=1 Xi. Find EY and V ar (Y ). Problem 16 Let X and Y be two jointly continuous random variables with joint PDF fXY (x, y) = ⎧⎪ ⎪ ⎨ ⎪ ⎪⎩ e −x + 0 ≤ x, 0 ≤ y ≤ 1 0 otherwise a . Find the constant c. b . Find P(0 ≤ X ≤ 1, 0 ≤ Y ≤ ). c. Find P(0 ≤ X ≤ 1). Problem 17 Let X and Y be two jointly continuous random variables with joint PDF fXY (x, y) = ⎧⎪ ⎨ ⎪⎩ e−xy 1 ≤ x ≤ e, y > 0 0 otherwise a . Find the marginal PDFs, fX(x) and fY (y). b . Write an integral to compute P(0 ≤ Y ≤ 1, 1 ≤ X ≤ √e). Problem 18 Let X and Y be two jointly continuous random variables with joint PDF 1 2 cy (1+x)2 1 2 fXY (x, y) = ⎧⎪ ⎪ ⎨ ⎪ ⎪⎩ x2 + y −1 ≤ x ≤ 1, 0 ≤ y ≤ 2 0 otherwise a . Find the marginal PDFs, fX(x) and fY (y). b . Find P(X > 0, Y < 1). c. Find P(X > 0 or Y < 1). d. Find P(X > 0|Y < 1). e. Find P(X + Y > 0). Problem 19 Let X and Y be two jointly continuous random variables with joint CDF FXY (x, y) = ⎧⎪ ⎨ ⎪⎩ 1 − e −x − e −2y + e −(x+2y) x, y > 0 0 otherwise a . Find the joint PDF , fXY (x, y). b . Find P(X < 2Y ). c. Are X and Y independent? Problem 20 Let X ∼ N(0, 1). a . Find the conditional PDF and CDF of X given X > 0. b . Find E[X|X > 0]. c. Find V ar (X|X > 0). Problem 21 Let X and Y be two jointly continuous random variables with joint PDF fXY (x, y) = ⎧⎪ ⎪ ⎨ ⎪ ⎪⎩ x 2 + y −1 ≤ x ≤ 1, 0 ≤ y ≤ 1 0 otherwise For 0 ≤ y ≤ 1, find the following: a . The conditional PDF of X given Y = y. 1 4 1 6 1 3 b . P(X > 0|Y = y). Does this value depend on y? c. Are X and Y independent? Problem 22 Let X and Y be two jointly continuous random variables with joint PDF fXY (x, y) = ⎧⎪ ⎪ ⎨ ⎪ ⎪⎩ x2 + y −1 ≤ x ≤ 1, 0 ≤ y ≤ 1 0 otherwise Find E[Y |X = 0] and V ar (Y |X = 0). Problem 23 Consider the set E = {(x, y)||x| + |y| ≤ 1}. Suppose that we choose a point (X, Y ) uniformly at random in E. That is, the joint PDF of X and Y is given by fXY (x, y) = ⎧⎪ ⎨ ⎪⎩ c (x, y) ∈ E 0 otherwise a . Find the constant c. b . Find the marginal PDFs fX(x) and fY (y). c. Find the conditional PDF of X given Y = y, where −1 ≤ y ≤ 1. d. Are X and Y independent? Problem 24 Let X and Y be two independent Uniform(0, 2) random variables. Find P(XY < 1). Problem 25 Suppose X ∼ Exponential(1) and given X = x, Y is a uniform random variable in [0, x] , i.e., Y |X = x ∼ Uniform(0, x), or equivalently Y |X ∼ Uniform(0, X). 1 2 2 3 a . Find EY . b . Find V ar(Y ). Problem 26 Let X and Y be two independent Uniform(0, 1) random variables. Find a . E[XY ] b . E[e X+Y ] c. E[X2 + Y 2 + XY ] d. E[Y e XY ] Problem 27 Let X and Y be two independent Uniform(0, 1) random variables, and Z = . Find the CDF and PDF of Z. Problem 28 Let X and Y be two independent N(0, 1) random variables, and U = X + Y . a . Find the conditional PDF of U given X = x, fU |X(u|x). b . Find the PDF of U, fU (u). c. Find the conditional PDF of X given U = u, fX|U (x|u). d. Find E[X|U = u], and V ar(X|U = u). Problem 29 Let X and Y be two independent standard normal random variables. Consider the point (X, Y ) in the x − y plane. Let (R, Θ) be the corresponding polar coordinates as shown in Figure 5.1 1. The inverse transformation is given by { X = R cos Θ Y = R sin Θ where, R ≥ 0 and −π < Θ ≤ π. Find the joint PDF of R and Θ. Show that R and Θ are independent. X Y Problem 30 In Problem 29, suppose that X and Y are independent Uniform(0, 1) random variables. Find the joint PDF of R and Θ. Are R and Θ independent? Problem 31 Consider two random variables X and Y with joint PMF given in T able 5.6. Table 5.6: Joint PMF of X and Y in Problem 31   Y = 0 Y = 1 Y = 2 X = 0 X = 1 Find Cov(X, Y ) and ρ(X, Y ). Problem 32 Let X and Y be two independent N(0, 1) random variable and 1 6 1 4 1 8 1 8 1 6 1 6 Z = 11 − X + X2Y , W = 3 − Y . Find Cov (Z, W). Problem 33 Let X and Y be two random variables. Suppose that σ2 X = 4, and σ2 Y = 9. If we know that the two random variables Z = 2X − Y and W = X + Y are independent, find Cov(X, Y ) and ρ(X, Y ). Problem 34 Let X ∼ Uniform(1, 3) and Y |X ∼ Exponential(X). Find Cov(X, Y ). Problem 35 Let X and Y be two independent N(0, 1) random variable and Z = 7 + X + Y , W = 1 + Y . Find ρ(Z, W). Problem 36 Let X and Y be jointly normal random variables with parameters μX = −1, σ2 X = 4, μY = 1, σ2 Y = 1, and ρ = − . a . Find P(X + 2Y ≤ 3). b . Find Cov(X − Y , X + 2Y ). Problem 37 Let X and Y be jointly normal random variables with parameters μX = 1, σ2 X = 4, μY = 1, σ2 Y = 1, and ρ = 0. a . Find P(X + 2Y > 4). b . Find E[X2Y 2]. Problem 38 Let X and Y be jointly normal random variables with parameters μX = 2, σ2 X = 4, μY = 1, σ2 Y = 9, and ρ = − . a . Find E[Y |X = 3]. 1 2 1 2 b . Find V ar(Y |X = 2). c. Find P(X + 2Y ≤ 5|X + Y = 3). 6.0.0 Introduction All the concepts that we have seen regarding one and two random variables can be extended to more random variables. In particular , we can define joint PDF , joint PMF , and joint CDF for three or more random variables. W e provide these definitions in Section 6.1.1. However , working with these functions quickly becomes computationally intractable as the number of random variables grows. Thus, in dealing with multiple random variables we usually resort to other techniques. W e discuss some of these techniques, such as moment generating functions and probability bounds, in this chapter . 6.1.1 Joint Distributions and Independence For three or more random variables, the joint PDF , joint PMF , and joint CDF are defined in a similar way to what we have already seen for the case of two random variables. Let X1, X2, ⋯, Xn be n discrete random variables. The joint PMF of X1, X2, ⋯, Xn is defined as PX1,X2,...,Xn (x1, x2, . . . , xn) = P(X1 = x1, X2 = x2, . . . , Xn = xn). For n jointly continuous random variables X1, X2, ⋯, Xn, the joint PDF is defined to be the function fX1X2...Xn (x1, x2, . . . , xn) such that the probability of any set A ⊂ Rn is given by the integral of the PDF over the set A. In particular , for a set A ∈ Rn, we can write P((X1, X2, ⋯ , Xn) ∈ A) = ∫ ⋯ ∫ A ⋯ ∫ fX1X2⋯Xn (x1, x2, ⋯ , xn)dx1dx2 ⋯ dxn. The marginal PDF of Xi can be obtained by integrating all other Xj's. For example, fX1 (x1) = ∫ ∞ −∞ ⋯ ∫ ∞ −∞ fX1X2...Xn (x1, x2, . . . , xn)dx2 ⋯ dxn. The joint CDF of n random variables X1, X2,..., Xn is defined as FX1,X2,...,Xn (x1, x2, . . . , xn) = P(X1 ≤ x1, X2 ≤ x2, . . . , Xn ≤ xn). Example 6. 1 Let X, Y and Z be three jointly continuous random variables with joint PDF fXY Z(x, y, z) = ⎧⎪ ⎨ ⎪⎩ c(x + 2y + 3z) 0 ≤ x, y, z ≤ 1 0 otherwise 1 . Find the constant c. 2 . Find the marginal PDF of X. Solution 1 . 1 = ∫ ∞ −∞ ∫ ∞ −∞ ∫ ∞ −∞ fXY Z(x, y, z)dxdydz = ∫ 1 0 ∫ 1 0 ∫ 1 0 c(x + 2y + 3z) dxdydz = ∫ 1 0 ∫ 1 0 c ( + 2y + 3z) dydz = ∫ 1 0 c ( + 3z) dz = 3c. Thus, c = . 2 . To find the marginal PDF of X, we note that RX = [0, 1]. For 0 ≤ x ≤ 1, we can write fX(x) = ∫ ∞ −∞ ∫ ∞ −∞ fXY Z(x, y, z)dydz = ∫ 1 0 ∫ 1 0 (x + 2y + 3z) dydz = ∫ 1 0 (x + 1 + 3z) dz = (x + ) . Thus, fX(x) = ⎧⎪ ⎪ ⎨ ⎪ ⎪⎩ (x + ) 0 ≤ x ≤ 1 0 otherwise Independence: The idea of independence is exactly the same as what we have seen before. W e restate it here in terms of the joint PMF , joint PDF , and joint CDF . Random variables X1, X2, . . . , Xn are independent, if for all (x1, x2, . . . , xn) ∈ Rn, FX1,X2,...,Xn (x1, x2, . . . , xn) = FX1 (x1)FX2 (x2) ⋯ FXn (xn). Equivalently , if X1, X2, ..., Xn are discrete, then they are independent if for all (x1, x2, . . . , xn) ∈ Rn, we have PX1,X2,...,Xn (x1, x2, . . . , xn) = PX1 (x1)PX2 (x2) ⋯ PXn (xn). 1 2 3 2 1 3 1 3 1 3 1 3 5 2 1 3 5 2 If X1, X2, ..., Xn are continuous, then they are independent if for all (x1, x2, . . . , xn) ∈ Rn, we have fX1,X2,...,Xn (x1, x2, . . . , xn) = fX1 (x1)fX2 (x2) ⋯ fXn (xn). If random variables X1, X2, ..., Xn are independent, then we have E[X1X2 ⋯ Xn] = E[X1]E[X2] ⋯ E[Xn]. In some situations we are dealing with random variables that are independent and are also identically distributed, i.e, they have the same CDFs. It is usually easier to deal with such random variables, since independence and being identically distributed often simplify the analysis. W e will see examples of such analyses shortly . Definition 6. 1 . Random variables X1, X2, ..., Xn are said to be independent and identically distributed (i.i.d.) if they are independent , and they have the same marginal distributions : FX1 (x) = FX2 (x) =. . . = FXn (x),  for all x ∈ R. For example, if random variables X1, X2, ..., Xn are i.i.d., they will have the same means and variances, so we can write E[X1X2 ⋯ Xn] = E[X1]E[X2] ⋯ E[Xn] (because the Xi's are indepenednt) = E[X1]E[X1] ⋯ E[X1] (because the Xi's are identically distributed) = E[X1] n. 6.1.2 Sums of Random V ariables In many applications, we need to work with a sum of several random variables. In particular , we might need to study a random variable Y given by Y = X1 + X2 + ⋯ + Xn. The linearity of expectation tells us that EY = EX1 + EX2 + ⋯ + EXn. We can also find the variance of Y based on our discussion in Section 5.3. In particular , we saw that the variance of a sum of two random variables is Var(X1 + X2) = Var(X1) + Var(X2) + 2Cov(X1, X2). For Y = X1 + X2 + ⋯ + Xn, we can obtain a more general version of the above equation. W e can write Var(Y ) = Cov ( n ∑ i=1 Xi, n ∑ j=1 Xj) = n ∑ i=1 n ∑ j=1 Cov(Xi, Xj) (using part 7 of Lemma 5.3) = n ∑ i=1 Var(Xi) + 2 ∑ i<j Cov(Xi, Xj). Var ( n ∑ i=1 Xi) = n ∑ i=1 Var(Xi) + 2 ∑ i<j Cov(Xi, Xj) If the Xi's are independent, then Cov(Xi, Xj) = 0 for i ≠ j. In this case, we can write If X1, X2,...,Xn are independent, Var ( n ∑ i=1 Xi) = n ∑ i=1 Var(Xi). Example 6. 2 N people sit around a round table, where N > 5. Each person tosses a coin. Anyone whose outcome is different from his/her two neighbors will receive a present. Let X be the number of people who receive presents. Find EX and Var(X). Solution Number the N people from 1 to N. Let Xi be the indicator random variable for the ith person, that is, Xi = 1 if the ith person receives a present and zero otherwise. Then X = X1 + X2+. . . +XN . First note that P(Xi = 1) = . This is the probability that the person to the right has a dif ferent outcome times the probability that the person to the left has a dif ferent outcome. In other words, if we define Hi and Ti be the events that the ith person's outcome is heads and tails respectively , then we can write EXi = P(Xi = 1) = P(Hi−1, Ti, Hi+1) + P(Ti−1, Hi, Ti+1) = + = . Thus, we find EX = EX1 + EX2+. . . +EXN = . Next, we can write Var(X) = N ∑ i=1 Var(Xi) + N ∑ i=1 ∑ j≠i Cov(Xi, Xj). Since Xi ∼ Bernoulli( ), we have Var(Xi) = . = . It remains to find Cov(Xi, Xj). First note that Xi and Xj are independent if there are at least two people between the ith person and the jth person. In other words, if 2 < |i − j| < N − 2, then Xi and Xj are independent, so Cov(Xi, Xj) = 0,  for 2 < |i − j| < N − 2. Also, note that there is a lot of symmetry in the problem: Cov(X1, X2) = Cov(X2, X3) = Cov(X3, X4) =. . . = Cov(XN−1, XN ) = Cov(XN , X1), Cov(X1, X3) = Cov(X2, X4) = Cov(X3, X5) =. . . = Cov(XN−1, X1) = Cov(XN , X2). Thus, we can write Var(X) = NVar(X1) + 2NCov(X1, X2) + 2NCov(X1, X3) = + 2NCov(X1, X2) + 2NCov(X1, X3). 1 4 1 8 1 8 1 4 N 4 1 4 1 4 3 4 3 16 3N 16 So we need to find Cov(X1, X2) and Cov(X1, X3). W e have E[X1X2] = P(X1 = 1, X2 = 1) = P(HN , T1, H2, T3) + P(TN , H1, T2, H3) = + = . Thus, Cov(X1, X2) = E[X1X2] − E[X1]E[X2] = − = , E[X1X3] = P(X1 = 1, X3 = 1) = P(HN , T1, H2, T3, H4) + P(TN , H1, T2, H3, T4) = + = . Thus, Cov(X1, X3) = E[X1X3] − E[X1]E[X3] = − = 0. Therefore, Var(X) = + 2Ncov(X1, X2) + 2Ncov(X1, X3) = + = . We now know how to find the mean and variance of a sum of n random variables, but we might need to go beyond that. Specifically , what if we need to know the PDF of Y = X1 + X2+... +Xn? In fact we have addressed that problem for the case where Y = X1 + X2 and X1 and X2 are independent (Example 5.30 in Section 5.2.4). For this case, we found out the PDF is given by convolving the PDF of X1 and X2, that is fY (y) = fX1 (y) ∗ fX2 (y) = ∫ ∞ −∞ fX1 (x)fX2 (y − x)dx. For Y = X1 + X2+... +Xn, we can use the above formula repeatedly to obtain the PDF of Y : fY (y) = fX1 (y) ∗ fX2 (y)∗. . . ∗fXn (y). Nevertheless, this quickly becomes computationally dif ficult. Thus, we often resort to other methods if we can. One method that is often useful is using moment generating functions, as we discuss in the next section. 1 16 1 16 1 8 1 8 1 16 1 16 1 32 1 32 1 16 1 16 1 16 3N 16 3N 16 2N 16 5N 16 6.1.3 Moment Generating Functions Here, we will introduce and discuss moment generating functions (MGFs) . Moment generating functions are useful for several reasons, one of which is their application to analysis of sums of random variables. Before discussing MGFs, let's define moments. Definition 6. 2 . The nth moment of a random variable X is defined to be E[Xn]. The nth central moment of X is defined to be E[(X − EX) n]. For example, the first moment is the expected value E[X]. The second central moment is the variance of X. Similar to mean and variance, other moments give useful information about random variables. The moment generating function (MGF) of a random variable X is a function MX(s) defined as MX(s) = E [e sX] . We say that MGF of X exists, if there exists a positive constant a such that MX(s) is finite for all s ∈ [−a, a]. Before going any further , let's look at an example. Example 6. 3 For each of the following random variables, find the MGF . a . X is a discrete random variable, with PMF PX(k) = ⎧⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪⎩ k = 1 k = 2 1 3 2 3 b . Y is a Uniform(0, 1) random variable. Solution a . For X, we have MX(s) = E [e sX] = e s + e 2s. which is well-defined for all s ∈ R. b . For Y , we can write MY (s) = E [e sY ] = ∫ 1 0 e sydy = . Note that we always have MY (0) = E[e 0⋅Y ] = 1, thus MY (s) is also well-defined for all s ∈ R. Why is the MGF useful? There are basically two reasons for this. First, the MGF of X gives us all moments of X. That is why it is called the moment generating function. Second, the MGF (if it exists) uniquely determines the distribution. That is, if two random variables have the same MGF , then they must have the same distribution. Thus, if you find the MGF of a random variable, you have indeed determined its distribution. W e will see that this method is very useful when we work on sums of several independent random variables. Let's discuss these in detail. Finding Moments from MGF: Remember the T aylor series for e x: for all x ∈ R, we have e x = 1 + x + + +. . . = ∞ ∑ k=0 . Now , we can write 1 3 2 3 es − 1 s x2 2! x 3 3! x k k! e sX = ∞ ∑ k=0 = ∞ ∑ k=0 . Thus, we have MX(s) = E[e sX] = ∞ ∑ k=0 E[Xk] . We conclude that the kth moment of X is the coef ficient of in the T aylor series of MX(s). Thus, if we have the T aylor series of MX(s), we can obtain all moments of X. Example 6. 4 If Y ∼ Uniform(0, 1), find E[Y k] using MY (s). Solution We found MY (s) in Example 6.3, so we have MY (s) = = ( ∞ ∑ k=0 − 1) = ∞ ∑ k=1 = ∞ ∑ k=1 = ∞ ∑ k=0 . Thus, the coef ficient of in the T aylor series for MY (s) is , so E[Xk] = . We remember from calculus that the coef ficient of in the T aylor series of MX(s) is obtained by taking the kth derivative of MX(s) and evaluating it at s = 0. Thus, we can write (sX) k k! Xksk k! sk k! sk k! e s − 1 s 1 s sk k! 1 s sk k! sk−1 k! 1 k + 1 sk k! sk k! 1 k+1 1 k + 1 sk k! E[Xk] = MX(s)|s=0. We can obtain all moments of Xk from its MGF: MX(s) = ∞ ∑ k=0 E[Xk] , E[Xk] = MX(s)|s=0. Example 6. 5 Let X ∼ Exponential(λ). Find the MGF of X, MX(s), and all of its moments, E[Xk]. Solution Recall that the PDF of X is fX(x) = λe −λxu(x), where u(x) is the unit step function. W e conclude MX(s) = E[e sX] = ∫ ∞ 0 λe −λxe sxdx = [− e −(λ−s)x] ∞ 0 , for s < λ = , for s < λ. Therefore, MX(s) exists for all s < λ. To find the moments of X, we can write dk dsk sk k! dk dsk λ λ − s λ λ − s MX(s) = = = ∞ ∑ k=0 ( ) k, for  ∣∣ ∣ ∣∣ ∣ < 1 = ∞ ∑ k=0 . We conclude that E[Xk] = ,  for k = 0, 1, 2, . . . Example 6. 6 Let X ∼ Poisson(λ). Find the MGF of X, MX(s). Solution We have PX(k) = e −λ ,  for k = 0, 1, 2, . . . Thus, MX(s) = E[e sX] = ∞ ∑ k=0 e ske −λ = e −λ ∞ ∑ k=0 e sk = e −λ ∞ ∑ k=0 = e −λe λe s (Taylor series for e x) = e λ(es−1), for all s ∈ R. As we discussed previously , the MGF uniquely determines the distribution. This is a λ λ − s 1 1 − s λs λ s λ k! λk sk k! k! λk λ k k! λ k k! λ k k! (λe s) k k! very useful fact. W e will see examples of how we use it shortly . Right now let's state this fact more precisely as a theorem. W e omit the proof here. Theorem 6. 1 Consider two random variables X and Y . Suppose that there exists a positive constant c such that MGFs of X and Y are finite and identical for all values of s in [−c, c]. Then, FX(t) = FY (t),  for all t ∈ R. Example 6. 7 For a random variable X, we know that MX(s) = ,  for s ∈ (−2, 2). Find the distribution of X. Solution We note that the above MGF is the MGF of an exponential random variable with λ = 2 (Example 6.5). Thus, we conclude that X ∼ Exponential(2). Sum of Independent Random V ariables: Suppose X1, X2, ..., Xn are n independent random variables, and the random variable Y is defined as Y = X1 + X2 + ⋯ + Xn. Then, MY (s) = E[e sY ] = E[e s(X1+X2+⋯+Xn)] = E[e sX1 e sX2 ⋯ e sXn ] = E[e sX1 ]E[e sX2 ] ⋯ E[e sXn ] (since the Xi's are independent) = MX1 (s)MX2 (s) ⋯ MXn (s). 2 2 − s If X1, X2, ..., Xn are n independent random variables, then MX1+X2+⋯+Xn (s) = MX1 (s)MX2 (s) ⋯ MXn (s). Example 6. 8 If X ∼ Binomial(n, p) find the MGF of X. Solution We can solve this question directly using the definition of MGF , but an easier way to solve it is to use the fact that a binomial random variable can be considered as the sum of n independent and identically distributed (i.i.d.) Bernoulli random variables. Thus, we can write X = X1 + X2 + ⋯ + Xn, where Xi ∼ Bernoulli(p). Thus, MX(s) = MX1 (s)MX2 (s) ⋯ MXn (s) = (MX1 (s)) n (since the Xi's are i.i.d.) Also, MX1 (s) = E[e sX1 ] = pe s + 1 − p. Thus, we conclude MX(s) = (pe s + 1 − p) n. Example 6. 9 Using MGFs prove that if X ∼ Binomial(m, p) and Y ∼ Binomial(n, p) are independent, then X + Y ∼ Binomial(m + n, p). Solution We have MX(s) = (pe s + 1 − p) m, MY (s) = (pe s + 1 − p) n. Since X and Y are independent, we conclude that MX+Y (s) = MX(s)MY (s) = (pe s + 1 − p) m+n, which is the MGF of a Binomial(m + n, p) random variable. Thus, X + Y ∼ Binomial(m + n, p). 6.1.4 Characteristic Functions There are random variables for which the moment generating function does not exist on any real interval with positive length. For example, consider the random variable X that has a Cauchy distribution fX(x) = , for all x ∈ R. You can show that for any nonzero real number s MX(s) = ∫ ∞ −∞ e sx dx = ∞. Therefore, the moment generating function does not exist for this random variable on any real interval with positive length. If a random variable does not have a well-defined MGF, we can use the characteristic function defined as ϕX(ω) = E[e jωX], where j = √−1 and ω is a real number . It is worth noting that e jωX is a complex-valued random variable. W e have not discussed complex-valued random variables. Nevertheless, you can imagine that a complex random variable can be written as X = Y + jZ, where Y and Z are ordinary real-valued random variables. Thus, working with a complex random variable is like working with two real-valued random variables. The advantage of the characteristic function is that it is defined for all real-valued random variables. Specifically , if X is a real-valued random variable, we can write |e jωX| = 1. Therefore, we conclude |ϕX(ω)| = |E[e jωX]| ≤ E[|e jωX|] ≤ 1. The characteristic function has similar properties to the MGF . For example, if X and Y are independent 1 π 1 + x2 1 π 1 + x2 ϕX+Y (ω) = E[e jω(X+Y )] = E[e jωXe jωY ] = E[e jωX]E[e jωY ] (since X and Y  are independent) = ϕX(ω)ϕY (ω). More generally , if X1, X2, ..., Xn are n independent random variables, then ϕX1+X2+⋯+Xn (ω) = ϕX1 (ω)ϕX2 (ω) ⋯ ϕXn (ω). Example 6. 10 If X ∼ Exponential(λ), show that ϕX(ω) = . Solution Recall that the PDF of X is fX(x) = λe −λxu(x), where u(x) is the unit step function. W e conclude ϕX(ω) = E[e jωX] = ∫ ∞ 0 λe −λxe jωxdx = [ e (jω−λ)x] ∞ 0 = . Note that since λ > 0, the value of e (jω−λ)x, when evaluated at x = +∞, is zero. λ λ − jω λ jω − λ λ λ − jω 6.1.5 Random V ectors When dealing with multiple random variables, it is sometimes useful to use vector and matrix notations. This makes the formulas more compact and lets us use facts from linear algebra. In this section, we briefly explore this avenue. The reader should be familiar with matrix algebra before reading this section. When we have n random variables X1, X2, ..., Xn we can put them in a (column) vector X: X = ⎡ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎣ X1 X2 . . . Xn ⎤ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎦ . We call X a random vector . Here X is an n-dimensional vector because it consists of n random variables. In this book, we usually use bold capital letters such as X, Y and Z to represent a random vector . To show a possible value of a random vector we usually use bold lowercase letters such as x, y and z. Thus, we can write the CDF of the random vector X as FX(x) = FX1,X2,...,Xn(x1, x2, . . . , xn) = P(X1 ≤ x1, X2 ≤ x2, . . . , Xn ≤ xn). If the Xi's are jointly continuous, the PDF of X can be written as fX(x) = fX1,X2,...,Xn(x1, x2, . . . , xn). Expectation: The expected value vector or the mean vector of the random vector X is defined as EX = ⎡ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎣ EX1 EX2 . . . EXn ⎤ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎦ . Similarly , a random matrix is a matrix whose elements are random variables. In particular , we can have an m by n random matrix M as M = ⎡ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎣ X11 X12 . . . X1n X21 X22 . . . X2n . . . . . . . . . . . . Xm1 Xm2 . . . Xmn ⎤ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎦ . We sometimes write this as M= [Xij], which means that Xij is the element in the ith row and jth column of M. The mean matrix of M is given by EM = ⎡ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎣ EX11 EX12 . . . EX1n EX21 EX22 . . . EX2n . . . . . . . . . . . . EXm1 EXm2 . . . EXmn ⎤ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎦ . Linearity of expectation is also valid for random vectors and matrices. In particular , let X be an n-dimensional random vector and the random vector Y be defined as Y = AX + b, where A is a fixed (non-random) m by n matrix and b is a fixed m-dimensional vector . Then we have EY = AEX + b. Also, if X1, X2, ⋯ , Xk are n-dimensional random vectors, then we have E[X1 + X2 + ⋯ + Xk] = EX1 + EX2 + ⋯ + EXk. Correlation and Covariance Matrix For a random vector X, we define the correlation matrix , RX, as RX = E[XXT] = E ⎡ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎣ X2 1 X1X2 . . . X1Xn X2X1 X2 2 . . . X2Xn . . . . . . . . . . . . XnX1 XnX2 . . . X2 n ⎤ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎦ = ⎡ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎣ EX2 1 E[X1X2] . . . E[X1Xn] EX2X1 E[X2 2 ] . . . E[X2Xn] . . . . . . . . . . . . E[XnX1] E[XnX2] . . . E[X2 n] ⎤ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎦ , where T shows matrix transposition. The covariance matrix , CX, is defined as CX = E[(X − EX)(X − EX) T] = E ⎡ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎣ (X1 − EX1) 2 (X1 − EX1)(X2 − EX2) . . . (X1 − EX1)(Xn − EXn) (X2 − EX2)(X1 − EX1) (X2 − EX2) 2 . . . (X2 − EX2)(Xn − EXn) . . . . . . . . . . . . (Xn − EXn)(X1 − EX1) (Xn − EXn)(X2 − EX2) . . . (Xn − EXn) 2 ⎤ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎦ = ⎡ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎣ Var(X1) Cov(X1, X2) . . . Cov(X1, Xn) Cov(X2, X1) Var(X2) . . . Cov(X2, Xn) . . . . . . . . . . . . Cov(Xn, X1) Cov(XnX2) . . . Var(Xn) ⎤ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎦ . The covariance matrix is a generalization of the variance of a random variable. Remember that for a random variable, we have Var(X) = EX2 − (EX) 2. The following example extends this formula to random vectors. Example 6. 1 1 For a random vector X , show CX = RX − EXEXT. Solution We have CX = E[(X − EX)(X − EX) T] = E[(X − EX)(XT − EXT)] = E[XXT] − EXEXT − EXEXT + EXEXT (by linearity of expectation) = RX − EXEXT. Correlation matrix of X:  RX = E[XXT] Covariance matrix of X:  CX = E[(X − EX)(X − EX) T] = RX − EXEXT Example 6. 12 Let X be an n-dimensional random vector and the random vector Y be defined as Y = AX + b, where A is a fixed m by n matrix and b is a fixed m-dimensional vector . Show that CY = ACXAT. Solution Note that by linearity of expectation, we have EY = AEX + b. By definition, we have CY = E[(Y − EY)(Y − EY) T] = E[(AX + b − AEX − b)(AX + b − AEX − b) T] = E[A(X − EX)(X − EX) TAT] = AE[(X − EX)(X − EX) T]AT (by linearity of expectation) = ACXAT. Example 6. 13 Let X and Y be two jointly continuous random variables with joint PDF fX,Y (x, y) = ⎧⎪ ⎪ ⎨ ⎪ ⎪⎩ x 2 + y 0 < x, y < 1 0 otherwise and let the random vector U be defined as 3 2 U = [ X Y ] . Find the correlation and covariance matrices of U. Solution We first obtain the marginal PDFs of X and Y . Note that RX = RY = (0, 1). We have for x ∈ RX fX(x) = ∫ 1 0 x 2 + y dy = x 2 + , for 0 < x < 1. Similarly , for y ∈ RY , we have fY (y) = ∫ 1 0 x 2 + y dx = y + , for 0 < y < 1. From these, we obtain EX = , EX2 = , EY = , and EY 2 = . We also need EXY . By LOTUS, we can write EXY = ∫ 1 0 ∫ 1 0 xy ( x 2 + y) dxdy = ∫ 1 0 y + y2dy = . From this, we also obtain Cov(X, Y ) = EXY − EXEY = − . = − . The correlation matrix RU is given by RU = E[UU T] = [ EX2 EXY EY X EY 2 ] = ⎡ ⎢ ⎣ ⎤ ⎥ ⎦ . 3 2 3 2 1 2 3 2 1 2 5 8 7 15 7 12 5 12 3 2 3 8 1 2 17 48 17 48 5 8 7 12 1 96 7 15 17 48 17 48 5 12 The covariance matrix CU is given by CU = [ Var(X) Cov(X, Y ) Cov(Y , X) Var(Y ) ] = ⎡ ⎢ ⎣ − − ⎤ ⎥ ⎦ . Properties of the Covariance Matrix: The covariance matrix is the generalization of the variance to random vectors. It is an important matrix and is used extensively . Let's take a moment and discuss its properties. Here, we use concepts from linear algebra such as eigenvalues and positive definiteness. First note that, for any random vector X , the covariance matrix CX is a symmetric matrix. This is because if CX = [cij], then cij = Cov(Xi, Xj) = Cov(Xj, Xi) = cji. Thus, the covariance matrix has all the nice properties of symmetric matrices. In particular , CX can be diagonalized and all the eigenvalues of CX are real. Here, we assume X is a real random vector , i.e., the Xi's can only take real values. A special important property of the covariance matrix is that it is positive semi-definite (PSD). Remember from linear algebra that a symmetric matrix M is positive semi-definite (PSD) if, for all vectors b, we have b TMb ≥ 0. Also, M is said to be positive definite (PD) , if for all vectors b≠ 0, we have b TMb > 0. By the above definitions, we note that every PD matrix is also PSD, but the converse is not generally true. Here, we show that covariance matrices are always PSD. Theorem 6. 2 . Let X be a random vector with n elements. Then, its covariance matrix CX is positive semi-definite(PSD). Proof. Let b be any fixed vector with n elements. Define the random variable Y as Y = b T(X − EX). We have 73 960 1 96 1 96 11 144 0 ≤ EY 2 = E(Y Y T ) = b TE[(X − EX)(X − EX) T]b = b TCXb. Note that the eigenvalues of a PSD matrix are always larger than or equal to zero. If all the eigenvalues are strictly larger than zero, then the matrix is positive definite. From linear algebra, we know that a real symmetric matrix is positive definite if and only if all its eigenvalues are positive. Since CX is a real symmetric matrix, we can state the following theorem. Theorem 6. 3 . Let X be a random vector with n elements. Then its covariance matrix CX is positive definite (PD), if and only if all its eigenvalues are larger than zero. Equivalently , CX is positive definite (PD), if and only if det(CX) > 0. Note that the second part of the theorem is implied by the first part. This is because the determinant of a matrix is the product of its eigenvalues, and we already know that all eigenvalues of CX are larger than or equal to zero. Example 6. 14 Let X and Y be two independent Uniform(0, 1) random variables. Let the random vectors U and V be defined as U = [ X X + Y ] , V = ⎡ ⎢ ⎣ X Y X + Y ⎤ ⎥ ⎦ . Determine whether CU and CV are positive definite. Solution Let us first find CU. We have CU = [ Var(X) Cov(X, X + Y ) Cov(X + Y , X) Var(X + Y ) ] . Since X and Y are independent Uniform(0, 1) random variables, we have Var(X) = Var(Y ) = , Cov(X, X + Y ) = Cov(X, X) + Cov(X, Y ) = + 0 = , Var(X + Y ) =Var(X) + Var(Y ) = . 1 12 1 12 1 12 1 6 Thus, CU = ⎡ ⎢ ⎣ ⎤ ⎥ ⎦ . So we conclude det(CU) = . − . = > 0. Therefore, CU is positive definite. For CV, we have CV = ⎡ ⎢ ⎣ Var(X) Cov(X, Y ) Cov(X, X + Y ) Cov(Y , X) Var(Y ) Cov(Y , X + Y ) Cov(X + Y , X) Cov(X + Y , Y ) Var(X + Y ) ⎤ ⎥ ⎦ = ⎡ ⎢ ⎢ ⎢ ⎢ ⎢ ⎣ 0 0 ⎤ ⎥ ⎥ ⎥ ⎥ ⎥ ⎦ . So we conclude det(CV) = ( ⋅ − ⋅ ) − 0 + (0 − ⋅ ) = 0. Thus, CV is not positive definite (we already know that it is positive semi-definite). Finally , if we have two random vectors, X and Y, we can define the cross correlation matrix of X and Y as RXY = E[XYT]. Also, the cross covariance matrix of X and Y is CXY = E[(X − EX)(Y − EY) T]. 1 12 1 12 1 12 1 6 1 12 1 6 1 12 1 12 1 144 1 12 1 12 1 12 1 12 1 12 1 12 1 6 1 12 1 12 1 6 1 12 1 12 1 12 1 12 1 12 Functions of Random V ectors: The Method of Transformations A function of a random vector is a random vector . Thus, the methods that we discussed regarding functions of two random variables can be used to find distributions of functions of random vectors. For example, we can state a more general form of Theorem 5.1 (method of transformations). Let us first explain the method and then see some examples on how to use it. Let X be an n-dimensional random vector with joint PDF fX(x). Let G : Rn ↦ Rn be a continuous and invertible function with continuous partial derivatives and let H = G−1. Suppose that the random vector Y is given by Y = G(X) and thus X = G−1(Y) = H(Y). That is, X = ⎡ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎣ X1 X2 . . . Xn ⎤ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎦ = ⎡ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎣ H1(Y1, Y2, . . . , Yn) H2(Y1, Y2, . . . , Yn) . . . Hn(Y1, Y2, . . . , Yn) ⎤ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎦ . Then, the PDF of Y , fY1,Y2,...,Yn (y1, y2, . . . , yn), is given by fY(y) = fX(H(y))|J| where J is the Jacobian of H defined by J = det ⎡ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎣ . . . . . . ⋮ ⋮ ⋮ ⋮ . . . ⎤ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎦ , and evaluated at (y1, y2, . . . , yn). Example 6. 15 Let X be an n-dimensional random vector . Let A be a fixed (non-random) invertible n by n matrix, and b be a fixed n-dimensional vector . Define the random vector Y as ∂H1 ∂y1 ∂H1 ∂y2 ∂H1 ∂yn ∂H2 ∂y1 ∂H2 ∂y2 ∂H2 ∂yn ∂Hn ∂y1 ∂Hn ∂y2 ∂Hn ∂yn Y = AX + b. Find the PDF of Y in terms of PDF of X. Solution Since A is invertible, we can write X = A−1(Y − b). We can also check that J = det(A−1) = . Thus, we conclude that fY (y) = fX(A−1(y − b)). Normal (Gaussian) Random V ectors: We discussed two jointly normal random variables previously in Section 5.3.2. In particular , two random variables X and Y are said to be bivariate normal or jointly normal , if aX + bY has normal distribution for all a, b ∈ R. We can extend this definition to n jointly normal random variables. Random variables X1, X2,..., Xn are said to be jointly normal if, for all a1,a2,..., an ∈ R, the random variable a1X1 + a2X2+. . . +anXn is a normal random variable. As before, we agree that the constant zero is a normal random variable with zero mean and variance, i.e., N(0, 0). When we have several jointly normal random 1 det(A) 1 | det(A)| variables, we often put them in a vector . The resulting random vector is a called a normal (Gaussian) random vector . A random vector X = ⎡ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎣ X1 X2 . . . Xn ⎤ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎦ is said to be normal or Gaussian if the random variables X1, X2,..., Xn are jointly normal. To find the general form for the PDF of a Gaussian random vector it is convenient to start from the simplest case where the Xi's are independent and identically distributed (i.i.d.), Xi ∼ N(0, 1). In this case, we know how to find the joint PDF . It is simply the product of the individual (marginal) PDFs. Let's call such a random vector the standard normal random vector . So, let Z = ⎡ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎣ Z1 Z2 . . . Zn ⎤ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎦ , where Zi's are i.i.d. and Zi ∼ N(0, 1). Then, we have fZ(z) = fZ1,Z2,...,Zn (z1, z2, . . . , zn) = n ∏ i=1 fZi(zi) = exp{− n ∑ i=1 z2 i } = exp{− z T z}. 1 (2π) n 2 1 2 1 (2π) n 2 1 2 For a standard normal random vector Z, where Zi's are i.i.d. and Zi ∼ N(0, 1), the PDF is given by fZ(z) = exp{− z T z}. Now , we need to extend this formula to a general normal random vector X with mean m and covariance matrix C . This is very similar to when we defined general normal random variables from the standard normal random variable. W e remember that if Z ∼ N(0, 1), then the random variable X = σZ + μ has N(μ, σ2) distribution. W e would like to do the same thing for normal random vectors. Assume that I have a normal random vector X with mean m and covariance matrix C . We write X ∼ N(m, C). Further, assume that C is a positive definite matrix.(The positive definiteness assumption here does not create any limitations. W e already know that C is positive semi-definite (Theorem 6.2), so det(C) ≥ 0. We also know that C is positive definite if and only if det(C) > 0 (Theorem 6.3). So here, we are only excluding the case det(C) = 0. If det(C) = 0, then you can show that you can write some Xi's as a linear combination of others, so indeed we can remove them from the vector without losing any information.) Then from linear algebra we know that there exists an n by n matrix Q such that QQT = I (I is the identity matrix), C = QDQT , where D is a diagonal matrix D = ⎡ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎣ d11 0 . . . 0 0 d22 . . . 0 . . . . . . . . . . . . 0 0 . . . dnn ⎤ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎦ . The positive definiteness assumption guarantees that all dii's are positive. Let's define 1 (2π) n 2 1 2 D = ⎡ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎣ √d11 0 . . . 0 0 √d22 . . . 0 . . . . . . . . . . . . 0 0 . . . √dnn ⎤ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎦ . We have D D = D and D = D T . Also define A = QD QT . Then, AAT = AT A = C. Now we are ready to define the transformation that converts a standard Gaussian vector to X ∼ N(m, C). Let Z be a standard Gaussian vector , i.e., Z ∼ N(0, I). Define X = AZ + m. We claim that X ∼ N(m, C). To see this, first note that X is a normal random vector . The reason is that any linear combination of components of X is indeed a linear combination of components of Z plus a constant. Thus, every linear combination of components of X is a normal random variable. It remains to show that EX = m and CX = C. First note that by linearity of expectation we have EX = E [AZ + m] = AE[Z] + m = m. Also, by Example 6.12 we have CX = ACZAT = AAT (since CZ = I) = C. Thus, we have shown that X is a random vector with mean m and covariance matrix C . Now we can use Example 6.15 to find the PDF of X . We have 1 2 1 2 1 2 1 2 1 2 1 2 fX(x) = fZ(A−1(x − m)) = exp{− (A−1(x − m)) T (A−1(x − m))} = exp{− (x − m) T A−T A−1(x − m)} = exp{− (x − m) T C −1(x − m)}. For a normal random vector X with mean m and covariance matrix C, the PDF is given by fX(x) = exp{− (x − m) T C −1(x − m)} (6.1) Example 6. 16 Let X and Y be two jointly normal random variables with X ∼ N(μX, σX), Y ∼ N(μY , σY ), and ρ(X, Y ) = ρ. Show that the above PDF formula for PDF of [ X Y ] is the same as fX,Y (x, y) given in Definition 5.4 in Section 5.3.2. That is, fXY (x, y) = ⋅ exp{− [( ) 2 + ( ) 2 − 2ρ ]}. Solution Both formulas are in the form ae − b. Thus, it suffices to show that they have the same a and b. Here we have m = [ μX μY ] . 1 | det(A)| 1 (2π) | det(A)| n 2 1 2 1 (2π) √det(C) n 2 1 2 1 (2π) √det(C) n 2 1 2 1 (2π) √det C n 2 1 2 1 2πσXσY √1 − ρ2 1 2(1 − ρ2) x − μX σX y − μY σY (x − μX)(y − μY ) σXσY 1 2 We also have C = [ Var(X) Cov(X, Y ) Cov(Y , X) Var(Y ) ] = [ σ2 X ρσXσY ρσXσY σ2 Y ] . From this, we obtain det C = σ2 Xσ2 Y (1 − ρ 2). Thus, in both formulas for PDF a is given by a = . Next, we check b. We have C −1 = [ σ2 Y −ρσXσY −ρσXσY σ2 X ] . Now by matrix multiplication we obtain (x − m) T C −1(x − m) = = [ x − μX y − μY ] T [ σ2 Y −ρσXσY −ρσXσY σ2 X ] [ x − μX y − μY ] = − [( ) 2 + ( ) 2 − 2ρ ] , which agrees with the formula in Definition 5.4. Remember that two jointly normal random variables X and Y are independent if and only if they are uncorrelated. We can extend this to multiple jointly normal random variables. Thus, if you have a normal random vector whose components are uncorrelated, you can conclude that the components are independent. To show this, note that if the Xi's are uncorrelated, then the covariance matrix CX is diagonal, so its inverse C −1 X is also diagonal. You can see that in this case the PDF (Equation 6.1) becomes the products of marginal PDFs. 1 2πσXσY √1 − ρ2 1 σ2 Xσ2 Y (1 − ρ2) 1 σ2 Xσ2 Y (1 − ρ2) 1 2(1 − ρ2) x − μX σX y − μY σY (x − μX)(y − μY ) σXσY If X = [X1, X2, . . . , Xn] T is a normal random vector , and we know Cov(Xi, Xj) = 0 for all i ≠ j, then X1,X2, ..., Xn are independent. Another important result is that if X = [X1, X2, . . . , Xn] T is a normal random vector then Y = AX + b is also a random vector because any linear combination of components of Y is also a linear combination of components of X plus a constant value. If X = [X1, X2, . . . , Xn] T is a normal random vector , X ∼ N(m, C), A is an m by n fixed matrix, and b is an m-dimensional fixed vector , then the random vector Y = AX + b is a normal random vector with mean AEX + b and covariance matrix ACAT . Y ∼ N(AEX + b, ACAT ) 6.1.6 Solved Problems Problem 1 Let X, Y and Z be three jointly continuous random variables with joint PDF fXY Z(x, y, z) = ⎧⎪ ⎪ ⎨ ⎪ ⎪⎩ (x + 2y + 3z) 0 ≤ x, y, z ≤ 1 0 otherwise Find the joint PDF of X and Y , fXY (x, y). Solution fXY (x, y) = ∫ ∞ −∞ fXY Z(x, y, z)dz = ∫ 1 0 (x + 2y + 3z)dz = [(x + 2y)z + z2] 1 0 = (x + 2y + ) , for 0 ≤ x, y ≤ 1. Thus, fXY (x, y) = ⎧⎪ ⎪ ⎨ ⎪ ⎪⎩ (x + 2y + ) 0 ≤ x ≤ 1, 0 ≤ y ≤ 1 0 otherwise Problem 2 Let X, Y and Z be three independent random variables with X ∼ N(μ, σ2), and Y , Z ∼ Uniform(0, 2). We also know that E[X2Y + XY Z] = 13, E[XY 2 + ZX2] = 14. Find μ and σ. 1 3 1 3 1 3 3 2 1 3 3 2 1 3 3 2 Solution X, Y , and Z are  independent  ⇒ { EX2 ⋅ EY + EX ⋅ EY ⋅ EZ = 13 EX ⋅ EY 2 + EZ ⋅ EX2 = 14 Since Y , Z ∼ Uniform(0, 2), we conclude EY = EZ = 1; Var(Y ) = Var(Z) = = . Therefore, EY 2 = + 1 = . Thus, { EX2 + EX = 13 EX + EX2 = 14 We conclude EX = 3, EX2 = 10. Therefore, { μ = 3 μ2 + σ2 = 10 So, we obtain μ = 3,σ = 1. Problem 3 Let X1, X2, and X3 be three i.i.d Bernoulli(p) random variables and Y1 = max(X1, X2), Y2 = max(X1, X3), Y3 = max(X2, X3), Y = Y1 + Y2 + Y3. Find EY and Var(Y ). Solution We have EY = EY1 + EY2 + EY3 = 3EY1, by symmetry. (2 − 0) 2 12 1 3 1 3 4 3 4 3 Also, Var(Y ) = Var(Y1) + Var(Y2) + Var(Y3) + 2Cov(Y1, Y2) + 2Cov(Y1, Y3) + 2Cov(Y2, Y3) = 3Var(Y1) + 6Cov(Y1, Y2), by symmetry. Note that Yi's are also Bernoulli random variables (but they are not independent). In particular , we have P(Y1 = 1) = P((X1 = 1) or (X2 = 1)) = P(X1 = 1) + P(X2 = 1) − P(X1 = 1, X2 = 1) (comma means “and”) = 2p − p 2. Thus, Y1 ∼ Bernoulli(2p − p 2), and we obtain EY1 = 2p − p 2 = p(2 − p), Var(Y1) = (2p − p 2)(1 − 2p + p 2) = p(2 − p)(1 − p) 2. It remains to find Cov(Y1, Y2). We can write Cov(Y1, Y2) = E[Y1Y2] − E[Y1]E[Y2] = E[Y1Y2] − p 2(2 − p) 2. Note that Y1Y2 is also a Bernoulli random variable. W e have E[Y1Y2] = P(Y1 = 1, Y2 = 1) = P((X1 = 1) or (X2 = 1, X3 = 1)) = P(X1 = 1) + P(X2 = 1, X3 = 1) − P(X1 = 1, X2 = 1, X3 = 1) = p + p 2 − p 3. Thus, we obtain Cov(Y1, Y2) = E[Y1Y2] − p 2(2 − p) 2 = p + p 2 − p 3 − p 2(2 − p) 2. Finally , we obtain EY = 3EY1 = 3p(2 − p). Also, Var(Y ) = 3Var(Y1) + 6Cov(Y1, Y2) = 3p(2 − p)(1 − p) 2 + 6(p + p 2 − p 3 − p 2(2 − p) 2). Problem 4 Let MX(s) be finite for s ∈ [−c, c], where c > 0. Show that MGF of Y = aX + b is given by MY (s) = e sbMX(as), and it is finite in [− , ]. Solution We have MY (s) = E[e sY ] = E[e saXe sb] = e sbE[e (sa)X] = e sbMX(as). Also, since MX(s) is finite for s ∈ [−c, c], MX(as) is finite for s ∈ [− , ]. Problem 5 Let Z ∼ N(0, 1) Find the MGF of Z. Extend your result to X ∼ N(μ, σ). Solution We have MZ(s) = E[e sZ] = ∫ ∞ −∞ e sxe − dx = ∫ ∞ −∞ e e − dx = e ∫ ∞ −∞ e − dx = e (PDF of normal integrates to 1). Using Problem 4, we obtain c |a| c |a| c |a| c |a| 1 √2π x2 2 1 √2π s2 2 (x−s)2 2 s2 2 1 √2π (x−s)2 2 s2 2 2 2 MX(s) = e sμ+ , for all s ∈ R. Problem 6 Let Y = X1 + X2 + X3+. . . +Xn, where Xi's are independent and Xi ∼ Poisson(λi). Find the distribution of Y . Solution We have MXi(s) = e λi(es−1),  for all s ∈ R. Thus, MY (s) = n ∏ i=1 e λi(es−1) = e (∑n i=1 λi)(es−1),  for all s ∈ R. which is the MGF of a Poisson random variable with parameter λ = ∑n i=1 λi, thus Y ∼ Poisson( n ∑ i=1 λi). Problem 7 Probability Generating Functions (PGFs): For many important discrete random variables, the range is a subset of {0, 1,2,...}. For these random variables it is usually more useful to work with probability generating functions (PGF)s defined as GX(z) = E[Z X] = ∞ ∑ n=0 P(X = n)Z n, for all Z ∈ R that GX(Z) is finite. 1 . Show that GX(Z) is always finite for |Z| ≤ 1. 2 . Show that if X and Y are independent, then GX+Y (Z) = GX(Z)GY (Z). 3 . Show that σ2s2 2 |z=0 = P(X = k). 4 . Show that |z=1 = E[X(X − 1)(X − 2). . . (X − k + 1)]. Solution 1 . If |Z| ≤ 1, then Z n ≤ |Z| ≤ 1, so we have GX(z) = ∞ ∑ n=0 P(X = n)Z n ≤ ∞ ∑ n=0 P(X = n) = 1. 2 . If X and Y are independent, then GX+Y (Z) = E[Z X+Y ] = E[Z XZ Y ] = E[Z X]E[Z Y ] (since X and Y  are independent) = GX(Z)GY (Z). 3 . By dif ferentiation we obtain = ∞ ∑ n=k n(n − 1)(n − 2). . . (n − k + 1)P(X = n)Z n−k. Thus, = k!P(X = k) + ∞ ∑ n=k+1 n(n − 1)(n − 2). . . (n − k + 1)P(X = n)Z n−k. Thus, |z=0 = P(X = k). 4 . By letting Z = 1 in = ∞ ∑ n=k n(n − 1)(n − 2). . . (n − k + 1)P(X = n)Z n−k, 1 k! dkGX(z) dzk dkGX(z) dzk dkGX(z) dzk dkGX(z) dzk 1 k! dkGX(z) dzk dkGX(z) dzk we obtain |z=1 = ∞ ∑ n=k n(n − 1)(n − 2). . . (n − k + 1)P(X = n), which by LOTUS is equal to E[X(X − 1)(X − 2). . . (X − k + 1)]. Problem 8 Let MX(s) be finite for s ∈ [−c, c] where c > 0. Prove lim n→∞ [MX( )]n = e sEX. Solution Equivalently , we show lim n→∞ n ln(MX( )) = sEX. We have lim n→∞ n ln(MX( )) = lim n→∞ = . So, we can use L'Hôpital's rule lim n→∞ = lim t→0 (let t = ) = lim t→0 (by L'Hô pital's rule) = = sμ (since M ′ X(0) = μ, MX(0) = 1). Problem 9 dkGX(z) dzk s n s n s n ln(MX( )) s n 1 n 0 0 ln(MX( )) s n 1 n ln(MX(ts)) t 1 n sM ′ X(ts) MX(ts) 1 sM ′ X(0) MX(0) Let MX(s) be finite for s ∈ [−c, c], where c > 0. Assume EX = 0, and Var(X) = 1. Prove lim n→∞ [MX ( )] n = e . Note: From this, we can prove the Central Limit Theorem (CL T) which is discussed in Section 7.1. Solution Equivalently , we show lim n→∞ n ln(MX( )) = . We have lim n→∞ n ln(MX( )) = lim n→∞ (let t = ) = lim t→0 = lim t→0 (by L'Hô pital's rule) = lim t→0 (again , ) = lim t→0 (by L'Hô pital's rule) = (since M ′′ X(0) = EX2 = 1). Problem 10 We can define MGF for jointly distributed random variables as well. For example, for two random variables (X, Y ), the MGF is defined by MXY (s, t) = E[e sX+tY ]. s √n s2 2 s √n s2 2 s √n ln(MX( )) s √n 1 n 1 √n ln(MX(ts)) t2 sM ′ X(ts) MX(ts) 2t sM ′ X(ts) 2t 0 0 s2M ′′ X(ts) 2 s2 2 Similar to the MGF of a single random variable, the MGF of the joint distributions uniquely determines the joint distribution. Let X and Y be two jointly normal random variables with EX = μX, EY = μY , Var(X) = σ2 X, Var(Y ) = σ2 Y , ρ(X, Y ) = ρ . Find MXY (s, t). Solution Note that U = sX + tY is a linear combination of X and Y and thus it is a normal random variable. W e have EU = sEX + tEY = sμX + tμY , Var(U) = s2Var(X) + t2Var(Y ) + 2stρ(X, Y )σXσY = s2σ2 X + t2σ2 Y + 2stρσXσY . Thus U ∼ N(sμX + tμY , s2σ2 X + t2σ2 Y + 2stρσXσY ). Note that for a normal random variable with mean μ and variance σ2 the MGF is given by e sμ+ . Thus MXY (s, t) = E[e U ] = MU (1) = e μU+ = e sμX+tμY + (s2σ2 X+t2σ2 Y +2stρσXσY ). Problem 1 1 Let X = [ X1 X2 ] be a normal random vector with the following mean vector and covariance matrix m = [ 0 1 ] , C = [ 1 −1 −1 2 ] . Let also A = ⎡ ⎢ ⎣ 1 2 2 1 1 1 ⎤ ⎥ ⎦ , b = ⎡ ⎢ ⎣ 0 1 2 ⎤ ⎥ ⎦ , Y = ⎡ ⎢ ⎣ Y1 Y2 Y3 ⎤ ⎥ ⎦ = AX + b. a . Find P(0 ≤ X2 ≤ 1). σ2s2 2 σ2 U 2 1 2 b . Find the expected value vector of Y, mY = EY. c. Find the covariance matrix of Y, CY. d. Find P(Y3 ≤ 4). Solution (a) From m and c we have X2 ∼ N(1, 2). Thus P(0 ≤ X2 ≤ 1) = Φ ( ) − Φ ( ) = Φ (0) − Φ ( ) = 0.2602 (b) mY = EY = AEX + b = ⎡ ⎢ ⎣ 1 2 2 1 1 1 ⎤ ⎥ ⎦ . [ 0 1 ] + ⎡ ⎢ ⎣ 0 1 2 ⎤ ⎥ ⎦ = ⎡ ⎢ ⎣ 2 2 3 ⎤ ⎥ ⎦ . (c) CY = ACXAT = ⎡ ⎢ ⎣ 1 2 2 1 1 1 ⎤ ⎥ ⎦ . [ 1 −1 −1 2 ] . [ 1 2 1 2 1 1 ] = ⎡ ⎢ ⎣ 5 1 2 1 2 1 2 1 1 ⎤ ⎥ ⎦ . (d) From mY and cY we have Y3 ∼ N(3, 1), thus P(Y3 ≤ 4) = Φ ( ) = Φ (1) = 0.8413 Problem 12 (Whitening/decorrelating transformation) Let X be an n-dimensional zero-mean 1 − 1 √2 0 − 1 √2 −1 √2 4 − 3 1 random vector . Since CX is a real symmetric matrix, we conclude that it can be diagonalized. That is, there exists an n by n matrix Q such that QQT = I (I is the identity matrix), CX = QDQT , where D is a diagonal matrix D = ⎡ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎣ d11 0 . . . 0 0 d22 . . . 0 . . . . . . . . . . . . 0 0 . . . dnn ⎤ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎦ . Now suppose we define a new random vector Y as Y = QT X, thus X = QY . Show that Y has a diagonal covariance matrix, and conclude that components of Y are uncorrelated, i.e., Cov(Yi, Yj) = 0 if i ≠ j. Solution CY = E[(Y − EY )(Y − EY ) T ] = E[(QT X − EQT X)(QT X − EQT X) T ] = E[QT (X − EX)(X − EX) T ]Q] = QT CXQ = QT QDQT Q = D (since QT Q = I). Therefore, Y has a diagonal covariance matrix, and Cov(Yi, Yj) = 0 if i ≠ j. 6.2.0 Probability Bounds In this section, we will discuss probability bounds. These are inequalities that are usually applicable to a general scenario. There are several scenarios in which we resort to inequalities. Sometimes we do not have enough information to calculate a desired quantity (such as the probability of an event or the expected value of a random variable). In other situations, the problem might be complicated and exact calculation might be very dif ficult. In other scenarios, we might want to provide a result that is general and applicable to wide range of problems. For example, suppose that you are an engineer and you design a communication system. Y our company wants to ensure that the error probability in your system be less than a given value, say 10−5. Calculating the exact value of probability might be difficult due to some unknown parameters or simply because the communication system is a complicated one. Here you do not actually need to find the error probability exactly , but all you need to do is to show that it is less than 10−5. In this section, we will discuss several inequalities. Depending on the problem you are dealing with, you might decide which one to use. 6.2.1 The Union Bound and Extension The union bound or Boole's inequality [ 13 ] is applicable when you need to show that the probability of union of some events is less than some value. Remember that for any two events A and B we have P(A ∪ B) = P(A) + P(B) − P(A ∩ B) ≤ P(A) + P(B). Similarly , for three events A, B, and C, we can write P(A ∪ B ∪ C) = P((A ∪ B) ∪ C) ≤ P(A ∪ B) + P(C) ≤ P(A) + P(B) + P(C). In general, using induction we prove the following The Union Bound For any events A1, A2, . . . , An, we have P( n ⋃ i=1 Ai) ≤ n ∑ i=1 P(Ai). (6.2) The union bound is a very simple but useful result. It is used frequently in dif ferent applications. Here, we look at one application in the area of random graphs . Random graphs are widely used when analyzing social networks, wireless networks, and the internet. A simple model for random graphs is the Erdös-Rényi model G(n, p) [ 1 1 ,12 ]. In this model, we have n nodes in the graph. In social networking context, each node might represent a person. Every pair of nodes are connected by an edge with probability p. The occurrence of each edge in the graph is independent from other edges in the graph. Figure 6.1 shows an example of a randomly generated graph using this model. Here, n = 5 and p was chosen to be . 1 2 Fig.6.1 - An example of a randomly generated graph based on the G(n, p) model. Here n = 5 and p was chosen to be . The question we are interested in here is the probability that there exists an isolated node in the graph [ 1 1 ,12 ]. An isolated node is a node that is not connected to any other nodes in the graph. In a wireless networking context, an isolated node is a node that cannot communicate with any other node in the network. Example 6. 17 Let Bn be the event that a graph randomly generated according to G(n, p) model has at least one isolated node. Show that P(Bn) ≤ n(1 − p) n−1. And conclude that for any ϵ > 0, if p = pn = (1 + ϵ) then lim n→∞ P(Bn) = 0. Solution There are n nodes in the network. Let's call them Node 1, Node 2,..., Node n. Let Ai be the event that the ith node is isolated. Then we have Bn = n ⋃ i=1 Ai. Thus, using the union bound we conclude that 1 2 ln(n) n P(Bn) = P( n ⋃ i=1 Ai) ≤ n ∑ i=1 P(Ai). By symmetry , for all i, j, we have P(Ai) = P(Aj), so P(Bn) ≤ nP(A1). Thus, we only need to find P(A1). The event A1 occurs if Node 1 is not connected to any of the other n − 1 nodes. Since the connections are independent, we conclude that P(A1) = (1 − p) n−1. Therefore, we obtain P(Bn) ≤ n(1 − p) n−1, which is the desired result. To prove the limit result, we use lim x→∞ (1 + ) x = e c,  for any constant c ∈ R. So, we obtain limn→∞ P(Bn) ≤ limn→∞ n(1 − pn) n−1 = limn→∞ n[1 − (1 + ϵ) ] n−1 = limn→∞ n[1 − ] n−1 = limn→∞ n([1 − ] ) = limn→∞ ne −(1+ϵ) ln n = limn→∞ = 0. But since P(Bn) ≥ 0, we conclude lim n→∞ P(Bn) = 0. It is an interesting exercise to calculate P(Bn) exactly using the inclusion-exclusion principle: c x ln n n 1+ϵ n ln n 1+ϵ n ln n n ln n (n−1) ln n n 1 nϵ P( n ⋃ i=1 Ai) = n ∑ i=1 P(Ai) − ∑ i<j P(Ai ∩ Aj) + ∑ i<j<k P(Ai ∩ Aj ∩ Ak) −   ⋯   + (−1) n−1 P( n ⋂ i=1 Ai). In fact, the union bound states that the probability of union of some events is smaller than the first term in the inclusion-exclusion formula. W e can in fact extend the union bound to obtain lower and upper bounds on the probability of union of events. These bounds are known as Bonferroni inequalities [ 13 ]. The idea is very simple. Start writing the inclusion-exclusion formula. If you stop at the first term, you obtain an upper bound on the probability of union. If you stop at the second term, you obtain a lower bound. If you stop at the third term, you obtain an upper bound, etc. So in general if you write an odd number of terms, you get an upper bound and if you write an even number of terms, you get a lower bound. Generalization of the Union Bound: Bonferroni Inequalities For any events A1, A2, . . . , An, we have P( n ⋃ i=1 Ai) ≤ n ∑ i=1 P(Ai); P( n ⋃ i=1 Ai) ≥ n ∑ i=1 P(Ai) − ∑ i<j P(Ai ∩ Aj); P( n ⋃ i=1 Ai) ≤ n ∑ i=1 P(Ai) − ∑ i<j P(Ai ∩ Aj) + ∑ i<j<k P(Ai ∩ Aj ∩ Ak). . . . Example 6. 18 Let Bn be the event that a graph randomly generated according to G(n, p) model has at least one isolated node. Show that P(Bn) ≥ n(1 − p) n−1 − ( )(1 − p) 2n−3. Solution n 2 Similar to Example 6.17, let Ai be the event that the ith node is isolated. Then we have Bn = n ⋃ i=1 Ai. Thus, using two terms in the inclusion-exclusion principle, we obtain P(Bn) = P( n ⋃ i=1 Ai) ≥ n ∑ i=1 P(Ai) − ∑ i<j P(Ai ∩ Aj). By symmetry , we obtain n ∑ i=1 P(Ai) = nP(A1), ∑ i<j P(Ai ∩ Aj) = ( )P(A1 ∩ A2). Thus, we conclude P(Bn) ≥ nP(A1) − ( )P(A1 ∩ A2). In Example 6.17 we found P(A1) = (1 − p) n−1. Similarly , we obtain P(A1 ∩ A2) = (1 − p) 2(n−2)+1 = (1 − p) 2n−3. The reason for this is that A1 ∩ A2 is the event that Nodes 1 and 2 are isolated. There are 2(n − 2) potential edges from the rest of the graph two Nodes 1 and 2, and there also is a potential edge from Node 1 to Node 2. These edges exist independently from each other and with probability pn. We conclude P(Bn) ≥ n(1 − p) n−1 − ( )(1 − p) 2n−3, which is the desired result. Expected V alue of the Number of Events: It is interesting to note that the union bound formula is also equal to the expected value of the number of occurred events. n 2 n 2 n 2 To see this, let A1, A2, . . . , An be any events. Define the indicator random variables X1, X2,...,Xn as Xi = ⎧⎪ ⎨ ⎪⎩ 1 if Ai occurs 0 otherwise If we define X = X1 + X2 + X3+. . . +Xn, then X shows the number of events that actually occur . We then have EX = EX1 + EX2 + EX3+. . . +EXn (by linearity of expectation) = P(A1) + P(A2)+. . . +P(An), which is indeed the righthand-side of the union bound. For example, from this we can conclude that the expected number of isolated nodes in a graph randomly generated according to G(n, p) is equal to EX = n(1 − p) n−1. 6.2.2 Markov and Chebyshev Inequalities Let X be any positive continuous random variable, we can write EX = ∫ ∞ −∞ xfX(x)dx = ∫ ∞ 0 xfX(x)dx ≥ ∫ ∞ a xfX(x)dx ≥ ∫ ∞ a afX(x)dx = a ∫ ∞ a fX(x)dx = aP(X ≥ a). Thus, we conclude P(X ≥ a) ≤ , for any a > 0. We can prove the above inequality for discrete or mixed random variables similarly (using the generalized PDF), so we have the following result, called Markov's inequality . Markov's Inequality If X is any nonnegative random variable, then P(X ≥ a) ≤ , Example 6. 19 Prove the union bound using Markov's inequality . Solution Similar to the discussion in the previous section, let A1, A2, . . . , An be any events and X be the number events Ai that occur . We saw that EX a EX a EX = P(A1) + P(A2)+. . . +P(An) = n ∑ i=1 P(Ai). Since X is a nonnegative random variable, we can apply Markov's inequality . Choosing a = 1, we have P(X ≥ 1) ≤ EX = n ∑ i=1 P(Ai). But note that P(X ≥ 1) = P(⋃ n i=1 Ai). Example 6. 20 Let X ∼ Binomial(n, p). Using Markov's inequality , find an upper bound on P(X ≥ αn) , where p < α < 1. Evaluate the bound for p = and α = . Solution Note that X is a nonnegative random variable and EX = np. Applying Markov's inequality , we obtain P(X ≥ αn) ≤ = = . For p = and α = , we obtain P(X ≥ ) ≤ . Chebyshev's Inequality: Let X be any random variable. If you define Y = (X − EX)2, then Y is a nonnegative random variable, so we can apply Markov's inequality to Y . In particular , for any positive real number b, we have P(Y ≥ b2) ≤ . But note that 1 2 3 4 EX αn pn αn p α 1 2 3 4 3n 4 2 3 EY b2 EY = E(X − EX) 2 = V ar(X), P(Y ≥ b2) = P((X − EX) 2 ≥ b2) = P(|X − EX| ≥ b). Thus, we conclude that P(|X − EX| ≥ b) ≤ . This is Chebyshev's inequality . Chebyshev's Inequality If X is any random variable, then for any b > 0 we have P(|X − EX| ≥ b) ≤ . Chebyshev's inequality states that the dif ference between X and EX is somehow limited by V ar(X). This is intuitively expected as variance shows on average how far we are from the mean. Example 6. 21 Let X ∼ Binomial(n, p). Using Chebyshev's inequality , find an upper bound on P(X ≥ αn), where p < α < 1. Evaluate the bound for p = and α = . Solution One way to obtain a bound is to write P(X ≥ αn) = P(X − np ≥ αn − np) ≤ P(|X − np| ≥ nα − np) ≤ = . For p = and α = , we obtain P(X ≥ ) ≤ . V ar(X) b2 V ar(X) b2 1 2 3 4 V ar(X) (nα−np)2 p(1−p) n(α−p)2 1 2 3 4 3n 4 4 n 6.2.3 Chernoff Bounds If X is a random variable, then for any a ∈ R, we can write P(X ≥ a) = P(e sX ≥ e sa),  for s > 0, P(X ≤ a) = P(e sX ≥ e sa),  for s < 0. Now , note that e sX is always a positive random variable for all s ∈ R. Thus, we can apply Markov's inequality . So for s > 0, we can write P(X ≥ a) = P(e sX ≥ e sa) ≤ ,  by Markov's inequality. Similarly , for s < 0, we can write P(X ≤ a) = P(e sX ≥ e sa) ≤ . Note that E[e sX] is in fact the moment generating function, MX(s). Thus, we conclude Chernof f Bounds: P(X ≥ a) ≤ e −saMX(s),  for all s > 0, P(X ≤ a) ≤ e −saMX(s),  for all s < 0 Since Chernof f bounds are valid for all values of s > 0 and s < 0, we can choose s in a way to obtain the best bound, that is we can write P(X ≥ a) ≤ min s>0 e −saMX(s), P(X ≤ a) ≤ min s<0 e −saMX(s). Let us look at an example to see how we can use Chernof f bounds. E[e sX] esa E[e sX] esa Example 6. 22 Let X ∼ Binomial(n, p). Using Chernof f bounds, find an upper bound on P(X ≥ αn), where p < α < 1. Evaluate the bound for p = and α = . Solution For X ∼ Binomial(n, p), we have MX(s) = (pe s + q) n,  where q = 1 − p. Thus, the Chernof f bound for P(X ≥ a) can be written as P(X ≥ αn) ≤ min s>0 e −saMX(s)   = min s>0 e −sa(pe s + q) n. To find the minimizing value of s, we can write e −sa(pe s + q) n = 0, which results in e s = . By using this value of s in Equation 6.3 and some algebra, we obtain P(X ≥ αn) ≤ ( ) (1−α)n( ) αn. For p = and α = , we obtain P(X ≥ n) ≤ ( ) . Comparison between Markov , Chebyshev , and Chernoff Bounds: Above, we found upper bounds on P(X ≥ αn) for X ∼ Binomial(n, p). It is interesting to compare them. Here are the results that we obtain for p = and α = : 1 2 3 4 d ds aq np(1 − α) 1 − p 1 − α p α 1 2 3 4 3 4 16 27 n 4 1 4 3 4 P(X ≥ ) ≤ Markov, P(X ≥ ) ≤ Chebyshev, P(X ≥ ) ≤ ( ) Chernoff. The bound given by Markov is the \"weakest\" one. It is constant and does not change as n increases. The bound given by Chebyshev's inequality is \"stronger\" than the one given by Markov's inequality . In particular , note that goes to zero as n goes to infinity . The strongest bound is the Chernof f bound. It goes to zero exponentially fast. 3n 4 2 3 3n 4 4 n 3n 4 16 27 n 4 4 n 6.2.4 Cauchy-Schwarz Inequality You might have seen the Cauchy-Schwarz inequality in your linear algebra course. The same inequality is valid for random variables. Let us state and prove the Cauchy- Schwarz inequality for random variables. Cauchy-Schwarz Inequality For any two random variables X and Y , we have |EXY | ≤ √E[X2]E[Y 2], where equality holds if and only if X = αY , for some constant α ∈ R. You can prove the Cauchy-Schwarz inequality with the same methods that we used to prove |ρ(X, Y )| ≤ 1 in Section 5.3.1. Here we provide another proof. Define the random variable W = (X − αY ) 2. Clearly , W is a nonnegative random variable for any value of α ∈ R. Thus, we obtain 0 ≤ EW = E(X − αY ) 2 = E[X2 − 2αXY + α 2Y 2] = E[X2] − 2αE[XY ] + α 2E[Y 2]. So, if we let f(α) = E[X2] − 2αE[XY ] + α 2E[Y 2], then we know that f(α) ≥ 0, for all α ∈ R. Moreover , if f(α) = 0 for some α, then we have EW = E(X − αY ) 2 = 0, which essentially means X = αY with probability one. T o prove the Cauchy-Schwarz inequality , choose α = . W e obtain 0 ≤ E[X2] − 2αE[XY ] + α2E[Y 2] = E[X2] − 2 E[XY ] + E[Y 2] = E[X2] − . Thus, we conclude (E[XY ]) 2 ≤ E[X2]E[Y 2], EXY EY 2 EXY EY 2 (EXY )2 (EY 2)2 (E[XY ])2 EY 2 which implies |EXY | ≤ √E[X2]E[Y 2]. Also, if |EXY | = √E[X2]E[Y 2], we conclude that f( ) = 0, which implies X = Y with probability one. Example 6. 23 Using the Cauchy-Schwarz inequality , show that for any two random variables X and Y |ρ(X, Y )| ≤ 1. Also, |ρ(X, Y )| = 1 if and only if Y = aX + b for some constants a, b ∈ R. Solution Let U = , V = . Then EU = EV = 0, and V ar(U) = V ar(V ) = 1. Using the Cauchy-Schwarz inequality for U and V , we obtain |EUV | ≤ √E[U 2]E[V 2] = 1. But note that EUV = ρ(X, Y ), thus we conclude |ρ(X, Y )| ≤ 1, where equality holds if and only if V = αU for some constant α ∈ R. That is = α , which implies Y = X + (EY − EX). In the Solved Problems section, we provide a generalization of the Cauchy-Schwrarz inequality , called Hölder's inequality . EXY EY 2 EXY EY 2 X − EX σX Y − EY σY Y − EY σY X − EX σX ασY σX ασY σX 6.2.5 Jensen's Inequality Remember that variance of every random variable X is a positive value, i.e., V ar(X) = EX2 − (EX) 2 ≥ 0. Thus, EX2 ≥ (EX) 2. If we define g(x) = x 2, we can write the above inequality as E[g(X)] ≥ g(E[X]). The function g(x) = x 2 is an example of convex function. Jensen's inequality states that, for any convex function g, we have E[g(X)] ≥ g(E[X]). So what is a convex function? Figure 6.2 depicts a convex function. A function is convex if, when you pick any two points on the graph of the function and draw a line segment between the two points, the entire segment lies above the graph. On the other hand, if the line segment always lies below the graph, the function is said to be concave . In other words, g(x) is convex if and only if −g(x) is concave. Fig.6.2 - Pictorial representation of a convex function and a concave function. We can state the definition for convex and concave functions in the following way: Definition 6. 3 Consider a function g : I → R, where I is an interval in R. We say that g is a convex function if, for any two points x and y in I and any α ∈ [0, 1], we have g(αx + (1 − α)y) ≤ αg(x) + (1 − α)g(y). We say that g is concave if g(αx + (1 − α)y) ≥ αg(x) + (1 − α)g(y). Note that in the above definition the term αx + (1 − α)y is the weighted average of x and y. Also, αg(x) + (1 − α)g(y) is the weighted average of g(x) and g(y). More generally , for a convex function g : I → R, and x1, x2,...,xn in I and nonnegative real numbers αi such that α1 + α2+. . . +αn = 1, we have g(α1x1 + α2x2+. . . +αnxn) ≤ α1g(x1) + α2g(x2)+. . . +αng(xn) (6.4) If n = 2, the above statement is the definition of convex functions. Y ou can extend it to higher values of n by induction. Now , consider a discrete random variable X with n possible values x1, x2,...,xn. In Equation 6.4, we can choose αi = P(X = xi) = PX(xi). Then, the left-hand side of 6.4 becomes g(EX) and the right-hand side becomes E[g(X)] (by LOTUS). So we can prove the Jensen's inequality in this case. Using limiting arguments, this result can be extended to other types of random variables. Jensen's Inequality: If g(x) is a convex function on RX, and E[g(X)] and g(E[X]) are finite, then E[g(X)] ≥ g(E[X]). To use Jensen's inequality , we need to determine if a function g is convex. A useful method is the second derivative. A twice-differentiable function g : I → R is convex if and only if g ′′(x) ≥ 0 for all x ∈ I. For example, if g(x) = x 2, then g ′′(x) = 2 ≥ 0, thus g(x) = x 2 is convex over R. Example 6. 24 Let X be a positive random variable. Compare E[Xa] with (E[X]) a for all values of a ∈ R. Solution First note E[Xa] = 1 = (E[X]) a,  if a = 0, E[Xa] = EX = (E[X]) a,  if a = 1. So let's assume a ≠ 0, 1. Letting g(x) = x a, we have g ′′(x) = a(a − 1)x a−2. On (0, ∞), we can say g ′′(x) is positive, if a < 0 or a > 1. It is negative, if 0 < a < 1. Therefore we conclude that g(x) is convex, if a < 0 or a > 1. It is concave, if 0 < a < 1. Using Jensen's inequality we conclude E[Xa] ≥ (E[X]) a,  if a < 0 or a > 1, E[Xa] ≤ (E[X]) a,  if 0 < a < 1. 6.2.6 Solved Problems Problem 1 Your friend tells you that he had four job interviews last week. He says that based on how the interviews went, he thinks he has a 20% chance of receiving an of fer from each of the companies he interviewed with. Nevertheless, since he interviewed with four companies, he is 90% sure that he will receive at least one of fer. Is he right? Solution Let Ai be the event that your friend receives an of fer from the ith company , i=1,2,3,4. Then, by the union bound: P (⋃ 4 i=1 Ai) ≤ ∑ P(Ai) = 0.2 + 0.2 + 0.2 + 0.2 = 0.8 Thus the probability of receiving at least one of fer is less than or equal to 80%. Problem 2 An isolated edge in a network is an edge that connects two nodes in the network such that neither of the two nodes is connected to any other nodes in the network. Let Cn be the event that a graph randomly generated according to G(n, p) model has at least one isolated edge. a . Show that P(Cn) ≤ ( )p(1 − p) 2(n−2) b . Show that, for any constant b > , if p = pn = b then lim n→∞ P(Cn) = 0. Solution n 2 1 2 ln(n) n There are ( ) possible edges in the graph. Let Ei be the event that the ith edge is an isolated edge, then P(Ei) = p(1 − p) 2(n−2), where p in the above equation is the probability that the ith edge is present and (1 − p) 2(n−2) is the probability that no other nodes are connected to this edge. By the union bound, we have P(Cn) = P (⋃ Ei) ≤ ∑i P(Ei) = ( )p(1 − p) 2(n−2), which is the desired result. Now , let p = b , where b > . Here, it is convenient to use the following inequality: 1 − x ≤ e −x, for all x ∈ R. You can prove it by dif ferentiating f(x) = e −x + x − 1, and showing that the minimum occurs at x = 0. Now , we can write P(Cn) = ( )p(1 − p) 2(n−2) = (1 − p) 2(n−2) ≤ e −2p(n−2) (using 1 − x ≤ e −x) = be −2 (n−2). Thus, limn→∞ P(Cn) ≤ limn→∞ be −2 (n−2) = limn→∞ bn−2b = limn→∞(n1−2b) = 0 (since b > ). Problem 3 Let X ∼ Exponential(λ). Using Markov's inequality find an upper bound for P(X ≥ a). Compare the upper bound with the actual value of P(X ≥ a). n 2 n 2 ln n n 1 2 n 2 n(n−1) 2 b ln n n (n−1)b 2 (n−1) 2 b ln n n (n−1) 2 b ln n n (n−1) 2 b 2 1 2 Solution If X ∼ Exponential(λ), then EX = , using Markov's inequality P (X ≥ a) ≤ = . The actual value of P(X ≥ a) is e −λa, and we always have ≥ e −λa. Problem 4 Let X ∼ Exponential(λ). Using Chebyshev's inequality find an upper bound for P(|X − EX| ≥ b). Solution a . We have EX = and V arX = . Using Chebyshev's inequality , we have P (|X − EX| ≥ b) ≤ = . Problem 5 Let X ∼ Exponential(λ). Using Chernof f bounds find an upper bound for P(X ≥ a), where a > EX. Compare the upper bound with the actual value of P(X ≥ a). Solution If X ∼ Exponential(λ), then MX(s) = , for s < λ. Using Chernof f bounds, we have 1 λ EX a 1 λa 1 λa 1 λ 1 λ2 V ar(X) b2 1 λ2b2 λ λ − s P (X ≥ a) ≤ min s>0 [e −saMX(s)] = min s>0 [e −sa ] . If f(s) = e −sa , to find mins>0 f(s) we write f(s) = 0. Therefore, s∗ = λ − . Note since a > EX = , then λ − > 0. Thus, P (X ≥ a) ≤ e −s∗a = aλe 1−λa. The real value of P (X ≥ a) is e−λa and we have e−λa ≤ aλe1−λa, or equivalently , aλe ≥ 1, which is true since a > . Problem 6 Let X and Y be two random variables with EX = 1, V ar(X) = 4, and EY = 2, V ar(Y ) = 1. Find the maximum possible value for E[XY ]. Solution Using ρ(X, Y ) ≤ 1 and ρ(X, Y ) = , we conclude ≤ 1. Thus EXY ≤ σXσY + EXEY = 2 × 1 + 2 × 1 = 4. In fact, we can achieve EXY = 4, if we choose Y = aX + b. λ λ − s λ λ−s d ds 1 a 1 λ 1 a λ λ − s∗ 1 λ Cov(X,Y ) σXσY EXY − EXEY σXσY Y = aX + b ⇒ ⎧⎪ ⎨ ⎪⎩ 2 = a + b 1 = (a 2)(4) Solving for a and b, we obtain a = , b = . Note that if you use the Cauchy-Schwarz inequality directly , you obtain: |EXY | 2 ≤ EX2 ⋅ EY 2 = 5 × 5. Thus EXY ≤ 5. But EXY = 5 cannot be achieved because equality in the Cauchy-Schwarz is obtained only when Y = αX. But here this is not possible. Problem 7 (Hölder's Inequality) Prove E [|XY |] ≤ E[|X| p] E[|Y | q] , where 1 < p, q < ∞ and + = 1. Note that, for p = q = , Hölder's ineqality becomes the Cauchy-Schwarz inequality . Hint: You can use Y oung's inequality [ 4 ] which states that for nonnegative real numbers α and β and integers p and q such that 1 < p, q < ∞ and + = 1, we have αβ ≤ + , with equality only if αp = βq. Solution Using Y oung's inequality , we conclude that for random variables U and V we have E|UV | ≤ + . 1 2 3 2 1 p 1 q 1 p 1 q 1 2 1 p 1 q α p p βq q E|U| p p E|V | q q Choose U = and V = . We obtain ≤ + = + = 1. Problem 8 Show that if h : R ↦ R is convex and non-decreasing, and g : R ↦ R is convex, then h(g(x)) is a convex function. Solution Since g is convex, we have g(αx + (1 − α)y) ≤ αg(x) + (1 − α)g(y), for all α ∈ [0, 1]. Therefore, we have h(g(αx + (1 − α)y)) ≤ h(αg(x) + (1 − α)g(y)) (h is non-decreasing) ≤ αh(g(x)) + (1 − α)h(g(y)) (h is convex). Problem 9 Let X be a positive random variable with EX = 10. What can you say about the following quantities? a . E[ ] b . E[e ] c. E[ln √X] Solution a . g(x) = , g ′′ (x) = > 0, for x > 0. |X| (E|X| p) 1 p |Y | (E|Y | q) 1 q E|XY | (E|X| p) (E|Y | q) 1 p 1 q E|X| p pE|X| p E|Y | q qE|Y | q 1 p 1 q 1 X+1 1 X+1 1 x+1 2 (1+x)3 Thus g is convex on (0, ∞) E [ ] ≥ (Jensen's inequality) = = . b . If we let h(x) = e x, g(x) = then h is convex and non-decreasing and g is convex thus by problem 8, e is a convex function, thus E [e ] ≥ e (by Jensen's inequality) = e . c. If g(x) = ln √x = ln x, then g ′ (x) = for x > 0 and g ′′ (x) = − . Thus g is concave on (0, ∞). We conclude E [ln √X] = E [ ln X] ≤ ln EX (by Jensen's inequality) = ln 10. 1 X+1 1 1+EX 1 1+10 1 11 1 1+x 1 x+1 1 1+X 1 1+EX 1 11 1 2 1 2x 1 2x2 1 2 1 2 1 2 6.3.0 Chapter Problems Problem 1 Let X, Y and Z be three jointly continuous random variables with joint PDF fXY Z(x, y, z) = ⎧⎪ ⎨ ⎪⎩ x + y 0 ≤ x, y, z ≤ 1 0 otherwise 1 . Find the joint PDF of X and Y . 2 . Find the marginal PDF of X. 3 . Find the conditional PDF of fXY |Z(x, y|z) using fXY |Z(x, y|z) = . 4 . Are X and Y independent of Z? Problem 2 Suppose that X, Y , and Z are three independent random variables. If X, Y ∼ N(0, 1) and Z ∼ Exponential(1), find 1 . E[XY |Z = 1], 2 . E[X2Y 2Z 2|Z = 1]. Problem 3 Let X, Y , and Z be three independent N(1, 1) random variables. Find E[XY |Y + Z = 1]. Problem 4 Let X1,X2,⋯,Xn be i.i.d. random variables, where Xi ∼ Bernoulli(p). Define fXY Z(x, y, z) fZ(z) Y1 = X1X2, Y2 = X2X3, ⋮ Yn−1 = Xn−1Xn, Yn = XnX1. If Y = Y1 + Y2 + ⋯ + Yn, find 1 . E[Y ], 2 . Var(Y ). Problem 5 In this problem, our goal is to find the variance of the hypergeometric distribution. Let's remember the random experiment behind the hypergeometric distribution. Y ou have a bag that contains b blue marbles and r red marbles. Y ou choose k ≤ b + r marbles at random (without replacement) and let X be the number of blue marbles in your sample. Then X ∼ Hypergeometric(b, r, k). Now let us define the indicator random variables Xi as follows. Xi = { 1 if the ith chosen marble is blue 0 otherwise Then, we can write X = X1 + X2 + ⋯ + Xk. Using the above equation, show 1 . EX = , 2 . Var(X) = . Problem 6 (MGF of the geometric distribution) If X ∼ Geometric(p), find the MGF of X. Problem 7 If MX(s) = + e s + e 2s, find EX and Var(X). Problem 8 Using MGFs show that if X ∼ N(μX, σ2 X) and Y ∼ N(μY , σ2 Y ) are independent, then kb b+r kbr (b+r)2 b+r−k b+r−1 1 4 1 2 1 4 X + Y ∼ N(μX + μY , σ2 X + σ2 Y ). Problem 9 (MGF of the Laplace distribution) Let X be a continuous random variable with the following PDF fX(x) = e −λ|x|. Find the MGF of X, MX(s). Problem 10 (MGF of Gamma distribution) Remember that a continuous random variable X is said to have a Gamma distribution with parameters α > 0 and λ > 0, shown as X ∼ Gamma(α, λ), if its PDF is given by fX(x) = { x > 0 0 otherwise If X ∼ Gamma(α, λ), find the MGF of X. Hint: Remember that ∫ ∞ 0 xα−1e−λxdx = , for α, λ > 0. Problem 1 1 Using the MGFs show that if Y = X1 + X2 + ⋯ + Xn, where the Xi's are independent Exponential(λ) random variables, then Y ∼ Gamma(n, λ). Problem 12 Let X be a random variable with characteristic function ϕX(ω). If Y = aX + b, show that ϕY (ω) = e jωbϕX(aω). Problem 13 Let X and Y be two jointly continuous random variables with joint PDF λ 2 λαxα−1e−λx Γ(α) Γ(α) λα fX,Y (x, y) = ⎧⎪ ⎪ ⎨ ⎪ ⎪⎩ (3x + y) 0 ≤ x, y ≤ 1 0 otherwise and let the random vector U be defined as U = [ X Y ] . 1 . Find the mean vector of U, EU. 2 . Find the correlation matrix of U, RU. 3 . Find the covariance matrix of U, CU. Problem 14 Let X ∼ Uniform(0, 1). Suppose that given X = x, Y and Z are independent and Y |X = x ∼ Uniform(0, x) and Z|X = x ∼ Uniform(0, 2x). Define the random vector U as U = ⎡ ⎢ ⎣ X Y Z ⎤ ⎥ ⎦ . 1 . Find the PDFs of Y and Z. 2 . Find the PDF of U, fU(u), by using fU(u) = fXY Z(x, y, z) = fX(x)fY |X(y|x)fZ|X,Y (z|x, y). Problem 15 Let X = [ X1 X2 ] be a normal random vector with the following mean and covariance matrices m = [ 1 2 ] , C = [ 4 1 1 1 ] . Let also 1 2 A = ⎡ ⎢ ⎣ 2 1 −1 1 1 3 ⎤ ⎥ ⎦ , b = ⎡ ⎢ ⎣ −1 0 1 ⎤ ⎥ ⎦ , Y = ⎡ ⎢ ⎣ Y1 Y2 Y3 ⎤ ⎥ ⎦ = AX + b. 1 . Find P(X2 > 0). 2 . Find expected value vector of Y, mY = EY. 3 . Find the covariance matrix of Y, CY. 4 . Find P(Y2 ≤ 2). Problem 16 Let X = ⎡ ⎢ ⎣ X1 X2 X3 ⎤ ⎥ ⎦ be a normal random vector with the following mean and covariance m = ⎡ ⎢ ⎣ 1 2 0 ⎤ ⎥ ⎦ , C = ⎡ ⎢ ⎣ 9 1 −1 1 4 2 −1 2 4 ⎤ ⎥ ⎦ . Find the MGF of X defined as MX(s, t, r) = E [e sX1+tX2+rX3 ] . Problem 17 A system consists of 4 components in a series, so the system works properly if all of the components are functional. In other words, the system fails if and only if at least one of its components fails. Suppose the probability that the component i fails is less than or equal to pf = , for i = 1, 2, 3, 4. Find an upper bound on the probability that the system fails. 1 100 Problem 18 A sensor network consists of n sensors that are distributed randomly on the unit square. Each node's location is uniform over the unit square and is independent of the locations of the other node. A node is isolated if there are no nodes that are within distance r of that node, where 0 < r < 1. 1 . Show that the probability that a given node is isolated is less than or equal to pd = (1 − )(n−1). 2 . Using the union bound, find an upper bound on the probability that the sensor network contains at least one isolated node. Problem 19 Let X ∼ Geometric(p). Using Markov's inequality find an upper bound for P(X ≥ a), for a positive integer a. Compare the upper bound with the real value of P(X ≥ a). Problem 20 in Geometric(p). Using Chebyshev's inequality find an upper bound for P(|X − EX| ≥ b). Problem 21 (Cantelli's inequality [ 16 ]) Let X be a random variable with EX = 0 and Var(X) = σ2. We would like to prove that for any a > 0, we have P(X ≥ a) ≤ . This inequality is sometimes called the one-sided Chebyshev inequality . Hint: One way to show this is to use P(X ≥ a) = P(X + c ≥ a + c) for any constant c ∈ R. Problem 22 The number of customers visiting a store during a day is a random variable with mean EX = 100 and variance V ar(X) = 225. 1 . Using Chebyshev's inequality , find an upper bound for having more than 120 or less than 80 customers in a day . That is, find an upper bound on P(X ≤ 80 or X ≥ 120). πr2 4 σ2 σ2 + a2 2 . Using the one-sided Chebyshev inequality (Problem 21), find an upper bound for having more than 120 customers in a day . Problem 23 Let Xi be i.i.d. and Xi ∼ Exponential(λ). Using Chernof f bounds find an upper bound for P(X1 + X2 + ⋯ + Xn ≥ a), where a > . Show that the bound goes to zero exponentially fast as a function of n. Problem 24 (Minkowski's inequality [ 17 ]) Prove for two random variables X and Y with finite moments, and 1 ≤ p < ∞, we have E[|X + Y | p] ≤ E[|X| p] + E[|Y | p] . Hint: Note that |X + Y | p = |X + Y | p−1|X + Y | ≤ |X + Y | p−1(|X| + |Y |) ≤ |X + Y | p−1|X| + |X + Y | p−1|Y |. Therefore E|X + Y | p ≤ E[|X + Y | p−1|X|] + E[|X + Y | p−1|Y |]. Now , apply Hölder's inequality . Problem 25 Let X be a positive random variable with EX = 10. What can you say about the following quantities? 1 . E[X − X3] 2 . E[X ln √X] 3 . E[|2 − X|] Problem 26 Let X be a random variable with EX = 1 and RX = (0, 2). If Y = X3 − 6X2, show that EY ≤ −5. n λ 1 p 1 p 1 p 7.0.0 Introduction In this chapter , we will discuss limit theorems and convergence modes for random variables. Limit theorems are among the most fundamental results in probability theory . We will discuss two important limit theorems in Section 7.1: the law of large numbers (LLN) and the central limit theorem (CL T) . W e will also talk about the importance of these theorems as applied in practice. In Section 7.2 , we will discuss the convergence of sequences of random variables. 7.1.0 Limit Theorems In this section, we will discuss two important theorems in probability , the law of large numbers (LLN) and the central limit theorem (CL T) . The LLN basically states that the average of a large number of i.i.d. random variables converges to the expected value. The CL T states that, under some conditions, the sum of a large number of random variables has an approximately normal distribution. 7.1.1 Law of Large Numbers The law of large numbers has a very central role in probability and statistics. It states that if you repeat an experiment independently a large number of times and average the result, what you obtain should be close to the expected value. There are two main versions of the law of large numbers. They are called the weak and strong laws of the large numbers. The dif ference between them is mostly theoretical. In this section, we state and prove the weak law of large numbers (WLLN). The strong law of large numbers is discussed in Section 7.2 . Before discussing the WLLN, let us define the sample mean . Definition 7. 1 . For i.i.d. random variables X1, X2, . . . , Xn, the sample mean , denoted by ¯¯¯¯¯ X , is defined as ¯¯¯¯¯ X = . Another common notation for the sample mean is Mn. If the Xi's have CDF FX(x) , we might show the sample mean by Mn(X) to indicate the distribution of the Xi 's. Note that since the Xi's are random variables, the sample mean, ¯¯¯¯¯ X = Mn(X), is also a random variable. In particular , we have E[ ¯¯¯¯¯ X ] = (by linearity of expectation) = (since EXi = EX) = EX. Also, the variance of ¯¯¯¯¯ X is given by X1 + X2+. . . +Xn n EX1 + EX2+. . . +EXn n nEX n Var( ¯¯¯¯¯ X ) = (since Var(aX) = a 2Var(X)) = (since the Xi's are independent) = (since Var(Xi) = Var(X)) = . Now let us state and prove the weak law of large numbers (WLLN). The weak law of large numbers (WLLN) Let X1, X2 , ... , Xn be i.i.d. random variables with a finite expected value EXi = μ < ∞. Then, for any ϵ > 0, lim n→∞ P(| ¯¯¯¯¯ X − μ| ≥ ϵ) = 0. Proof The proof of the weak law of large number is easier if we assume Var(X) = σ2 is finite. In this case we can use Chebyshev's inequality to write P(| ¯¯¯¯¯ X − μ| ≥ ϵ) ≤ = , which goes to zero as n → ∞. Var(X1 + X2+. . . +Xn) n2 Var(X1) + Var(X2)+. . . +Var(Xn) n2 nVar(X) n2 Var(X) n Var( ¯¯¯¯¯ X ) ϵ2 Var(X) nϵ2 7.1.2 Central Limit Theorem The central limit theorem (CL T) is one of the most important results in probability theory . It states that, under certain conditions, the sum of a large number of random variables is approximately normal. Here, we state a version of the CL T that applies to i.i.d. random variables. Suppose that X1, X2 , ... , Xn are i.i.d. random variables with expected values EXi = μ < ∞ and variance Var(Xi) = σ2 < ∞. Then as we saw above, the sample mean ¯¯¯¯¯ X = has mean E¯¯¯¯¯ X = μ and variance Var( ¯¯¯¯¯ X ) = . Thus, the normalized random variable Zn = = has mean EZn = 0 and variance Var(Zn) = 1. The central limit theorem states that the CDF of Zn converges to the standard normal CDF . The Central Limit Theorem (CL T) Let X1,X2,...,Xn be i.i.d. random variables with expected value EXi = μ < ∞ and variance 0 < Var(Xi) = σ2 < ∞. Then, the random variable Zn = = converges in distribution to the standard normal random variable as n goes to infinity , that is lim n→∞ P(Zn ≤ x) = Φ(x),  for all x ∈ R, where Φ(x) is the standard normal CDF . An interesting thing about the CL T is that it does not matter what the distribution of the Xi's is. The Xi's can be discrete, continuous, or mixed random variables. T o get a X1+X2+...+Xn n σ2 n ¯¯¯¯¯ X − μ σ/√n X1 + X2+. . . +Xn − nμ √nσ ¯¯¯¯¯ X − μ σ/√n X1 + X2+. . . +Xn − nμ √nσ feeling for the CL T, let us look at some examples. Let's assume that Xi's are Bernoulli(p). Then EXi = p, Var(Xi) = p(1 − p). Also, Yn = X1 + X2+. . . +Xn has Binomial(n, p) distribution. Thus, Zn = , where Yn ∼ Binomial(n, p). Figure 7.1 shows the PMF of Zn for dif ferent values of n. As you see, the shape of the PMF gets closer to a normal PDF curve as n increases. Here, Zn is a discrete random variable, so mathematically speaking it has a PMF not a PDF. That is why the CL T states that the CDF (not the PDF) of Zn converges to the standard normal CDF . Nevertheless, since PMF and PDF are conceptually similar , the figure is useful in visualizing the convergence to normal distribution. Yn − np √np(1 − p) Fig.7.1 - Zn is the normalized sum of n independent Bernoulli(p) random variables. The shape of its PMF , PZn (z), resembles the normal curve as n increases. As another example, let's assume that Xi's are Uniform(0, 1). Then EXi = , Var(Xi) = . In this case, Zn = . Figure 7.2 shows the PDF of Zn for dif ferent values of n. As you see, the shape of the PDF gets closer to the normal PDF as n increases. 1 2 1 12 X1 + X2+. . . +Xn − n 2 √n/12 Fig. 7.2 - Zn is the normalized sum of n independent Uniform(0, 1) random variables. The shape of its PDF , fZn (z), gets closer to the normal curve as n increases. We could have directly looked at Yn = X1 + X2+. . . +Xn, so why do we normalize it first and say that the normalized version ( Zn) becomes approximately normal? This is because EYn = nEXi and Var(Yn) = nσ2 go to infinity as n goes to infinity . We normalize Yn in order to have a finite mean and variance ( EZn = 0, Var(Zn) = 1). Nevertheless, for any fixed n, the CDF of Zn is obtained by scaling and shifting the CDF of Yn. Thus, the two CDFs have similar shapes. The importance of the central limit theorem stems from the fact that, in many real applications, a certain random variable of interest is a sum of a large number of independent random variables. In these situations, we are often able to use the CL T to justify using the normal distribution. Examples of such random variables are found in almost every discipline. Here are a few: Laboratory measurement errors are usually modeled by normal random variables. In communication and signal processing, Gaussian noise is the most frequently used model for noise. In finance, the percentage changes in the prices of some assets are sometimes modeled by normal random variables. When we do random sampling from a population to obtain statistical knowledge about the population, we often model the resulting quantity as a normal random variable. The CLT is also very useful in the sense that it can simplify our computations significantly . If you have a problem in which you are interested in a sum of one thousand i.i.d. random variables, it might be extremely dif ficult, if not impossible, to find the distribution of the sum by direct calculation. Using the CL T we can immediately write the distribution, if we know the mean and variance of the Xi's. Another question that comes to mind is how large n should be so that we can use the normal approximation. The answer generally depends on the distribution of the Xis. Nevertheless, as a rule of thumb it is often stated that if n is larger than or equal to 30, then the normal approximation is very good. Let's summarize how we use the CL T to solve problems: How to Apply The Central Limit Theorem (CL T) Here are the steps that we need in order to apply the CL T: 1 . Write the random variable of interest, Y , as the sum of n i.i.d. random variable Xi's: Y = X1 + X2+. . . +Xn. 2 . Find EY and Var(Y ) by noting that EY = nμ, Var(Y ) = nσ2, where μ = EXi and σ2 = Var(Xi). 3 . According to the CL T, conclude that = is approximately standard normal; thus, to find P(y1 ≤ Y ≤ y2), we can write P(y1 ≤ Y ≤ y2) = P ( ≤ ≤ ) ≈ Φ ( ) − Φ ( ) . Let us look at some examples to see how we can use the central limit theorem. Example 7. 1 A bank teller serves customers standing in the queue one by one. Suppose that the service time Xi for customer i has mean EXi = 2 (minutes) and Var(Xi) = 1. We assume that service times for dif ferent bank customers are independent. Let Y be the total time the bank teller spends serving 50 customers. Find P(90 < Y < 110). Solution Y = X1 + X2+. . . +Xn, where n = 50, EXi = μ = 2, and Var(Xi) = σ2 = 1. Thus, we can write Y −EY √Var(Y ) Y −nμ √nσ y1 − nμ √nσ Y − nμ √nσ y2 − nμ √nσ y2 − nμ √nσ y1 − nμ √nσ P(90 < Y ≤ 110) = P ( < < ) = P ( < < ) = P (−√2 < < √2) . By the CL T, is approximately standard normal, so we can write P(90 < Y ≤ 110) ≈ Φ(√2) − Φ(−√2) = 0.8427 Example 7. 2 In a communication system each data packet consists of 1000 bits. Due to the noise, each bit may be received in error with probability 0.1. It is assumed bit errors occur independently . Find the probability that there are more than 120 errors in a certain data packet. Solution Let us define Xi as the indicator random variable for the ith bit in the packet. That is, Xi = 1 if the ith bit is received in error , and Xi = 0 otherwise. Then the Xi's are i.i.d. and Xi ∼ Bernoulli(p = 0.1). If Y is the total number of bit errors in the packet, we have Y = X1 + X2+. . . +Xn. Since Xi ∼ Bernoulli(p = 0.1), we have EXi = μ = p = 0.1, Var(Xi) = σ2 = p(1 − p) = 0.09 Using the CL T, we have 90 − nμ √nσ Y − nμ √nσ 110 − nμ √nσ 90 − 100 √50 Y − nμ √nσ 110 − 100 √50 Y − nμ √nσ Y −nμ √nσ P(Y > 120) = P ( > ) = P ( > ) ≈ 1 − Φ ( ) = 0.0175 Continuity Correction: Let us assume that Y ∼ Binomial(n = 20, p = ), and suppose that we are interested in P(8 ≤ Y ≤ 10). We know that a Binomial(n = 20, p = ) can be written as the sum of n i.i.d. Bernoulli(p) random variables: Y = X1 + X2+. . . +Xn. Since Xi ∼ Bernoulli(p = ), we have EXi = μ = p = , Var(Xi) = σ2 = p(1 − p) = . Thus, we may want to apply the CL T to write P(8 ≤ Y ≤ 10) = P ( < < ) = P ( < < ) ≈ Φ(0) − Φ ( ) = 0.3145 Since, here, n = 20 is relatively small, we can actually find P(8 ≤ Y ≤ 10) accurately . We have P(8 ≤ Y ≤ 10) = 10 ∑ k=8 ( )p k(1 − p) n−k = [( ) + ( ) + ( )]( ) 20 = 0.4565 Y − nμ √nσ 120 − nμ √nσ Y − nμ √nσ 120 − 100 √90 20 √90 1 2 1 2 1 2 1 2 1 4 8 − nμ √nσ Y − nμ √nσ 10 − nμ √nσ 8 − 10 √5 Y − nμ √nσ 10 − 10 √5 −2 √5 n k 20 8 20 9 20 10 1 2 We notice that our approximation is not so good. Part of the error is due to the fact that Y is a discrete random variable and we are using a continuous distribution to find P(8 ≤ Y ≤ 10). Here is a trick to get a better approximation, called continuity correction . Since Y can only take integer values, we can write P(8 ≤ Y ≤ 10) = P(7.5 < Y < 10.5) = P ( < < ) = P ( < < ) ≈ Φ ( ) − Φ ( ) = 0.4567 As we see, using continuity correction, our approximation improved significantly . The continuity correction is particularly useful when we would like to find P(y1 ≤ Y ≤ y2), where Y is binomial and y1 and y2 are close to each other . Continuity Correction for Discrete Random V ariables Let X1,X2, ⋯,Xn be independent discrete random variables and let Y = X1 + X2 + ⋯ + Xn. Suppose that we are interested in finding P(A) = P(l ≤ Y ≤ u) using the CL T, where l and u are integers. Since Y is an integer- valued random variable, we can write P(A) = P(l − ≤ Y ≤ u + ). It turns out that the above expression sometimes provides a better approximation for P(A) when applying the CL T. This is called the continuity correction and it is particularly useful when Xi's are Bernoulli (i.e., Y is binomial). 7.5 − nμ √nσ Y − nμ √nσ 10.5 − nμ √nσ 7.5 − 10 √5 Y − nμ √nσ 10.5 − 10 √5 0.5 √5 −2.5 √5 1 2 1 2 7.1.3 Solved Problems Problem 1 There are 100 men on a plane. Let Xi be the weight (in pounds) of the ith man on the plane. Suppose that the Xi's are i.i.d., and EXi = μ = 170 and σXi = σ = 30. Find the probability that the total weight of the men on the plane exceeds 18,000 pounds. Solution If W is the total weight, then W = X1 + X2 + ⋯ + Xn, where n = 100. We have EW = nμ = (100)(170) = 17000, Var(W) = 100Var(Xi) = (100)(30) 2 = 90000. Thus, σW = 300. We have P(W > 18000) = P ( > ) = P ( > ) = 1 − Φ ( ) (by CLT) ≈ 4.3 × 10−4. Problem 2 Let X1, X2, ⋯, X25 be i.i.d. with the following PMF PX(k) = ⎧⎪ ⎨ ⎪⎩ 0.6 k = 1 0.4 k = −1 0 otherwise And let W − 17000 300 18000 − 17000 300 W − 17000 300 10 3 10 3 Y = X1 + X2 + ⋯ + Xn. Using the CL T and continuity correction, estimate P(4 ≤ Y ≤ 6). Solution We have EXi = (0.6)(1) + (0.4)(−1) = , EX2 i = 0.6 + 0.4 = 1. Therefore, Var(Xi) = 1 − = ; thus, σXi = . Therefore, EY = 25 × = 5, Var(Y ) = 25 × = 24; thus, σY = 2√6. P(4 ≤ Y ≤ 6) = P(3.5 ≤ Y ≤ 6.5) (continuity correction) = P ( ≤ ≤ ) = P (−0.3062 ≤ ≤ +0.3062) ≈ Φ(0.3062) − Φ(−0.3062) (by the CLT) = 2Φ(0.3062) − 1 ≈ 0.2405 1 5 1 25 24 25 2√6 5 1 5 24 25 3.5 − 5 2√6 Y − 5 2√6 6.5 − 5 2√6 Y − 5 2√6 Problem 3 You have invited 64 guests to a party . You need to make sandwiches for the guests. You believe that a guest might need 0, 1 or 2 sandwiches with probabilities , , and respectively . You assume that the number of sandwiches each guest needs is independent from other guests. How many sandwiches should you make so that you are 95% sure that there is no shortage? Solution Let Xi be the number of sandwiches that the ith person needs, and let Y = X1 + X2 + ⋯ + X64. The goal is to find y such that P(Y ≤ y) ≥ 0.95 First note that EXi = (0) + (1) + (2) = 1, EX2 i = (02) + (12) + (22) = . Thus, Var(Xi) = EX2 i − (EXi) 2 = − 1 = → σXi = . Thus, EY = 64 × 1 = 64, Var(Y ) = 64 × = 32 → σY = 4√2. Now , we can use the CL T to find y 1 4 1 2 1 4 1 4 1 2 1 4 1 4 1 2 1 4 3 2 3 2 1 2 1 √2 1 2 P(Y ≤ y) = P ( ≤ ) = Φ ( ) (by CLT). We can write Φ ( ) = 0.95 Therefore, = Φ−1(0.95) ≈ 1.6449 Thus, y = 73.3. Therefore, if you make 74 sandwiches, you are 95% sure that there is no shortage. Note that you can find the numerical value of Φ−1(0.95) by running the norminv(0.95) command in MA TLAB. Problem 4 Let X1 , X2, ⋯, Xn be i.i.d. Exponential(λ) random variables with λ = 1. Let ¯¯¯¯¯ X = . How large n should be such that P (0.9 ≤ ¯¯¯¯¯ X ≤ 1.1) ≥ 0.95? Solution Let Y = X1 + X2 + ⋯ + Xn, so ¯¯¯¯¯ X = . Since Xi ∼ Exponential(1), we have E(Xi) = = 1, Var(Xi) = = 1. Therefore, E(Y ) = nEXi = n, Var(Y ) = nVar(Xi) = n, Y − 64 4√2 y − 64 4√2 y − 64 4√2 y − 64 4√2 y − 64 4√2 X1 + X2 + ⋯ + Xn n Y n 1 λ 1 λ2 P(0.9 ≤ ¯¯¯¯¯ X ≤ 1.1) = P (0.9 ≤ ≤ 1.1) = P(0.9n ≤ Y ≤ 1.1n) = P ( ≤ ≤ ) = P (−0.1√n ≤ ≤ 0.1√n) . By the CL T is approximately N(0, 1), so P(0.9 ≤ ¯¯¯¯¯ X ≤ 1.1) ≈ Φ (0.1√n) − Φ (−0.1√n) = 2Φ (0.1√n) − 1 (since Φ(−x) = 1 − Φ(x)). We need to have 2Φ (0.1√n) − 1 ≥ 0.95, so Φ (0.1√n) ≥ 0.975. Thus, 0.1√n ≥ Φ−1(0.975) = 1.96 √n ≥ 19.6 n ≥ 384.16 Since n is an integer , we conclude n ≥ 385. Problem 5 For this problem and the next, you will need to be familiar with moment generating functions (Section 6.1.3). The goal here is to prove the (weak) law of large numbers using MGFs. In particular , let X1, X2, … , Xn be i.i.d. random variables with expected value EXi = μ < ∞ and MGF MX(s) that is finite on some interval [−c, c] where c > 0 is a constant. As usual, let ¯¯¯¯¯ X = . Prove lim n→∞ M¯¯¯¯ X (s) = e sμ,  for all s ∈ [−c, c]. Since this is the MGF of constant random variable μ, we conclude that the distribution of ¯¯¯¯¯ X converges to μ. Hint: Use the result of Problem 8 in Section 6.1.6: for a random variable X with a well-defined MGF , MX(s), we have Y n 0.9n − n √n Y − n √n 1.1n − n √n Y − n √n Y −n √n X1 + X2 + ⋯ + Xn n lim n→∞ [MX ( )]n = esEX. Solution We have M¯¯¯¯ X (s) = E[e s ¯¯¯¯ X ] = E[e s ] = E[e s e s ⋯ e s ] = E[e ] ⋅ E[e ] ⋯ E[e ] (since the Xi's are independent) = [MX ( )]n (since the Xi's are identically distributed) Therefore, lim n→∞ M¯¯¯¯ X (s) = lim n→∞ [MX ( )] n = e sEX (by the hint) = e sμ. Note that esμ is the MGF of a constant random variable Y , with value Y = μ. This means that the random variable ¯¯¯¯¯ X converges to μ (in distribution). Problem 6 The goal in this problem is to prove the central limit theorem using MGFs. In particular , let X1, X2, ... , Xn be i.i.d. random variables with expected value EXi = μ < ∞, Var(Xi) = σ2 < ∞, and MGF MX(s) that is finite on some interval [−c, c], where c > 0 is a constant. As usual, let Zn = = . Prove lim n→∞ MZn (s) = e ,  for all s ∈ [−c, c]. Since this is the MGF of a standard normal random variable, we conclude that the distribution of Zn converges to the standard normal random variable. s n X1+X2+⋯+Xn n X1 n X2 n Xn n sX1 n sX2 n sXn n s n s n ¯¯¯¯¯ X − μ σ/√n X1 + X2 + ⋯ + Xn − nμ √nσ s2 2 Hint: Use the result of Problem 9 in Section 6.1.6: for a random variable Y with a well- defined MGF , MY (s), and EY = 0, Var(Y ) = 1, we have lim n→∞ [MY ( )] n = e . Solution Let Yi's be the normalized versions of the Xi's, i.e., Yi = . Then, Yi's are i.i.d. and EYi = 0, Var(Yi) = 1. We also have Zn = = . Thus, we have MZn (s) = E[e s ] = E[e ] ⋅ E[e ] ⋯ E[e ] (the since Yi's are independent) = MY1 ( ) n (theYi's are identically distributed). Thus, we conclude lim n→∞ MZn (s) = lim n→∞ MY1 ( ) n = e (by the hint). Since this is the MGF of a standard normal random variable, we conclude the CDF of Zn converges to the standard normal CDF . s √n s2 2 Xi − μ σ ¯¯¯¯¯ X − μ σ √n Y1 + Y2 + ⋯ + Yn √n Y1+Y2+⋯+Yn √n sY1 √n sY2 √n sYn √n s √n s √n s2 2 7.2.0 Convergence of Random V ariables In some situations, we would like to see if a sequence of random variables X1, X2, X3, ⋯ ''converges'' to a random variable X. That is, we would like to see if Xn gets closer and closer to X in some sense as n increases. For example, suppose that we are interested in knowing the value of a random variable X, but we are not able to observe X directly . Instead, you can do some measurements and come up with an estimate of X: call it X1. You then perform more measurements and update your estimate of X and call it X2. You continue this process to obtain X1, X2, X3, ⋯. Your hope is that as n increases, your estimate gets better and better . That is, you hope that as n increases, Xn gets closer and closer to X. In other words, you hope that Xn converges to X. In fact, we have already seen the concept of convergence in Section 7.1.0 when we discussed limit theorems (the weak law of large numbers (WLLN) and the central limit theorem (CL T)). The WLLN states that the average of a large number of i.i.d. random variables converges in probability to the expected value. The CL T states that the normalized average of a sequence of i.i.d. random variables converges in distribution to a standard normal distribution. In this section, we will develop the theoretical background to study the convergence of a sequence of random variables in more detail. In particular , we will define dif ferent types of convergence. When we say that the sequence Xn converges to X, it means that Xn's are getting ''closer and closer'' to X. Dif ferent types of convergence refer to dif ferent ways of defining what ''closer'' means. W e also discuss how dif ferent types of convergence are related. 7.2.1 Convergence of a Sequence of Numbers Before discussing convergence for a sequence of random variables, let us remember what convergence means for a sequence of real numbers. If we have a sequence of real numbers a1, a2, a3, ⋯, we can ask whether the sequence converges. For example, the sequence , , , ⋯ , , ⋯ is defined as an = ,  for n = 1, 2, 3, ⋯ This sequence converges to 1. W e say that a sequence a1, a2, a3, ⋯ converges to a limit L if an approaches L as n goes to infinity . Definition 7. 2 . A sequence a1, a2, a3, ⋯ converges to a limit L if lim n→∞ an = L. That is, for any ϵ > 0, there exists an N ∈ N such that |an − L| < ϵ,  for all n > N. 1 2 2 3 3 4 n n + 1 n n + 1 7.2.2 Sequence of Random V ariables Here, we would like to discuss what we precisely mean by a sequence of random variables. Remember that, in any probability model, we have a sample space S and a probability measure P. For simplicity , suppose that our sample space consists of a finite number of elements, i.e., S = {s1, s2, ⋯ , sk}. Then, a random variable X is a mapping that assigns a real number to any of the possible outcomes si, i = 1, 2, ⋯ , k. Thus, we may write X(si) = xi,  for i = 1, 2, ⋯ , k. When we have a sequence of random variables X1, X2, X3, ⋯, it is also useful to remember that we have an underlying sample space S. In particular , each Xn is a function from S to real numbers. Thus, we may write Xn(si) = xni,  for i = 1, 2, ⋯ , k. In sum, a sequence of random variables is in fact a sequence of functions Xn : S → R. Example 7. 3 Consider the following random experiment: A fair coin is tossed once. Here, the sample space has only two elements S = {H, T}. We define a sequence of random variables X1, X2, X3, ⋯ on this sample space as follows: Xn(s) = ⎧⎪ ⎪ ⎨ ⎪ ⎪⎩  if s = H 1  if s = T a . Are the Xi's independent? b . Find the PMF and CDF of Xn, FXn (x) for n = 1, 2, 3, ⋯. c. As n goes to infinity , what does FXn (x) look like? Solution 1 n+1 a . The Xi's are not independent because their values are determined by the same coin toss. In particular , to show that X1 and X2 are not independent, we can write P(X1 = 1, X2 = 1) = P(T) = , which is different from P(X1 = 1) ⋅ P(X2 = 1) = P(T) ⋅ P(T) = . b . Each Xi can take only two possible values that are equally likely . Thus, the PMF of Xn is given by PXn (x) = P(Xn = x) = ⎧⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪⎩  if x =  if x = 1 From this we can obtain the CDF of Xn FXn (x) = P(Xn ≤ x) = ⎧⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪⎩ 1  if x ≥ 1  if  ≤ x < 1 0  if x < c. Figure 7.3 shows the CDF of Xn for dif ferent values of n. We see in the figure that the CDF of Xn approaches the CDF of a Bernoulli ( ) random variable as n → ∞. As we will discuss in the next sections, this means that the sequence X1 , X2, X3, ⋯ converges in distribution to a Bernoulli ( ) random variable as n → ∞. 1 2 1 4 1 2 1 n+1 1 2 1 2 1 n+1 1 n+1 1 2 1 2 Fig.7.3 - CDFs of Xn for Example 7.12 The previous example was defined on a very simple sample space S = {H, T}. Let us look at an example that is defined on a more interesting sample space. Example 7. 4 Consider the following random experiment: A fair coin is tossed repeatedly forever . Here, the sample space S consists of all possible sequences of heads and tails. W e define the sequence of random variables X1, X2, X3, ⋯ as follows: Xn = ⎧⎪ ⎨ ⎪⎩ 0  if the nth coin toss results in a heads 1  if the nth coin toss results in a tails In this example, the Xi's are independent because each Xi is a result of a dif ferent coin toss. In fact, the Xi's are i.i.d. Bernoulli ( ) random variables. Thus, when we would like to refer to such a sequence, we usually say , ''Let X1, X2, X3, ⋯ be a sequence of i.i.d. Bernoulli ( ) random variables.'' W e usually do not state the sample space because it is implied that the sample space S consists of all possible sequences of heads and tails. 1 2 1 2 7.2.3 Different T ypes of Convergence for Sequences of Random V ariables Here, we would like to provide definitions of dif ferent types of convergence and discuss how they are related. Consider a sequence of random variables X1, X2, X3, ⋯, i.e, {Xn, n ∈ N}. This sequence might ''converge'' to a random variable X. There are four types of convergence that we will discuss in this section: 1 . Convergence in distribution, 2 . Convergence in probability , 3 . Convergence in mean, 4 . Almost sure convergence. These are all dif ferent kinds of convergence. A sequence might converge in one sense but not another . Some of these convergence types are ''stronger'' than others and some are ''weaker .'' By this, we mean the following: If T ype A convergence is stronger than T ype B convergence, it means that T ype A convergence implies T ype B convergence. Figure 7.4 summarizes how these types of convergence are related. In this figure, the stronger types of convergence are on top and, as we move to the bottom, the convergence becomes weaker . For example, using the figure, we conclude that if a sequence of random variables converges in probability to a random variable X, then the sequence converges in distribution to X as well. Fig.7.4 - Relations between dif ferent types of convergence 7.2.4 Convergence in Distribution Convergence in distribution is in some sense the weakest type of convergence. All it says is that the CDF of Xn's converges to the CDF of X as n goes to infinity . It does not require any dependence between the Xn's and X. We saw this type of convergence before when we discussed the central limit theorem. T o say that Xn converges in distribution to X, we write Xn  →  X. Here is a formal definition of convergence in distribution: Convergence in Distribution A sequence of random variables X1, X2, X3, ⋯ converges in distribution to a random variable X, shown by Xn  →  X, if lim n→∞ FXn (x) = FX(x), for all x at which FX(x) is continuous. Example 7. 5 If X1, X2, X3, ⋯ is a sequence of i.i.d. random variables with CDF FX(x), then Xn  →  X. This is because FXn (x) = FX(x),  for all x. Therefore, lim n→∞ FXn (x) = FX(x),  for all x. d d d Example 7. 6 Let X2, X3, X4, ⋯ be a sequence of random variable such that FXn (x) = ⎧⎪ ⎪ ⎨ ⎪ ⎪⎩ 1 − (1 − )nx x > 0 0 otherwise Show that Xn converges in distribution to Exponential(1). Solution Let X ∼ Exponential(1). For x ≤ 0, we have FXn (x) = FX(x) = 0,  for n = 2, 3, 4, ⋯ . For x ≥ 0, we have lim n→∞ FXn (x) = lim n→∞ (1 − (1 − ) nx) = 1 − lim n→∞ (1 − ) nx = 1 − e −x = FX(x),  for all x. Thus, we conclude that Xn  →  X. When working with integer-valued random variables, the following theorem is often useful. Theorem 7. 1 Consider the sequence X1, X2, X3, ⋯ and the random variable X. Assume that X and Xn (for all n) are non-negative and integer-valued, i.e., RX ⊂ {0, 1, 2, ⋯}, RXn ⊂ {0, 1, 2, ⋯},  for n = 1, 2, 3, ⋯ . Then Xn  →  X if and only if lim n→∞ PXn (k) = PX(k),  for k = 0, 1, 2, ⋯ . 1 n 1 n 1 n d d Proof Since X is integer-valued, its CDF , FX(x), is continuous at all x ∈ R − {0, 1, 2, . . . }. If Xn  →  X, then lim n→∞ FXn (x) = FX(x),  for all x ∈ R − {0, 1, 2, . . . }. Thus, for k = 0, 1, 2, ⋯, we have lim n→∞ PXn (k) = lim n→∞ [FXn (k + ) − FXn (k − )]  (Xn's are integer-valued) = lim n→∞ FXn (k + ) − lim n→∞ FXn (k − ) = FX (k + ) − FX (k − )  (since Xn  →  X) = PX(k)  (since X is integer-valued). To prove the converse, assume that we know lim n→∞ PXn (k) = PX(k),  for k = 0, 1, 2, ⋯ . Then, for all x ∈ R, we have lim n→∞ FXn (x) = lim n→∞ P(Xn ≤ x) = lim n→∞ ⌊x⌋ ∑ k=0 PXn (k), where ⌊x⌋ shows the largest integer less than or equal to x. Since for any fixed x, the set {0, 1, ⋯ , ⌊x⌋} is a finite set, we can change the order of the limit and the sum, so we obtain lim n→∞ FXn (x) = ⌊x⌋ ∑ k=0 lim n→∞ PXn (k) = ⌊x⌋ ∑ k=0 PX(k)  (by assumption) = P(X ≤ x) = FX(x). Example 7. 7 Let X1, X2, X3, ⋯ be a sequence of random variable such that d 1 2 1 2 1 2 1 2 1 2 1 2 d Xn ∼ Binomial (n, ) ,  for n ∈ N, n > λ, where λ > 0 is a constant. Show that Xn converges in distribution to Poisson(λ). Solution By Theorem 7.1 , it suffices to show that lim n→∞ PXn (k) = PX(k),  for all k = 0, 1, 2, ⋯ . We have lim n→∞ PXn (k) = lim n→∞ ( )( ) k(1 − ) n−k = λ k lim n→∞ ( ) (1 − ) n−k = . lim n→∞ ([ ] [(1 − ) n] [(1 − ) −k]) . Note that for a fixed k, we have lim n→∞ = 1, lim n→∞ (1 − ) −k = 1, lim n→∞ (1 − ) n = e −λ. Thus, we conclude lim n→∞ PXn (k) = . We end this section by reminding you that the most famous example of convergence in distribution is the central limit theorem (CL T). The CL T states that the normalized average of i.i.d. random variables X1, X2, X3, ⋯ converges in distribution to a standard normal random variable. λ n n k λ n λ n n! k!(n − k)! 1 nk λ n λ k k! n(n − 1)(n − 2). . . (n − k + 1) nk λ n λ n n(n − 1)(n − 2). . . (n − k + 1) nk λ n λ n e −λλ k k! 7.2.5 Convergence in Probability Convergence in probability is stronger than convergence in distribution. In particular , for a sequence X1, X2, X3, ⋯ to converge to a random variable X, we must have that P(|Xn − X| ≥ ϵ) goes to 0 as n → ∞, for any ϵ > 0. T o say that Xn converges in probability to X, we write Xn  →  X. Here is the formal definition of convergence in probability: Convergence in Probability A sequence of random variables X1, X2, X3, ⋯ converges in probability to a random variable X, shown by Xn  →  X, if lim n→∞ P(|Xn − X| ≥ ϵ) = 0,  for all ϵ > 0. Example 7. 8 Let Xn ∼ Exponential(n), show that Xn  →  0. That is, the sequence X1, X2, X3, ⋯ converges in probability to the zero random variable X. Solution We have lim n→∞ P(|Xn − 0| ≥ ϵ) = lim n→∞ P(Xn ≥ ϵ) ( since Xn ≥ 0 ) = lim n→∞ e −nϵ ( since Xn ∼ Exponential(n) ) = 0,  for all ϵ > 0. p p p Example 7. 9 Let X be a random variable, and Xn = X + Yn, where EYn = , Var(Yn) = , where σ > 0 is a constant. Show that Xn  →  X. Solution First note that by the triangle inequality , for all a, b ∈ R, we have |a + b| ≤ |a| + |b|. Choosing a = Yn − EYn and b = EYn, we obtain |Yn| ≤ |Yn − EYn| + . Now , for any ϵ > 0, we have P(|Xn − X| ≥ ϵ) = P(|Yn| ≥ ϵ) ≤ P (|Yn − EYn| + ≥ ϵ) = P (|Yn − EYn| ≥ ϵ − ) ≤ (by Chebyshev's inequality) = → 0  as n → ∞. Therefore, we conclude Xn  →  X. As we mentioned previously , convergence in probability is stronger than convergence in distribution. That is, if Xn  →  X, then Xn  →  X. The converse is not necessarily true. For example, let X1, X2, X3, ⋯ be a sequence of i.i.d. Bernoulli ( ) random variables. Let also X ∼ Bernoulli ( ) be independent from the Xi's. Then, Xn  →  X. However , Xn does not converge in probability to X, since |Xn − X| is in fact also a Bernoulli ( ) random variable and 1 n σ2 n p 1 n 1 n 1 n Var(Yn) (ϵ − )2 1 n σ2 n(ϵ − )2 1 n p p d 1 2 1 2 d 1 2 P(|Xn − X| ≥ ϵ) = ,  for 0 < ϵ < 1. A special case in which the converse is true is when Xn  →  c, where c is a constant. In this case, convergence in distribution implies convergence in probability . W e can state the following theorem: Theorem 7. 2 If Xn  →  c, where c is a constant, then Xn  →  c. Proof Since Xn  →  c, we conclude that for any ϵ > 0, we have lim n→∞ FXn (c − ϵ) = 0, lim n→∞ FXn (c + ) = 1. We can write for any ϵ > 0, lim n→∞ P(|Xn − c| ≥ ϵ) = lim n→∞ [P(Xn ≤ c − ϵ) + P(Xn ≥ c + ϵ)] = lim n→∞ P(Xn ≤ c − ϵ) + lim n→∞ P(Xn ≥ c + ϵ) = lim n→∞ FXn (c − ϵ) + lim n→∞ P(Xn ≥ c + ϵ) = 0 + lim n→∞ P(Xn ≥ c + ϵ) (since  lim n→∞ FXn (c − ϵ) = 0) ≤ lim n→∞ P(Xn > c + ) = 1 − lim n→∞ FXn (c + ) = 0 (since  lim n→∞ FXn (c + ) = 1). Since lim n→∞ P(|Xn − c| ≥ ϵ) ≥ 0, we conclude that lim n→∞ P(|Xn − c| ≥ ϵ) = 0,  for all ϵ > 0, which means Xn  →  c. The most famous example of convergence in probability is the weak law of large numbers (WLLN). W e proved WLLN in Section 7.1.1 . The WLLN states that if X1, X2, 1 2 d d p d ϵ 2 ϵ 2 ϵ 2 ϵ 2 p X3, ⋯ are i.i.d. random variables with mean EXi = μ < ∞, then the average sequence defined by ¯¯¯¯¯ X n = converges in probability to μ. It is called the \"weak\" law because it refers to convergence in probability . There is another version of the law of large numbers that is called the strong law of large numbers (SLLN). W e will discuss SLLN in Section 7.2.7 . X1 + X2+. . . +Xn n 7.2.6 Convergence in Mean One way of interpreting the convergence of a sequence Xn to X is to say that the ''distance'' between X and Xn is getting smaller and smaller . For example, if we define the distance between Xn and X as P(|Xn − X| ≥ ϵ), we have convergence in probability . One way to define the distance between Xn and X is E (|Xn − X| r) , where r ≥ 1 is a fixed number . This refers to convergence in mean . ( Note: for convergence in mean, it is usually required that E|Xr n| < ∞. ) The most common choice is r = 2, in which case it is called the mean-square convergence . ( Note: Some authors refer to the case r = 1 as convergence in mean. ) Convergence in Mean Let r ≥ 1 be a fixed number . A sequence of random variables X1, X2, X3, ⋯ converges in the rth mean or in the Lr norm to a random variable X, shown by Xn  −→  X, if lim n→∞ E (|Xn − X| r) = 0. If r = 2, it is called the mean-square convergence , and it is shown by Xn  −−→  X. Example 7. 10 Let Xn ∼ Uniform (0, ). Show that Xn  −→  0, for any r ≥ 1. Solution The PDF of Xn is given by Lr m.s. 1 n Lr 1 fXn (x) = ⎧⎪ ⎨ ⎪⎩ n 0 ≤ x ≤ 0 otherwise We have E (|Xn − 0| r) = ∫0 xrn dx = → 0,  for all r ≥ 1. Theorem 7. 3 Let 1 ≤ r ≤ s. If Xn  −→  X, then Xn  −→  X. Proof We can use Hölder's inequality , which was proved in Section . Hölder's Inequality states that E|XY | ≤ (E|X| p) (E|Y | q) , where 1 < p , q < ∞ and + = 1. In Hölder's inequality , choose X = |Xn − X| r, Y = 1, p = > 1. We obtain E|Xn − X| r ≤ (E|Xn − X| s) . Now , by assumption Xn  −→  X, which means lim n→∞ E (|Xn − X| s) = 0. We conclude 1 n 1 n 1 (r + 1)nr Ls Lr 1 p 1 q 1 p 1 q s r 1 p Ls 1 lim n→∞ E (|Xn − X| r) ≤ lim n→∞ (E|Xn − X| s) = 0. Therefore, Xn  −→  X. As we mentioned before, convergence in mean is stronger than convergence in probability . W e can prove this using Markov's inequality . Theorem 7. 4 If Xn  −→  X for some r ≥ 1, then Xn  →  X. Proof For any ϵ > 0, we have P(|Xn − X| ≥ ϵ) = P(|Xn − X| r ≥ ϵr)  (since r ≥ 1) ≤  (by Markov's inequality). Since by assumption lim n→∞ E (|Xn − X| r) = 0, we conclude lim n→∞ P(|Xn − X| ≥ ϵ) = 0,  for all ϵ > 0. The converse of Theorem 7.4 is not true in general. That is, there are sequences that converge in probability but not in mean. Let us look at an example. Example 7. 1 1 Consider a sequence {Xn, n = 1, 2, 3, ⋯} such that Xn = ⎧⎪ ⎪ ⎨ ⎪ ⎪⎩ n2 with probability  0 with probability 1 − 1 p Lr Lr p E|Xn − X| r ϵr 1 n 1 n Show that a . Xn  →  0. b . Xn does not converge in the rth mean for any r ≥ 1. Solution a . To show Xn  →  0, we can write, for any ϵ > 0 lim n→∞ P(|Xn| ≥ ϵ) = lim n→∞ P(Xn = n2) = lim n→∞ = 0. We conclude that Xn  →  0. b . For any r ≥ 1, we can write lim n→∞ E (|Xn| r) = lim n→∞ (n2r ⋅ + 0 ⋅ (1 − )) = lim n→∞ n2r−1   = ∞ (since r ≥ 1). Therefore, Xn does not converge in the rth mean for any r ≥ 1. In particular , it is interesting to note that, although Xn  →  0, the expected value of Xn does not converge to 0. p p 1 n p 1 n 1 n p 7.2.7 Almost Sure Convergence Consider a sequence of random variables X1, X2, X3, ⋯ that is defined on an underlying sample space S. For simplicity , let us assume that S is a finite set, so we can write S = {s1, s2, ⋯ , sk}. Remember that each Xn is a function from S to the set of real numbers. Thus, we may write Xn(si) = xni,  for i = 1, 2, ⋯ , k. After this random experiment is performed, one of the si's will be the outcome of the experiment, and the values of the Xn's are known. If sj is the outcome of the experiment, we observe the following sequence: x1j, x2j, x3j, ⋯ . Since this is a sequence of real numbers, we can talk about its convergence. Does it converge? If yes, what does it converge to? Almost sure convergence is defined based on the convergence of such sequences. Before introducing almost sure convergence let us look at an example. Example 7. 12 Consider the following random experiment: A fair coin is tossed once. Here, the sample space has only two elements S = {H, T}. W e define a sequence of random variables X1, X2, X3, ⋯ on this sample space as follows: Xn(s) = ⎧⎪ ⎪ ⎨ ⎪ ⎪⎩  if s = H (−1) n  if s = T a . For each of the possible outcomes ( H or T), determine whether the resulting sequence of real numbers converges or not. b . Find P ({si ∈ S : lim n→∞ Xn(si) = 1}) . n n+1 Solution a . If the outcome is H, then we have Xn(H) = , so we obtain the following sequence , , , , ⋯ . This sequence converges to 1 as n goes to infinity . If the outcome is T, then we have Xn(T) = (−1)n, so we obtain the following sequence −1, 1, −1, 1, −1, ⋯ . This sequence does not converge as it oscillates between −1 and 1 forever . b . By part (a), the event {si ∈ S : limn→∞ Xn(si) = 1} happens if and only if the outcome is H, so P ({si ∈ S : lim n→∞ Xn(si) = 1}) = P(H) = . In the above example, we saw that the sequence Xn(s) converged when s = H and did not converge when s = T. In general, if the probability that the sequence Xn(s) converges to X(s) is equal to 1, we say that Xn converges to X almost surely and write Xn  −−→  X. Almost Sure Convergence A sequence of random variables X1, X2, X3, ⋯ converges almost surely to a random variable X, shown by Xn  −−→  X, if P ({s ∈ S : lim n→∞ Xn(s) = X(s)}) = 1. n n+1 1 2 2 3 3 4 4 5 1 2 a.s. a.s. Example 7. 13 Consider the sample space S = [0, 1] with a probability measure that is uniform on this space, i.e., P([a, b]) = b − a,  for all 0 ≤ a ≤ b ≤ 1. Define the sequence {Xn, n = 1, 2, ⋯ } as follows: Xn(s) = ⎧⎪ ⎪ ⎨ ⎪ ⎪⎩ 1 0 ≤ s < 0 otherwise Also, define the random variable X on this sample space as follows: X(s) = ⎧⎪ ⎪ ⎨ ⎪ ⎪⎩ 1 0 ≤ s < 0 otherwise Show that Xn  −−→  X. Solution Define the set A as follows: A = {s ∈ S : lim n→∞ Xn(s) = X(s)} . We need to prove that P(A) = 1. Let's first find A. Note that > , so for any s ∈ [0, ), we have Xn(s) = X(s) = 1. Therefore, we conclude that [0, 0.5) ⊂ A. Now if s > , then X(s) = 0. Also, since 2s − 1 > 0, we can write Xn(s) = 0,  for all n > . Therefore, lim n→∞ Xn(s) = 0 = X(s),  for all s > . n+1 2n 1 2 a.s. n+1 2n 1 2 1 2 1 2 1 2s − 1 1 2 We conclude ( , 1] ⊂ S. Y ou can check that s = ∉ A, since Xn ( ) = 1,  for all n, while X ( ) = 0. W e conclude A = [0, ) ∪ ( , 1] = S − { } . Since P(A) = 1, we conclude Xn  −−→  X. In some problems, proving almost sure convergence directly can be dif ficult. Thus, it is desirable to know some suf ficient conditions for almost sure convergence. Here is a result that is sometimes useful when we would like to prove almost sure convergence. Theorem 7. 5 Consider the sequence X1, X2, X3, ⋯. If for all ϵ > 0, we have ∞ ∑ n=1 P(|Xn − X| > ϵ) < ∞, then Xn  −−→  X. Example 7. 14 Consider a sequence {Xn, n = 1, 2, 3, ⋯} such that Xn = ⎧⎪ ⎪ ⎨ ⎪ ⎪⎩ − with probability  with probability  Show that Xn  −−→  0. Solution By the Theorem above, it suf fices to show that ∞ ∑ n=1 P(|Xn| > ϵ) < ∞. 1 2 1 2 1 2 1 2 1 2 1 2 1 2 a.s. a.s. 1 n 1 2 1 n 1 2 a.s. Note that |Xn| = . Thus, |Xn| > ϵ if and only if n < . Thus, we conclude ∞ ∑ n=1 P(|Xn| > ϵ) ≤ ⌊ ⌋ ∑ n=1 P(|Xn| > ϵ) = ⌊ ⌋ < ∞. Theorem 7.5 provides only a suf ficient condition for almost sure convergence. In particular , if we obtain ∞ ∑ n=1 P(|Xn − X| > ϵ) = ∞, then we still don't know whether the Xn's converge to X almost surely or not. Here, we provide a condition that is both necessary and suf ficient. Theorem 7. 6 Consider the sequence X1, X2, X3, ⋯. For any ϵ > 0, define the set of events Am = {|Xn − X| < ϵ, for all n ≥ m}. Then Xn  −−→  X if and only if for any ϵ > 0, we have lim m→∞ P(Am) = 1. Example 7. 15 Let X1, X2, X3, ⋯ be independent random variables, where Xn ∼ Bernoulli ( ) for n = 2, 3, ⋯. The goal here is to check whether Xn  −−→  0. 1 . Check that ∑∞ n=1 P(|Xn| > ϵ) = ∞. 2 . Show that the sequence X1, X2, . . . does not converge to 0 almost surely using Theorem 7.6 . Solution 1 . We first note that for 0 < ϵ < 1, we have 1 n 1 ϵ 1 ϵ 1 ϵ a.s. 1 n a.s. ∞ ∑ n=1 P(|Xn| > ϵ) = ∞ ∑ n=1 P(Xn = 1) = ∞ ∑ n=1 = ∞. 2 . To use Theorem 7.6, we define Am = {|Xn| < ϵ, for all n ≥ m}. Note that for 0 < ϵ < 1, we have Am = {Xn = 0, for all n ≥ m}. According to Theorem 7.6 , it suf fices to show that lim m→∞ P(Am) < 1. We can in fact show that limm→∞ P(Am) = 0. T o show this, we will prove P(Am) = 0, for every m ≥ 2. For 0 < ϵ < 1, we have P(Am) = P({Xn = 0, for all n ≥ m}) ≤ P({Xn = 0, for n = m, m + 1, ⋯ , N}) (for every positive integer N ≥ m) = P(Xm = 0)P(Xm+1 = 0) ⋯ P(XN = 0) (since the Xi's are independent) = ⋅ ⋯ = . Thus, by choosing N large enough, we can show that P(Am) is less than any positive number . Therefore, P(Am) = 0 for all m ≥ 2. We conclude that limm→∞ P(Am) = 0. Thus, according to Theorem 7.6, the sequence X1, X2, . . . does not converge to 0 almost surely . An important example for almost sure convergence is the strong law of large numbers (SLLN) . Here, we state the SLLN without proof. The interested reader can find a proof of SLLN in [19] . A simpler proof can be obtained if we assume the finiteness of the fourth moment. (See [20] for example.) 1 n m − 1 m m m + 1 N − 1 N m − 1 N The strong law of large numbers (SLLN) Let X1, X2,..., Xn be i.i.d. random variables with a finite expected value EXi = μ < ∞. Let also Mn = . Then Mn  −−→  μ. We end this section by stating a version of the continuous mapping theorem . This theorem is sometimes useful when proving the convergence of random variables. Theorem 7. 7 Let X1, X2, X3, ⋯ be a sequence of random variables. Let also h : R ↦ R be a continuous function. Then, the following statements are true: 1 . If Xn  →  X, then h(Xn)  →  h(X). 2 . If Xn  →  X, then h(Xn)  →  h(X). 3 . If Xn  −−→  X, then h(Xn)  −−→  h(X). X1 + X2+. . . +Xn n a.s. d d p p a.s. a.s. 7.2.8 Solved Problems Problem 1 Let X1, X2, X3, ⋯ be a sequence of random variables such that Xn ∼ Geometric ( ) ,  for n = 1, 2, 3, ⋯ , where λ > 0 is a constant. Define a new sequence Yn as Yn = Xn,  for n = 1, 2, 3, ⋯ . Show that Yn converges in distribution to Exponential(λ). Solution Note that if W ∼ Geometric(p), then for any positive integer l, we have P(W ≤ l) = l ∑ k=1(1 − p)k−1p = p l ∑ k=1(1 − p)k−1 = p ⋅ = 1 − (1 − p) l. Now , since Yn = Xn, for any positive real number , we can write P(Yn ≤ y) = P(Xn ≤ ny) = 1 − (1 − )⌊ny⌋, where ⌊ny⌋ is the largest integer less than or equal to ny. W e then write ⌊ ⌋ λ n 1 n 1 − (1 − p) l 1 − (1 − p) 1 n λ n lim n→∞ FYn (y) = lim n→∞ 1 − (1 − )⌊ny⌋ = 1 − lim n→∞ (1 − )⌊ny⌋ = 1 − e −λy. The last equality holds because ny − 1 ≤ ⌊ny⌋ ≤ ny, and lim n→∞ (1 − )ny = e−λy. Problem 2 Let X1, X2, X3, ⋯ be a sequence of i.i.d. Uniform(0, 1) random variables. Define the sequence Yn as Yn = min(X1, X2, ⋯ , Xn). Prove the following convergence results independently (i.e, do not conclude the weaker convergence modes from the stronger ones). a . Yn  →  0. b . Yn  →  0. c. Yn  −→  0, for all r ≥ 1. d. Yn  −→  0. Solution a . Yn  →  0: Note that FXn (x) = ⎧⎪ ⎨ ⎪⎩ 0 x < 0 x 0 ≤ x ≤ 1 1 x > 1 Also, note that RYn = [0, 1]. For 0 ≤ y ≤ 1, we can write λ n λ n λ n d p Lr a.s d FYn (y) = P(Yn ≤ y) = 1 − P(Yn > y) = 1 − P(X1 > y, X2 > y, ⋯ , Xn > y) = 1 − P(X1 > y)P(X2 > y) ⋯ P(Xn > y) (since Xi's are independent) = 1 − (1 − FX1 (y))(1 − FX2 (y)) ⋯ (1 − FXn (y)) = 1 − (1 − y) n. Therefore, we conclude lim n→∞ FYn (y) = { 0 y ≤ 0 1 y > 0 Therefore, Yn  →  0. b . Yn  →  0: Note that as we found in part (a) FYn (y) = ⎧⎪ ⎨ ⎪⎩ 0 y < 0 1 − (1 − y)n 0 ≤ y ≤ 1 1 y > 1 In particular , note that Yn is a continuous random variable. T o show Yn  →  0, we need to show that lim n→∞ P(|Yn| ≥ ϵ) = 0,  for all ϵ > 0. Since Yn ≥ 0, it suf fices to show that lim n→∞ P(Yn ≥ ϵ) = 0,  for all ϵ > 0. For ϵ ∈ (0, 1), we have P(Yn ≥ ϵ) = 1 − P(Yn < ϵ) = 1 − P(Yn ≤ ϵ) (since Yn is a continuous random variable) = 1 − FYn (ϵ) = (1 − ϵ) n. Therefore, lim n→∞ P(|Yn| ≥ ϵ) = lim n→∞ (1 − ϵ) n = 0,  for all ϵ ∈ (0, 1]. c. Yn  −→  0, for all r ≥ 1: By dif ferentiating FYn (y), we obtain d p p Lr fYn (y) = { n(1 − y) n−1 0 ≤ y ≤ 1 0 otherwise Thus, for r ≥ 1, we can write E|Yn| r = ∫ 1 0 nyr(1 − y) n−1dy ≤ ∫ 1 0 ny(1 − y) n−1dy (since r ≥ 1) = [ − y(1 − y) n] 1 0 + ∫ 1 0 (1 − y) ndy (integration by parts) = . Therefore lim n→∞ E (|Yn| r) = 0. d. Yn  −→  0: We will prove ∞ ∑ n=1 P(|Yn| > ϵ) < ∞, which implies Yn  −→  0. By our discussion in part (b), ∞ ∑ n=1 P(|Yn| > ϵ) = ∞ ∑ n=1(1 − ϵ) n = < ∞ (geometric series). Problem 3 Let Xn ∼ N(0, ). Show that Xn  −−→  0. Hint: You may decide to use the inequality given in Equation 4.7 , which is 1 − Φ(x) ≤ e − . Solution We will prove 1 n + 1 a.s a.s 1 − ϵ ϵ 1 n a.s. 1 √2π 1 x x2 2 ∞ ∑ n=1 P(|Xn| > ϵ) < ∞, which implies Xn  −→  0. In particular , P(|Xn| > ϵ) = 2(1 − Φ(ϵn)) (since Xn ∼ N(0, )) ≤ e − ≤ e − ≤ e − . Therefore, ∞ ∑ n=1 P(|Xn| > ϵ) ≤ ∞ ∑ n=1 e − = ∞ ∑ n=1 e − = < ∞ (geometric series). Problem 4 Consider the sample space S = [0, 1] with uniform probability distribution, i.e., P([a, b]) = b − a,  for all 0 ≤ a ≤ b ≤ 1. Define the sequence {Xn, n = 1, 2, ⋯ } as Xn(s) = s + (1 − s)n. Also, define the random variable X on this sample space as X(s) = s. Show that Xn  −−→  X. Solution For any s ∈ (0, 1], we have lim n→∞ Xn(s) = lim n→∞ [ s + (1 − s) n] = s = X(s). a.s 1 n 1 √2π 2 ϵn ϵ2n2 2 1 √2π 2 ϵ ϵ2n2 2 1 √2π 2 ϵ ϵ2n 2 1 √2π 2 ϵ ϵ2n 2 1 √2π 2 ϵ ϵ2n 2 1 √2π 2 ϵ e − ϵ2 2 1 − e − ϵ2 2 n n+1 a.s. n n + 1 However , if s = 0, then lim n→∞ Xn(0) = lim n→∞ [ ⋅ 0 + (1 − 0) n] = 1. Thus, we conclude lim n→∞ Xn(s) = X(s),  for all s ∈ (0, 1]. Since P((0, 1]) = 1, we conclude Xn  −−→  X. Problem 5 Let {Xn, n = 1, 2, ⋯} and {Yn, n = 1, 2, ⋯} be two sequences of random variables, defined on the sample space S. Suppose that we know Xn  −−→  X, Yn  −−→  Y . Prove that Xn + Yn  −−→  X + Y . Solution Define the sets A and B as follows: A = {s ∈ S : lim n→∞ Xn(s) = X(s)} , B = {s ∈ S : lim n→∞ Yn(s) = Y (s)} . By definition of almost sure convergence, we conclude P(A) = P(B) = 1. Therefore, P(Ac) = P(B c) = 0. W e conclude P(A ∩ B) = 1 − P(Ac ∪ B c) ≥ 1 − P(Ac) − P(B c) = 1. Thus, P(A ∩ B) = 1. Now , consider the sequence {Zn, n = 1, 2, ⋯}, where Zn = Xn + Yn, and define the set C as C = {s ∈ S : lim n→∞ Zn(s) = X(s) + Y (s)} . n n + 1 a.s. a.s. a.s. a.s. We claim A ∩ B ⊂ C. Specifically , if s ∈ A ∩ B, then we have lim n→∞ Xn(s) = X(s), lim n→∞ Yn(s) = Y (s). Therefore, lim n→∞ Zn(s) = lim n→∞ [Xn(s) + Yn(s)] = lim n→∞ Xn(s) + lim n→∞ Yn(s) = X(s) + Y (s). Thus, s ∈ C. W e conclude A ∩ B ⊂ C. Thus, P(C) ≥ P(A ∩ B) = 1, which implies P(C) = 1. This means that Zn  −−→  X + Y . Problem 6 Let {Xn, n = 1, 2, ⋯} and {Yn, n = 1, 2, ⋯} be two sequences of random variables, defined on the sample space S. Suppose that we know Xn  →  X, Yn  →  Y . Prove that Xn + Yn  →  X + Y . Solution For n ∈ N, define the following events An = {|Xn − X| < }, Bn = {|Yn − Y | < }. Since Xn  →  X and Yn  →  Y , we have for all ϵ > 0 lim n→∞ P(An) = 1, lim n→∞ P(Bn) = 1. We can also write a.s. p p p ϵ 2 ϵ 2 p p P(An ∩ Bn) = P(An) + P(Bn) − P(An ∪ Bn) ≥ P(An) + P(Bn) − 1. Therefore, lim n→∞ P(An ∩ Bn) = 1. Now , let us define the events Cn and Dn as follows: Cn = {|Xn − X| + |Yn − Y | < ϵ}, Dn = {|Xn + Yn − X − Y | < ϵ}. Now , note that (An ∩ Bn) ⊂ Cn, thus P(An ∩ Bn) ≤ P(Cn). Also, by the triangle inequality for absolute values, we have |(Xn − X) + (Yn − Y )| ≤ |Xn − X| + |Yn − Y |. Therefore, Cn ⊂ Dn, which implies P(Cn) ≤ P(Dn). We conclude P(An ∩ Bn) ≤ P(Cn) ≤ P(Dn). Since limn→∞ P(An ∩ Bn) = 1, we conclude limn→∞ P(Dn) = 1. This by definition means that Xn + Yn  →  X + Y . p 7.3.0 End of Chapter Problems Problem 1 Let Xi be i.i.d. Uniform(0, 1). W e define the sample mean as Mn = . a . Find E[Mn] and Var(Mn) as a function of n. b . Using Chebyshev's inequality , find an upper bound on P (∣ ∣ ∣ Mn − ∣ ∣ ∣ ≥ ) . c. Using your bound, show that lim n→∞ P (∣ ∣ ∣ Mn − ∣ ∣ ∣ ≥ ) = 0. Problem 2 The number of accidents in a certain city is modeled by a Poisson random variable with an average rate of 10 accidents per day . Suppose that the number of accidents on different days are independent. Use the central limit theorem to find the probability that there will be more than 3800 accidents in a certain year . Assume that there are 365 days in a year . Problem 3 In a communication system, each codeword consists of 1000 bits. Due to the noise, each bit may be received in error with probability 0.1. It is assumed bit errors occur independently . Since error correcting codes are used in this system, each codeword can be decoded reliably if there are less than or equal to 125 errors in the received codeword, otherwise the decoding fails. Using the CL T, find the probability of decoding failure. X1 + X2+. . . +Xn n 1 2 1 100 1 2 1 100 Problem 4 50 students live in a dormitory . The parking lot has the capacity for 30 cars. Each student has a car with probability , independently from other students. Use the CLT (with continuity correction) to find the probability that there won't be enough parking spaces for all the cars. Problem 5 The amount of time needed for a certain machine to process a job is a random variable with mean EXi = 10 minutes and Var(Xi) = 2 minutes 2. The times needed for different jobs are independent from each other . Find the probability that the machine processes less than or equal to 40 jobs in 7 hours. Problem 6 You have a fair coin. Y ou toss the coin n times. Let X be the portion of times that you observe heads. How large n has to be so that you are 95% sure that 0.45 ≤ X ≤ 0.55? In other words, how large n has to be so that P(0.45 ≤ X ≤ 0.55) ≥ .95 ? Problem 7 An engineer is measuring a quantity q. It is assumed that there is a random error in each measurement, so the engineer will take n measurements and reports the average of the measurements as the estimated value of q. Specifically , if Yi is the value that is obtained in the i'th measurement, we assume that Yi = q + Xi, where Xi is the error in the ith measurement. W e assume that Xi's are i.i.d. with EXi = 0 and Var(Xi) = 4 units. The engineer reports the average of measurements Mn = . 1 2 Y1 + Y2+. . . +Yn n How many measurements does the engineer need to make until he is 95% sure that the final error is less than 0.1 units? In other words, what should the value of n be such that P(q − 0.1 ≤ Mn ≤ q + 0.1) ≥ 0.95 ? Problem 8 Let X2, X3, X4, ⋯ be a sequence of random variables such that FXn (x) = ⎧⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪⎩ x > 0 0 otherwise Show that Xn converges in distribution to X = 1. Problem 9 Let X2, X3, X4, ⋯ be a sequence of random variables such that FXn (x) = ⎧⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪⎩ 0 ≤ x ≤ 1 x > 1 Show that Xn converges in distribution to Uniform(0, 1). Problem 10 Consider a sequence {Xn, n = 1, 2, 3, ⋯} such that Xn = ⎧⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪⎩ n with probability  0 with probability 1 − Show that a . Xn  →  0. b . Xn  −→  0, for r < 2. c. Xn does not converge to 0 in the rth mean for any r ≥ 2. d. Xn  −−→  0. en(x−1) 1+en(x−1) enx+xen enx+( )enn+1 n enx+en enx+( )enn+1 n 1 n2 1 n2 p Lr a.s. Problem 1 1 We perform the following random experiment. W e put n ≥ 10 blue balls and n red balls in a bag. W e pick 10 balls at random (without replacement) from the bag. Let Xn be the number of blue balls. W e perform this experiment for n = 10, 11, 12, ⋯. Prove that Xn  →  Binomial (10, ). Problem 12 Find two sequences of random variables {Xn, n = 1, 2, ⋯} and {Yn, n = 1, 2, ⋯} such that Xn  →  X, and Yn  →  Y , but Xn + Yn does not converge in distribution to X + Y . Problem 13 Let X1, X2, X3, ⋯ be a sequence of continuous random variable such that fXn (x) = e−n|x|. Show that Xn converges in probability to 0. Problem 14 Let X1, X2, X3, ⋯ be a sequence of continuous random variable such that fXn (x) = ⎧⎪ ⎪ ⎨ ⎪ ⎪⎩ x > 0 otherwise Show that Xn converges in probability to 0. Problem 15 Let Y1, Y2, Y3, ⋯ be a sequence of i.i.d. random variables with mean EYi = μ and finite variance Var(Yi) = σ2. Define the sequence {Xn, n = 2, 3, . . . } as d 1 2 d d n 2 1 nx2 1 n Xn = ,  for n = 2, 3, ⋯ . Show that Xn  →  μ2. Problem 16 Let Y1, Y2, Y3, ⋯ be a sequence of positive i.i.d. random variables with 0 < E[ln Yi] = γ < ∞. Define the sequence {Xn, n = 1, 2, 3, . . . } as Xn = (Y1Y2Y3 ⋯ Yn−1Yn) ,  for n = 1, 2, 3, ⋯ . Show that Xn  →  e γ. Problem 17 Let X1, X2, X3, ⋯ be a sequence of random variable such that Xn ∼ Poisson(nλ),  for n = 1, 2, 3, ⋯ , where λ > 0 is a constant. Define a new sequence Yn as Yn = Xn,  for n = 1, 2, 3, ⋯ . Show that Yn converges in mean square to λ, i.e., Yn  −−→  λ. Problem 18 Let {Xn, n = 1, 2, ⋯} and {Yn, n = 1, 2, ⋯} be two sequences of random variables, defined on the sample space S. Suppose that we know Xn  −→  X, Yn  −→  Y . Prove that Xn + Yn  −→  X + Y . Hint: You may want to use Minkowski's inequality which states that for two random variables X and Y with finite moments, and 1 ≤ p < ∞, we have E[∣∣X + Y ∣∣ p] ≤ E[|X| p] + E[|Y | p] . Y1Y2 + Y2Y3 + ⋯ Yn−1Yn + YnY1 n p 1 n p 1 n m.s. Lr Lr Lr 1 p 1 p Problem 19 Let X1, X2, X3, ⋯ be a sequence of random variable such that Xn ∼ Rayleigh( ), i.e., fXn (x) = { n2x exp{− } x > 0 0 otherwise Show that Xn  −−→  0. Problem 20 Let Y1, Y2, ⋯ be independent random variables, where Yn ∼ Bernoulli ( ) for n = 1, 2, 3, ⋯. W e define the sequence {Xn, n = 2, 3, 4, ⋯} as Xn+1 = Y1Y2Y3 ⋯ Yn,  for n = 1, 2, 3, ⋯ . Show that Xn  −−→  0. 1 n n2x 2 2 a.s. n n+1 a.s. 8.1.0 Introduction In real life, we work with data that are af fected by randomness, and we need to extract information and draw conclusions from the data. The randomness might come from a variety of sources. Here are two examples of such situations: 1 . Suppose that we would like to predict the outcome of an election. Since we cannot poll the entire population, we will choose a random sample from the population and ask them who they plan to vote for . In this experiment, the randomness comes from the sampling. Note also that if our poll is conducted one month before the election, another source of randomness is that people might change their opinions during the one month period. 2 . In a wireless communication system, a message is transmitted from a transmitter to a receiver . However , the receiver receives a corrupted version (a noisy version) of the transmitted signal. The receiver needs to extract the original message from the received noisy version. Here, the randomness comes from the noise. Examples like these are abundant. Dealing with such situations is the subject of the field of statistical inference. Statistical inference is a collection of methods that deal with drawing conclusions from data that are prone to random variation. Clearly , we use our knowledge of probability theory when we work on statistical inference problems. However , the big addition here is that we need to work with real data . The probability problems that we have seen in this book so far were clearly defined and the probability models were given to us. For example, you might have seen a problem like this: Let X be a normal random variable with mean μ = 100 and variance σ2 = 15. Find the probability that X > 110. In real life, we might not know the distribution of X, so we need to collect data, and from the data we should conclude whether X has a normal distribution or not. Now , suppose that we can use the central limit theorem to argue that X is normally distributed. Even in that case, we need to collect data to be able estimate μ and σ. Here is a general setup for a statistical inference problem: There is an unknown quantity that we would like to estimate. W e get some data. From the data, we estimate the desired quantity . There are two major approaches to this problem: 1 . Frequentist (classical) Inference: In this approach, the unknown quantity θ is assumed to be a fixed quantity . That is, θ is a deterministic (non-random) quantity that is to be estimated by the observed data. For example, in the polling problem stated above we might consider θ as the percentage of people who will vote for a certain candidate, call him/her Candidate A. After asking n randomly chosen voters, we might estimate θ by ^Θ = , where Y is the number of people (among the randomly chosen voters) who say they will vote for Candidate A. Although θ is assumed to be a non-random quantity , our estimate of θ, which we show by ^Θ is a random variable, because it depends on our random sample. 2 . Bayesian Inference: In the Bayesian approach the unknown quantity Θ is assumed to be a random variable, and we assume that we have some initial guess about the distribution of Θ. After observing the data, we update the distribution of Θ using Bayes' Rule. As an example, consider the communication system in which the information is transmitted in the form of bits, i.e., 0's and 1's. Let's assume that, in each transmission, the transmitter sends a 1 with probability p, or it sends a 0 with probability 1 − p. Thus, if Θ is the transmitted bit, then Θ ∼ Bernoulli(p). At the receiver, X, which is a noisy version of Θ, is received. The receiver has to recover Θ from X. Here, to estimate Θ, we use our prior knowledge that Θ ∼ Bernoulli(p). Y n In summary , you may say that frequentist (classical) inference deals with estimating non-random quantities, while Bayesian inference deals with estimating random variables. W e will discuss frequentist and Bayesian approaches more in detail in this and the next chapter . Nevertheless, it is important to note that both approaches are very useful and widely used in practice. In this chapter , we will focus on frequentist methods, while in the next chapter we will discuss Bayesian methods. 8.1.1 Random Sampling When collecting data, we often make several observations on a random variable. For example, suppose that our goal is to investigate the height distribution of people in a well defined population (i.e., adults between 25 and 50 in a certain country). T o do this, we define random variables X1, X2, X3, . . ., Xn as follows: W e choose a random sample of size n with replacement from the population and let Xi be the height of the i th chosen person. More specifically , 1 . We chose a person uniformly at random from the population and let X1 be the height of that person. Here, every person in the population has the same chance of being chosen. 2 . To determine the value of X2, again we choose a person uniformly (and independently from the first person) at random and let X2 be the height of that person. Again, every person in the population has the same chance of being chosen. 3 . In general, Xi is the height of the ith person that is chosen uniformly and independently from the population. You might ask why do we do the sampling with replacement? In practice, we often do the sampling without replacement, that is, we do not allow one person to be chosen twice. However , if the population is large, then the probability of choosing one person twice is extremely low , and it can be shown that the results obtained from sampling with replacement are very close to the results obtained using sampling without replacement. The big advantage of sampling with replacement (the above procedure) is that Xi's will be independent and this makes the analysis much simpler . Now for example, if we would like to estimate the average height in the population, we may define an estimator as ^Θ = . The random variables X1, X2, X3, . . ., Xn defined above are independent and identically distributed (i.i.d.) and we refer to them collectively as a (simple) random sample. X1 + X2 + ⋯ + Xn n The collection of random variables X1, X2, X3, . . ., Xn is said to be a random sample of size n if they are independent and identically distributed (i.i.d.), i.e., 1 . X1, X2, X3, . . ., Xn are independent random variables, and 2 . they have the same distribution, i.e, FX1 (x) = FX2 (x) =. . . = FXn (x),  for all x ∈ R. In the above example, the random variable ^Θ = is called a point estimator for the average height in the population. After performing the above experiment, we will obtain ^Θ = ^θ . Here, ^θ is called an estimate of the average height in the population. In general, a point estimator is a function of the random sample ^Θ = h(X1, X2, ⋯ , Xn) that is used to estimate an unknown quantity . It is worth noting that there are dif ferent methods for sampling from a population. We refer to the above sampling method as simple random sampling . In general, \"sampling is concerned with the selection of a subset of individuals from within a statistical population to estimate characteristics of the whole population\" [18] . Nevertheless, for the material that we cover in this book simple random sampling is suf ficient. Unless otherwise stated, when we refer to random samples, we assume they are simple random samples. Some Properties of Random Samples: Since we will be working with random samples, we would like to review some properties of random samples in this section. Here, we assume that X1, X2, X3, . . ., Xn are a random sample. Specifically , we assume 1 . the Xi's are independent; 2 . FX1 (x) = FX2 (x) =. . . = FXn(x) = FX(x); 3 . EXi = EX = μ < ∞; 4 . 0 < Var(Xi) = Var(X) = σ2 < ∞. Sample Mean: The sample mean is defined as X1+X2+⋯+Xn n ¯¯¯¯¯ X = . Another common notation for the sample mean is Mn. Since Xi are assumed to have the CDF FX(x), the sample mean is sometimes denoted by Mn(X) to indicate the distribution of Xi's. Properties of the sample mean 1 . E¯¯¯¯¯ X = μ. 2 . Var( ¯¯¯¯¯ X ) = . 3 . Weak Law of Large Numbers (WLLN): lim n→∞ P(| ¯¯¯¯¯ X − μ| ≥ ϵ) = 0. 4 . Central Limit Theorem: The random variable Zn = = converges in distribution to the standard normal random variable as n goes to infinity , that is lim n→∞ P(Zn ≤ x) = Φ(x),  for all x ∈ R where Φ(x) is the standard normal CDF . Order Statistics: Given a random sample, we might be interested in quantities such as the largest, the smallest, or the middle value in the sample. Thus, we often order the observed data from the smallest to the largest. We call the resulting ordered random variables order statistics . More specifically , let X1, X2, X3, . . ., Xn be a random sample from a continuous distribution with CDF FX(x). Let us order Xi's from the smallest to the largest and denote the resulting sequence of random variables as X(1), X(2), ⋯ , X(n). Thus, we have X1 + X2+. . . +Xn n σ2 n ¯¯¯¯¯ X − μ σ/√n X1 + X2+. . . +Xn − nμ √nσ X(1) = min (X1, X2, ⋯ , Xn); and X(n) = max (X1, X2, ⋯ , Xn). We call X(1), X(2), ⋯ , X(n) the order statistics of the random sample X1, X2, X3, . . ., Xn. W e are often interested in the PDFs or CDFs of the X(i)'s. The following theorem provides these functions. Theorem 8. 1 Let X1, X2, . . ., Xn be a random sample from a continuous distribution with CDF FX(x) and PDF fX(x). Let X(1), X(2), ⋯ , X(n) be the order statistics of X1, X2, X3, . . ., Xn. Then the CDF and PDF of X(i) are given by fX(i) (x) = fX(x)[FX(x)]i−1[1 − FX(x)]n−i, FX(i) (x) = n ∑ k=i ( )[FX(x)]k[1 − FX(x)]n−k. Also, the joint PDF of X(1), X(2), ⋯ , X(n) is given by fX(1),⋯,X(n) (x1, x2, ⋯ , xn) = ⎧⎪ ⎨ ⎪⎩ n!fX(x1)fX(x2) ⋯ fX(xn) for x1 ≤ x2 ≤ x2 ⋯ ≤ xn 0 otherwise A method to prove the above theorem is outlined in the End of Chapter Problems section. Let's look at an example. Example 8. 1 Let X1, X2, X3, X4 be a random sample from the Uniform(0, 1) distribution, and let X(1), X(2), X(3) , X(4). Find the PDFs of X(1), X(2), and X(4). Solution n! (i − 1)!(n − i)! n k Here, the ranges of the random variables are [0, 1], so the PDFs and CDFs are zero outside of [0, 1]. W e have fX(x) = 1,  for x ∈ [0, 1], and FX(x) = x,  for x ∈ [0, 1]. By Theorem 8.1 , we obtain fX(1) (x) = fX(x)[FX(x)]1−1[1 − FX(x)]4−1 = 4fX(x)[1 − FX(x)]3 = 4(1 − x) 3,  for x ∈ [0, 1]. fX(2) (x) = fX(x)[FX(x)]2−1[1 − FX(x)]4−2 = 12fX(x)FX(x)[1 − FX(x)]2 = 12x(1 − x) 2,  for x ∈ [0, 1]. fX(4) (x) = fX(x)[FX(x)]4−1[1 − FX(x)]4−4 = 4fX(x)[FX(x)]3 = 4x 3,  for x ∈ [0, 1]. 4! (1 − 1)!(4 − 1)! 4! (2 − 1)!(4 − 2)! 4! (4 − 1)!(4 − 4)! 8.2.0 Point Estimation Here, we assume that θ is an unknown parameter to be estimated. For example, θ might be the expected value of a random variable, θ = EX. The important assumption here is that θ is a fixed (non-random) quantity . To estimate θ, we need to collect some data. Specifically , we get a random sample X1, X2, X3, . . ., Xn such that Xi's have the same distribution as X. To estimate θ, we define a point estimator ^Θ that is a function of the random sample, i.e., ^Θ = h(X1, X2, ⋯ , Xn). For example, if θ = EX, we may choose ^Θ to be the sample mean ^Θ = ¯¯¯¯¯ X = . There are infinitely many possible estimators for θ, so how can we make sure that we have chosen a good estimator? How do we compare dif ferent possible estimators? T o do this, we provide a list of some desirable properties that we would like our estimators to have. Intuitively , we know that a good estimator should be able to give us values that are \"close\" to the real value of θ. To make this notion more precise we provide some definitions. X1 + X2+. . . +Xn n 8.2.1 Evaluating Estimators We define three main desirable properties for point estimators. The first one is related to the estimator's bias . The bias of an estimator ^Θ tells us on average how far ^Θ is from the real value of θ. Let ^Θ = h(X1, X2, ⋯ , Xn) be a point estimator for θ. The bias of point estimator ^Θ is defined by B( ^Θ) = E[ ^Θ] − θ. In general, we would like to have a bias that is close to 0, indicating that on average, ^Θ is close to θ. It is worth noting that B( ^Θ) might depend on the actual value of θ. In other words, you might have an estimator for which B( ^Θ) is small for some values of θ and large for some other values of θ. A desirable scenario is when B( ^Θ) = 0, i.e, E[ ^Θ] = θ, for all values of θ. In this case, we say that ^Θ is an unbiased estimator of θ. Let ^Θ = h(X1, X2, ⋯ , Xn) be a point estimator for a parameter θ. We say that ^Θ is an unbiased of estimator of θ if B( ^Θ) = 0,  for all possible values of θ. Example 8. 2 Let X1, X2, X3, . . ., Xn be a random sample. Show that the sample mean ^Θ = ¯¯¯¯¯ X = is an unbiased estimator of θ = EXi. X1 + X2+. . . +Xn n Solution We have B( ^Θ) = E[ ^Θ] − θ = E [¯¯¯¯¯ X ] − θ = EXi − θ = 0. Note that if an estimator is unbiased, it is not necessarily a good estimator . In the above example, if we choose ^Θ1 = X1, then ^Θ1 is also an unbiased estimator of θ: B( ^Θ1) = E[ ^Θ1] − θ = EX1 − θ = 0. Nevertheless, we suspect that ^Θ1 is probably not as good as the sample mean ¯¯¯¯¯ X . Therefore, we need other measures to ensure that an estimator is a \"good\" estimator . A very common measure is the mean squared error defined by E[( ^Θ − θ) 2]. The mean squared error (MSE) of a point estimator ^Θ, shown by M SE( ^Θ), is defined as M SE( ^Θ) = E[( ^Θ − θ) 2]. Note that ^Θ − θ is the error that we make when we estimate θ by ^Θ. Thus, the MSE is a measure of the distance between ^Θ and θ, and a smaller MSE is generally indicative of a better estimator . Example 8. 3 Let X1, X2, X3, . . ., Xn be a random sample from a distribution with mean EXi = θ, and variance Var(Xi) = σ2. Consider the following two estimators for θ: 1 . ^Θ1 = X1. 2 . ^Θ2 = ¯¯¯¯¯ X = . X1+X2+...+Xn n Find M SE( ^Θ1) and M SE( ^Θ2) and show that for n > 1, we have M SE( ^Θ1) > M SE( ^Θ2). Solution We have M SE( ^Θ1) = E[( ^Θ1 − θ) 2] = E[(X1 − EX1) 2] = Var(X1) = σ2. To find M SE( ^Θ2), we can write M SE( ^Θ2) = E[( ^Θ2 − θ) 2] = E[( ¯¯¯¯¯ X − θ) 2] = Var( ¯¯¯¯¯ X − θ) + (E[ ¯¯¯¯¯ X − θ]) 2. The last equality results from EY 2 = Var(Y ) + (EY ) 2, where Y = ¯¯¯¯¯ X − θ. Now , note that Var( ¯¯¯¯¯ X − θ) = Var( ¯¯¯¯¯ X ) since θ is a constant. Also, E[ ¯¯¯¯¯ X − θ] = 0. Thus, we conclude M SE( ^Θ2) = Var( ¯¯¯¯¯ X ) = . Thus, we conclude for n > 1, M SE( ^Θ1) > M SE( ^Θ2). From the above example, we conclude that although both ^Θ1 and ^Θ2 are unbiased estimators of the mean, ^Θ2 = ¯¯¯¯¯ X is probably a better estimator since it has a smaller MSE. In general, if ^Θ is a point estimator for θ, we can write M SE( ^Θ) = E[( ^Θ − θ) 2] = Var( ^Θ − θ) + (E[ ^Θ − θ]) 2 = Var( ^Θ) + B( ^Θ) 2. σ2 n If ^Θ is a point estimator for θ, M SE( ^Θ) = Var( ^Θ) + B( ^Θ) 2, where B( ^Θ) = E[ ^Θ] − θ is the bias of ^Θ. The last property that we discuss for point estimators is consistency . Loosely speaking, we say that an estimator is consistent if as the sample size n gets larger , ^Θ converges to the real value of θ. More precisely , we have the following definition: Let ^Θ1, ^Θ2, ⋯, ^Θn, ⋯, be a sequence of point estimators of θ. W e say that ^Θn is a consistent estimator of θ, if lim n→∞ P(| ^Θn − θ| ≥ ϵ) = 0,  for all ϵ > 0. Example 8. 4 Let X1, X2, X3, . . ., Xn be a random sample with mean EXi = θ, and variance Var(Xi) = σ2. Show that ^Θn = ¯¯¯¯¯ X is a consistent estimator of θ. Solution We need to show that lim n→∞ P(| ¯¯¯¯¯ X − θ| ≥ ϵ) = 0,  for all ϵ > 0. But this is true because of the weak law of large numbers. In particular , we can use Chebyshev's inequality to write P(| ¯¯¯¯¯ X − θ| ≥ ϵ) ≤ = , which goes to 0 as n → ∞. Var( ¯¯¯¯¯ X ) ϵ2 σ2 nϵ2 We could also show the consistency of ^Θn = ¯¯¯¯¯ X by looking at the MSE. As we found previously , the MSE of ^Θn = ¯¯¯¯¯ X is given by M SE( ^Θn) = . Thus, M SE( ^Θn) goes to 0 as n → ∞. From this, we can conclude that ^Θn = ¯¯¯¯¯ X is a consistent estimator for θ. In fact, we can state the following theorem: Theorem 8. 2 Let ^Θ1, ^Θ2, ⋯ be a sequence of point estimators of θ. If lim n→∞ M SE( ^Θn) = 0, then ^Θn is a consistent estimator of θ. Proof We can write P(| ^Θn − θ| ≥ ϵ) = P(| ^Θn − θ| 2 ≥ ϵ2) ≤ (by Markov's inequality) = , which goes to 0 as n → ∞ by the assumption. σ2 n E[ ^Θn − θ]2 ϵ2 M SE( ^Θn) ϵ2 8.2.2 Point Estimators for Mean and V ariance The above discussion suggests the sample mean, ¯¯¯¯¯ X , is often a reasonable point estimator for the mean. Now , suppose that we would like to estimate the variance of a distribution σ2. Assuming 0 < σ2 < ∞, by definition σ2 = E[(X − μ) 2]. Thus, the variance itself is the mean of the random variable Y = (X − μ) 2. This suggests the following estimator for the variance ^σ 2 = n ∑ k=1(Xk − μ) 2. By linearity of expectation, ^σ 2 is an unbiased estimator of σ2. Also, by the weak law of large numbers, ^σ 2 is also a consistent estimator of σ2. However , in practice we often do not know the value of μ. Thus, we may replace μ by our estimate of the μ, the sample mean, to obtain the following estimator for σ2: ¯¯¯¯ S 2 = n ∑ k=1(Xk − ¯¯¯¯¯ X ) 2. Using a little algebra, you can show that ¯¯¯¯ S 2 = ( n ∑ k=1 X2 k − n¯¯¯¯¯ X 2) . Example 8. 5 Let X1, X2, X3, . . ., Xn be a random sample with mean EXi = μ, and variance Var(Xi) = σ2. Suppose that we use ¯¯¯¯ S 2 = n ∑ k=1(Xk − ¯¯¯¯¯ X ) 2 = ( n ∑ k=1 X2 k − n¯¯¯¯¯ X 2) to estimate σ2. Find the bias of this estimator B( ¯¯¯¯ S 2) = E[ ¯¯¯¯ S 2] − σ2. 1 n 1 n 1 n 1 n 1 n Solution First note that E¯¯¯¯¯ X 2 = (E¯¯¯¯¯ X ) 2 + Var( ¯¯¯¯¯ X ) = μ2 + . Thus, E[ ¯¯¯¯ S 2] = ( n ∑ k=1 EX2 k − nE¯¯¯¯¯ X 2) = (n(μ2 + σ2) − n (μ2 + )) = σ2. Therefore, B( ¯¯¯¯ S 2) = E[ ¯¯¯¯ S 2] − σ2 = − . We conclude that ¯¯¯¯ S 2 is a biased estimator of the variance. Nevertheless, note that if n is relatively large, the bias is very small. Since E[ ¯¯¯¯ S 2] = σ2, we can obtain an unbiased estimator of σ2 by multiplying ¯¯¯¯ S 2 by . Thus, we define S 2 = n ∑ k=1(Xk − ¯¯¯¯¯ X ) 2 = ( n ∑ k=1 X2 k − n¯¯¯¯¯ X 2) . By the above discussion, S 2 is an unbiased estimator of the variance. W e call it the sample variance . W e should note that if n is large, the dif ference between S 2 and ¯¯¯¯ S 2 is very small. W e also define the sample standard deviation as S = √S 2. Although the sample standard deviation is usually used as an estimator for the standard deviation, it is a biased estimator . T o see this, note that S is random, so Var(S) > 0. Thus, σ2 n 1 n 1 n σ2 n n − 1 n σ2 n n−1 n n n−1 1 n − 1 1 n − 1 0 < Var(S) = ES 2 − (ES) 2 = σ2 − (ES) 2. Therefore, ES < σ, which means that S is a biased estimator of σ. Let X1, X2, X3, . . ., Xn be a random sample with mean EXi = μ < ∞, and variance 0 < Var(Xi) = σ2 < ∞. The sample variance of this random sample is defined as S 2 = n ∑ k=1(Xk − ¯¯¯¯¯ X ) 2 = ( n ∑ k=1 X2 k − n¯¯¯¯¯ X 2) . The sample variance is an unbiased estimator of σ2. The sample standard deviation is defined as S = √S 2, and is commonly used as an estimator for σ. Nevertheless, S is a biased estimator of σ. You can use the mean command in MA TLAB to compute the sample mean for a given sample. More specifically , for a given vector x = [x1, x2, ⋯, xn ], mean(x) returns the sample average . Also, the functions var and std can be used to compute the sample variance and the sample standard deviation respectively . Example 8. 6 Let T be the time that is needed for a specific task in a factory to be completed. In order to estimate the mean and variance of T, we observe a random sample T1, T2, ⋯, T6. Thus, Ti's are i.i.d. and have the same distribution as T. W e obtain the following values (in minutes): 18, 21, 17, 16, 24, 20. Find the values of the sample mean, the sample variance, and the sample standard deviation for the observed sample. 1 n − 1 1 n − 1 x1 + x2 + ⋯ + xn n Solution The sample mean is ¯¯¯¯ T = = = 19.33 The sample variance is given by S 2 = 6 ∑ k=1(Tk − 19.333) 2 = 8.67 Finally , the sample standard deviation is given by S = √S 2 = 2.94 You can use the following MA TLAB code to compute the above values: t = [18, 21, 17, 16, 24, 20]; m = mean(t); v = var(t); s = std(t); T1 + T2 + T3 + T4 + T5 + T6 6 18 + 21 + 17 + 16 + 24 + 20 6 1 6 − 1 8.2.3 Maximum Likelihood Estimation So far , we have discussed estimating the mean and variance of a distribution. Our methods have been somewhat ad hoc. More specifically , it is not clear how we can estimate other parameters. W e now would like to talk about a systematic way of parameter estimation. Specifically , we would like to introduce an estimation method, called maximum likelihood estimation (MLE). To give you the idea behind MLE let us look at an example. Example 8. 7 I have a bag that contains 3 balls. Each ball is either red or blue, but I have no information in addition to this. Thus, the number of blue balls, call it θ, might be 0, 1, 2, or 3. I am allowed to choose 4 balls at random from the bag with replacement. We define the random variables X1, X2, X3, and X4 as follows Xi = ⎧⎪ ⎨ ⎪⎩ 1 if the ith chosen ball is blue 0 if the ith chosen ball is red Note that Xi's are i.i.d. and Xi ∼ Bernoulli( ). After doing my experiment, I observe the following values for Xi's. x1 = 1, x2 = 0, x3 = 1, x4 = 1. Thus, I observe 3 blue balls and 1 red balls. 1 . For each possible value of θ, find the probability of the observed sample, (x1, x2, x3, x4) = (1, 0, 1, 1). 2 . For which value of θ is the probability of the observed sample is the largest? Solution Since Xi ∼ Bernoulli( ), we have θ 3 θ 3 PXi(x) = ⎧⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪⎩  for x = 1 1 −  for x = 0 Since Xi's are independent, the joint PMF of X1, X2, X3, and X4 can be written as PX1X2X3X4 (x1, x2, x3, x4) = PX1 (x1)PX2 (x2)PX3 (x3)PX4 (x4) Therefore, PX1X2X3X4 (1, 0, 1, 1) = ⋅ (1 − ) ⋅ ⋅ = ( ) 3 (1 − ) . Note that the joint PMF depends on θ, so we write it as PX1X2X3X4 (x1, x2, x3, x4; θ). We obtain the values given in T able 8.1 for the probability of (1, 0, 1, 1). θ PX1X2X3X4 (1, 0, 1, 1; θ) 0 0 1 0.0247 2 0.0988 3 0 Table 8.1: V alues of PX1X2X3X4 (1, 0, 1, 1; θ) for Example 8.1 The probability of observed sample for θ = 0 and θ = 3 is zero. This makes sense because our sample included both red and blue balls. From the table we see that the probability of the observed data is maximized for θ = 2. This means that the observed data is most likely to occur for θ = 2. For this reason, we may choose ^θ = 2 as our estimate of θ. This is called the maximum likelihood estimate (MLE) of θ. The above example gives us the idea behind the maximum likelihood estimation. Here, we introduce this method formally . To do so, we first define the likelihood function. Let X1, X2, X3, . . ., Xn be a random sample from a distribution with a parameter θ (In general, θ might be a vector , θ = (θ1, θ2, ⋯ , θk).) Suppose that x1, x2, θ 3 θ 3 θ 3 θ 3 θ 3 θ 3 θ 3 θ 3 x3, . . ., xn are the observed values of X1, X2, X3, . . ., Xn. If Xi's are discrete random variables, we define the likelihood function as the probability of the observed sample sample as a function of θ: L(x1, x2, ⋯ , xn; θ) = P(X1 = x1, X2 = x2, ⋯ , Xn = xn; θ) = PX1X2⋯Xn (x1, x2, ⋯ , xn; θ). To get a more compact formula, we may use the vector notation, X = (X1, X2, ⋯ , Xn). Thus, we may write L(x; θ) = PX(x; θ). If X1, X2, X3, . . ., Xn are jointly continuous, we use the joint PDF instead of the joint PMF. Thus, the likelihood is defined by L(x1, x2, ⋯ , xn; θ) = fX1X2⋯Xn (x1, x2, ⋯ , xn; θ). Let X1, X2, X3, . . ., Xn be a random sample from a distribution with a parameter θ. Suppose that we have observed X1 = x1, X2 = x2, ⋯, Xn = xn. 1 . If Xi's are discrete, then the likelihood function is defined as L(x1, x2, ⋯ , xn; θ) = PX1X2⋯Xn (x1, x2, ⋯ , xn; θ). 2 . If Xi's are jointly continuous, then the likelihood function is defined as L(x1, x2, ⋯ , xn; θ) = fX1X2⋯Xn (x1, x2, ⋯ , xn; θ). In some problems, it is easier to work with the log likelihood function given by ln L(x1, x2, ⋯ , xn; θ). Example 8. 8 For the following random samples, find the likelihood function: 1 . Xi ∼ Binomial(3, θ), and we have observed (x1, x2, x3, x4) = (1, 3, 2, 2). 2 . Xi ∼ Exponential(θ) and we have observed (x1, x2, x3, x4) = (1.23, 3.32, 1.98, 2.12). Solution Remember that when we have a random sample, Xi's are i.i.d., so we can obtain the joint PMF and PDF by multiplying the marginal (individual) PMFs and PDFs. 1 . If Xi ∼ Binomial(3, θ), then PXi(x; θ) = ( )θx(1 − θ) 3−x Thus, L(x1, x2, x3, x4; θ) = PX1X2X3X4 (x1, x2, x3, x4; θ) = PX1 (x1; θ)PX2 (x2; θ)PX3 (x3; θ)PX4 (x4; θ) = ( )( )( )( )θx1+x2+x3+x4 (1 − θ) 12−(x1+x2+x3+x4). Since we have observed (x1, x2, x3, x4) = (1, 3, 2, 2), we have L(1, 3, 2, 2; θ) = ( )( )( )( )θ8(1 − θ) 4 = 27 θ8(1 − θ) 4. 2 . If Xi ∼ Exponential(θ), then fXi(x; θ) = θe −θxu(x), where u(x) is the unit step function, i.e., u(x) = 1 for x ≥ 0 and u(x) = 0 for x < 0 . Thus, for xi ≥ 0, we can write L(x1, x2, x3, x4; θ) = fX1X2X3X4 (x1, x2, x3, x4; θ) = fX1 (x1; θ)fX2 (x2; θ)fX3 (x3; θ)fX4 (x4; θ) = θ4e −(x1+x2+x3+x4)θ. Since we have observed (x1, x2, x3, x4) = (1.23, 3.32, 1.98, 2.12), we have L(1.23, 3.32, 1.98, 2.12; θ) = θ4e −8.65θ. Now that we have defined the likelihood function, we are ready to define maximum likelihood estimation. Let X1, X2, X3, . . ., Xn be a random sample from a distribution 3 x 3 x1 3 x2 3 x3 3 x4 3 1 3 3 3 2 3 2 with a parameter θ. Suppose that we have observed X1 = x1, X2 = x2, ⋯, Xn = xn. The maximum likelihood estimate of θ, shown by ^θ ML is the value that maximizes the likelihood function L(x1, x2, ⋯ , xn; θ). Figure 8.1 illustrates finding the maximum likelihood estimate as the maximizing value of θ for the likelihood function. There are two cases shown in the figure: In the first graph, θ is a discrete-valued parameter , such as the one in Example 8.7 . In the second one, θ is a continuous-valued parameter , such as the ones in Example 8.8. In both cases, the maximum likelihood estimate of θ is the value that maximizes the likelihood function. Figure 8.1 - The maximum likelihood estimate for θ. Let us find the maximum likelihood estimates for the observations of Example 8.8. Example 8. 9 For the following random samples, find the maximum likelihood estimate of θ: 1 . Xi ∼ Binomial(3, θ), and we have observed (x1, x2, x3, x4) = (1, 3, 2, 2). 2 . Xi ∼ Exponential(θ) and we have observed (x1, x2, x3, x4) = (1.23, 3.32, 1.98, 2.12). Solution 1 . In Example 8.8. , we found the likelihood function as L(1, 3, 2, 2; θ) = 27 θ8(1 − θ) 4. To find the value of θ that maximizes the likelihood function, we can take the derivative and set it to zero. W e have = 27[ 8θ7(1 − θ) 4 − 4θ8(1 − θ) 3]. Thus, we obtain dL(1, 3, 2, 2; θ) dθ ^θ ML = . 2 . In Example 8.8. , we found the likelihood function as L(1.23, 3.32, 1.98, 2.12; θ) = θ4e −8.65θ. Here, it is easier to work with the log likelihood function, ln L(1.23, 3.32, 1.98, 2.12; θ). Specifically , ln L(1.23, 3.32, 1.98, 2.12; θ) = 4 ln θ − 8.65θ. By dif ferentiating, we obtain − 8.65 = 0, which results in ^θ ML = 0.46 It is worth noting that technically , we need to look at the second derivatives and endpoints to make sure that the values that we obtained above are the maximizing values. For this example, it turns out that the obtained values are indeed the maximizing values. Note that the value of the maximum likelihood estimate is a function of the observed data. Thus, as any other estimator , the maximum likelihood estimator (MLE), shown by ^ΘML is indeed a random variable. The MLE estimates ^θ ML that we found above were the values of the random variable ^ΘML for the specified observed d 2 3 4 θ The Maximum Likelihood Estimator (MLE) Let X1, X2, X3, . . ., Xn be a random sample from a distribution with a parameter θ. Given that we have observed X1 = x1, X2 = x2, ⋯, Xn = xn, a maximum likelihood estimate of θ, shown by ^θ ML is a value of θ that maximizes the likelihood function L(x1, x2, ⋯ , xn; θ). A maximum likelihood estimator (MLE) of the parameter θ, shown by ^ΘML is a random variable ^ΘML = ^ΘML(X1, X2, ⋯ , Xn) whose value when X1 = x1, X2 = x2, ⋯, Xn = xn is given by ^θ ML. Example 8. 10 For the following examples, find the maximum likelihood estimator (MLE) of θ: 1 . Xi ∼ Binomial(m, θ), and we have observed X1, X2, X3, . . ., Xn. 2 . Xi ∼ Exponential(θ) and we have observed X1, X2, X3, . . ., Xn. Solution 1 . Similar to our calculation in Example 8.8. , for the observed values of X1 = x1, X2 = x2, ⋯, Xn = xn, the likelihood function is given by L(x1, x2, ⋯ , xn; θ) = fX1X2⋯Xn (x1, x2, ⋯ , xn; θ) = n ∏ i=1 fXi(xi; θ) = n ∏ i=1 ( )θxi(1 − θ) m−xi = [ n ∏ i=1 ( )] θ∑n i=1 xi(1 − θ) mn−∑n i=1 xi. Note that the first term does not depend on θ, so we may write L(x1, x2, ⋯ , xn; θ) as L(x1, x2, ⋯ , xn; θ) = c θs(1 − θ) mn−s, m xi m xi where c does not depend on θ, and s = ∑n k=1 xi. By dif ferentiating and setting the derivative to 0 we obtain ^θ ML = n ∑ k=1 xi. This suggests that the MLE can be written as ^ΘML = n ∑ k=1 Xi. 2 . Similar to our calculation in Example 8.8. , for the observed values of X1 = x1, X2 = x2, ⋯, Xn = xn, the likelihood function is given by L(x1, x2, ⋯ , xn; θ) = n ∏ i=1 fXi(xi; θ) = n ∏ i=1 θe −θxi = θne −θ ∑n k=1 xi. Therefore, ln L(x1, x2, ⋯ , xn; θ) = n ln θ − n ∑ k=1 xiθ. By dif ferentiating and setting the derivative to 0 we obtain ^θ ML = . This suggests that the MLE can be written as ^ΘML = . The examples that we have discussed had only one unknown parameter θ. In general, θ could be a vector of parameters, and we can apply the same methodology to obtain the MLE. More specifically , if we have k unknown parameters θ1, θ2, ⋯, θk, then we need to maximize the likelihood function L(x1, x2, ⋯ , xn; θ1, θ2, ⋯ , θk) to obtain the maximum likelihood estimators ^Θ1, ^Θ2, ⋯, ^Θk. Let's look at an example. 1 mn 1 mn n ∑n k=1 xi n ∑n k=1 Xi Example 8. 1 1 Suppose that we have observed the random sample X1, X2, X3, . . ., Xn, where Xi ∼ N(θ1, θ2), so fXi(xi; θ1, θ2) = e − . Find the maximum likelihood estimators for θ1 and θ2. Solution The likelihood function is given by L(x1, x2, ⋯ , xn; θ1, θ2) = exp(− n ∑ i=1 (xi − θ1) 2). Here again, it is easier to work with the log likelihood function ln L(x1, x2, ⋯ , xn; θ1, θ2) = − ln(2π) − ln θ2 − n ∑ i=1 (xi − θ1) 2. We take the derivatives with respect to θ1 and θ2 and set them to zero: ln L(x1, x2, ⋯ , xn; θ1, θ2) = n ∑ i=1 (xi − θ1) = 0 ln L(x1, x2, ⋯ , xn; θ1, θ2) = − + n ∑ i=1 (xi − θ1) 2 = 0. By solving the above equations, we obtain the following maximum likelihood estimates for θ1 and θ2: ^θ 1 = n ∑ i=1 xi, ^θ 2 = n ∑ i=1 (xi − θ1) 2. We can write the MLE of θ1 and θ2 as random variables ^Θ1 and ^Θ1: 1 √2πθ2 (xi−θ1)2 2θ2 1 (2π) θ2 n 2 n 2 1 2θ2 n 2 n 2 1 2θ2 ∂ ∂θ1 1 θ2 ∂ ∂θ2 n 2θ2 1 2θ2 2 1 n 1 n ^Θ1 = n ∑ i=1 Xi, ^Θ2 = n ∑ i=1 (Xi − Θ1) 2. Note that ^Θ1 is the sample mean, ¯¯¯¯¯ X , and therefore it is an unbiased estimator of the mean. Here, ^Θ2 is very close to the sample variance which we defined as S 2 = n ∑ i=1 (Xi − ¯¯¯¯¯ X ) 2. In fact, ^Θ2 = S 2. Since we already know that the sample variance of unbiased estimator of the variance, we conclude that ^Θ2 is a biased estimator of the variance: E ^Θ2 = θ2. Nevertheless, the bias is very small here and it goes to zero as n gets large. Note: Here, we caution that we cannot always find the maximum likelihood estimator by setting the derivative to zero. For example, if θ is an integer-valued parameter (such as the number of blue balls in Example 8.9. ), then we cannot use dif ferentiation and we need to find the maximizing value in another way . Even if θ is a real-valued parameter , we cannot always find the MLE by setting the derivative to zero. For example, the maximum might be obtained at the endpoints of the acceptable ranges. We will see an example of such scenarios in the Solved Problems section ( Section 8.2.5 ). 1 n 1 n 1 n − 1 n − 1 n n − 1 n 8.2.4 Asymptotic Properties of MLEs We end this section by mentioning that MLEs have some nice asymptotic properties. By asymptotic properties we mean properties that are true when the sample size becomes large. Here, we state these properties without proofs. Asymptotic Properties of MLEs Let X1, X2, X3, . . ., Xn be a random sample from a distribution with a parameter θ. Let ^ΘML denote the maximum likelihood estimator (MLE) of θ. Then, under some mild regularity conditions, 1 . ^ΘML is asymptotically consistent, i.e., lim n→∞ P(| ^ΘML − θ| > ϵ) = 0. \\item ^ΘML is asymptotically unbiased, i.e., lim n→∞ E[ ^ΘML] = θ. 2 . As n becomes large, ^ΘML is approximately a normal random variable. More precisely , the random variable converges in distribution to N(0, 1). ^ΘML − θ √Var( ^ΘML) 8.2.5 Solved Problems Problem 1 Let X be the height of a randomly chosen individual from a population. In order to estimate the mean and variance of X, we observe a random sample X1, X2, ⋯, X7. Thus, Xi's are i.i.d. and have the same distribution as X. W e obtain the following values (in centimeters): 166.8, 171.4, 169.1, 178.5, 168.0, 157.9, 170.1 Find the values of the sample mean, the sample variance, and the sample standard deviation for the observed sample. Solution ¯¯¯¯¯ X = = = 168.8 The sample variance is given by S 2 = 7 ∑ k=1(Xk − 168.8) 2 = 37.7 Finally , the sample standard deviation is given by = √S 2 = 6.1 The following MA TLAB code can be used to obtain these values: X1 + X2 + X3 + X4 + X5 + X6 + X7 7 166.8 + 171.4 + 169.1 + 178.5 + 168.0 + 157.9 + 170.1 7 1 7 − 1 x=[166.8, 171.4, 169.1, 178.5, 168.0, 157.9, 170.1]; m=mean(x); v=var(x); s=std(x); Problem 2 Prove the following: a . If ^Θ1 is an unbiased estimator for θ, and W is a zero mean random variable, then ^Θ2 = ^Θ1 + W is also an unbiased estimator for θ. b . If ^Θ1 is an estimator for θ such that E[ ^Θ1] = aθ + b, where a ≠ 0, show that ^Θ2 = is an unbiased estimator for θ. Solution a . We have E[ ^Θ2] = E[ ^Θ1] + E[W] (by linearity of expectation) = θ + 0 (since  ^Θ1 is unbiased and EW = 0) = θ. Thus, ^Θ2 is an unbiased estimator for θ. b . We have E[ ^Θ2] = (by linearity of expectation) = = θ. Thus, ^Θ2 is an unbiased estimator for θ. ^Θ1 − b a E[ ^Θ1] − b a aθ + b − b a Problem 3 Let X1, X2, X3, . . ., Xn be a random sample from a Uniform(0, θ) distribution, where θ is unknown. Define the estimator ^Θn = max{X1, X2, ⋯ , Xn}. a . Find the bias of ^Θn, B( ^Θn). b . Find the MSE of ^Θn, M SE( ^Θn). c. Is ^Θn a consistent estimator of θ? Solution If X ∼ Uniform(0, θ), then the PDF and CDF of X are given by fX(x) = ⎧⎪ ⎪ ⎨ ⎪ ⎪⎩ 0 ≤ x ≤ θ 0 otherwise and FX(x) = ⎧⎪ ⎨ ⎪⎩ 0 ≤ x ≤ θ 0 otherwise By Theorem 8.1 , the PDF of ^Θn is given by f ^Θn (y) = nfX(x)[FX(x)]n−1 = ⎧⎪ ⎪ ⎨ ⎪ ⎪⎩ 0 ≤ y ≤ θ 0 otherwise a . To find the bias of ^Θn, we have E[ ^Θn] = ∫ θ 0 y ⋅ dy = θ. Thus, the bias is given by ^ ^ 1 θ x θ ny n−1 θn nyn−1 θn n n + 1 B( ^Θn) = E[ ^Θn] − θ = θ − θ = − . b . To find M SE( ^Θn), we can write M SE( ^Θn) = Var( ^Θn) + B( ^Θn) 2 = Var( ^Θn) + . Thus, we need to find Var( ^Θ). W e have E [ ^Θ 2 n] = ∫ θ 0 y2 ⋅ dy = θ2. Thus, Var( ^Θn) = E [ ^Θ 2 n] − (E[ ^Θn]) 2 = θ2. Therefore, M SE( ^Θn) = θ2 + = . c. Note that lim n→∞ M SE( ^Θn) = lim n→∞ = 0. Thus, by Theorem 8.2 , ^Θn is a consistent estimator of θ. Problem 4 Let X1, X2, X3, . . ., Xn be a random sample from a Geometric(θ) distribution, where θ is unknown. Find the maximum likelihood estimator (MLE) of θ based on this random sample. n n + 1 θ n + 1 θ2 (n + 1)2 nyn−1 θn n n + 2 n (n + 2)(n + 1)2 n (n + 2)(n + 1)2 θ2 (n + 1)2 2θ2 (n + 2)(n + 1) 2θ2 (n + 2)(n + 1) Solution If Xi ∼ Geometric(θ), then PXi(x; θ) = (1 − θ) x−1θ. Thus, the likelihood function is given by L(x1, x2, ⋯ , xn; θ) = PX1X2⋯Xn (x1, x2, ⋯ , xn; θ) = PX1 (x1; θ)PX2 (x2; θ) ⋯ PXn (xn; θ) = (1 − θ) [∑n i=1 xi−n]θn. Then, the log likelihood function is given by ln L(x1, x2, ⋯ , xn; θ) = ( n ∑ i=1 xi − n) ln(1 − θ) + n ln θ. Thus, = ( n ∑ i=1 xi − n) ⋅ + . By setting the derivative to zero, we can check that the maximizing value of θ is given by ^θ ML = . Thus, the MLE can be written as ^ΘML = . Problem 5 Let X1, X2, X3, . . ., Xn be a random sample from a Uniform(0, θ) distribution, where θ is unknown. Find the maximum likelihood estimator (MLE) of θ based on this random sample. Solution If Xi ∼ Uniform(0, θ), then d ln L(x1, x2, ⋯ , xn; θ) dθ −1 1 − θ n θ n ∑n i=1 xi n ∑n i=1 Xi 1 fX(x) = ⎧⎪ ⎪ ⎨ ⎪ ⎪⎩ 0 ≤ x ≤ θ 0 otherwise The likelihood function is given by L(x1, x2, ⋯ , xn; θ) = fX1X2⋯Xn (x1, x2, ⋯ , xn; θ) = fX1 (x1; θ)fX2 (x2; θ) ⋯ fXn (xn; θ) = ⎧⎪ ⎪ ⎨ ⎪ ⎪⎩ 0 ≤ x1, x2, ⋯ , xn ≤ θ 0 otherwise Note that is a decreasing function of θ. Thus, to minimize it, we need to choose the smallest possible value for θ. For i = 1, 2, . . . , n, we need to have θ ≥ xi. Thus, the smallest possible value for θ is ^θ ML = max(x1, x2, ⋯ , xn). Therefore, the MLE can be written as ^ΘML = max(X1, X2, ⋯ , Xn). Note that this is one of those cases wherein ^θ ML cannot be obtained by setting the derivative of the likelihood function to zero. Here, the maximum is achieved at an endpoint of the acceptable interval. 1 θ 1 θn 1 θn 8.3.0 Interval Estimation (Confidence Intervals) Let X1, X2, X3, . . ., Xn be a random sample from a distribution with a parameter θ that is to be estimated. Suppose that we have observed X1 = x1, X2 = x2, ⋯, Xn = xn. So far , we have discussed point estimation for θ. The point estimate ^θ alone does not give much information about θ. In particular , without additional information, we do not know how close ^θ is to the real θ. Here, we will introduce the concept of interval estimation . In this approach, instead of giving just one value ^θ as the estimate for θ, we will produce an interval that is likely to include the true value of θ. Thus, instead of saying ^θ = 34.25, we might report the interval [^θ l, ^θ h] = [30.69, 37.81], which we hope includes the real value of θ. That is, we produce two estimates for θ, a high estimate ^θ h and a low estimate ^θ l. In interval estimation, there are two important concepts. One is the length of the reported interval, ^θ h − ^θ l. The length of the interval shows the precision with which we can estimate θ. The smaller the interval, the higher the precision with which we can estimate θ. The second important factor is the confidence level that shows how confident we are about the interval. The confidence level is the probability that the interval that we construct includes the real value of θ. Therefore, high confidence levels are desirable. W e will discuss these concepts in this section. 8.3.1 The General Framework of Interval Estimation Let X1, X2, X3, . . ., Xn be a random sample from a distribution with a parameter θ that is to be estimated. Our goal is to find two estimators for θ: 1 . the low estimator , ^Θl = ^Θl(X1, X2, ⋯ , Xn), and 2 . the high estimator , ^Θh = ^Θh(X1, X2, ⋯ , Xn). The interval estimator is given by the interval [ ^Θl, ^Θh]. The estimators ^Θl and ^Θh are chosen such that the probability that the interval [ ^Θl, ^Θh] includes θ is larger than 1 − α. Here, 1 − α is said to be confidence level . We would like α to be small. Common values for α are 0.1, .05, and .01 which correspond to confidence levels 90%, 95%, and 99% respectively . Thus, when we are asked to find a 95% confidence interval for a parameter θ, we need to find ^Θl and ^Θh such that P( ^Θl < θ and ^Θh > θ) ≥ 0.95 The above discussion will become clearer as we go through examples. Before doing that let's formally define interval estimation. Interval Estimation Let X1, X2, X3, . . ., Xn be a random sample from a distribution with a parameter θ that is to be estimated. An interval estimator with confidence level 1 − α consists of two estimators ^Θl(X1, X2, ⋯ , Xn) and ^Θh(X1, X2, ⋯ , Xn) such that P( ^Θl ≤ θ and ^Θh ≥ θ) ≥ 1 − α, for every possible value of θ. Equivalently , we say that [ ^Θl, ^Θh] is a (1 − α)100% confidence interval for θ. Note that the condition P( ^Θl ≤ θ and ^Θh ≥ θ) ≥ 1 − α can be equivalently written as P( ^Θl ≤ θ ≤ ^Θh) ≥ 1 − α, or P(θ ∈ [ ^Θl, ^Θh]) ≥ 1 − α. The randomness in these terms is due to ^Θl and ^Θh, not θ. Here, θ is the unknown quantity which is assumed to be non-random (frequentist inference). On the other hand, ^Θl and ^Θh are random variables because they are functions of the observed random variables X1, X2, X3, . . ., Xn. 8.3.2 Finding Interval Estimators Here we would like to discuss how we find interval estimators. Before doing so, let's review a simple fact from random variables and their distributions. Let X be a continuous random variable with CDF FX(x) = P(X ≤ x). Suppose that we are interested in finding two values xh and xl such that P(xl ≤ X ≤ xh) = 1 − α. One way to do this, is to chose xl and xh such that P(X ≤ xl) = , and P(X ≥ xh) = . Equivalently , FX(xl) = , and FX(xh) = 1 − . We can rewrite these equations by using the inverse function F −1 X as xl = F −1 X ( ) , and xh = F −1 X (1 − ) . We call the interval [xl, xh] a ( 1 − α) interval for X. Figure 8.2 shows the values of xl and xh using the CDF of X, and also using the PDF of X. Figure 8.2 - [xl, xh] is a ( 1 − α) interval for X, that is, P(xl ≤ X ≤ xh) = 1 − α. Example 8. 12 Let Z ∼ N(0, 1), find xl and xh such that α 2 α 2 α 2 α 2 α 2 α 2 P(xl ≤ Z ≤ xh) = 0.95 Solution Here, α = 0.05 and the CDF of Z is given by the Φ function. Thus, we can choose xl = Φ−1(0.025) = −1.96, and xh = Φ−1(1 − 0.025) = 1.96 Thus, for a standard normal random variable Z, we have P( − 1.96 ≤ Z ≤ 1.96) = 0.95 More generally , we can find a (1 − α) interval for the standard normal random variable. Assume Z ∼ N(0, 1). Let us define a notation that is commonly used. For any p ∈ [0, 1], we define zp as the real value for which P(Z > zp) = p. Therefore, Φ(zp) = 1 − p, zp = Φ−1(1 − p). By symmetry of the normal distribution, we also conclude z1−p = −zp. Figure 8.3 shows zp and z1−p = −zp on the real line. In MA TLAB, to compute zp you can use the following command: norminv(1 − p). Figure 8.3 - By definition, zp is the real number , for which we have Φ(zp) = 1 − p. Now , using the zp notation, we can state a (1 − α) interval for the standard normal random variable Z as P (−z ≤ Z ≤ z ) = 1 − α. Figure 8.4 shows the (1 − α) interval for the standard normal random variable Z. Figure 8.4 - A (1 − α) interval for N(0, 1) distribution. In particular , in this figure, we have P (Z ∈ [ − z , z ]) = 1 − α. Now , let's talk about how we can find interval estimators. A general approach is to start with a point estimator ^Θ, such as the MLE, and create the interval [ ^Θl, ^Θh] around it such that P(θ ∈ [ ^Θl, ^Θh]) ≥ 1 − α. How do we do this? Let's look at an example. Example 8. 13 Let X1, X2, X3, . . ., Xn be a random sample from a normal distribution N(θ, 1). Find a 95% confidence interval for θ. Solution Let's start with a point estimator ^Θ for θ. Since θ is the mean of the distribution, we can use the sample mean ^Θ = ¯¯¯¯¯ X = . Since Xi ∼ N(θ, 1) and the Xi's are independent, we conclude that ¯¯¯¯¯ X ∼ N (θ, ) . By normalizing ¯¯¯¯¯ X , we conclude that the random variable α 2 α 2 α 2 α 2 X1 + X2+. . . +Xn n 1 n = √n( ¯¯¯¯¯ X − θ) has a N(0, 1) distribution. Therefore, by Example 8.12 , we conclude P( − 1.96 ≤ √n( ¯¯¯¯¯ X − θ) ≤ 1.96) = 0.95 which is equivalent to (by rearranging the terms) P(¯¯¯¯¯ X − ≤ θ ≤ ¯¯¯¯¯ X + ) = 0.95 Therefore, we can report the interval [ ^Θl, ^Θh] = [ ¯¯¯¯¯ X − , ¯¯¯¯¯ X + ] as our 95% confidence interval for θ. At first, it might seem that our solution to Example 8.13 is not based on a systematic method. Y ou might have asked: \"How should I know that I need to work with the normalized ¯¯¯¯¯ X ?\" However , by thinking more deeply about the way we solved this example, we can suggest a general method to solve confidence interval problems. The crucial fact about the random variable ¯¯¯¯¯ X − θ is that its distribution does not depend on the unknown parameter θ. Thus, we could easily find a 95% interval for the random variable √n( ¯¯¯¯¯ X − θ) that did not depend on θ. Such a random variable is called a pivot or a pivotal quantity . Let us define this more precisely . ¯¯¯¯¯ X − θ 1 √n 1.96 √n 1.96 √n 1.96 √n 1.96 √n Pivotal Quantity Let X1, X2, X3, . . ., Xn be a random sample from a distribution with a parameter θ that is to be estimated. The random variable Q is said to be a pivot or a pivotal quantity , if it has the following properties: 1 . It is a function of the observed data X1, X2, X3, . . ., Xn and the unknown parameter θ, but it does not depend on any other unknown parameters: Q = Q(X1, X2, ⋯ , Xn, θ). 2 . The probability distribution of Q does not depend on θ or any other unknown parameters. Example 8. 14 Check that the random variables Q1 = ¯¯¯¯¯ X − θ and Q2 = √n( ¯¯¯¯¯ X − θ) are both valid pivots in Example 8.13 . Solution We note that Q1 and Q2 by definitions are functions of ¯¯¯¯¯ X and θ. Since ¯¯¯¯¯ X = , we conclude Q1 and Q2 are both functions of the observed data X1, X2, X3, . . ., Xn and the unknown parameter θ, and they do not depend on any other unknown parameters. Also, Q1 ∼ N(0, ), Q2 ∼ N(0, 1). Thus, their distributions do not depend on θ or any other unknown parameters. W e conclude that Q1 and Q2 are both valid pivots. X1 + X2+. . . +Xn n 1 n To summarize, here are the steps in the pivotal method for finding confidence intervals: 1 . First, find a pivotal quantity Q(X1, X2, ⋯ , Xn, θ). 2 . Find an interval for Q such that P(ql ≤ Q ≤ qh) = 1 − α. 3 . Using algebraic manipulations, convert the above equation to an equation of the form P( ^Θl ≤ θ ≤ ^Θh) = 1 − α. You are probably still not sure how exactly you can perform these steps. The most crucial one is the first step. How do we find a pivotal quantity? Luckily , for many important cases that appear frequently in practice, statisticians have already found the pivotal quantities, so we can use their results directly . In practice, many of the interval estimation problems you encounter are of the forms for which general confidence intervals have been found previously . Therefore, to solve many confidence interval problems, it suf fices to write the problem in a format similar to a previously solved problem. As you see more examples, you will feel more confident about solving confidence interval problems. Example 8. 15 Let X1, X2, X3, . . ., Xn be a random sample from a distribution with known variance Var(Xi) = σ2, and unknown mean EXi = θ. Find a (1 − α) confidence interval for θ. Assume that n is large. Solution As usual, to find a confidence interval, we start with a point estimate. Since θ = EXi, a natural choice is the sample mean ¯¯¯¯¯ X = . Since n is large, by the Central Limit Theorem (CL T), we conclude that Q = X1 + X2+. . . +Xn n ¯¯¯¯¯ X − θ σ √n has approximately N(0, 1) distribution. In particular , Q is a function of the Xi's and θ, and its distribution does not depend on θ, or any other unknown parameters. Thus, Q is a pivotal quantity . The next step is to find a (1 − α) interval for Q. As we saw before, a (1 − α) interval for the standard normal random variable Q can be stated as P (−z ≤ Q ≤ z ) = 1 − α. Therefore, P ⎛ ⎜ ⎝−z ≤ ≤ z ⎞ ⎟ ⎠ = 1 − α. which is equivalent to P (¯¯¯¯¯ X − z ≤ θ ≤ ¯¯¯¯¯ X + z ) = 1 − α. We conclude that [¯¯¯¯¯ X − z , ¯¯¯¯¯ X + z ] is a (1 − α)100% confidence interval for θ. The above example is our first important case of known interval estimators, so let's summarize what we have shown: Assumptions: A random sample X1, X2, X3, . . ., Xn is given from a distribution with known variance Var(Xi) = σ2 < ∞; n is large. Parameter to be Estimated: θ = EXi. Confidence Interval: [¯¯¯¯¯ X − z , ¯¯¯¯¯ X + z ] is approximately a (1 − α)100% confidence interval for θ. Note that to obtain the above interval, we used the CL T. Thus, what we found is an approximate confidence interval. Nevertheless, for large n, the approximation is very good. α 2 α 2 α 2 ¯¯¯¯¯ X − θ σ √n α 2 α 2 σ √n α 2 σ √n α 2 σ √n α 2 σ √n α 2 σ √n α 2 σ √n Example 8. 16 An engineer is measuring a quantity θ. It is assumed that there is a random error in each measurement, so the engineer will take n measurements and report the average of the measurements as the estimated value of θ. Here, n is assumed to be large enough so that the central limit theorem applies. If Xi is the value that is obtained in the ith measurement, we assume that Xi = θ + Wi, where Wi is the error in the ith measurement. W e assume that the Wi's are i.i.d. with EWi = 0 and Var(Wi) = 4 units. The engineer reports the average of the measurements ¯¯¯¯¯ X = . How many measurements does the engineer need to make until he is 90% sure that the final error is less than 0.25 units? In other words, what should the value of n be such that P(θ − 0.25 ≤ ¯¯¯¯¯ X ≤ θ + 0.25) ≥ .90 ? Solution Note that, here, the Xi's are i.i.d. with mean EXi = θ + EWi = θ, and variance Var(Xi) = Var(Wi) = 4. Thus, we can restate the problem using our confidence interval terminology: \"Let X1, X2, X3, . . ., Xn be a random sample from a distribution with known variance Var(Xi) = σ2 = 4. How large n should be so that the interval [¯¯¯¯¯ X − 0.25, ¯¯¯¯¯ X + 0.25] is a 90% confidence interval for θ = EXi?\" By our discussion above, the 95% confidence interval for θ = EXi is given by X1 + X2+. . . +Xn n [ ¯¯¯¯¯ X − z , ¯¯¯¯¯ X + z ] Thus, we need z = 0.25, where σ = 2, α = 1 − 0.90 = 0.1. In particular , z = z0.05 = Φ−1(1 − 0.05) = 1.645 Thus, we need to have 1.645 = 0.25 We conclude that n ≥ 174 is sufficient. Now suppose that X1, X2, X3, . . ., Xn is a random sample from a distribution with unknown variance Var(Xi) = σ2. Our goal is to find a 1 − α confidence interval for θ = EXi. We also assume that n is large. By the above discussion, we can say P (¯¯¯¯¯ X − z ≤ θ ≤ ¯¯¯¯¯ X + z ) = 1 − α. However , there is a problem here. W e do not know the value of σ. How do we deal with this issue? There are two general approaches: we can either find an upper bound for σ, or we can estimate σ. 1 . An upper bound for σ2: Suppose that we can somehow show that σ ≤ σmax, where σmax < ∞ is a real number . Then, if we replace σ in [¯¯¯¯¯ X − z , ¯¯¯¯¯ X + z ] by σmax, the interval gets bigger . In other words, the interval [ ¯¯¯¯¯ X − z , ¯¯¯¯¯ X + z ] is still a valid (1 − α)100% confidence interval for θ. α 2 σ √n α 2 σ √n α 2 σ √n α 2 2 √n α 2 σ √n α 2 σ √n α 2 σ √n α 2 σ √n α 2 σmax √n α 2 σmax √n 2 . Estimate σ2: Note that here, since n is large, we should be able to find a relatively good estimate for σ2. After estimating σ2, we can use that estimate and [¯¯¯¯¯ X − z , ¯¯¯¯¯ X + z ] to find an approximate (1 − α)100% confidence interval for θ. We now provide examples of each approach. Example 8. 17 (Public Opinion Polling) W e would like to estimate the portion of people who plan to vote for Candidate A in an upcoming election. It is assumed that the number of voters is large, and θ is the portion of voters who plan to vote for Candidate A. W e define the random variable X as follows. A voter is chosen uniformly at random among all voters and we ask her/him: \"Do you plan to vote for Candidate A?\" If she/he says \"yes,\" then X = 1, otherwise X = 0. Then, X ∼ Bernoulli(θ). Let X1, X2, X3, . . ., Xn be a random sample from this distribution, which means that the Xi's are i.i.d. and Xi ∼ Bernoulli(θ). In other words, we randomly select n voters (with replacement) and we ask each of them if they plan to vote for Candidate A. Find a (1 − α)100% confidence interval for θ based on X1, X2, X3, . . ., Xn. Solution Note that, here, EXi = θ. Thus, we want to estimate the mean of the distribution. Note also that Var(Xi) = σ2 = θ(1 − θ). Thus, to find σ, we need to know θ. But θ is the parameter that we would like to estimate in the first place. By the above discussion, we know that if we can find an upper bound for σ, we can use it to build a confidence interval for θ. Luckily , it is easy to find an upper bound for σ in this problem. More specifically , if you define f(θ) = θ(1 − θ),  for θ ∈ [0, 1]. By taking derivatives, you can show that the maximum value for f(θ) is obtained at θ = and that α 2 σ √n α 2 σ √n 1 2 f(θ) ≤ f ( ) = ,  for θ ∈ [0, 1]. We conclude that σmax = is an upper bound for σ. We conclude that the interval [ ¯¯¯¯¯ X − z , ¯¯¯¯¯ X + z ] is a (1 − α)100% confidence interval for θ, where σmax = . Thus, [ ¯¯¯¯¯ X − , ¯¯¯¯¯ X + ] is a (1 − α)100% confidence interval for θ. Note that we obtained the interval by using the CLT, so it is an approximate interval. Nevertheless, for large n, the approximation is very good. Also, since we have used an upper bound for σ, this confidence interval might be too conservative, specifically if θ is far from . The above setting is another important case of known interval estimators, so let's summarize it: Assumptions: A random sample X1, X2, X3, . . ., Xn is given from a Bernoulli(θ); n is large. Parameter to be Estimated: θ Confidence Interval: [ ¯¯¯¯¯ X − , ¯¯¯¯¯ X + ] is approximately a (1 − α)100% confidence interval for θ. This is a conservative confidence interval as it is obtained using an upper bound for σ. 1 2 1 4 1 2 α 2 σmax √n α 2 σmax √n 1 2 z α 2 2√n z α 2 2√n 1 2 z α 2 2√n z α 2 2√n Example 8. 18 There are two candidates in a presidential election: Candidate A and Candidate B. Let θ be the portion of people who plan to vote for Candidate A. Our goal is to find a confidence interval for θ. Specifically , we choose a random sample (with replacement) of n voters and ask them if they plan to vote for Candidate A. Our goal is to estimate the θ such that the margin of error is 3 percentage points. Assume a 95% confidence level. That is, we would like to choose n such that P (¯¯¯¯¯ X − 0.03 ≤ θ ≤ ¯¯¯¯¯ X + 0.03) ≥ 0.95, where ¯¯¯¯¯ X is the portion of people in our random sample that say they plan to vote for Candidate A. How large does n need to be? Solution Based on the above discussion, [ ¯¯¯¯¯ X − , ¯¯¯¯¯ X + ] is a valid (1 − α)100% confidence interval for θ. Therefore, we need to have = 0.03 Here, α = 0.05, so z = z0.025 = 1.96. Therefore, we obtain n = ( ) 2. We conclude n ≥ 1068 is enough. The above calculation provides a reason why most polls before elections are conducted with a sample size of around one thousand. As we mentioned, the above calculation might be a little conservative. Another approach would be to estimate σ2 instead of using an upper bound. In this example, the structure of the problem suggests a way to estimate σ2. Specifically , since σ2 = θ(1 − θ), z α 2 2√n z α 2 2√n z α 2 2√n α 2 1.96 2 × 0.03 we may use ^σ 2 = ^θ(1 − ^θ) = ¯¯¯¯¯ X (1 − ¯¯¯¯¯ X ) as an estimate for θ, where ^θ = ¯¯¯¯¯ X . The rationale behind this approximation is that since n is large, ¯¯¯¯¯ X is likely a good estimate of θ, thus ^σ 2 = ^θ(1 − ^θ) is a good estimate of σ2. After estimating σ2, we can use [¯¯¯¯¯ X − z , ¯¯¯¯¯ X + z ] as an approximate (1 − α)100% confidence interval for θ. To summarize, we have the following confidence interval rule: Assumptions: A random sample X1, X2, X3, . . ., Xn is given from a Bernoulli(θ); n is large. Parameter to be Estimated: θ Confidence Interval: [ ¯¯¯¯¯ X − z √ , ¯¯¯¯¯ X + z √ ] is approximately a (1 − α)100% confidence interval for θ. Again, the above confidence interval is an approximate confidence interval because we used two approximations: the CL T and an approximation for σ2. The above scenario is a special case ( Bernoulli(θ)) for which we could come up with a point estimator for σ2. Can we have a more general estimator for σ2 that we can use for any distribution? W e have already discussed such a point estimator and we called it the sample variance: S 2 = n ∑ k=1(Xk − ¯¯¯¯¯ X ) 2 = ( n ∑ k=1 X2 k − n¯¯¯¯¯ X 2) . Thus, using the sample variance, S 2, we can have an estimate for σ2. If n is large, this estimate is likely to be close to the real value of σ2. So let us summarize this discussion as follows: α 2 ^σ √n α 2 ^σ √n α 2 ¯¯¯¯ X (1−¯¯¯¯ X ) n α 2 ¯¯¯¯ X (1−¯¯¯¯ X ) n 1 n − 1 1 n − 1 Assumptions: A random sample X1, X2, X3, . . ., Xn is given from a distribution with unknown variance Var(Xi) = σ2 < ∞; n is large. Parameter to be Estimated: θ = EXi. Confidence Interval: If S is the sample standard deviation S =    ⎷ n ∑ k=1(Xk − ¯¯¯¯¯ X ) 2 =    ⎷ ( n ∑ k=1 X2 k − n¯¯¯¯¯ X 2), then the interval [ ¯¯¯¯¯ X − z , ¯¯¯¯¯ X + z ] is approximately a (1 − α)100% confidence interval for θ. Example 8. 19 We have collected a random sample X1, X2, X3, . . ., X100 from an unknown distribution. The sample mean and the sample variance for this random sample are given by ¯¯¯¯¯ X = 15.6, S 2 = 8.4 Construct an approximate 99% confidence interval for θ = EXi. Solution Here, the interval [ ¯¯¯¯¯ X − z , ¯¯¯¯¯ X + z ] is approximately a (1 − α)100% confidence interval for θ. Since α = 0.01, we have z = z0.005 = 2.576 1 n − 1 1 n − 1 α 2 S √n α 2 S √n α 2 S √n α 2 S √n α 2 Using n = 100, ¯¯¯¯¯ X = 15.6, S 2 = 8.4, we obtain the following interval [15.6 − 2.576 , 15.6 + 2.576 ] = [14.85, 16.34]. √8.4 √100 √8.4 √100 8.3.3 Confidence Intervals for Normal Samples In the above discussion, we assumed n to be large so that we could use the CL T. An interesting aspect of the confidence intervals that we obtained was that they often did not depend on the details of the distribution from which we obtained the random sample. That is, the confidence intervals only depended on statistics such as ¯¯¯¯¯ X and S 2. What if n is not large? In this case, we cannot use the CL T, so we need to use the probability distribution from which the random sample is obtained. A very important case is when we have a sample X1, X2, X3, . . ., Xn from a normal distribution. Here, we would like to discuss how to find interval estimators for the mean and the variance of a normal distribution. Before doing so, we need to introduce two probability distributions that are related to the normal distribution. These distributions are useful when finding interval estimators for the mean and the variance of a normal distribution. C hi-Sq uared D istributio n Let us remember the gamma distribution. A continuous random variable X is said to have a gamma distribution with parameters α > 0 and λ > 0, shown as X ∼ Gamma(α, λ), if its PDF is given by fX(x) = { x > 0 0 otherwise Now , we would like to define a closely related distribution, called the chi-squared distribution. W e know that if Z1, Z2, ⋯, Zn are independent standard normal random variables, then the random variable X = Z1 + Z2 + ⋯ + Zn is also normal. More specifically , X ∼ N(0, n). Now , if we define a random variable Y as Y = Z 2 1 + Z 2 2 + ⋯ + Z 2 n , then Y is said to have a chi-squared distribution with n degrees of freedom shown by Y ∼ χ 2(n). λαx α−1e −λx Γ(α) It can be shown that the random variable Y has, in fact, a gamma distribution with parameters α = and λ = , Y ∼ Gamma ( , ) . Figure 8.5 shows the PDF of χ 2(n) distribution for some values of n. Figure 8.5 - The PDF of χ 2(n) distribution for some values of n. So, let us summarize the definition and some properties of the chi-squared distribution. n 2 1 2 n 2 1 2 The Chi-Squared Distribution Definition 8. 1 . If Z1, Z2, ⋯, Zn are independent standard normal random variables, the random variable Y defined as Y = Z 2 1 + Z 2 2 + ⋯ + Z 2 n is said to have a chi-squared distribution with n degrees of freedom shown by Y ∼ χ 2(n). Properties: 1 . The chi-squared distribution is a special case of the gamma distribution. More specifically , Y ∼ Gamma ( , ) . Thus, fY (y) = y −1e − , for y > 0. 2 . EY = n, Var(Y ) = 2n. 3 . For any p ∈ [0, 1] and n ∈ N, we define χ 2 p,n as the real value for which P(Y > χ 2 p,n) = p, where Y ∼ χ 2(n). Figure 8.6 shows χ 2 p,n. In MA TLAB, to compute χ 2 p,n you can use the following command: chi2inv(1 − p, n) n 2 1 2 1 2 Γ ( ) n 2 n 2 n 2 y 2 Figure 8.6 - The definition of χ 2 p,n. Now , why do we need the chi-squared distribution? One reason is the following theorem, which we will use in estimating the variance of normal random variables. Theorem 8. 3 . Let X1, X2, ⋯, Xn be i.i.d. N(μ, σ) random variables. Also, let S 2 be the standard variance for this random sample. Then, the random variable Y defined as Y = = n ∑ i=1 (Xi − ¯¯¯¯¯ X ) 2 has a chi-squared distribution with n − 1 degrees of freedom, i.e., Y ∼ χ 2(n − 1). Moreover , ¯¯¯¯¯ X and S 2 are independent random variables. The t-D istributio n The next distribution that we need is the Student's t-distribution (or simply the t- distribution ). Here, we provide the definition and some properties of the t-distribution. (n − 1)S 2 σ2 1 σ2 The t-Distribution Definition 8. 2 . Let Z ∼ N(0, 1), and Y ∼ χ 2(n), where n ∈ N. Also assume that Z and Y are independent. The random variable T defined as T = is said to have a t-distribution with n degrees of freedom shown by T ∼ T(n). Properties: 1 . The t-distribution has a bell-shaped PDF centered at 0, but its PDF is more spread out than the normal PDF (Figure 8.7). 2 . ET = 0, for n > 0. But ET, is undefined for n = 1. 3 . Var(T) = , for n > 2. But, Var(T) is undefined for n = 1, 2. 4 . As n becomes large, the t density approaches the standard normal PDF . More formally , we can write T(n)  →  N(0, 1). 5 . For any p ∈ [0, 1] and n ∈ N, we define tp,n as the real value for which P(T > tp,n) = p. Since the t-distribution has a symmetric PDF , we have t1−p,n = −tp,n. In MA TLAB, to compute tp,n you can use the following command: tinv(1 − p, n). Figure 8.7 shows the PDF of t-distribution for some values of n and compares them with the PDF of the standard normal distribution. As we see, the t density is more spread out than the standard normal PDF . Figure 8.8 shows tp,n. Z √Y /n n n−2 d Figure 8.7 - The PDF of t-distribution for some values of n compared with the standard normal PDF . Figure 8.8 - The definition of tp,n. Why do we need the t-distribution? One reason is the following theorem which we will use in estimating the mean of normal random variables. Theorem 8. 4 . Let X1, X2, ⋯, Xn be i.i.d. N(μ, σ) random variables. Also, let S 2 be the standard variance for this random sample. Then, the random variable T defined as T = has a t-distribution with n − 1 degrees of freedom, i.e., T ∼ T(n − 1). Proof: Define the random variable Z as Z = . Then, Z ∼ N(0, 1). Also, define the random variable Y as Y = . Then by Theorem Theorem 8.3 , Y ∼ χ 2(n − 1). We conclude that the random variable T = = has a t-distribution with n − 1 degrees of freedom. C o n fiden ce In tervals fo r the Mean o f No rmal Ran do m Variables Here, we assume that X1, X2, X3, . . ., Xn is a random sample from a normal distribution N(μ, σ2), and our goal is to find an interval estimator for μ. We no longer require n to be large. Thus, n could be any positive integer . There are two possible scenarios depending on whether σ2 is known or not. If the value of σ2 is known, we can easily find a confidence interval for μ. This can be done using exactly the same method that we used to estimate μ for a general distribution for the case of large n. More specifically , we know that the random variable Q = ¯¯¯¯¯ X − μ S/√n ¯¯¯¯¯ X − μ σ/√n (n − 1)S 2 σ2 Z √ Y n−1 ¯¯¯¯¯ X − μ S/√n ¯¯¯¯¯ X − μ σ/√n has N(0, 1) distribution. In particular , Q is a function of the Xi's and μ, and its distribution does not depend on μ. Thus, Q is a pivotal quantity , and we conclude that [¯¯¯¯¯ X − z , ¯¯¯¯¯ X + z ] is (1 − α)100% confidence interval for μ. Assumptions: A random sample X1, X2, X3, . . ., Xn is given from a N(μ, σ2) distribution, where Var(Xi) = σ2 is known . Parameter to be Estimated: μ = EXi. Confidence Interval: [¯¯¯¯¯ X − z , ¯¯¯¯¯ X + z ] is a (1 − α)100% confidence interval for μ. The more interesting case is when we do not know the variance σ2. More specifically , we are given X1, X2, X3, . . ., Xn, which is a random sample from a normal distribution N(μ, σ2), and our goal is to find an interval estimator for μ. However , σ2 is also unknown. In this case, using Theorem 8.4 , we conclude that the random variable T defined as T = has a t-distribution with n − 1 degrees of freedom, i.e., T ∼ T(n − 1). Here, the random variable T is a pivotal quantity , since it is a function of the Xi's and μ, and its distribution does not depend on μ or any other unknown parameters. Now that we have a pivot, the next step is to find a (1 − α) interval for T. Using the definition of tp,n, a (1 − α) interval for T can be stated as P (−t ,n−1 ≤ T ≤ t ,n−1) = 1 − α. Therefore, P (−t ,n−1 ≤ ≤ t ,n−1) = 1 − α, which is equivalent to α 2 σ √n α 2 σ √n α 2 σ √n α 2 σ √n ¯¯¯¯¯ X − μ S/√n α 2 α 2 α 2 ¯¯¯¯¯ X − μ S/√n α 2 P (¯¯¯¯¯ X − t ,n−1 ≤ μ ≤ ¯¯¯¯¯ X + t ,n−1 ) = 1 − α. We conclude that [¯¯¯¯¯ X − t ,n−1 , ¯¯¯¯¯ X + t ,n−1 ] is (1 − α)100% confidence interval for μ. Assumptions: A random sample X1, X2, X3, . . ., Xn is given from a N(μ, σ2) distribution, where μ = EXi and Var(Xi) = σ2 are unknown . Parameter to be Estimated: μ = EXi. Confidence Interval: [¯¯¯¯¯ X − t ,n−1 , ¯¯¯¯¯ X + t ,n−1 ] is a (1 − α) confidence interval for μ. Example 8. 20 A farmer weighs 10 randomly chosen watermelons from his farm and he obtains the following values (in lbs): 7.72 9.58 12.38 7.77 11.27 8.80 11.10 7.80 10.17 6.00 Assuming that the weight is normally distributed with mean μ and and variance σ, find a 95% confidence interval for μ. Solution Using the data we obtain ¯¯¯¯¯ X = 9.26, S 2 = 3.96 Here, n = 10, α = 0.05, so we need t0.025,9 ≈ 2.262 The above value can be obtained in MA TLAB using the command tinv(0.975, 9). Thus, we can obtain a 95% confidence interval for μ as α 2 S √n α 2 S √n α 2 S √n α 2 S √n α 2 S √n α 2 S √n [ ¯¯¯¯¯ X − t ,n−1 , ¯¯¯¯¯ X + t ,n−1 ] = = [9.26 − 2.26 ⋅ , 9.26 + 2.26 ⋅ ] = [7.84, 10.68]. Therefore, [7.84, 10.68] is a 95% confidence interval for μ. C o n fiden ce In tervals fo r the Varian ce o f No rmal Ran do m Variables Now , suppose that we would like to estimate the variance of a normal distribution. More specifically , assume that X1, X2, X3, . . ., Xn is a random sample from a normal distribution N(μ, σ2), and our goal is to find an interval estimator for σ2. We assume that μ is also unknown. Again, n could be any positive integer . By Theorem 8.3 , the random variable Y defined as Q = = n ∑ i=1 (Xi − ¯¯¯¯¯ X ) 2 has a chi-squared distribution with n − 1 degrees of freedom, i.e., Q ∼ χ 2(n − 1). In particular , Q is a pivotal quantity since it is a function of the Xi's and σ2, and its distribution does not depend on σ2 or any other unknown parameters. Using the definition of χ 2 p,n, a (1 − α) interval for Q can be stated as P (χ 2 1− ,n−1 ≤ Q ≤ χ 2 ,n−1) = 1 − α. Therefore, P (χ 2 1− ,n−1 ≤ ≤ χ 2 ,n−1) = 1 − α. which is equivalent to P ⎛ ⎜ ⎝ ≤ σ2 ≤ ⎞ ⎟ ⎠ = 1 − α. We conclude that ⎡ ⎣ , ⎤ ⎦ is a (1 − α)100% confidence interval for σ2. α 2 S √n α 2 S √n √3.96 √10 √3.96 √10 (n − 1)S 2 σ2 1 σ2 α 2 α 2 α 2 (n − 1)S 2 σ2 α 2 (n − 1)S 2 χ 2 ,n−1 α 2 (n − 1)S 2 χ 2 1− ,n−1 α 2 (n−1)S 2 χ 2 ,n−1 α 2 (n−1)S 2 χ 2 1− ,n−1 α 2 Assumptions: A random sample X1, X2, X3, . . ., Xn is given from a N(μ, σ2) distribution, where μ = EXi and Var(Xi) = σ2 are unknown . Parameter to be Estimated: Var(Xi) = σ2. Confidence Interval: ⎡ ⎣ , ⎤ ⎦ is a (1 − α)100% confidence interval for σ2. Example 8. 21 For the data given in Example 8.20 , find a 95% confidence interval for σ2. Again, assume that the weight is normally distributed with mean μ and and variance σ, where μ and σ are unknown. Solution As before, using the data we obtain ¯¯¯¯¯ X = 9.26, S 2 = 3.96 Here, n = 10, α = 0.05, so we need χ 2 0.025,9 = 19.02, χ 2 0.975,9 = 2.70 The above values can obtained in MA TLAB using the commands chi2inv(0.975, 9) and chi2inv(0.025, 9), respectively . Thus, we can obtain a 95% confidence interval for σ2 as ⎡ ⎢ ⎣ , ⎤ ⎥ ⎦ = [ , ] = [1.87, 13.20]. Therefore, [1.87, 13.20] is a 95% confidence interval for σ2. (n−1)S 2 χ 2 ,n−1 α 2 (n−1)S 2 χ 2 1− ,n−1 α 2 (n − 1)S 2 χ 2 ,n−1 α 2 (n − 1)S 2 χ 2 1− ,n−1 α 2 9 × 3.96 19.02 9 × 3.96 2.70 8.3.4 Solved Problems Problem 1 Let X1, X2, X3, . . ., Xn be a random sample from an exponential distribution with parameter θ, i.e., fXi(x; θ) = θe −θxu(x). Our goal is to find a (1 − α)100% confidence interval for θ. T o do this, we need to remember a few facts about the gamma distribution. More specifically , If Y = X1 + X2 + ⋯ + Xn, where the Xi's are independent Exponential(θ) random variables, then Y ∼ Gamma(n, θ). Thus, the random variable Q defined as Q = θ(X1 + X2 + ⋯ + Xn) has a Gamma(n, 1) distribution. Let us define γp,n as follows. For any p ∈ [0, 1] and n ∈ N, we define γp,n as the real value for which P(Q > γp,n) = p, where Q ∼ Gamma(n, 1). a . Explain why Q = θ(X1 + X2 + ⋯ + Xn) is a pivotal quantity . b . Using Q and the definition of γp,n, construct a (1 − α)100% confidence interval for θ. Solution a . Q is a function of the Xi's and θ, and its distribution does not depend on θ or any other unknown parameters. Thus, Q is a pivotal quantity . b . Using the definition of γp,n, a (1 − α) interval for Q can be stated as P (γ1− ,n−1 ≤ Q ≤ γ ,n−1) = 1 − α. Therefore, P (γ1− ,n−1 ≤ θ(X1 + X2 + ⋯ + Xn) ≤ γ ,n−1) = 1 − α. α 2 α 2 α 2 α 2 Since X1 + X2 + ⋯ + Xn is always a positive quantity , the above equation is equivalent to P ( ≤ θ ≤ ) = 1 − α. We conclude that [ , ] is a (1 − α)100% confidence interval for θ. Problem 2 A random sample X1, X2, X3, . . ., X100 is given from a distribution with known variance Var(Xi) = 16. For the observed sample, the sample mean is ¯¯¯¯¯ X = 23.5. Find an approximate 95% confidence interval for θ = EXi. Solution Here, [¯¯¯¯¯ X − z , ¯¯¯¯¯ X + z ] is an approximate (1 − α)100% confidence interval. Since α = 0.05, we have z = z0.025 = Φ−1(1 − 0.025) = 1.96 Also, σ = 4. Therefore, the approximate confidence interval is [23.5 − 1.96 , 23.5 − 1.96 ] ≈ [22.7, 24.3]. Problem 3 To estimate the portion of voters who plan to vote for Candidate A in an election, a random sample of size n from the voters is chosen. The sampling is done with replacement. Let θ be the portion of voters who plan to vote for Candidate A among all voters. How large does n need to be so that we can obtain a 90% confidence interval with 3% margin of error? That is, how large n needs to be such that P (¯¯¯¯¯ X − 0.03 ≤ θ ≤ ¯¯¯¯¯ X + 0.03) ≥ 0.90, γ1− ,n−1 α 2 X1 + X2 + ⋯ + Xn γ ,n−1 α 2 X1 + X2 + ⋯ + Xn γ1− ,n−1 α 2 X1+X2+⋯+Xn γ ,n−1 α 2 X1+X2+⋯+Xn α 2 σ √n α 2 σ √n α 2 4 √100 4 √100 where ¯¯¯¯¯ X is the portion of people in our random sample that say they plan to vote for Candidate A. Solution Here, [ ¯¯¯¯¯ X − , ¯¯¯¯¯ X + ] is an approximate (1 − α)100% confidence interval for θ. Since α = 0.1, we have z = z0.05 = Φ−1(1 − 0.05) = 1.645 Therefore, we need to have = 0.03 Therefore, we obtain n = ( ) 2. We conclude n ≥ 752 is enough. Problem 4 a . Let X be a random variable such that RX ⊂ [a, b], i.e., we always have a ≤ X ≤ b. Show that Var(X) ≤ . b . Let X1, X2, X3, . . ., Xn be a random sample from an unknown distribution with CDF FX(x) such that RX ⊂ [a, b]. Specifically , EX and Var(X) are unknown. Find a (1 − α)100% confidence interval for θ = EX. Assume that n is large. Solution z α 2 2√n z α 2 2√n α 2 1.645 2√n 1.645 2 × 0.03 (b − a) 2 4 a . Define Y = X − . Thus, RY ⊂ [− , ]. Then, Var(X) = Var(Y ) = E[Y 2] − μ2 Y ≤ E[Y 2] ≤ ( ) 2 (since Y 2 ≤ ( ) 2) = . b . Here, we have an upper bound on σ, which is σmax = . Thus, the interval [ ¯¯¯¯¯ X − z , ¯¯¯¯¯ X + z ] is a (1 − α)100% confidence interval for θ. More specifically , [ ¯¯¯¯¯ X − z , ¯¯¯¯¯ X + z ] is a (1 − α)100% confidence interval for θ. Problem 5 A random sample X1, X2, X3, . . ., X144 is given from a distribution with unknown variance Var(Xi) = σ2. For the observed sample, the sample mean is ¯¯¯¯¯ X = 55.2, and the sample variance is S 2 = 34.5. Find a 99% confidence interval for θ = EXi. Solution The interval [ ¯¯¯¯¯ X − z , ¯¯¯¯¯ X + z ] is approximately a (1 − α)100% confidence interval for θ. Here, n = 144, α = 0.01, so we need z = z0.005 = Φ−1(1 − 0.005) ≈ 2.58 a+b 2 b−a 2 b−a 2 b − a 2 b − a 2 (b − a)2 4 (b−a) 2 α 2 σmax √n α 2 σmax √n α 2 b − a 2√n α 2 b − a 2√n α 2 S √n α 2 S √n α 2 Thus, we can obtain a 99% confidence interval for θ as [ ¯¯¯¯¯ X − z , ¯¯¯¯¯ X + z ] = [55.2 − 2.58 ⋅ , 55.2 + 2.58 ⋅ ] ≈ [53.94, 56.46]. Therefore, [53.94, 56.46] is an approximate 99% confidence interval for θ. Problem 6 A random sample X1, X2, X3, . . ., X16 is given from a normal distribution with unknown mean μ = EXi and unknown variance Var(Xi) = σ2. For the observed sample, the sample mean is ¯¯¯¯¯ X = 16.7, and the sample variance is S 2 = 7.5. a . Find a 95% confidence interval for μ. b . Find a 95% confidence interval for σ2. Solution a . Here, the interval [ ¯¯¯¯¯ X − t ,n−1 , ¯¯¯¯¯ X + t ,n−1 ] is a (1 − α)100% confidence interval for μ. Let n = 16, α = 0.05, then t0.025,15 ≈ 2.13 The above value can obtained in MA TLAB using the command tinv(0.975, 15). Thus, we can obtain a 95% confidence interval for μ as [16.7 − 2.13 , 16.7 + 2.13 ] ≈ [15.24, 18.16]. Therefore, [15.24, 18.16] is a 95% confidence interval for μ. b . Here, ⎡ ⎣ , ⎤ ⎦ is a (1 − α)100% confidence interval for σ2. In this problem, n = 16, α = .05, so we need χ 2 0.025,15 ≈ 27.49, χ 2 0.975,15 ≈ 6.26 α 2 S √n α 2 S √n √34.5 12 √34.5 12 α 2 S √n α 2 S √n √7.5 4 √7.5 4 (n−1)S 2 χ 2 ,n−1 α 2 (n−1)S 2 χ 2 1− ,n−1 α 2 The above values can obtained in MA TLAB using the commands chi2inv(0.975, 15) and chi2inv(0.025, 15), respectively . Thus, we can obtain a 95% confidence interval for σ2 as ⎡ ⎢ ⎣ , ⎤ ⎥ ⎦ = [ , ] ≈ [4.09, 17.97]. Therefore, [4.09, 17.97] is a 95% confidence interval for σ2. (n − 1)S 2 χ 2 ,n−1 α 2 (n − 1)S 2 χ 2 1− ,n−1 α 2 15 × 7.5 27.49 15 × 7.5 6.26 8.4.1 Introduction Often, we need to test whether a hypothesis is true or false. For example, a pharmaceutical company might be interested in knowing if a new drug is ef fective in treating a disease. Here, there are two hypotheses. The first one is that the drug is not effective, while the second hypothesis is that the drug is ef fective. We call these hypotheses H0 and H1 respectively . As another example, consider a radar system that uses radio waves to detect aircraft. The system receives a signal and, based on the received signal, it needs to decide whether an aircraft is present or not. Here, there are again two opposing hypotheses: H0: No aircraft is present. H1: An aircraft is present. The hypothesis H0 is called the null hypothesis and the hypothesis H1 is called the alternative hypothesis . The null hypothesis, H0, is usually referred to as the default hypothesis, i.e., the hypothesis that is initially assumed to be true. The alternative hypothesis, H1, is the statement contradictory to H0. Based on the observed data, we need to decide either to accept H0, or to reject it, in which case we say we accept H1. These are problems of hypothesis testing . In this section, we will discuss how to approach such problems from a classical (frequentist) point of view . We will start with an example, and then provide a general framework to approach hypothesis testing problems. When looking at the example, we will introduce some terminology that is commonly used in hypothesis testing. Do not worry much about the terminology when reading this example as we will provide more precise definitions later on. Example 8. 22 You have a coin and you would like to check whether it is fair or not. More specifically , let θ be the probability of heads, θ = P(H). You have two hypotheses: H0 (the null hypothesis): The coin is fair , i.e. θ = θ0 = . H1 (the alternative hypothesis): The coin is not fair , i.e., θ ≠ . 1 2 1 2 Solution We need to design a test to either accept H0 or H1. To check whether the coin is fair or not, we perform the following experiment. W e toss the coin 100 times and record the number of heads. Let X be the number of heads that we observe, so X ∼ Binomial(100, θ). Now , if H0 is true, then θ = θ0 = , so we expect the number of heads to be close to 50. Thus, intuitively we can say that if we observe close to 50 heads we should accept H0, otherwise we should reject it. More specifically , we suggest the following criteria: If |X − 50| is less than or equal to some threshold, we accept H0. On the other hand, if |X − 50| is larger than the threshold we reject H0 and accept H1. Let's call that threshold t. If |X − 50| ≤ t, accept H0. If |X − 50| > t, accept H1. But how do we choose the threshold t? To choose t properly , we need to state some requirements for our test. An important factor here is probability of error . One way to make an error is when we reject H0 while in fact it is true. W e call this type I error . More specifically , this is the event that |X − 50| > t when H0 is true. Thus, P(type I error) = P(|X − 50| > t | H0). We read this as the probability that |X − 50| > t when H0 is true. (Note that, here, P(|X − 50| > t | H0) is not a conditional probability , since in classical statistics we do not treat H0 and H1 as random events. Another common notation is P(|X − 50| > t when H0 is true).) To be able to decide what t needs to be, we can choose a desired value for P(type I error). For example, we might want to have a test for which P(type I error) ≤ α = 0.05 Here, α is called the level of significance . We can choose P(|X − 50| > t | H0) = α = 0.05 (8.2) to satisfy the desired level of significance. Since we know the distribution of X under H0, i.e., X|H0 ∼ Binomial(100, θ = ), we should be able to choose t such that Equation 8.2 holds. Note that by the central limit theorem (CL T), for large values of n, 1 2 1 2 we can approximate a Binomial(n, θ) distribution by a normal distribution. More specifically , we can say that for large values of n, if X ∼ Binomial(n, θ0 = ), then Y = = is (approximately) a standard normal random variable, N(0, 1). Thus, to be able to use the CLT, instead of looking at X directly , we can look at Y . Note that P(type I error) = P(|X − 50| > t|H0) = P (∣ ∣ ∣ ∣ ∣ ∣ > ∣∣∣ H0) = P (|Y | > ∣∣ H0) . For simplicity , let's put c = , so we can summarize our test as follows: If |Y | ≤ c, accept H0. If |Y | > c, accept H1. where Y = . Now , we need to decide what c should be. W e need to have α = P (|Y | > c) = 1 − P (−c ≤ Y ≤ c) ≈ 2 − 2Φ (c) (using Φ(x) = 1 − Φ(−x)). Thus, we need to have 2 − 2Φ(c) = 0.05 So we obtain c = Φ−1(0.975) = 1.96 Thus, we conclude the following test If |Y | ≤ 1.96, accept H0. If |Y | > 1.96, accept H1. The set A = [−1.96, 1.96] is called the acceptance region , because it includes the points that result in accepting H0. The set R = (−∞, −1.96) ∪ (1.96, ∞) is called the rejection region because it includes the points that correspond to rejecting H0. Figure 8.9 summarizes these concepts. 1 2 X − nθ0 √nθ0(1 − θ0) X − 50 5 X − 50 5 t 5 t 5 t 5 X−50 5 Figure 8.9 - Acceptance rejection, rejection region, and type I error for Example 8.22 Note that since Y = , we can equivalently state the test as If |X − 50| ≤ 9.8, accept H0. If |X − 50| > 9.8, accept H1. Or equivalently , If the observed number of heads is in {41, 42, ⋯ , 59}, accept H0. If the observed number of heads is in {0, 1, ⋯ , 40} ∪ {60, 61, ⋯ , 100}, reject H0 (accept H1). In summary , if the observed number of heads is more than 9 counts away from 50, we reject H0. Before ending our discussion on this example, we would like to mention another point. Suppose that we toss the coin 100 times and observe 55 heads. Based on the above discussion we should accept H0. However , it is often recommended to say \"we failed to reject H0\" instead of saying \"we are accepting H0.\" The reason is that we have not really proved that H0 is true. In fact, all we know is that the result of our experiment was not statistically contradictory to H0. Nevertheless, we will not worry about this terminology in this book. X−50 5 8.4.2 General Setting and Definitions Example 8.22 provided a basic introduction to hypothesis testing. Here, we would like to provide a general setting for problems of hypothesis testing and formally define the terminology that is used in hypothesis testing. Although there are several new phrases such as null hypothesis, type I error , significance level, etc., there are not many new concepts or tools here. Thus, after going through a few examples, the concepts should become clear . Suppose that θ is an unknown parameter . A hypothesis is a statement such as θ = 1, θ > 1.3, θ ≠ 0.5, etc. In hypothesis testing problems, we need to decide between two contradictory hypotheses. More precisely , let S be the set of possible values for θ. Suppose that we can partition S into two disjoint sets S0 and S1. Let H0 be the hypothesis that θ ∈ S0, and let H1 be the hypothesis that θ ∈ S1. H0 (the null hypothesis): θ ∈ S0. H1 (the alternative hypothesis): θ ∈ S1. In Example 8.22 , S = [0, 1], S0 = { }, and S1 = [0, 1] − { }. Here, H0 is an example of a simple hypothesis because S0 contains only one value of θ. On the other hand, H1 is an example of composite hypothesis since S1 contains more than one element. It is often the case that the null hypothesis is chosen to be a simple hypothesis. Often, to decide between H0 and H1, we look at a function of the observed data. For instance, in Example 8.22 , we looked at the random variable Y , defined as Y = , where X was the total number of heads. Here, X is a function of the observed data (sequence of heads and tails), and thus Y is a function of the observed data. W e call Y a statistic. 1 2 1 2 X − nθ0 √nθ0(1 − θ0) Definition 8. 3 . Let X1, X2, ⋯, Xn be a random sample of interest. A statistic is a real-valued function of the data. For example, the sample mean, defined as W(X1, X2, ⋯ , Xn) = , is a statistic. A test statistic is a statistic based on which we build our test. To decide whether to choose H0 or H1, we choose a test statistic, W = W(X1, X2, ⋯ , Xn). Now , assuming H0, we can define the set A ⊂ R as the set of possible values of W for which we would accept H0. The set A is called the acceptance region , while the set R = R − A is said to be the rejection region . In Example 8.22 , the acceptance region was found to be the set A = [−1.96, 1.96], and the set R = (−∞, −1.96) ∪ (1.96, ∞) was the rejection region. There are two possible errors that we can make. W e define type I error as the event that we reject H0 when H0 is true. Note that the probability of type I error in general depends on the real value of θ. More specifically , P(type I error | θ) = P(Reject H0 | θ) = P(W ∈ R | θ),  for θ ∈ S0. If the probability of type I error satisfies P(type I error) ≤ α,  for all θ ∈ S0, then we say the test has significance level α or simply the test is a level α test. Note that it is often the case that the null hypothesis is a simple hypothesis, so S0 has only one element (as in Example 8.22 ). The second possible error that we can make is to accept H0 when H0 is false. This is called the type II error . Since the alternative hypothesis, H1, is usually a composite hypothesis (so it includes more than one value of θ), the probability of type II error is usually a function of θ. The probability of type II error is usually shown by β: β(θ) = P(Accept H0 | θ),  for θ ∈ S1. We now go through an example to practice the above concepts. X1 + X2+. . . +Xn n Example 8. 23 Consider a radar system that uses radio waves to detect aircraft. The system receives a signal and, based on the received signal, it needs to decide whether an aircraft is present or not. Let X be the received signal. Suppose that we know X = W, if no aircraft is present. X = 1 + W , if an aircraft is present. where W ∼ N(0, σ2 = ). Thus, we can write X = θ + W , where θ = 0 if there is no aircraft, and θ = 1 if there is an aircraft. Suppose that we define H0 and H1 as follows: H0 (null hypothesis): No aircraft is present. H1 (alternative hypothesis): An aircraft is present. a . Write the null hypothesis, H0, and the alternative hypothesis, H1, in terms of possible values of θ. b . Design a level 0.05 test ( α = 0.05) to decide between H0 and H1. c. Find the probability of type II error , β, for the above test. Note that this is the probability of missing a present aircraft. d. If we observe X = 0.6, is there enough evidence to reject H0 at significance level α = 0.01? e. If we would like the probability of missing a present aircraft to be less than 5%, what is the smallest significance level that we can achieve? Solution a . The null hypothesis corresponds to θ = 0 and the alternative hypothesis corresponds to θ = 1. Thus, we can write H0 (null hypothesis): No aircraft is present: θ = 0. H1 (alternative hypothesis): An aircraft is present: θ = 1. Note that here both hypotheses are simple. 1 9 b . To decide between H0 and H1, we look at the observed data. Here, the situation is relatively simple. The observed data is just the random variable X. Under H0, X ∼ N(0, ), and under H1, X ∼ N(1, ). Thus, we can suggest the following test: We choose a threshold c. If the observed value of X is less than c, we choose H0 (i.e., θ = EX = 0). If the observed value of X is larger than c, we choose H1 (i.e., θ = EX = 1). T o choose c, we use the required α: P(type I error) = P(Reject H0 | H0) = P(X > c | H0) = P(W > c) = 1 − Φ(3c) (since assuming H0, X ∼ N(0, )). Letting P(type I error) = α, we obtain c = Φ−1(1 − α). Letting α = 0.05, we obtain c = Φ−1(0.95) = 0.548 c. Note that, here, the alternative hypothesis is a simple hypothesis. That is, it includes only one value of θ (i.e., θ = 1). Thus, we can write β = P(type II error) = P(accept H0 | H1) = P(X < c | H1) = P(1 + W < c) = P(W < c − 1) = Φ(3(c − 1)). Since c = 0.548, we obtain β = 0.088. d. In part (b), we obtained c = Φ−1(1 − α). For α = 0.01, we have c = Φ−1(0.99) = 0.775 which is larger than 0.6. Thus, we cannot reject H0 at significance level α = 0.01. e. In part (c), we obtained β = Φ(3(c − 1)). 1 9 1 9 1 9 1 3 1 3 1 3 1 3 To have β = 0.05, we obtain c = 1 + Φ−1(β) = 1 + Φ−1(0.05) = 0.452 Thus, we need to have c ≤ 0.452 to obtain β ≤ 0.05. Therefore, P(type I error) = 1 − Φ(3c) = 1 − Φ(3 × 0.452) = 0.0875, which means that the smallest significance level that we can achieve is α = 0.0875. Trade-off Between α and β: Since α and β indicate error probabilities, we would ideally like both of them to be small. However , there is in fact a trade-of f between α and β. That is, if we want to decrease the probability of type I error ( α), then the probability of type II error ( β) increases, and vise versa. T o see this, we can look at our analysis in Example 8.23 . In that example, we found α = 1 − Φ(3c), β = Φ(3(c − 1)). Note that Φ(x) is an increasing function. If we make c larger , α becomes smaller , and β becomes larger . On the other hand, if we make c smaller , α becomes larger , and β becomes smaller . Figure 8.10 shows type I and type II error probabilities for Example 8.23 . Figure 8.10 - T ype I and type II errors in Example 8.23 . 1 3 1 3 8.4.3 Hypothesis T esting for the Mean Here, we would like to discuss some common hypothesis testing problems. W e assume that we have a random sample X1,X2,...,Xn from a distribution and our goal is to make inference about the mean of the distribution μ. We consider three hypothesis testing problems. The first one is a test to decide between the following hypotheses: H0: μ = μ0, H1: μ ≠ μ0. In this case, the null hypothesis is a simple hypothesis and the alternative hypothesis is a two-sided hypothesis (i.e., it includes both μ < μ0 and μ > μ0). We call this hypothesis test a two-sided test. The second and the third cases are one-sided tests. More specifically , the second case is H0: μ ≤ μ0, H1: μ > μ0. Here, both H0 and H1 are one-sided, so we call this test a one-sided test. The third case is very similar to the second case. More specifically , the third scenario is H0: μ ≥ μ0, H1: μ < μ0. In all of the three cases, we use the sample mean ¯¯¯¯¯ X = to define our statistic. In particular , if we know the variance of the Xi's, Var(Xi) = σ2, then we define our test statistic as the normalized sample mean (assuming H0): W(X1, X2, ⋯ , Xn) = . X1 + X2+. . . +Xn n ¯¯¯¯¯ X − μ0 σ/√n If we do not know the variance of the Xi's, we use W(X1, X2, ⋯ , Xn) = , where S is the sample standard deviation, S =    ⎷ n ∑ k=1(Xk − ¯¯¯¯¯ X ) 2 =    ⎷ ( n ∑ k=1 X2 k − n¯¯¯¯¯ X 2). In any case, we will be able to find the distribution of W, and thus we can design our tests by calculating error probabilities. Let us start with the first case. Two -sided Tests fo r the Mea n : Here, we are given a random sample X1,X2,...,Xn from a distribution. Let μ = EXi. Our goal is to decide between H0: μ = μ0, H1: μ ≠ μ0. Example 8.22 , which we saw previously is an instance of this case. If H0 is true, we expect ¯¯¯¯¯ X to be close to μ0, and so we expect W(X1, X2, ⋯ , Xn) to be close to 0 (see the definition of W above). Therefore, we can suggest the following test. Choose a threshold, and call it c. If |W| ≤ c, accept H0, and if |W| > c, accept H1. How do we choose c? If α is the required significance level, we must have P(type I error) = P(Reject H0 | H0) = P(|W| > c | H0) ≤ α. Thus, we can choose c such that P(|W| > c | H0) = α. Let us look at an example. Example 8. 24 Let X1,X2,...,Xn be a random sample from a N(μ, σ2) distribution, where μ is unknown but σ is known. Design a level α test to choose between H0: μ = μ0, ¯¯¯¯¯ X − μ0 S/√n 1 n − 1 1 n − 1 H1: μ ≠ μ0. Solution As discussed above, we let W(X1, X2, ⋯ , Xn) = . Note that, assuming H0, W ∼ N(0, 1). We will choose a threshold, c. If |W| ≤ c, we accept H0, and if |W| > c, accept H1. To choose c, we let P(|W| > c | H0) = α. Since the standard normal PDF is symmetric around 0, we have P(|W| > c | H0) = 2P(W > c| H0). Thus, we conclude P(W > c| H0) = . Therefore, c = z . Therefore, we accept H0 if ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ≤ z , and reject it otherwise. Relation to Confidence Intervals : It is interesting to examine the above acceptance region. Here, we accept H0 if ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ≤ z . We can rewrite the above condition as μ0 ∈ [ ¯¯¯¯¯ X − z , ¯¯¯¯¯ X + z ] . ¯¯¯¯¯ X − μ0 σ/√n α 2 α 2 ¯¯¯¯¯ X − μ0 σ/√n α 2 ¯¯¯¯¯ X − μ0 σ/√n α 2 α 2 σ √n α 2 σ √n The above interval should look familiar to you. It is the (1 − α)100% confidence interval for μ0. This is not a coincidence as there is a general relationship between confidence interval problems and hypothesis testing problems. Example 8. 25 For the above example ( Example 8.24 ), find β, the probability of type II error , as a function of μ. Solution We have β(μ) = P(type II error) = P(accept H0 | μ) = P (∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ < z | μ) . If Xi ∼ N(μ, σ2), then ¯¯¯¯¯ X ∼ N(μ, ). Thus, β(μ) = P (∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ < z | μ) = P (μ0 − z ≤ ¯¯¯¯¯ X ≤ μ0 + z ) = Φ (z + ) − Φ (−z + ) . Unknown variance: The above results ( Example 8.25 ) can be extended to the case when we do not know the variance using the T distribution. More specifically , consider the following example. Example 8. 26 Let X1,X2,...,Xn be a random sample from a N(μ, σ2) distribution, where μ and σ are unknown. Design a level α test to choose between H0: μ = μ0, ¯¯¯¯¯ X − μ0 σ/√n α 2 σ2 n ¯¯¯¯¯ X − μ0 σ/√n α 2 α 2 σ √n α 2 σ √n α 2 μ0 − μ σ/√n α 2 μ0 − μ σ/√n H1: μ ≠ μ0. Solution Let S 2 be the standard variance for this random sample. Then, the random variable W defined as W(X1, X2, ⋯ , Xn) = has a t-distribution with n − 1 degrees of freedom, i.e., W ∼ T(n − 1). Thus, we can repeat the analysis of Example 8.24 here. The only dif ference is that we need to replace σ by S and z by t ,n−1. Therefore, we accept H0 if |W| ≤ t ,n−1, and reject it otherwise. Let us look at a numerical example of this case. Example 8. 27 The average adult male height in a certain country is 170 cm. We suspect that the men in a certain city in that country might have a dif ferent average height due to some environmental factors. W e pick a random sample of size 9 from the adult males in the city and obtain the following values for their heights (in cm): 176.2 157.9 160.1 180.9 165.1 167.2 162.9 155.7 166.2 Assume that the height distribution in this population is normally distributed. Here, we need to decide between H0: μ = 170, H1: μ ≠ 170. Based on the observed data, is there enough evidence to reject H0 at significance level α = 0.05? Solution ¯¯¯¯¯ X − μ0 S/√n α 2 α 2 α 2 Let's first compute the sample mean and the sample standard deviation. The sample mean is ¯¯¯¯¯ X = = 165.8 The sample variance is given by S 2 = 9 ∑ k=1(Xk − ¯¯¯¯¯ X ) 2 = 68.01 The sample standard deviation is given by S = √S 2 = 8.25 The following MA TLAB code can be used to obtain these values: x=[176.2,157.9,160.1,180.9,165.1,167.2,162.9,155.7,166.2]; m=mean(x); v=var(x); s=std(x); Now , our test statistic is W(X1, X2, ⋯ , X9) = = = −1.52 Thus, |W| = 1.52. Also, we have t ,n−1 = t0.025,8 ≈ 2.31 The above value can be obtained in MA TLAB using the command tinv(0.975, 8). Thus, we conclude |W| ≤ t ,n−1. Therefore, we accept H0. In other words, we do not have enough evidence to conclude that the average height in the city is dif ferent from the average height in the X1 + X2 + X3 + X4 + X5 + X6 + X7 + X8 + X9 9 1 9 − 1 ¯¯¯¯¯ X − μ0 S/√n 165.8 − 170 8.25/3 α 2 α 2 country . What if the sample is not from a normal distribution? In the case that n is large, we can say that W(X1, X2, ⋯ , Xn) = is approximately standard normal. Therefore, we accept H0 : μ = μ0 if ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ≤ z , and reject it otherwise (i.e., accept H1 : μ ≠ μ0). Let us summarize what we have obtained for the two-sided test for the mean. Table 8.2: T wo-sided hypothesis testing for the mean: H0 : μ = μ0, H1: μ ≠ μ0. Case Test Statistic Acceptance Region Xi ∼ N(μ, σ2), σ known W = |W| ≤ z n large, Xi non-normal W = |W| ≤ z Xi ∼ N(μ, σ2), σ unknown W = |W| ≤ t ,n−1 On e-sided Tests fo r the Mea n : We can provide a similar analysis when we have a one-sided test. Let's show this by an example. Example 8. 28 Let X1,X2,...,Xn be a random sample from a N(μ, σ2) distribution, where μ is unknown and σ is known. Design a level α test to choose between ¯¯¯¯¯ X − μ0 S/√n ¯¯¯¯¯ X − μ0 S/√n α 2 ¯¯¯¯ X −μ0 σ/√n α 2 ¯¯¯¯ X −μ0 S/√n α 2 ¯¯¯¯ X −μ0 S/√n α 2 H0: μ ≤ μ0, H1: μ > μ0. Solution As before, we define the test statistic as W(X1, X2, ⋯ , Xn) = . If H0 is true (i.e., μ ≤ μ0), we expect ¯¯¯¯¯ X (and thus W) to be relatively small, while if H1 is true, we expect ¯¯¯¯¯ X (and thus W) to be larger . This suggests the following test: Choose a threshold, and call it c. If W ≤ c, accept H0, and if W > c, accept H1. How do we choose c? If α is the required significance level, we must have P(type I error) = P(Reject H0 | H0) = P(W > c | μ ≤ μ0) ≤ α. Here, the probability of type I error depends on μ. More specifically , for any μ0 ≤ μ, we can write P(type I error | μ) = P(Reject H0 | μ) = P(W > c | μ) = P ( > c | μ) = P ( + > c | μ) = P ( > c + | μ) ≤ P ( > c | μ) ( since μ ≤ μ0) = 1 − Φ(c) ( since given μ, ∼ N(0, 1)). Thus, we can choose α = 1 − Φ(c), which results in c = zα. Therefore, we accept H0 if ¯¯¯¯¯ X − μ0 σ/√n ¯¯¯¯¯ X − μ0 σ/√n ¯¯¯¯¯ X − μ σ/√n μ − μ0 σ/√n ¯¯¯¯¯ X − μ σ/√n μ0 − μ σ/√n ¯¯¯¯¯ X − μ σ/√n ¯¯¯¯¯ X − μ σ/√n ≤ zα, and reject it otherwise. The above analysis can be repeated for other cases. More generally , suppose that we are given a random sample X1,X2,...,Xn from a distribution. Let μ = EXi. Our goal is to decide between H0: μ ≤ μ0, H1: μ > μ0. We define the test statistic as before, i.e., we define W as W(X1, X2, ⋯ , Xn) = , if σ = √Var(Xi) is known, and as W(X1, X2, ⋯ , Xn) = , if σ is unknown. If H0 is true (i.e., μ ≤ μ0), we expect that ¯¯¯¯¯ X (and thus W) to be relatively small, while if H1 is true, we expect ¯¯¯¯¯ X (and thus W) to be larger . This suggests the following test: Choose a threshold c. If W ≤ c, accept H0, and if W > c, accept H1. To choose c, note that P(type I error) = P(Reject H0 | H0) = P(W > c | μ ≤ μ0) ≤ P(W > c | μ = μ0). Note that the last inequality resulted because if we make μ larger , the probability of W > c can only increase. In other words, we assumed the worst case scenario, i.e, μ = μ0 for the probability of error . Thus, we can choose c such that P(W > c | μ = μ0) = c. By doing this procedure, we obtain the acceptance regions reflected in T able 8.3. Table 8.3: One-sided hypothesis testing for the mean: H0 : μ ≤ μ0, H1: μ > μ0. Case Test Statistic Acceptance Region ¯¯¯¯¯ X − μ0 σ/√n ¯¯¯¯¯ X − μ0 σ/√n ¯¯¯¯¯ X − μ0 S/√n Xi ∼ N(μ, σ2), σ known W = W ≤ zα n large, Xi non-normal W = W ≤ zα Xi ∼ N(μ, σ2), σ unknown W = W ≤ tα,n−1 Note that the tests mentioned in T able 8.3 remain valid if we replace the null hypothesis by μ = μ0. The reason for this is that in choosing the threshold c, we assumed the worst case scenario, i.e, μ = μ0. Finally , if we need to decide between H0: μ ≥ μ0, H1: μ < μ0, we can again repeat the above analysis and we obtain the acceptance regions reflected in T able 8.4. Table 8.4: One-sided hypothesis testing for the mean: H0 : μ ≥ μ0, H1: μ < μ0. Case Test Statistic Acceptance Region Xi ∼ N(μ, σ2), σ known W = W ≥ −zα n large, Xi non-normal W = W ≥ −zα Xi ∼ N(μ, σ2), σ unknown W = W ≥ −tα,n−1 ¯¯¯¯ X −μ0 σ/√n ¯¯¯¯ X −μ0 S/√n ¯¯¯¯ X −μ0 S/√n ¯¯¯¯ X −μ0 σ/√n ¯¯¯¯ X −μ0 S/√n ¯¯¯¯ X −μ0 S/√n 8.4.4 P-V alues In the above discussions, we only reported an \"accept\" or a \"reject\" decision as the conclusion of a hypothesis test. However , we can provide more information using what we call P-values . In other words, we could indicate how close the decision was. More specifically , suppose we end up rejecting H0 at at significance level α = 0.05. Then we could ask: \"How about if we require significance level α = 0.01?\" Can we still reject H0 ? More specifically , we can ask the following question: What is the lowest significance level α that results in rejecting the null hypothesis? The answer to the above question is called the P-value . P-value is the lowest significance level α that results in rejecting the null hypothesis. Intuitively , if the P-value is small, it means that the observed data is very unlikely to have occurred under H0, so we are more confident in rejecting the null hypothesis. How do we find P-values? Let's look at an example. Example 8. 29 You have a coin and you would like to check whether it is fair or biased. More specifically , let θ be the probability of heads, θ = P(H). Suppose that you need to choose between the following hypotheses: H0 (the null hypothesis): The coin is fair , i.e., θ = θ0 = . H1 (the alternative hypothesis): The coin is not fair , i.e., θ > . We toss the coin 100 times and observe 60 heads. 1 . Can we reject H0 at significance level α = 0.05? 2 . Can we reject H0 at significance level α = 0.01? 1 2 1 2 3 . What is the P-value? Solution Let X be the random variable showing the number of observed heads. In our experiment, we observed X = 60. Since n = 100 is relatively large, assuming H0 is true, the random variable W = = is (approximately) a standard normal random variable, N(0, 1). If H0 is true, we expect X to be close to 50, while if H1 is true, we expect X to be larger . Thus, we can suggest the following test: W e choose a threshold c. If W ≤ c, we accept H0; otherwise, we accept H1. To calculate P(type I error), we can write P(type I error) = P(Reject H0 | H0) = P(W > c | H0). Since W ∼ N(0, 1) under H0, we need to choose c = zα, to ensure significance level α. In this example, we obtain W = = = 2. 1 . If we require significance level α = 0.05, then c = z0.05 = 1.645 The above value can be obtained in MA TLAB using the following command: norminv(1 − 0.05). Since we have W = 2 > 1.645, we reject H0, and accept H1. 2 . If we require significance level α = 0.01, then c = z0.01 = 2.33 The above value can be obtained in MA TLAB using the following command: norminv(1 − 0.01). Since we have W = 2 ≤ 2.33, we fail to reject H0, so we accept H0. 3 . P-value is the lowest significance level α that results in rejecting H0. Here, since W = 2, we will reject H0 if and only if c < 2. Note that zα = c, thus α = 1 − Φ(c). X − nθ0 √nθ0(1 − θ0) X − 50 5 X − 50 5 60 − 50 5 If c = 2, we obtain α = 1 − Φ(2) = 0.023 Therefore, we reject H0 for α > 0.023. Thus, the P-value is equal to 0.023. The above example suggests the following way to compute P-values: Computing P-V alues Consider a hypothesis test for choosing between H0 and H1. Let W be the test statistic, and w1 be the observed value of W. 1 . Assume H0 is true. 2 . The P-value is P(type I error) when the test threshold c is chosen to be c = w1. To see how we can use the above method, again consider Example 8.29 . Here, W = , which is approximately N(0, 1) under H0. The observed value of W is w1 = = 2. Thus, P − value = P(type I error when c = 2) = P(W > 2) = 1 − Φ(2) = 0.023 X − 50 5 60 − 50 5 8.4.5 Likelihood Ratio T ests So far we have focused on specific examples of hypothesis testing problems. Here, we would like to introduce a relatively general hypothesis testing procedure called the likelihood ratio test . Before doing so, let us quickly review the definition of the likelihood function, which was previously discussed in Section 8.2.3 . Review o f the L ikeliho o d F un c tio n : Let X1, X2, X3, . . ., Xn be a random sample from a distribution with a parameter θ. Suppose that we have observed X1 = x1, X2 = x2, ⋯, Xn = xn. - If the Xi's are discrete, then the likelihood function is defined as L(x1, x2, ⋯ , xn; θ) = PX1X2⋯Xn (x1, x2, ⋯ , xn; θ). - If the Xi's are jointly continuous, then the likelihood function is defined as L(x1, x2, ⋯ , xn; θ) = fX1X2⋯Xn (x1, x2, ⋯ , xn; θ). L ikeliho o d Ra tio Tests: Consider a hypothesis testing problem in which both the null and the alternative hypotheses are simple. That is H0: θ = θ0, H1: θ = θ1. Now , let X1, X2, X3, . . ., Xn be a random sample from a distribution with a parameter θ . Suppose that we have observed X1 = x1, X2 = x2, ⋯, Xn = xn. One way to decide between H0 and H1 is to compare the corresponding likelihood functions: l0 = L(x1, x2, ⋯ , xn; θ0), l1 = L(x1, x2, ⋯ , xn; θ1). More specifically , if l0 is much larger than l1, we should accept H0. On the other hand if l1 is much larger , we tend to reject H0. Therefore, we can look at the ratio to decide between H0 and H1. This is the idea behind likelihood ratio tests . l0 l1 Likelihood Ratio T est for Simple Hypotheses Let X1, X2, X3, . . ., Xn be a random sample from a distribution with a parameter θ. Suppose that we have observed X1 = x1, X2 = x2, ⋯, Xn = xn. To decide between two simple hypotheses H0: θ = θ0, H1: θ = θ1, we define λ(x1, x2, ⋯ , xn) = . To perform a likelihood ratio test (LRT) , we choose a constant c. We reject H0 if λ < c and accept it if λ ≥ c. The value of c can be chosen based on the desired α. Let's look at an example to see how we can perform a likelihood ratio test. Example 8. 30 Here, we look again at the radar problem ( Example 8.23 ). More specifically , we observe the random variable X: X = θ + W, where W ∼ N(0, σ2 = ). We need to decide between H0: θ = θ0 = 0, H1: θ = θ1 = 1. Let X = x. Design a level 0.05 test (α = 0.05) to decide between H0 and H1. Solution L(x1, x2, ⋯ , xn; θ0) L(x1, x2, ⋯ , xn; θ1) 1 9 If θ = θ0 = 0, then X ∼ N(0, σ2 = ). Therefore, L(x; θ0) = fX(x; θ0) = e − . On the other hand, if θ = θ1 = 1, then X ∼ N(1, σ2 = ). Therefore, L(x; θ1) = fX(x; θ1) = e − . Therefore, λ(x) = = exp{− + } = exp{ }. Thus, we accept H0 if exp{ } ≥ c, where c is the threshold. Equivalently , we accept H0 if x ≤ (1 − ln c) . Let us define c′ = (1 − ln c), where c′ is a new threshold. Remember that x is the observed value of the random variable X. Thus, we can summarize the decision rule as follows. W e accept H0 if X ≤ c′. How to do we choose c′? We use the required α. P(type I error) = P(Reject H0 | H0) = P(X > c′ | H0) = P(X > c′) (where X ∼ N (0, ) ) = 1 − Φ(3c′). Letting P(type I error) = α, we obtain c′ = Φ−1(1 − α). 1 9 3 √2π 9x2 2 1 9 3 √2π 9(x−1)2 2 L(x; θ0) L(x; θ1) 9x 2 2 9(x − 1) 2 2 9(1 − 2x) 2 9(1 − 2x) 2 1 2 2 9 1 2 2 9 1 3 1 3 Letting α = 0.05, we obtain c′ = Φ−1(.95) = 0.548 As we see, in this case, the likelihood ratio test is exactly the same test that we obtained in Example 8.23 . How do we perform the likelihood ratio test if the hypotheses are not simple? Suppose that θ is an unknown parameter . Let S be the set of possible values for θ and suppose that we can partition S into two disjoint sets S0 and S1. Consider the following hypotheses: H0: θ ∈ S0, H1: θ ∈ S1. The idea behind the general likelihood ratio test can be explained as follows: W e first find the likelihoods corresponding to the most likely values of θ in S0 and S1 respectively . That is, we find l0 = max{L(x1, x2, ⋯ , xn; θ) : θ ∈ S0}, l = max{L(x1, x2, ⋯ , xn; θ) : θ ∈ S}. (To be more accurate, we need to replace max by sup.) Let us consider two extreme cases. First, if l0 = l, then we can say that the most likely value of θ belongs to S0. This indicates that we should not reject H0. On the other hand, if is much smaller than 1, we should probably reject H0 in favor of H1. To conduct a likelihood ratio test, we choose a threshold 0 ≤ c ≤ 1 and compare to c. If ≥ c, we accept H0. If < c, we reject H0. The value of c can be chosen based on the desired α. 1 3 l0 l1 l0 l l0 l l0 l Likelihood Ratio T ests Let X1, X2, X3, . . ., Xn be a random sample from a distribution with a parameter θ. Suppose that we have observed X1 = x1, X2 = x2, ⋯, Xn = xn. Define λ(x1, x2, ⋯ , xn) = . To perform a likelihood ratio test (LRT) , we choose a constant c in [0, 1]. We reject H0 if λ < c and accept it if λ ≥ c. The value of c can be chosen based on the desired α. sup{L(x1, x2, ⋯ , xn; θ) : θ ∈ S0} sup{L(x1, x2, ⋯ , xn; θ) : θ ∈ S} 8.4.6 Solved Problems Problem 1 Let X ∼ Geometric(θ). We observe X and we need to decide between H0: θ = θ0 = 0.5, H1: θ = θ1 = 0.1 a . Design a level 0.05 test ( α = 0.05) to decide between H0 and H1. b . Find the probability of type-II error β. Solution a . We choose a threshold c ∈ N and compare the observed value of X = x to c. We accept H0 if x ≤ c and reject it if x > c. The probability of type I error is given by P(type I error) = P(Reject H0 | H0) = P(Reject H0 | θ = 0.5) = P(X > c | θ = 0.5) = ∞ ∑ k=c+1 P(X = k) (where X ∼ Geometric(θ0 = 0.5)) = ∞ ∑ k=c+1(1 − θ0) k−1θ0 = (1 − θ0) cθ0 ∞ ∑ l=0 (1 − θ0) l = (1 − θ0) c. To have α = 0.05, we need to choose c such that (1 − θ0) c ≤ α = 0.05, so we obtain l c ≥ = = 4.32 Since we would like c ∈ N, we can let c = 5. To summarize, we have the following decision rule: Accept H0 if the observed value of X is in the set A = {1, 2, 3, 4, 5}, and reject H0 otherwise. b . Since the alternative hypothesis H1 is a simple hypothesis ( θ = θ1), there is only one value for β, β = P(type II error) = P(accept H0 | H1) = P(X ≤ c | H1) = 1 − (1 − θ1) c = 1 − (0.9) 5 = 0.41 Problem 2 Let X1,X2,X3, X4 be a random sample from a N(μ, 1) distribution, where μ is unknown. Suppose that we have observed the following values 2.82 2.71 3.22 2.67 We would like to decide between H0: μ = μ0 = 2, H1: μ ≠ 2. a . Assuming α = 0.1, Do you accept H0 or H1? b . If we require significance level α, find β as a function of μ and α. Solution a . We have a sample from a normal distribution with known variance, so using the first row in Table 8.2 , we define the test statistic as W = . ln α ln(1 − θ0) ln(0.05) ln(.5) ¯¯¯¯¯ X − μ0 σ/√n We have ¯¯¯¯¯ X = 2.85, μ0 = 2, σ = 1, and n = 4. So, we obtain W = = 1.7 Here, α = 0.1, so z = z0.05 = 1.645. Since |W| > z , we reject H0 and accept H1. b . Here, the test statistic W is W ∼ = 2( ¯¯¯¯¯ X − 2). If X ∼ (μ, 1), then ¯¯¯¯¯ X ∼ N (μ, ) , and W ∼ N(2(μ − 2), 1). Thus, we have β = P(type II error) = P(accept H0 | μ) = P(|W| < z | μ) = P(|W| < z ) (when W ∼ N(2(μ − 2), 1)) = Φ (z − 2μ + 4) − Φ (−z − 2μ + 4) . Problem 3 Let X1,X2,...,X100 be a random sample from an unknown distribution. After observing this sample, the sample mean and the sample variance are calculated to be ¯¯¯¯¯ X = 21.32, S 2 = 27.6 Design a level 0.05 test to choose between H0: μ = 20, H1: μ > 20. 2.85 − 2 1/2 α 2 α 2 1 4 α 2 α 2 α 2 α 2 Do you accept or reject H0? Solution Here, we have a non-normal sample, where n = 100 is large. As we have discussed previously , to test for the above hypotheses, we can use the results of Table 8.3 . More specifically , using the second row of Table 8.3 , we define the test statistic as W = = = 2.51 Here, α = 0.05, so zα = z0.05 = 1.645. Since W > zα, we reject H0 and accept H1. Problem 4 Let X1,X2,X3, X4 be a random sample from a N(μ, σ2) distribution, where μ and σ are unknown. Suppose that we have observed the following values 3.58 10.03 4.77 14.66 We would like to decide between H0: μ ≥ 10, H1: μ < 10. Assuming α = 0.05, Do you accept H0 or H1? Solution Here, we have a sample from a normal distribution with unknown mean and unknown variance. Thus, using the third row in Table 8.4 , we define the test statistic as W = . ¯¯¯¯¯ X − μ0 S/√n 21.32 − 20 √27.6/√100 ¯¯¯¯¯ X − μ0 S/√n Using the data we obtain ¯¯¯¯¯ X = 8.26, S = 5.10 Therefore, we obtain W = = −0.68 Here, α = 0.05, so n = 4, tα,n−1 = t0.05,3 = 2.35. Since W > −tα,n−1, we fail to reject H0, so we accept H0. Problem 5 Let X1,X2,...,X81 be a random sample from an unknown distribution. After observing this sample, the sample mean and the sample variance are calculated to be ¯¯¯¯¯ X = 8.25, S 2 = 14.6 Design a test to decide between H0: μ = 9, H1: μ < 9, and calculate the P-value for the observed data. Solution Here, we have a non-normal sample, where n = 81 is large. As we have discussed previously , to test for the above hypotheses, we can use the results of Table 8.4 . More specifically , using the second row of Table 8.4 , we define the test statistic as W = = = −1.767 8.26 − 10 5.10/2 ¯¯¯¯¯ X − μ0 S/√n 8.25 − 9 √14.6/√81 The P-value is P(type I error) when the test threshold c is chosen to be c = −1.767. Since the threshold for this test (as indicated by Table 8.4 ) is −zα, we obtain −zα = −1.767 Noting that by definition zα = Φ−1(1 − α), we obtain P(type I error) as α = 1 − Φ(1.767) ≈ 0.0386 Therefore, P − value ≈ 0.0386 8.5.0 Linear Regression Sometimes we are interested in obtaining a simple model that explains the relationship between two or more variables. For example, suppose that we are interested in studying the relationship between the income of parents and the income of their children in a certain country . In general, many factors can impact the income of a person. Nevertheless, we suspect that children from wealthier families generally tend to become wealthier when they grow up. Here, we can consider two variables: 1 . The family income can be defined as the average income of parents at a certain period. 2 . The child income can be defined as his/her average income at a certain period (e.g, age). To examine the relationship between the two variables, we collect some data (xi, yi),  for i = 1, 2, ⋯ , n, where yi is the average income of the ith child, and xi is the average income of his/her parents. W e are often interested in finding a simple model. A linear model is probably the simplest model that we can define, where we write yi ≈ β0 + β1xi. Of course, there are other factors that impact each child's future income, so we might write yi = β0 + β1xi + ϵi, where ϵi is modeled as a random variable. More specifically , if we approximate a child's future income by ^yi = β0 + β1xi, then ϵi indicates the error in our approximation. The goal here is to obtain the best values of β0 and β1 that result in the smallest errors. In other words, we would like to draw a \"line\" in the x − y plane that best fits our data points. The line ^y = β0 + β1x is called the regression line . Figure 8.1 1 shows the regression line. Figure 8.1 1 - Regression line is the line that best represents the data points (xi, yi). We may summarize our model as Y = β0 + β1x + ϵ. Note that since ϵ is a random variable, Y is also a random variable. The variable x is called the predictor or the explanatory variable , and the random variable Y is called the response variable. That is, here, we use x to predict/estimate Y . 8.5.1 Simple Linear Regression Model Here, we provide a model that is called the simple linear regression model. Our model is Yi = β0 + β1xi + ϵi, where we model ϵi's as independent and zero-mean normal random variables, ϵi ∼ N(0, σ2). The parameters β0, β1, and σ2 are considered fixed but unknown. The assumption is that we have data points (x1, y1), (x2, y2), ⋯, (xn, yn) and our goal is to find the \"best\" values for β0 and β1 resulting in the line that provides the \"best\" fit for the data points. Here, yi's are the observed values of the random variables Yi's. T o have a well-defined problem we add the following assumptions. W e assume n ≥ 3. W e also assume that not all xi's are identical. There are several common methods for finding good values for β0 and β1. These methods will result in the same answers; however , they are philosophically based on different ideas. Here, we will provide two methods for estimating β0 and β1. A third method will be discussed in the Solved Problems section. 8.5.2 The First Method for Finding β0 and β1 Here, we assume that xi's are observed values of a random variable X. Therefore, we can summarize our model as Y = β0 + β1X + ϵ, where ϵ is a N(0, σ2) random variable independent of X. First, we take expectation from both sides to obtain EY = β0 + β1EX + E[ϵ] = β0 + β1EX Thus, β0 = EY − β1EX. Next, we look at Cov(X, Y ), Cov(X, Y ) = Cov(X, β0 + β1X + ϵ) = β0Cov(X, 1) + β1Cov(X, X) + Cov(X, ϵ) = 0 + β1Cov(X, X) + 0 (since X and ϵ are independent) = β1Var(X). Therefore, we obtain β1 = , β0 = EY − β1EX. Now , we can find β0 and β1 if we know EX, EY , . Here, we have the observed pairs (x1, y1), (x2, y2), ⋯, (xn, yn), so we may estimate these quantities. More specifically , we define ¯¯¯x = , ¯¯¯y = , sxx = n ∑ i=1 (xi − ¯¯¯x) 2, sxy = n ∑ i=1 (xi − ¯¯¯x)(yi − ¯¯¯y ). Cov(X, Y ) Var(X) Cov(X,Y ) Var(X) x1 + x2+. . . +xn n y1 + y2+. . . +yn n We can then estimate β0 and β1 as ^β1 = , ^β0 = ¯¯¯y − ^β1¯¯¯x. The above formulas give us the regression line ^y = ^β0 + ^β1x. For each xi, the fitted value ^y i is obtained by ^y i = ^β0 + ^β1xi. Here, ^y i is the predicted value of yi using the regression formula. The errors in this prediction are given by ei = yi − ^y i, which are called the residuals . sxy sxx Simple Linear Regression Given the observations (x1, y1), (x2, y2), ⋯, (xn, yn), we can write the regression line as ^y = β0 + β1x. We can estimate β0 and β1 as ^β1 = , ^β0 = ¯¯¯y − ^β1¯¯¯x, where sxx = n ∑ i=1 (xi − ¯¯¯x) 2, sxy = n ∑ i=1 (xi − ¯¯¯x)(yi − ¯¯¯y ). For each xi, the fitted value ^y i is obtained by ^y i = ^β0 + ^β1xi. The quantities ei = yi − ^y i are called the residuals . Example 8. 31 Consider the following observed values of (xi, yi): (1, 3) (2, 4) (3, 8) (4, 9) 1 . Find the estimated regression line ^y = ^β0 + ^β1x, based on the observed data. sxy sxx 2 . For each xi, compute the fitted value of yi using ^y i = ^β0 + ^β1xi. 3 . Compute the residuals, ei = yi − ^y i and note that 4 ∑ i=1 ei = 0. Solution 1 . We have ¯¯¯x = = 2.5, ¯¯¯y = = 6, sxx = (1 − 2.5) 2 + (2 − 2.5) 2 + (3 − 2.5) 2 + (4 − 2.5) 2 = 5, sxy = (1 − 2.5)(3 − 6) + (2 − 2.5)(4 − 6) + (3 − 2.5)(8 − 6) + (4 − 2.5)(9 − 6) = 11. Therefore, we obtain ^β1 = = = 2.2 ^β0 = 6 − (2.2)(2.5) = 0.5 2 . The fitted values are given by ^y i = 0.5 + 2.2xi, so we obtain ^y 1 = 2.7, ^y 2 = 4.9, ^y 3 = 7.1, ^y 4 = 9.3 3 . We have e1 = y1 − ^y 1 = 3 − 2.7 = 0.3, e2 = y2 − ^y 2 = 4 − 4.9 = −0.9, e3 = y3 − ^y 3 = 8 − 7.1 = 0.9, e4 = y4 − ^y 4 = 9 − 9.3 = −0.3 So, e1 + e2 + e3 + e4 = 0. 1 + 2 + 3 + 4 4 3 + 4 + 8 + 9 4 sxy sxx 11 5 We can use MA TLAB or other software packages to do regression analysis. For example, the following MA TLAB code can be used to obtain the estimated regression line in Example 8.31 . x=[1;2;3;4]; x0=ones(size(x)); y=[3;4;8;9]; beta = regress(y ,[x0,x]); C o effic ien t o f D etermin a tio n ( R- Sq ua red) : Let's look again at the above model for regression. W e wrote Y = β0 + β1X + ϵ, where ϵ is a N(0, σ2) random variable independent of X. Note that, here, X is the only variable that we observe, so we estimate Y using X. That is, we can write ^Y = β0 + β1X. The error in our estimate is Y − ^Y = ϵ. Note that the randomness in Y comes from two sources: X and ϵ. More specifically , if we look at Var(Y ), we can write Var(Y ) = β2 1 Var(X) + Var(ϵ) (since X and ϵ are assumed to be independent). The above equation can be interpreted as follows. The total variation in Y can be divided into two parts. The first part, β2 1 Var(X), is due to variation in X. The second part, Var(ϵ), is the variance of error . In other words, Var(ϵ) is the variance left in Y after we know X. If the variance of error , Var(ϵ), is small, then Y is close to ^Y , so our regression model will be successful in estimating Y . From the above discussion, we can define ρ 2 = β2 1 Var(X) Var(Y ) as the portion of variance of Y that is explained by variation in X. From the above discussion, we can also conclude that 0 ≤ ρ 2 ≤ 1. More specifically , if ρ 2 is close to 1, Y can be estimated very well as a linear function of X. On the other hand if ρ 2 is small, then the variance of error is large and Y cannot be accurately estimated as a linear function of X. Since β1 = , we can write ρ 2 = = (8.6) The above equation should look familiar to you. Here, ρ is the correlation coef ficient that we have seen before. Here, we are basically saying that if X and Y are highly correlated (i.e., ρ(X, Y ) is large), then Y can be well approximated by a linear function of X, i.e., Y ≈ ^Y = β0 + β1X. We conclude that ρ 2 is an indicator showing the strength of our regression model in estimating (predicting) Y from X. In practice, we often do not have ρ but we have the observed pairs (x1, y1), (x2, y2), ⋯, (xn, yn). We can estimate ρ 2 from the observed data. W e show it by r 2 and call it R-squared or coefficient of determination . Coef ficient of Determination For the observed data pairs, (x1, y1), (x2, y2), ⋯, (xn, yn), we define coefficient of determination , r 2 as r 2 = , where sxx = n ∑ i=1 (xi − ¯¯¯x) 2, syy = n ∑ i=1 (yi − ¯¯¯y ) 2, sxy = n ∑ i=1 (xi − ¯¯¯x)(yi − ¯¯¯y ). We have 0 ≤ r 2 ≤ 1. Larger values of r 2 generally suggest that our linear model ^yi = ^β0 + ^β1xi is a good fit for the data. Cov(X,Y ) Var(X) β2 1 Var(X) Var(Y ) [Cov(X, Y )] 2 Var(X)Var(Y ) s2 xy sxxsyy Two sets of data pairs are shown in Figure 8.12. In both data sets, the values of the yi 's (the heights of the data points) have considerable variation. The data points shown in (a) are very close to the regression line. Therefore, most of the variation in y is explained by the regression formula. That is, here, the ^yi's are relatively close to the yi 's, so r 2 is close to 1. On the other hand, for the data shown in (b), a lot of variation in y is left unexplained by the regression model. Therefore, r 2 for this data set is much smaller than r 2 for the data set in (a). Figure 8.12 - The data in (a) results in a high value of r 2, while the data shown in (b) results in a low value of r 2. Example 8. 32 For the data in Example 8.31 , find the coef ficient of determination. Solution In Example Example 8.31 , we found sxx = 5, sxy = 11. We also have syy = (3 − 6) 2 + (4 − 6) 2 + (8 − 6) 2 + (9 − 6) 2 = 26. We conclude r 2 = ≈ 0.93 112 5 × 26 8.5.3 The Method of Least Squares Here, we use a dif ferent method to estimate β0 and β1. This method will result in the same estimates as before; however , it is based on a dif ferent idea. Suppose that we have data points (x1, y1), (x2, y2), ⋯, (xn, yn). Consider the model ^y = β0 + β1x. The errors (residuals) are given by ei = yi − ^y i = yi − β0 − β1x. The sum of the squared errors is given by g(β0, β1) = n ∑ i=1 e 2 i = n ∑ i=1 (yi − β0 − β1xi) 2. (8.7) To find the best fit for the data, we find the values of ^β0 and ^β1 such that g(β0, β1) is minimized. This can be done by taking partial derivatives with respect to β0 and β1, and setting them to zero. W e obtain = n ∑ i=1 2(−1)(yi − β0 − β1xi) = 0, (8.8) = n ∑ i=1 2(−xi)(yi − β0 − β1xi) = 0. (8.9) By solving the above equations, we obtain the same values of ^β0 and ^β1 as before ^β1 = , ^β0 = ¯¯¯y − ^β1¯¯¯x, where sxx = n ∑ i=1 (xi − ¯¯¯x) 2, sxy = n ∑ i=1 (xi − ¯¯¯x)(yi − ¯¯¯y ). ∂g ∂β0 ∂g ∂β1 sxy sxx This method is called the method of least squares , and for this reason, we call the above values of ^β0 and ^β1 the least squares estimates of β0 and β1. 8.5.4 Extensions and Issues In practice, the models that we use might be more complicated than the simple linear regression model. W e almost always use computers to do regression analysis (MA TLAB, EXCEL, etc). Therefore, when we understand the basics of the simple linear regression, we can simply use computers to do more complicated analyses. A natural extension to the simple linear regression is multiple linear regression . Mul ti p l e Li n ear Regressi o n : In the above discussion, our model had only one predictor (explanatory variable), x. We can consider models with more than one explanatory variable. For example, suppose that you would like to have a model to predict house prices based on square footage, age, number of bedrooms, etc. Here, your response variable y is the house price. Y our goal is to have a linear model y = β0 + β1x + β2z + ⋯ + βkw + ϵ, where x, z, ⋯, w are the explanatory variables (square footage, age, number of bedrooms, etc). Such a model is an example of a multiple linear regression model. It is possible to extend the method of least squares to this case to compute estimates of β0, β1, ⋯, βk. In MA TLAB, the command regress can be used for multiple linear regression. It is worth noting that when we say linear regression, we mean linear in the unknown parameters βi. For example, the model y = β0 + β1x + β2x 2 + ϵ is a linear regression model since it is linear in β0, β1, and β2. We would like to end this section by mentioning that when running regression algorithms, one needs to be mindful about some practical considerations. Issues such as overfitting [21] , heteroscedasticity [22] , and multicollinearity [23] might cause problems in regression analysis. 8.5.5 Solved Problems Problem 1 Consider the following observed values of (xi, yi): (−1, 6), (0, 3), (1, 2), (2, −1) a . Find the estimated regression line ^y = ^β0 + ^β1x, based on the observed data. b . For each xi, compute the fitted value of yi using ^y i = ^β0 + ^β1xi. c. Compute the residuals, ei = yi − ^y i. d. Find R-squared (the coef ficient of determination). Solution a . We have ¯¯¯x = = 0.5, ¯¯¯y = = 2.5, sxx = (−1 − 0.5) 2 + (0 − 0.5) 2 + (1 − 0.5) 2 + (2 − 0.5) 2 = 5, sxy = (−1 − 0.5)(6 − 2.5) + (0 − 0.5)(3 − 2.5) + (1 − 0.5)(2 − 2.5) + (2 − 0.5)(−1 − 2.5) = −11. Therefore, we obtain ^β1 = = = −2.2, ^β0 = 2.5 − (−2.2)(0.5) = 3.6 The following MA TLAB code can be used to obtain the estimated regression line −1 + 0 + 1 + 2 4 6 + 3 + 2 + (−1) 4 sxy sxx −11 5 x=[-1;0;1;2]; x0=ones(size(x)); y=[6;3;2;-1]; beta = regress(y ,[x0,x]); b . The fitted values are given by ^y i = 3.6 − 2.2xi, so we obtain ^y 1 = 5.8, ^y 2 = 3.6, ^y 3 = 1.4, ^y 4 = −0.8 c. We have e1 = y1 − ^y 1 = 6 − 5.8 = 0.2, e2 = y2 − ^y 2 = 3 − 3.6 = −0.6, e3 = y3 − ^y 3 = 2 − 1.4 = 0.6, e4 = y4 − ^y 4 = −1 − (−0.8) = −0.2 d. We have syy = (6 − 2.5) 2 + (3 − 2.5) 2 + (2 − 2.5) 2 + (−1 − 2.5) 2 = 25. We conclude r 2 = ≈ 0.968 Problem 2 Consider the model Y = β0 + β1X + ϵ, where ϵ is a N(0, σ2) random variable independent of X. Let also ^Y = β0 + β1X. Show that E[(Y − EY ) 2] = E[( ^Y − EY ) 2] + E[(Y − ^Y ) 2]. (−11) 2 5 × 25 Solution Since X and ϵ are independent, we can write Var(Y ) = β2 1 Var(X) + Var(ϵ) (8.10) Note that, ^Y − EY = (β0 + β1X) − (β0 + β1EX) = β1(X − EX). Therefore, E[( ^Y − EY ) 2] = β2 1 Var(X). Also, E[(Y − EY ) 2] = Var(Y ), E[(Y − ^Y ) 2] = Var(ϵ). Combining with Equation 8.10 , we conclude E[(Y − EY ) 2] = E[( ^Y − EY ) 2] + E[(Y − ^Y ) 2]. Problem 3 Show that, in a simple linear regression, the estimated coef ficients ^β0 and ^β1 (least squares estimates of β0 and β1) satisfy the following equations n ∑ i=1 ei = 0, n ∑ i=1 eixi = 0, n ∑ i=1 ei ^yi = 0, where ei = yi − ^yi = yi − ^β0 − ^β1x. Hint: ^β0 and ^β1 satisfy Equation 8.8 and Equation 8.9 . By cancelling the (−2) factor , you can write n ∑ i=1 (yi − ^β0 − ^β1xi) = 0, n ∑ i=1 (yi − ^β0 − ^β1xi)xi = 0. Use the above equations to show the desired equations. Solution We have n ∑ i=1 (yi − ^β0 − ^β1xi) = 0, n ∑ i=1 (yi − ^β0 − ^β1xi)xi = 0. Since ei = yi − ^β0 − ^β1x, we conclude n ∑ i=1 ei = 0, n ∑ i=1 eixi = 0. Moreover , n ∑ i=1 ei ^yi = n ∑ i=1 ei( ^β0 + ^β1xi) = ^β0 n ∑ i=1 ei + ^β1 n ∑ i=1 eixi = 0 + 0 = 0. Problem 4 Show that the coef ficient of determination can also be obtained as r 2 = . Solution We know ^yi = β0 + β1xi, ¯¯¯y = β0 + β1¯¯¯x. Therefore, ∑n i=1( ^yi − ¯¯¯y )2 ∑n i=1(yi − ¯¯¯y )2 n ∑ i=1 ( ^yi − ¯¯¯y ) 2 = n ∑ i=1 (β1xi − β1¯¯¯x) 2 = β2 1 n ∑ i=1 (xi − ¯¯¯x) 2 = β2 1 sxx. Therefore, = = (since β1 = ) = r 2. Problem 5 (The Method of Maximum Likelihood ) This problem assumes that you are familiar with the maximum likelihood method discussed in Section 8.2.3 . Consider the model Yi = β0 + β1xi + ϵi, where ϵi's are independent N(0, σ2) random variables. Our goal is to estimate β0 and β1. We have the observed data pairs (x1, y1), (x2, y2), ⋯, (xn, yn). a . Argue that, for given values of β0, β1, and xi, Yi is a normal random variable with mean β0 + β1xi and variance σ2. Moreover , show that the Yi's are independent. b . Find the likelihood function L(y1, y2, ⋯ , yn; β0, β1) = fY1Y2⋯Yn (y1, y2, ⋯ , yn; β0, β1). c. Show that the maximum likelihood estimates of β0 and β1 are the same as the ones we obtained using the least squares method. Solution a . Given values of β0, β1, and xi, c = β0 + β1xi is a constant. Therefore, Yi = c + ϵi is a normal random variable with mean c and variance σ2. Also, since the ϵi's are independent, we conclude that Yi's are also independent random variables. b . By the previous part, for given values of β0, β1, and xi, ∑n i=1( ^yi − ¯¯¯y ) 2 ∑n i=1(yi − ¯¯¯y )2 β2 1 sxx syy s2 xy sxxsyy sxy sxx fYi(y; β0, β1) = exp{− (y − β0 − β1xi) 2}. Therefore, the likelihood function is given by L(y1, y2, ⋯ , yn; β0, β1) = fY1Y2⋯Yn (y1, y2, ⋯ , yn; β0, β1) = fY1 (y1; β0, β1)fY2 (y2; β0, β1) ⋯ fYn (yn; β0, β1) = exp{− n ∑ i=1 (y − β0 − β1xi) 2}. c. To find the maximum likelihood estimates (MLE) of β0 and β1, we need to find ^β0 and ^β1 such that the likelihood function L(y1, y2, ⋯ , yn; β0, β1) = exp{− n ∑ i=1 (y − β0 − β1xi) 2} is maximized. This is equivalent to minimizing n ∑ i=1 (y − β0 − β1xi) 2. The above expression is the sum of the squared errors, g(β0, β1) (Equation 8.7 ). Therefore, the maximum likelihood estimation for this model is the same as the least squares method. 1 √2πσ2 1 2 1 (2πσ2) n 2 1 2 1 (2πσ2) n 2 1 2 8.6.0 End of Chapter Problems Problem 1 Let X be the weight of a randomly chosen individual from a population of adult men. In order to estimate the mean and variance of X, we observe a random sample X1,X2,⋯ ,X10. Thus, the Xi's are i.i.d. and have the same distribution as X. We obtain the following values (in pounds): 165.5, 175.4, 144.1, 178.5, 168.0, 157.9, 170.1, 202.5, 145.5, 135.7 Find the values of the sample mean, the sample variance, and the sample standard deviation for the observed sample. Problem 2 Let X1, X2, X3, . . ., Xn be a random sample with unknown mean EXi = μ, and unknown variance Var(Xi) = σ2. Suppose that we would like to estimate θ = μ2. We define the estimator ^Θ as ^Θ = ( ¯¯¯¯¯ X ) 2 = [ n ∑ k=1 Xk] 2 to estimate θ. Is ^Θ an unbiased estimator of θ? Why? Problem 3 Let X1, X2, X3, . . ., Xn be a random sample from the following distribution fX(x) = ⎧⎪ ⎪ ⎨ ⎪ ⎪⎩ θ (x − ) + 1 for 0 ≤ x ≤ 1 0 otherwise where θ ∈ [−2, 2] is an unknown parameter . We define the estimator ^Θn as ^Θn = 12 ¯¯¯¯¯ X − 6 1 n 1 2 to estimate θ. a . Is ^Θn an unbiased estimator of θ? b . Is ^Θn a consistent estimator of θ? c. Find the mean squared error (MSE) of ^Θn. Problem 4 Let X1, … , X4 be a random sample from a Geometric(p) distribution. Suppose we observed (x1, x2, x3, x4) = (2, 3, 3, 5). Find the likelihood function using PXi(xi; p) = p(1 − p)xi−1 as the PMF . Problem 5 Let X1, … , X4 be a random sample from an Exponential(θ) distribution. Suppose we observed (x1, x2, x3, x4) = (2.35, 1.55, 3.25, 2.65). Find the likelihood function using fXi(xi; θ) = θe −θxi,  for xi ≥ 0 as the PDF . Problem 6 Often when working with maximum likelihood functions, out of ease we maximize the log-likelihood rather than the likelihood to find the maximum likelihood estimator . Why is maximizing L(x; θ) as a function of θ equivalent to maximizing log L(x; θ)? Problem 7 Let X be one observation from a N(0, σ2) distribution. a . Find an unbiased estimator of σ2. b . Find the log likelihood, log (L(x; σ2)), using 2 fX(x; σ2) = exp {− } as the PDF . c. Find the Maximum Likelihood Estimate (MLE) for the standard deviation σ, ^σML. Problem 8 Let X1, … , Xn be a random sample from a Poisson(λ) distribution. a . Find the likelihood equation, L(x1, … , xn; λ), using PXi(x1, … , xn; λ) = as the PMF . b . Find the log likelihood function and use that to obtain the MLE for λ, ^λML. Problem 9 In this problem, we would like to find the CDFs of the order statistics. Let X1, … , Xn be a random sample from a continuous distribution with CDF FX(x) and PDF fX(x). Define X(1), … , X(n) as the order statistics and show that FX(i) (x) = n ∑ k=i ( )[FX(x)]k[1 − FX(x)]n−k. Hint: Fix x ∈ R. Let Y be a random variable that counts the number of Xj's ≤ x. Define {Xj ≤ x} as a \"success\" and {Xj > x} as a \"failure,\" and show that Y ∼ Binomial(n, p = FX(x)). Problem 10 In this problem, we would like to find the PDFs of order statistics. Let X1, … , Xn be a random sample from a continuous distribution with CDF FX(x) and PDF fX(x). Define X(1), … , X(n) as the order statistics. Our goal here is to show that 1 √2πσ x 2 2σ2 e −λλ xi xi! n k fX(i) (x) = fX(x)[FX(x)]i−1[1 − FX(x)]n−i. One way to do this is to dif ferentiate the CDF (found in Problem 9 ). However , here, we would like to derive the PDF directly . Let fX(i) (x) be the PDF of X(i). By definition of the PDF, for small δ, we can write fX(i) (x)δ ≈ P(x ≤ X(i) ≤ x + δ)δ. Note that the event {x ≤ X(i) ≤ x + δ} occurs if i − 1 of the Xj's are less than x, one of them is in [x, x + δ], and n − i of them are larger than x + δ. Using this, find fX(i) (x). Hint: Remember the multinomial distribution. More specifically , suppose that an experiment has 3 possible outcomes, so the sample space is given by S = {s1, s2, s3}. Also, suppose that P(si) = pi for i = 1, 2, 3. Then for n = n1 + n2 + n3 independent trials of this experiment, the probability that each si appears ni times is given by ( )p n1 1 p n2 2 p n3 3 = p n1 1 p n2 2 p n3 3 . Problem 1 1 A random sample X1, X2, X3, . . ., X100 is given from a distribution with known variance Var(Xi) = 81. For the observed sample, the sample mean is ¯¯¯¯¯ X = 50.1. Find an approximate 95% confidence interval for θ = EXi. Problem 12 To estimate the portion of voters who plan to vote for Candidate A in an election, a random sample of size n from the voters is chosen. The sampling is done with replacement. Let θ be the portion of voters who plan to vote for Candidate A among all voters. a . How large does n need to be so that we can obtain a 90% confidence interval with 3% margin of error? n! (i − 1)!(n − i)! n n1, n2, n3 n! n1!n2!n3! b . How large does n need to be so that we can obtain a 99% confidence interval with 3% margin of error? Problem 13 Let X1, X2, X3, . . ., X100 be a random sample from a distribution with unknown variance Var(Xi) = σ2 < ∞. For the observed sample, the sample mean is ¯¯¯¯¯ X = 110.5, and the sample variance is S 2 = 45.6. Find a 95% confidence interval for θ = EXi. Problem 14 A random sample X1, X2, X3, . . ., X36 is given from a normal distribution with unknown mean μ = EXi and unknown variance Var(Xi) = σ2. For the observed sample, the sample mean is ¯¯¯¯¯ X = 35.8, and the sample variance is S 2 = 12.5. a . Find and compare 90%, 95%, and 99% confidence interval for μ. b . Find and compare 90%, 95%, and 99% confidence interval for σ2. Problem 15 Let X1, X2, X3, X4, X5 be a random sample from a N(μ, 1) distribution, where μ is unknown. Suppose that we have observed the following values 5.45, 4.23, 7.22, 6.94, 5.98 We would like to decide between H0: μ = μ0 = 5, H1: μ ≠ 5. a . Define a test statistic to test the hypotheses and draw a conclusion assuming α = 0.05. b . Find a 95% confidence interval around ¯¯¯¯¯ X . Is μ0 included in the interval? How does the exclusion of μ0 in the interval relate to the hypotheses we are testing? Problem 16 Let X1, … , X9 be a random sample from a N(μ, 1) distribution, where μ is unknown. Suppose that we have observed the following values 16.34, 18.57, 18.22, 16.94, 15.98, 15.23, 17.22, 16.54, 17.54 We would like to decide between H0: μ = μ0 = 16, H1: μ ≠ 16. a . Find a 90% confidence interval around ¯¯¯¯¯ X . Is μ0 included in the interval? How does this relate to our hypothesis test? b . Define a test statistic to test the hypotheses and draw a conclusion assuming α = 0.1. Problem 17 Let X1, X2 ,..., X150 be a random sample from an unknown distribution. After observing this sample, the sample mean and the sample variance are calculated to be ¯¯¯¯¯ X = 52.28, S 2 = 30.9 Design a level 0.05 test to choose between H0: μ = 50, H1: μ > 50. Do you accept or reject H0? Problem 18 Let X1, X2, X3, X4, X5 be a random sample from a N(μ, σ2) distribution, where μ and σ are both unknown. Suppose that we have observed the following values 27.72, 22.24, 32.86, 19.66, 35.34 We would like to decide between H0: μ ≥ 30, H1: μ < 30. Assuming α = 0.05, what do you conclude? Problem 19 Let X1, X2 ,..., X121 be a random sample from an unknown distribution. After observing this sample, the sample mean and the sample variance are calculated to be ¯¯¯¯¯ X = 29.25, S 2 = 20.7 Design a test to decide between H0: μ = 30, H1: μ < 30, and calculate the P-value for the observed data. Problem 20 Suppose we would like to test the hypothesis that at least 10% of students suf fer from allergies. W e collect a random sample of 225 students and 21 of them suf fer from allergies. a . State the null and alternative hypotheses. b . Obtain a test statistic and a P-value. c. State the conclusion at the α = 0.05 level. Problem 21 Consider the following observed values of (xi, yi): (−5, −2), (−3, 1), (0, 4), (2, 6), (1, 3). a . Find the estimated regression line ^y = ^β0 + ^β1x based on the observed data. b . For each xi, compute the fitted value of yi using ^y i = ^β0 + ^β1xi. c. Compute the residuals, ei = yi − ^y i. d. Calculate R-squared. Problem 22 Consider the following observed values of (xi, yi): (1, 3), (3, 7). a . Find the estimated regression line ^y = ^β0 + ^β1x based on the observed data. b . For each xi, compute the fitted value of yi using ^y i = ^β0 + ^β1xi. c. Compute the residuals, ei = yi − ^y i. d. Calculate R-squared. e. Explain the above results. In particular , can you conclude that the obtained regression line is a good model here? Problem 23 Consider the simple linear regression model Yi = β0 + β1xi + ϵi, where ϵi's are independent N(0, σ2) random variables. Therefore, Yi is a normal random variable with mean β0 + β1xi and variance σ2. Moreover , Yi's are independent. As usual, we have the observed data pairs (x1, y1), (x2, y2), ⋯, (xn, yn) from which we would like to estimate β0 and β1. In this chapter , we found the following estimators ^β1 = , ^β0 = ¯¯¯¯ Y − ^β1¯¯¯x. where sxx = n ∑ i=1 (xi − ¯¯¯x) 2, sxy = n ∑ i=1 (xi − ¯¯¯x)(Yi − ¯¯¯¯ Y ). a . Show that ^β1 is a normal random variable. b . Show that ^β1 is an unbiased estimator of β1, i.e., E[ ^β1] = β1. c. Show that Var( ^β1) = . Problem 24 Again consider the simple linear regression model Yi = β0 + β1xi + ϵi, where ϵi's are independent N(0, σ2) random variables, and ^β1 = , ^β0 = ¯¯¯¯ Y − ^β1¯¯¯x. sxy sxx σ2 sxx sxy sxx a . Show that ^β0 is a normal random variable. b . Show that ^β0 is an unbiased estimator of β0, i.e., E[ ^β0] = β0. c. For any i = 1, 2, 3, . . . , n, show that Cov( ^β1, Yi) = σ2. d. Show that Cov( ^β1, ¯¯¯¯ Y ) = 0. e. Show that Var( ^β0) = σ2. xi − ¯¯¯x sxx ∑n i=1 x 2 i nsxx 9.1.0 Bayesian Inference The following is a general setup for a statistical inference problem: There is an unknown quantity that we would like to estimate. W e get some data. From the data, we estimate the desired quantity . In the previous chapter , we discussed the frequentist approach to this problem. In that approach, the unknown quantity θ is assumed to be a fixed (non-random) quantity that is to be estimated by the observed data. In this chapter , we would like to discuss a dif ferent framework for inference, namely the Bayesian approach. In the Bayesian framework, we treat the unknown quantity , Θ, as a random variable. More specifically , we assume that we have some initial guess about the distribution of Θ. This distribution is called the prior distribution . After observing some data, we update the distribution of Θ (based on the observed data). This step is usually done using Bayes' Rule . That is why this approach is called the Bayesian approach. The details of this approach will be clearer as you go through the chapter . Here, to motivate the Bayesian approach, we will provide two examples of statistical problems that might be solved using the Bayesian approach. Example 9. 1 Suppose that you would like to estimate the portion of voters in your town that plan to vote for Party A in an upcoming election. T o do so, you take a random sample of size n from the likely voters in the town. Since you have a limited amount of time and resources, your sample is relatively small. Specifically , suppose that n = 20. After doing your sampling, you find out that 6 people in your sample say they will vote for Party A. Solution Let θ be the true portion of voters in your town who plan to vote for Party A. Y ou might want to estimate θ as ^θ = = 0.3 6 20 In fact, in absence of any other data, that seems to be a reasonable estimate. However , you might feel that n = 20 is too small. Thus, your guess is that the error in your estimation might be too high. While thinking about this problem, you remember that the data from the previous election is available to you. Y ou look at that data and find out that, in the previous election, 40% of the people in your town voted for Party A. How can you use this data to possibly improve your estimate of θ? You might argue as follows: Although the portion of votes for Party A changes from one election to another , the change is not usually very drastic. Therefore, given that in the previous election 40% of the voters voted for Party A, you might want to model the portion of votes for Party A in the next election as a random variable Θ with a probability density function, fΘ(θ), that is mostly concentrated around θ = 0.4. For example, you might want to choose the density such that E[Θ] = 0.4 Figure 9.1 shows an example of such density functions. Such a distribution shows your prior belief about Θ in the absence of any additional data. That is, before taking your random sample of size n = 20, this is your guess about the distribution of Θ. Figure 9.1 - An example of a prior distribution for Θ in Example 9.1 Therefore, you initially have the prior distribution fΘ(θ). Then you collect some data, shown by D. More specifically , here your data is a random sample of size n = 20 voters, 6 of whom are voting for Party A. As we will discuss in more detail, you can then proceed to find an updated distribution for Θ, called the posterior distribution, using Bayes' rule: fΘ|D(θ|D) = . We can now use the posterior density , fΘ|D(θ|D), to further draw inferences about Θ. More specifically , we might use it to find point or interval estimates of Θ. P(D|θ)fΘ(θ) P(D) Example 9. 2 Consider a communication channel as shown in Figure 9.2. W e can model the communication over this channel as follows. At time n, a random variable Xn is generated and is transmitted over the channel. However , the channel is noisy . Thus, at the receiver, a noisy version of Xn is received. More specifically , the received signal is Yn = Xn + Wn, where Wn ∼ N(0, σ2) is the noise added to Xn. We assume that the receiver knows the distribution of Xn. The goal here is to recover (estimate) the value of Xn based on the observed value of Yn. Figure 9.2 - Noisy communication channel in Example 9.2 Solution Again, we are dealing with estimating a random variable ( Xn). In this case, the prior distribution is fX(x). After observing Yn, the posterior distribution can be written as fXn|Yn (x|y) = . Here, we have assumed both X and Y are continuous random variables. The above formula is a version of Bayes' rule. W e will discuss the details of this approach shortly; however , as you'll notice, we are using the same framework as Example 9.1 . After finding the posterior distribution, fXn|Yn (x|y), we can then use it to estimate the value of Xn. If you think about Examples 9.1 and 9.2 carefully , you will notice that they have similar structures. Basically , in both problems, our goal is to draw an inference about the fYn|Xn (y|x)fX(x) fY (y) value of an unobserved random variable ( Θ or Xn). We observe some data ( D or Yn). We then use Bayes' rule to make inference about the unobserved random variable. This is generally how we approach inference problems in Bayesian statistics . It is worth noting that Examples 9.1 and 9.2 are conceptually dif ferent in the following sense: In Example 9.1 , the choice of prior distribution fΘ(θ) is somewhat unclear . That is, different people might use dif ferent prior distributions. In other words, the choice of prior distribution is subjective here. On the other hand, in Example 9.2 , the prior distribution fXn (x) might be determined as a part of the communication system design. In other words, for this example, the prior distribution might be known without any ambiguity . Nevertheless, once the prior distribution is determined, then one uses similar methods to attack both problems. For this reason, we study both problems under the umbrella of Bayesian statistics. Bayesian Statistical Inference The goal is to draw inferences about an unknown variable X by observing a related random variable Y . The unknown variable is modeled as a random variable X, with prior distribution fX(x), if X is continuous, PX(x), if X is discrete. After observing the value of the random variable Y , we find the posterior distribution of X. This is the conditional PDF (or PMF) of X given Y = y, fX|Y (x|y)  or  PX|Y (x|y). The posterior distribution is usually found using Bayes' formula. Using the posterior distribution, we can then find point or interval estimates of X. Note that in the above setting, X or Y (or possibly both) could be random vectors. For example, X = (X1, X2, ⋯ , Xn) might consist of several random variables. However , the general idea of Bayesian statistics stays the same. W e will specifically talk about estimating random vectors in Section 9.1.7 . 9.1.10 Solved Problems Problem 1 Let X ∼ N(0, 1). Suppose that we know Y | X = x ∼ N(x, 1). Show that the posterior density of X given Y = y, fX|Y (x|y), is given by X | Y = y ∼ N ( , ) . Solution Our goal is to show that fX|Y (x|y) is normal with mean and variance . Therefore, it suffices to show that fX|Y (x|y) = c(y) exp{−(x − ) 2}, where c(y) is just a function of y. That is, for a given y, c(y) is just the normalizing constant ensuring that fX|Y (x|y) integrates to one. By the assumptions, fY |X(y|x) = exp{− }, fX(x) = exp{− }. Therefore, y 2 1 2 y 2 1 2 y 2 1 √2π (y − x) 2 2 1 √2π x 2 2 fX|Y (x|y) = = (a function of y) ⋅ fY |X(y|x)fX(x) = (a function of y) ⋅ exp{− } = (a function of y) ⋅ exp{−(x − ) 2 + } = (a function of y) ⋅ exp{−(x − ) 2}. Problem 2 We can generalize the result of Problem 9.1 using the same method. In particular , assuming X ∼ N(μ, τ 2) and Y | X = x ∼ N(x, σ2), it can be shown that the posterior density of X given Y = y is given by X | Y = y ∼ N ( , ) . In this problem, you can use the above result. Let X ∼ N(μ, τ 2) and Y | X = x ∼ N(x, σ2). Suppose that we have observed the random sample Y1, Y2, ⋯, Yn such that the Yi's are i.i.d. and have the same distribution as Y . a . Show that the posterior density of X given ¯¯¯¯ Y (the sample mean) is X | ¯¯¯¯ Y ∼ N ( , ) . b . Find the MAP and the MMSE estimates of X given ¯¯¯¯ Y . Solution a . Since Y | X = x ∼ N(x, σ2), we conclude fY |X(y|x)fX(x) fY (y) (y − x) 2 + x 2 2 y 2 y2 4 y 2 y/σ2 + μ/τ 2 1/σ2 + 1/τ 2 1 1/σ2 + 1/τ 2 n ¯¯¯¯ Y /σ2 + μ/τ 2 n/σ2 + 1/τ 2 1 n/σ2 + 1/τ 2 2 ¯¯¯¯ Y | X = x ∼ N (x, ) . Therefore, we can use the posterior density given in the problem statement (we need to replace σ2 by ). Thus, the posterior density of X given ¯¯¯¯ Y is X | ¯¯¯¯ Y ∼ N ( , ) . b . To find the MAP estimate of X given ¯¯¯¯ Y , we need to find the value that maximizes the posterior density . Since the posterior density is normal, the maximum value is obtained at the mean which is ^XMAP = . Also, the MMSE estimate of X given ¯¯¯¯ Y is ^XM = E[X|¯¯¯¯ Y ] = . Problem 3 Let ^XM be the MMSE estimate of X given Y . Show that the MSE of this estimator is M SE = E[Var(X|Y )]. Solution We have Var(X|Y ) = E[(X − E[X|Y ]) 2|Y ] (by definition of Var(X|Y )) = E[(X − ^XM ) 2|Y ]. Therefore, E[Var(X|Y )] = E[E[(X − ^X2 M )|Y ]] = E[(X − ^XM ) 2] (by the law of iterated expectations) = M SE (by definition of MSE). σ2 n σ2 n n ¯¯¯¯ Y /σ2 + μ/τ 2 n/σ2 + 1/τ 2 1 n/σ2 + 1/τ 2 n ¯¯¯¯ Y /σ2 + μ/τ 2 n/σ2 + 1/τ 2 n ¯¯¯¯ Y /σ2 + μ/τ 2 n/σ2 + 1/τ 2 Problem 4 Consider two random variables X and Y with the joint PMF given in Table 9.1. Table 9.1: Joint PMF of X and Y for Problem 4   Y = 0 Y = 1 X = 0 X = 1 0 a . Find the linear MMSE estimator of X given Y , ( ^XL). b . Find the MMSE estimator of X given Y , ( ^XM ). c. Find the MSE of ^XM . Solution Using the table we find out PX(0) = + = , PX(1) = + 0 = , PY (0) = + = , PY (1) = + 0 = . Thus, the marginal distributions of X and Y are both Bernoulli( ). Therefore, we have EX = EY = , Var(X) = Var(Y ) = ⋅ = . a . To find the linear MMSE estimator of X given Y , we also need Cov(X, Y ). W e have EXY = ∑ xiyjPXY (x, y) = 0. Therefore, 1 5 2 5 2 5 1 5 2 5 3 5 2 5 2 5 1 5 2 5 3 5 2 5 2 5 2 5 2 5 2 5 3 5 6 25 Cov(X, Y ) = EXY − EXEY = − . The linear MMSE estimator of X given Y is ^XL = (Y − EY ) + EX = (Y − ) + = − Y + . Since Y can only take two values, we can summarize ^XL in the following table. Table 9.2: The linear MMSE estimator of X given Y for Problem 4   Y = 0 Y = 1 ^XL 0 b . To find the MMSE estimator of X given Y , we need the conditional PMFs. W e have PX|Y (0|0) = = = . Thus, PX|Y (1|0) = 1 − = . We conclude X|Y = 0 ∼ Bernoulli ( ) . Similarly , we find PX|Y (0|1) = 1, PX|Y (1|1) = 0. Thus, given Y = 1, we have always X = 0. The MMSE estimator of X given Y is ^ 4 25 Cov(X, Y ) Var(Y ) −4/25 6/25 2 5 2 5 2 3 2 3 2 3 PXY (0, 0) PY (0) 1 5 3 5 1 3 1 3 2 3 2 3 ^XM = E[X|Y ]. We have E[X|Y = 0] = , E[X|Y = 1] = 0. Thus, we can summarize ^XM in the following table. Table 9.3: The MMSE estimator of X given Y for Problem Problem 4   Y = 0 Y = 1 ^XM 0 We notice that, for this problem, the MMSE and the linear MMSE estimators are the same. In fact, this is not surprising since here, Y can only take two possible values, and for each value we have a corresponding MMSE estimator . The linear MMSE estimator is just the line passing through the two resulting points. c. The MSE of ^XM can be obtained as M SE = E[ ~ X2] = EX2 − E[ ^X2 M ] = − E[ ^X2 M ]. From the table for ^XM , we obtain E[ ^X2 M ] = . Therefore, M SE = . Note that here the MMSE and the linear MMSE estimators are equal, so they have the same MSE. Thus, we can use the formula for the MSE of ^XL as well: M SE = (1 − ρ(X, Y ) 2)Var(X) = (1 − ) Var(X) = (1 − ) = . 2 3 2 3 2 5 4 15 2 15 Cov(X, Y )2 Var(X)Var(Y ) (−4/25)2 6/25 ⋅ 6/25 6 25 2 15 Problem 5 Consider Example 9.9 in which X is an unobserved random variable with EX = 0, Var(X) = 4. Assume that we have observed Y1 and Y2 given by Y1 = X + W1, Y2 = X + W2, where EW1 = EW2 = 0, Var(W1) = 1, and Var(W2) = 4. Assume that W1, W2 , and X are independent random variables. Find the linear MMSE estimator of X given Y1 and Y2 using the vector formula ^XL = CXYCY−1(Y − E[Y]) + E[X]. Solution Note that, here, X is a one dimensional vector , and Y is a two dimensional vector Y = [ Y1 Y2 ] = [ X + W1 X + W2 ] . We have CY = [ Var(Y1) Cov(Y1, Y2) Cov(Y2, Y1) Var(Y2) ] = [ 5 4 4 8 ] , CXY = [ Cov(X, Y1) Cov(X, Y2) ] = [ 4 4 ] . Therefore, ^XL = [ 4 4 ] [ 5 4 4 8 ] −1 ([ Y1 Y2 ] − [ 0 0 ]) + 0 = [ ] [ Y1 Y2 ] = Y1 + Y2, which is the same as the result that we obtained using the orthogonality principle in Example 9.9 . Problem 6 2 3 1 6 2 3 1 6 Suppose that we need to decide between two opposing hypotheses H0 and H1. Let Cij be the cost of accepting Hi given that Hj is true. That is C00: The cost of choosing H0, given that H0 is true. C10: The cost of choosing H1, given that H0 is true. C01: The cost of choosing H0, given that H1 is true. C11: The cost of choosing H1, given that H1 is true. It is reasonable to assume that the associated cost to a correct decision is less than the cost of an incorrect decision. That is, c00 < c10 and c11 < c01. The average cost can be written as C = ∑ i,j CijP(choose Hi|Hj)P(Hj) =C00P(choose H0|H0)P(H0) + C01P(choose H0|H1)P(H1) + C10P(choose H1|H0)P(H0) + C11P(choose H1|H1)P(H1). Our goal is to find the decision rule such that the average cost is minimized. Show that the decision rule can be stated as follows: Choose H0 if and only if fY (y|H0)P(H0)(C10 − C00) ≥ fY (y|H1)P(H1)(C01 − C11) (9.8) Solution First, note that P(choose H0|H0) = 1 − P(choose H1|H0), P(choose H1|H1) = 1 − P(choose H0|H1). Therefore, C =C00[1 − P(choose H1|H0)]P(H0) + C01P(choose H0|H1)P(H1) + C10P(choose H1|H0)P(H0) + C11[1 − P(choose H0|H1)]P(H1) =(C10 − C00)P(choose H1|H0)P(H0) + (C01 − C11)P(choose H0|H1)P(H1) + C00p(H0) + C11p(H1). The term C00p(H0) + C11P(H1) is constant (i.e., it does not depend on the decision rule). Therefore, to minimize the cost, we need to minimize D = P(choose H1|H0)P(H0)(C10 − C00) + P(choose H0|H1)P(H1)(C01 − C11). The above expression is very similar to the average error probability of the MAP test ( Equation 9.8 ). The only dif ference is that we have p(H0)(C10 − C00) instead of P(H0), and we have p(H1)(C01 − C11) instead of P(H1). Therefore, we can use a decision rule similar to the MAP decision rule. More specifically , we choose H0 if and only if fY (y|H0)P(H0)(C10 − C00) ≥ fY (y|H1)P(H1)(C01 − C11). Problem 7 Let X ∼ N(0, 4) and Y | X = x ∼ N(x, 1). Suppose that we have observed the random sample Y1, Y2, ⋯, Y25 such that the Yi's are i.i.d. and have the same distribution as Y . Find a 95% credible interval for X, given that we have observed ¯¯¯¯ Y = = 0.56 Hint: Use the result of Problem 9.2 . Solution By part (a) of Problem 9.2 , we have X | ¯¯¯¯ Y ∼ N ( , ) = N(0.5545, 0.0396). Therefore, we choose the interval in the form of [0.5545 − c, 0.5545 + c]. We need to have P(0.5545 − c ≤ X ≤ 0.5545 + c∣∣¯¯¯¯ Y = 0.56) = Φ ( ) − Φ ( ) = 2Φ ( ) − 1 = 0.95 Y1 + Y2+. . . +Yn n 25(0.56)/1 + 0/4 25/1 + 1/4 1 25/1 + 1/4 c √0.0396 −c √0.0396 c √0.0396 Solving for c, we obtain c = √0.0396Φ−1(0.975) ≈ 0.39 Therefore, the 95% credible interval for X is [0.5545 − 0.39, 0.5545 + 0.39] ≈ [0.1645, 0.9445]. 9.1.1 Prior and Posterior Let X be the random variable whose value we try to estimate. Let Y be the observed random variable. That is, we have observed Y = y, and we would like to estimate X. Assuming both X and Y are discrete, we can write P(X = x|Y = y) = = . Using our notation for PMF and conditional PMF , the above equation can be rewritten as PX|Y (x|y) = . The above equation, as we have seen before, is just one way of writing Bayes' rule. If either X or Y are continuous random variables, we can replace the corresponding PMF with PDF in the above formula. For example, if X is a continuous random variable, while Y is discrete we can write fX|Y (x|y) = . To find the denominator ( PY (y) or fY (y)), we often use the law of total probability . Let's look at an example. Example 9. 3 Let X ∼ Uniform(0, 1). Suppose that we know Y | X = x ∼ Geometric(x). Find the posterior density of X given Y = 2, fX|Y (x|2). Solution P(X = x, Y = y) P(Y = y) P(Y = y|X = x)P(X = x) P(Y = y) PY |X(y|x)PX(x) PY (y) PY |X(y|x)fX(x) PY (y) Using Bayes' rule we have fX|Y (x|2) = . We know Y | X = x ∼ Geometric(x), so PY |X(y|x) = x(1 − x) y−1,  for y = 1, 2, ⋯ . Therefore, PY |X(2|x) = x(1 − x). To find PY (2), we can use the law of total probability PY (2) = ∫ ∞ −∞ PY |X(2|x)fX(x) dx = ∫ 1 0 x(1 − x) ⋅ 1 dx = . Therefore, we obtain fX|Y (x|2) = = 6x(1 − x),  for 0 ≤ x ≤ 1. For the remainder of this chapter , for simplicity , we often write the posterior PDF as fX|Y (x|y) = , which implies that both X and Y are continuous. Nevertheless, we understand that if either X or Y is discrete, we need to replace the PDF by the corresponding PMF . PY |X(2|x)fX(x) PY (2) 1 6 x(1 − x) ⋅ 1 1 6 fY |X(y|x)fX(x) fY (y) 9.1.2 Maximum A Posteriori (MAP) Estimation The posterior distribution, fX|Y (x|y) (or PX|Y (x|y)), contains all the knowledge about the unknown quantity X. Therefore, we can use the posterior distribution to find point or interval estimates of X. One way to obtain a point estimate is to choose the value of x that maximizes the posterior PDF (or PMF). This is called the maximum a posteriori (MAP) estimation . Figure 9.3 - The maximum a posteriori (MAP) estimate of X given Y = y is the value of x that maximizes the posterior PDF or PMF . The MAP estimate of X is usually shown by ^xMAP . Maximum A Posteriori (MAP) Estimation The MAP estimate of the random variable X, given that we have observed Y = y, is given by the value of x that maximizes fX|Y (x|y) if X is a continuous random variable,  PX|Y (x|y) if X is a discrete random variable.  The MAP estimate is shown by ^xMAP . To find the MAP estimate, we need to find the value of x that maximizes fX|Y (x|y) = . fY |X(y|x)fX(x) fY (y) Note that fY (y) does not depend on the value of x. Therefore, we can equivalently find the value of x that maximizes fY |X(y|x)fX(x). This can simplify finding the MAP estimate significantly , because finding fY (y) might be complicated. More specifically , finding fY (y) usually is done using the law of total probability , which involves integration or summation, such as the one in Example 9.3 . To find the MAP estimate of X given that we have observed Y = y, we find the value of x that maximizes fY |X(y|x)fX(x). If either X or Y is discrete, we replace its PDF in the above expression by the corresponding PMF . Example 9. 4 Let X be a continuous random variable with the following PDF: fX(x) = ⎧⎪ ⎨ ⎪⎩ 2x if 0 ≤ x ≤ 1 0 otherwise Also, suppose that Y | X = x ∼ Geometric(x). Find the MAP estimate of X given Y = 3. Solution We know that Y | X = x ∼ Geometric(x), so PY |X(y|x) = x(1 − x) y−1,  for y = 1, 2, ⋯ . Therefore, PY |X(3|x) = x(1 − x) 2. We need to find the value of x ∈ [0, 1] that maximizes PY |X(y|x)fX(x) = x(1 − x) 2 ⋅ 2x = 2x 2(1 − x) 2. We can find the maximizing value by dif ferentiation. W e obtain [x 2(1 − x) 2] = 2x(1 − x) 2 − 2(1 − x)x 2 = 0. Solving for x (and checking for maximization criteria), we obtain the MAP estimate as ^xMAP = . d dx 1 2 9.1.3 Comparison to ML Estimation We discussed maximum likelihood estimation in the previous chapter . Assuming that we have observed Y = y, the maximum likelihood (ML) estimate of X is the value of x that maximizes fY |X(y|x) (9.1) We show the ML estimate of X by ^xML. On the other hand, the MAP estimate of X is the value of x that maximizes fY |X(y|x)fX(x) (9.2) The two expressions in Equations 9.1 and 9.2 are somewhat similar . The dif ference is that Equation 9.2 has an extra term, fX(x). For example, if X is uniformly distributed over a finite interval, then the ML and the MAP estimate will be the same. Example 9. 5 Suppose that the signal X ∼ N(0, σ2 X) is transmitted over a communication channel. Assume that the received signal is given by Y = X + W, where W ∼ N(0, σ2 W ) is independent of X. 1 . Find the ML estimate of X, given Y = y is observed. 2 . Find the MAP estimate of X, given Y = y is observed. Solution Here, we have fX(x) = e − . We also have, Y |X = x ∼ N(x, σ2 W ), so 1 √2πσX x2 2σ2 X 2 fY |X(y|x) = e − . 1 . The ML estimate of X, given Y = y, is the value of x that maximizes fY |X(y|x) = e − . To maximize the above function, we should minimize (y − x) 2. Therefore, we conclude ^xML = y. 2 . The MAP estimate of X, given Y = y, is the value of x that maximizes fY |X(y|x)fX(x) = c exp{− [ + ]}, where c is a constant. T o maximize the above function, we should minimize + . By dif ferentiation, we obtain the MAP estimate of x as ^xMAP = y. 1 √2πσW (y−x)2 2σ2 W 1 √2πσW (y−x)2 2σ2 W (y − x) 2 2σ2 W x2 2σ2 X (y − x) 2 2σ2 W x 2 2σ2 X σ2 X σ2 X + σ2 W 9.1.4 Conditional Expectation (MMSE) Remember that the posterior distribution, fX|Y (x|y), contains all the knowledge that we have about the unknown quantity X. Therefore, to find a point estimate of X, we can just choose a summary statistic of the posterior such as its mean, median, or mode. If we choose the mode (the value of x that maximizes fX|Y (x|y)), we obtain the MAP estimate of X. Another option would be to choose the posterior mean, i.e., ^x = E[X|Y = y]. We will show that E[X|Y = y] will give us the best estimate of X in terms of the mean squared error . For this reason, the conditional expectation is called the minimum mean squared error (MMSE) estimate of X. It is also called the least mean squares (LMS) estimate or simply the Bayes' estimate of X. Minimum Mean Squared Error (MMSE) Estimation The minimum mean squared error (MMSE) estimate of the random variable X, given that we have observed Y = y, is given by ^xM = E[X|Y = y]. Example 9. 6 Let X be a continuous random variable with the following PDF fX(x) = ⎧⎪ ⎨ ⎪⎩ 2x if 0 ≤ x ≤ 1 0 otherwise We also know that fY |X(y|x) = ⎧⎪ ⎨ ⎪⎩ 2xy − x + 1 if 0 ≤ y ≤ 1 0 otherwise Find the MMSE estimate of X, given Y = y is observed. Solution First we need to find the posterior density , fX|Y (x|y). W e have fX|Y (x|y) = . We can find fY (y) as fY (y) = ∫ 1 0 fY |X(y|x)fX(x)dx = ∫ 1 0 (2xy − x + 1)2xdx = y + ,  for 0 ≤ y ≤ 1. We conclude fX|Y (x|y) = ,  for 0 ≤ x ≤ 1. The MMSE estimate of X given Y = y is then given by ^xM = E[X|Y = y] = ∫ 1 0 xfX|Y (x|y)dx = ∫ 1 0 6x 2(2xy − x + 1)dx = . fY |X(y|x)fX(x) fY (y) 4 3 1 3 6x(2xy − x + 1) 4y + 1 1 4y + 1 3y + 1 2 4y + 1 9.1.5 Mean Squared Error (MSE) Suppose that we would like to estimate the value of an unobserved random variable X given that we have observed Y = y. In general, our estimate ^x is a function of y: ^x = g(y). The error in our estimate is given by ~ X = X − ^x = X − g(y). Often, we are interested in the mean squared error (MSE) given by E[(X − ^x) 2|Y = y] = E[(X − g(y)) 2|Y = y]. One way of finding a point estimate ^x = g(y) is to find a function g(Y ) that minimizes the mean squared error (MSE). Here, we show that g(y) = E[X|Y = y] has the lowest MSE among all possible estimators. That is why it is called the minimum mean squared error (MMSE) estimate . For simplicity , let us first consider the case that we would like to estimate X without observing anything. What would be our best estimate of X in that case? Let a be our estimate of X. Then, the MSE is given by h(a) = E[(X − a) 2] = EX2 − 2aEX + a 2. This is a quadratic function of a, and we can find the minimizing value of a by differentiation: h ′(a) = −2EX + 2a. Therefore, we conclude the minimizing value of a is a = EX. Now , if we have observed Y = y, we can repeat the above argument. The only difference is that everything is conditioned on Y = y. More specifically , the MSE is given by h(a) = E[(X − a) 2|Y = y] = E[X2|Y = y] − 2aE[X|Y = y] + a 2. Again, we obtain a quadratic function of a, and by dif ferentiation we obtain the MMSE estimate of X given Y = y as ^xM = E[X|Y = y]. Suppose that we would like to estimate the value of an unobserved random variable X , by observing the value of a random variable Y = y. In general, our estimate ^x is a function of y, so we can write ^X = g(Y ). Note that, since Y is a random variable, the estimator ^X = g(Y ) is also a random variable. The error in our estimate is given by ~ X = X − ^X = X − g(Y ), which is also a random variable. W e can then define the mean squared error (MSE) of this estimator by E[(X − ^X) 2] = E[(X − g(Y )) 2]. From our discussion above we can conclude that the conditional expectation ^XM = E[X|Y ] has the lowest MSE among all other estimators g(Y ). Mean Squared Error (MSE) of an Estimator Let ^X = g(Y ) be an estimator of the random variable X, given that we have observed the random variable Y . The mean squared error (MSE) of this estimator is defined as E[(X − ^X) 2] = E[(X − g(Y )) 2]. The MMSE estimator of X, ^XM = E[X|Y ], has the lowest MSE among all possible estimators. P ro p erties o f the Estimatio n Erro r: Here, we would like to study the MSE of the conditional expectation. First, note that E[ ^XM ] = E[E[X|Y ]] = E[X] (by the law of iterated expectations). Therefore, ^XM = E[X|Y ] is an unbiased estimator of X. In other words, for ^XM = E[X|Y ], the estimation error , ~ X, is a zero-mean random variable E[ ~ X] = EX − E[ ^XM ] = 0. Before going any further , let us state and prove a useful lemma. Lemma 9. 1 Define the random variable W = E[ ~ X|Y ]. Let ^XM = E[X|Y ] be the MMSE estimator of X given Y , and let ~ X = X − ^XM be the estimation error . Then, we have a . W = 0. b . For any function g(Y ), we have E[ ~ X ⋅ g(Y )] = 0. Proof: a . We can write W = E[ ~ X|Y ] = E[X − ^XM |Y ] = E[X|Y ] − E[ ^XM |Y ] = ^XM − E[ ^XM |Y ] = ^XM − ^XM = 0. The last line resulted because ^XM is a function of Y , so E[ ^XM |Y ] = ^XM . b . First, note that E[ ~ X ⋅ g(Y )|Y ] = g(Y )E[ ~ X|Y ] = g(Y ) ⋅ W = 0. Next, by the law of iterated expectations, we have E[ ~ X ⋅ g(Y )] = E[E[ ~ X ⋅ g(Y )|Y ]] = 0. We are now ready to state a very interesting property of the estimation error for the MMSE estimator . Namely , we show that the estimation error , ~ X, and ^XM are uncorrelated. T o see this, note that ^ ^ ^ Cov( ~ X, ^XM ) = E[ ~ X ⋅ ^XM ] − E[ ~ X]E[ ^XM ] = E[ ~ X ⋅ ^XM ] (since E[ ~ X] = 0) = E[ ~ X ⋅ g(Y )] (since  ^XM  is a function of Y ) = 0 (by Lemma 9.1). Now , let us look at Var(X). The estimation error is ~ X = X − ^XM , so X = ~ X + ^XM . Since Cov( ~ X, ^XM ) = 0, we conclude Var(X) = Var( ^XM ) + Var( ~ X). (9.3) The above formula can be interpreted as follows. Part of the variance of X is explained by the variance in ^XM . The remaining part is the variance in estimation error . In other words, if ^XM captures most of the variation in X, then the error will be small. Note also that we can rewrite Equation 9.3 as E[X2] − E[X] 2 = E[ ^X2 M ] − E[ ^XM ] 2 + E[ ~ X2] − E[ ~ X] 2. Note that E[ ^XM ] = E[X], E[ ~ X] = 0. We conclude E[X2] = E[ ^X2 M ] + E[ ~ X2]. Some Additional Properties of the MMSE Estimator - The MMSE estimator , ^XM = E[X|Y ], is an unbiased estimator of X, i.e., E[ ^XM ] = EX, E[ ~ X] = 0. - The estimation error , ~ X, and ^XM are uncorrelated Cov( ~ X, ^XM ) = 0. - We have Var(X) = Var( ^XM ) + Var( ~ X), E[X2] = E[ ^X2 M ] + E[ ~ X2]. Let us look at an example to practice the above concepts. This is an example involving jointly normal random variables. Thus, before solving the example, it is useful to remember the properties of jointly normal random variables. Remember that two random variables X and Y are jointly normal if aX + bY has a normal distribution for all a, b ∈ R. As we have seen before, if X and Y are jointly normal random variables with parameters μX, σ2 X, μY , σ2 Y , and ρ, then, given Y = y, X is normally distributed with E[X|Y = y] = μX + ρσX , Var(X|Y = y) = (1 − ρ 2)σ2 X. Example 9. 7 Let X ∼ N(0, 1) and Y = X + W, where W ∼ N(0, 1) is independent of X. a . Find the MMSE estimator of X given Y , ( ^XM ). b . Find the MSE of this estimator , using M SE = E[(X − ^XM ) 2]. c. Check that E[X2] = E[ ^X2 M ] + E[ ~ X2]. y − μY σY Solution Since X and W are independent and normal, Y is also normal. Moreover , X and Y are also jointly normal, since for all a, b ∈ R, we have aX + bY = (a + b)X + bW, which is also a normal random variable. Note also, Cov(X, Y ) = Cov(X, X + W) = Cov(X, X) + Cov(X, W) = Var(X) = 1. Therefore, ρ(X, Y ) = = = . a . The MMSE estimator of X given Y is ^XM = E[X|Y ] = μX + ρσX = . b . The MSE of this estimator is given by E[(X − ^XM ) 2] = E [(X − ) 2] = E [X2 − XY + ] = EX2 − E[X(X + W)] + = EX2 − EX2 − EXEW + = = = . c. Note that E[X2] = 1. Also, Cov(X, Y ) σXσY 1 1 ⋅ √2 1 √2 Y − μY σY Y 2 Y 2 Y 2 4 EY 2 4 EY 2 4 Var(Y ) + (EY ) 2 4 2 + 0 4 1 2 2 E[ ^X2 M ] = = . In the above, we also found M SE = E[ ~ X2] = . Therefore, we have E[X2] = E[ ^X2 M ] + E[ ~ X2]. EY 2 4 1 2 1 2 9.1.6 Linear MMSE Estimation of Random V ariables Suppose that we would like to estimate the value of an unobserved random variable X , given that we have observed Y = y. In general, our estimate ^x is a function of y ^x = g(y). For example, the MMSE estimate of X given Y = y is g(y) = E[X|Y = y]. We might face some dif ficulties if we want to use the MMSE in practice. First, the function g(y) = E[X|Y = y] might have a complicated form. Specifically , if X and Y are random vectors, computing E[X|Y = y] might not be easy . Moreover , to find E[X|Y = y] we need to know fX|Y (y), which might not be easy to find in some problems. T o address these issues, we might want to use a simpler function g(y) to estimate X. In particular , we might want g(y) to be a linear function of y. Suppose that we would like to have an estimator for X of the form ^XL = g(Y ) = aY + b, where a and b are some real numbers to be determined. More specifically , our goal is to choose a and b such that the MSE of the above estimator M SE = E[(X − ^XL) 2] is minimized. W e call the resulting estimator the linear MMSE estimator . The following theorem gives us the optimal values for a and b. Theorem 9. 1 Let X and Y be two random variables with finite means and variances. Also, let ρ be the correlation coef ficient of X and Y . Consider the function h(a, b) = E[(X − aY − b) 2]. Then, 1 . The function h(a, b) is minimized if a = a ∗ = , b = b∗ = EX − aEY . 2 . We have h(a ∗, b∗) = (1 − ρ 2)Var(X). 3 . E[(X − a ∗Y − b∗)Y ] = 0 (orthogonality principle). Proof: We have h(a, b) = E[(X − aY − b) 2] = E[X2 + a 2Y 2 + b2 − 2aXY − 2bX + 2abY ] = EX2 + a 2EY 2 + b2 − 2aEXY − 2bEX + 2abEY . Thus, h(a, b) is a quadratic function of a and b. W e take the derivatives with respect to a and b and set them to zero, so we obtain EY 2 ⋅ a + EY ⋅ b = EXY (9.4) EY ⋅ a + b = EX (9.5) Solving for a and b, we obtain a ∗ = , b∗ = EX − aEY . It can be verified that the above values do in fact minimize h(a, b). Note that Equation 9.5 implies that E[X − a ∗Y − b∗] = 0. Therefore, h(a ∗, b∗) = E[(X − a ∗Y − b∗) 2] = Var(X − a ∗Y − b∗) = Var(X − a ∗Y ) = Var(X) + a ∗2Var(Y ) − 2a ∗Cov(X, Y ) = Var(X) + Var(Y ) − 2 Cov(X, Y ) = Var(X) − = (1 − ρ 2)Var(X). Finally , note that E[(X − a ∗Y − b∗)Y ] = EXY − a ∗EY 2 − b∗EY = 0 (by Equation 9.4). Note that ~ X = X − a ∗Y − b∗ is the error in the linear MMSE estimation of X given Y . From the above theorem, we conclude that ~ Cov(X, Y ) Var(Y ) Cov(X, Y ) Var(Y ) Cov(X, Y ) 2 Var(Y )2 Cov(X, Y ) Var(Y ) Cov(X, Y ) 2 Var(Y ) E[ ~ X] = 0, E[ ~ XY ] = 0. In sum, we can write the linear MMSE estimator of X given Y as ^XL = (Y − EY ) + EX. If ρ = ρ(X, Y ) is the correlation coef ficient of X and Y , then Cov(X, Y ) = ρσXσY , so the above formula can be written as ^XL = (Y − EY ) + EX. Linear MMSE Estimator The linear MMSE estimator of the random variable X, given that we have observed Y , is given by ^XL = (Y − EY ) + EX = (Y − EY ) + EX. The estimation error , defined as ~ X = X − ^XL, satisfies the orthogonality principle : E[ ~ X] = 0, Cov( ~ X, Y ) = E[ ~ XY ] = 0. The MSE of the linear MMSE is given by E[(X − XL) 2] = E[ ~ X2] = (1 − ρ 2)Var(X). Note that to compute the linear MMSE estimates, we only need to know expected values, variances, and the covariance. Let us look at an example. Example 9. 8 Suppose X ∼ Uniform(1, 2), and given X = x, Y is exponential with parameter λ = . Cov(X, Y ) Var(Y ) ρσX σY Cov(X, Y ) Var(Y ) ρσX σY 1 x a . Find the linear MMSE estimate of X given Y . b . Find the MSE of this estimator . c. Check that E[ ~ XY ] = 0. Solution We have ^XL = (Y − EY ) + EX. Therefore, we need to find EX, EY , Var(Y ), and Cov(X, Y ). First, note that we have EX = , and EY = E[E[Y |X]] (law of iterated expectations) = E [X] (since Y |X ∼ Exponential( )) = . EY 2 = E[E[Y 2|X]] (law of iterated expectations) = E [2X2] (since Y |X ∼ Exponential( )) = ∫ 2 1 2x 2dx = . Therefore, Var(Y ) = EY 2 − (EY ) 2 = − = . We also have Cov(X, Y ) Var(Y ) 3 2 1 X 3 2 1 X 14 3 14 3 9 4 29 12 EXY = E[E[XY |X]] (law of iterated expectations) EXY = E[XE[Y |X]] (given X, X is a constant) = E [X ⋅ X] (since Y |X ∼ Exponential( )) = ∫ 2 1 x 2dx = . Thus, Cov(X, Y ) = E[XY ] − (EX)(EY ) = − ⋅ = . a . The linear MMSE estimate of X given Y is ^XL = (Y − EY ) + EX = (Y − ) + = + . b . The MSE of ^XL is M SE = (1 − ρ 2)Var(X). Since X ∼ Uniform(1, 2), Var(X) = . Also, ρ 2 = = . Thus, M SE = (1 − ) = . c. We have ~ X = X − ^XL = X − − . 1 X 7 3 7 3 3 2 3 2 1 12 Cov(X, Y ) Var(Y ) 1 29 3 2 3 2 Y 29 42 29 1 12 Cov 2(X, Y ) Var(X)Var(Y ) 1 29 1 29 1 12 7 87 Y 29 42 29 Therefore, E[ ~ XY ] = E [(X − − ) Y ] = E[XY ] − − EY = − − ⋅ = 0. Y 29 42 29 EY 2 29 42 29 7 3 14 3 ⋅ 29 42 29 3 2 9.1.7 Estimation for Random V ectors The examples that we have seen so far involved only two random variables X and Y . In practice, we often need to estimate several random variables and we might observe several random variables. In other words, we might want to estimate the value of an unobserved random vector X: X = ⎡ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎣ X1 X2 . . . Xm ⎤ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎦ , given that we have observed the random vector Y, Y = ⎡ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎣ Y1 Y2 . . . Yn ⎤ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎦ . Almost everything that we have discussed can be extended to the case of random vectors. For example, to find the MMSE estimate of X given Y = y, we can write ^XM = E[X|Y] = ⎡ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎣ E[X1|Y1, Y2, ⋯ , Yn] E[X2|Y1, Y2, ⋯ , Yn] . . . E[Xm|Y1, Y2, ⋯ , Yn] ⎤ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎦ . However , the above conditional expectations might be too complicated computationally . Therefore, for random vectors, it is very common to consider simpler estimators such as the linear MMSE. Let's now discuss linear MMSE for random vectors. L in ea r MMSE fo r Ra n do m Vec to rs: Suppose that we would like to have an estimator for the random vector X in the form of ^XL = AY + b, where A and b are fixed matrices to be determined. Remember that for two random variables X and Y , the linear MMSE estimator of X given Y is ^XL = (Y − EY ) + EX = (Y − EY ) + EX. We can extend this result to the case of random vectors. More specifically , we can show that the linear MMSE estimator of the random vector X given the random vector Y is given by ^XL = CXYCY−1(Y − E[Y]) + E[X]. In the above equation, CY is the covariance matrix of Y, defined as CY = E[(Y − EY)(Y − EY) T ], and CXY is the cross covariance matrix of X and Y, defined as CXY = E[(X − EX)(Y − EY) T ]. The above calculations can easily be done using MA TLAB or other packages. However , it is sometimes easier to use the orthogonality principle to find ^XL. W e now explain how to use the orthogonality principle to find linear MMSE estimators. Usin g the Ortho go n a lity Prin c ip le to F in d L in ea r MMSE Estima to rs fo r Ra n do m Vec to rs: Suppose that we are estimating a vector X: X = ⎡ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎣ X1 X2 . . . Xm ⎤ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎦ given that we have observed the random vector Y. Let ^ Cov(X, Y ) Var(Y ) Cov(X, Y ) Cov(Y , Y ) ^XL = ⎡ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎣ ^X1 ^X2 . . . ^Xm ⎤ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎦ be the vector estimate. W e define the MSE as M SE = m ∑ k=1 E[(Xk − ^Xk) 2]. Therefore, to minimize the MSE, it suf fices to minimize each E[(Xk − ^Xk) 2] individually . This means that we only need to discuss estimating a random variable X given that we have observed the random vector Y. Since we would like our estimator to be linear , we can write ^XL = n ∑ k=1 akYk + b. The error in our estimate ~ X is then given by ~ X = X − ^XL = X − n ∑ k=1 akYk − b. Similar to the proof of Theorem 9.1 , we can show that the linear MMSE should satisfy E[ ~ X] = 0, Cov( ~ X, Yj) = E[ ~ XYj] = 0,  for all j = 1, 2, ⋯ , n. The above equations are called the orthogonality principle . The orthogonality principle is often stated as follows: The error ( ~ X) must be orthogonal to the observations ( Y1, Y2, ⋯, Yn). Note that there are n + 1 unknowns (a1, a2, ⋯, an and b) and n + 1 equations. Let us look at an example to see how we can apply the orthogonality principle. Example 9. 9 Let X be an unobserved random variable with EX = 0, Var(X) = 4. Assume that we have observed Y1 and Y2 given by Y1 = X + W1, Y2 = X + W2, where EW1 = EW2 = 0, Var(W1) = 1, and Var(W2) = 4. Assume that W1, W2 , and X are independent random variables. Find the linear MMSE estimator of X, given Y1 and Y2. Solution The linear MMSE of X given Y has the form ^XL = aY1 + bY2 + c. We use the orthogonality principle. W e have E[ ~ X] = aEY1 + bEY2 + c = a ⋅ 0 + b ⋅ 0 + c = c. Using E[ ~ X] = 0, we conclude c = 0. Next, we note Cov( ^XL, Y1) = Cov(aY1 + bY2, Y1) = aCov(Y1, Y1) + bCov(Y1, Y2) = aCov(X + W1, X + W1) + bCov(X + W1, X + W2) = a(Var(X) + Var(W1)) + bVar(X) = 5a + 4b. Similarly , we find Cov( ^XL, Y2) = Cov(aY1 + bY2, Y2) = aVar(X) + b(Var(X) + Var(W2)) = 4a + 8b. We need to have Cov( ~ X, Yj) = 0,  for j = 1, 2, which is equivalent to Cov( ^XL, Yj) = Cov(X, Yj),  for j = 1, 2. Since Cov(X, Y1) = Cov(X, Y2) = Var(X) = 4, we conclude 5a + 4b = 4, 4a + 8b = 4. Solving for a and b, we obtain a = , and b = . Therefore, the linear MMSE estimator of X, given Y1 and Y2, is 2 3 1 6 ^XL = Y1 + Y2. 2 3 1 6 9.1.8 Bayesian Hypothesis T esting Suppose that we need to decide between two hypotheses H0 and H1. In the Bayesian setting, we assume that we know prior probabilities of H0 and H1. That is, we know P(H0) = p0 and P(H1) = p1, where p0 + p1 = 1. We observe the random variable (or the random vector) Y . We know the distribution of Y under the two hypotheses, i.e, we know fY (y|H0), and fY (y|H1). Using Bayes' rule, we can obtain the posterior probabilities of H0 and H1: P(H0|Y = y) = , P(H1|Y = y) = . One way to decide between H0 and H1 is to compare P(H0|Y = y) and P(H1|Y = y), and accept the hypothesis with the higher posterior probability . This is the idea behind the maximum a posteriori (MAP) test . Here, since we are choosing the hypothesis with the highest probability , it is relatively easy to show that the error probability is minimized. To be more specific, according to the MAP test, we choose H0 if and only if P(H0|Y = y) ≥ P(H1|Y = y). In other words, we choose H0 if and only if fY (y|H0)P(H0) ≥ fY (y|H1)P(H1). Note that as always, we use the PMF instead of the PDF if Y is a discrete random variable. W e can generalize the MAP test to the case where you have more than two hypotheses. In that case, again we choose the hypothesis with the highest posterior probability . fY (y|H0)P(H0) fY (y) fY (y|H1)P(H1) fY (y) MAP Hypothesis T est Choose the hypothesis with the highest posterior probability , P(Hi|Y = y). Equivalently , choose hypothesis Hi with the highest fY (y|Hi)P(Hi). Example 9. 10 Suppose that the random variable X is transmitted over a communication channel. Assume that the received signal is given by Y = X + W, where W ∼ N(0, σ2) is independent of X. Suppose that X = 1 with probability p, and X = −1 with probability 1 − p. The goal is to decide between X = 1 and X = −1 by observing the random variable Y . Find the MAP test for this problem. Solution Here, we have two hypotheses: H0: X = 1, H1: X = −1. Under H0, Y = 1 + W , so Y |H0 ∼ N(1, σ2). Therefore, fY (y|H0) = e − . Under H1, Y = −1 + W , so Y |H1 ∼ N(−1, σ2). Therefore, fY (y|H1) = e − . Thus, we choose H0 if and only if 1 σ√2π (y−1)2 2σ2 1 σ√2π (y+1)2 2σ2 2 2 e − P(H0) ≥ e − P(H1). We have P(H0) = p, and P(H1) = 1 − p. Therefore, we choose H0 if and only if exp( ) ≥ . Equivalently , we choose H0 if and only if y ≥ ln( ). Note that the average error probability for a hypothesis test can be written as Pe = P(choose H1|H0)P(H0) + P(choose H0|H1)P(H1). (9.6) As we mentioned earlier , the MAP test achieves the minimum possible average error probability . Example 9. 1 1 Find the average error probability in Example 9.10 Solution in Example 9.10 , we arrived at the following decision rule: W e choose H0 if and only if y ≥ c, where c = ln( ). Since Y |H0 ∼ N(1, σ2), P(choose H1|H0) = P(Y < c|H0) = Φ ( ) = Φ ( ln( ) − ) . 1 σ√2π (y−1)2 2σ2 1 σ√2π (y+1)2 2σ2 2y σ2 1 − p p σ2 2 1 − p p σ2 2 1 − p p c − 1 σ σ 2 1 − p p 1 σ Since Y |H1 ∼ N(−1, σ2), P(choose H0|H1) = P(Y ≥ c|H1) = 1 − Φ ( ) = 1 − Φ ( ln( ) + ) . Figure 9.4 shows the two error probabilities for this example. Therefore, the average error probability is given by Pe = P(choose H1|H0)P(H0) + P(choose H0|H1)P(H1) = p ⋅ Φ ( ln( ) − ) + (1 − p) ⋅ [1 − Φ ( ln( ) + )] . Figure 9.4 - Error probabilities for Example 9.10 and Example 9.1 1 Min imum C o st Hyp o th e sis Te st: Suppose that you are building a sensor network to detect fires in a forest. Based on the information collected by the sensors, the system needs to decide between two opposing hypotheses: H0: There is no fire, H1: There is a fire. There are two possible types of errors that we can make: W e might accept H0 while H1 is true, or we might accept H1 while H0 is true. Note that the cost associated with these two errors are not the same. In other words, if there is a fire and we miss it, we will be making a costlier error . To address situations like this, we associate a cost to c + 1 σ σ 2 1 − p p 1 σ σ 2 1 − p p 1 σ σ 2 1 − p p 1 σ each error type: C10: The cost of choosing H1, given that H0 is true. C01: The cost of choosing H0, given that H1 is true. Then, the average cost can be written as C = C10P(choose H1|H0)P(H0) + C01P(choose H0|H1)P(H1). The goal of minimum cost hypothesis testing is to minimise the above expression. Luckily , this can be done easily . Note that we can rewrite the average cost as C = P(choose H1|H0) ⋅ [P(H0)C10] + P(choose H0|H1) ⋅ [P(H1)C01]. The above expression is very similar to the average error probability of the MAP test (Equation 9.6 ). The only dif ference is that we have p(H0)C10 instead of P(H0), and we have p(H1)C01 instead of P(H1). Therefore, we can use a decision rule similar to the MAP decision rule. More specifically , we choose H0 if and only if fY (y|H0)P(H0)C10 ≥ fY (y|H1)P(H1)C01 (9.7) Here is another way to interpret the above decision rule. If we divide both sides of Equation 9.7 by fY (y) and apply Bayes' rule, we conclude the following: W e choose H0 if and only if P(H0|y)C10 ≥ P(H1|y)C01. Note that P(H0|y)C10 is the expected cost of accepting H1. We call this the posterior risk of accepting H1. Similarly , P(H1|y)C01 is the posterior risk (expected cost) of accepting H0. Therefore, we can summarize the minimum cost test as follows: W e accept the hypothesis with the lowest posterior risk. Minimum Cost Hypothesis T est Assuming the following costs C10: The cost of choosing H1, given that H0 is true. C01: The cost of choosing H0, given that H1 is true. We choose H0 if and only if ≥ . Equivalently , we choose H0 if and only if P(H0|y)C10 ≥ P(H1|y)C01. Example 9. 12 A surveillance system is in charge of detecting intruders to a facility . There are two hypotheses to choose from: H0: No intruder is present. H1: There is an intruder . The system sends an alarm message if it accepts H1. Suppose that after processing the data, we obtain P(H1|y) = 0.05. Also, assume that the cost of missing an intruder is 10 times the cost of a false alarm. Should the system send an alarm message (accept H1)? Solution First note that P(H0|y) = 1 − P(H1|y) = 0.95 The posterior risk of accepting H1 is fY (y|H0) fY (y|H1) P(H1)C01 P(H0)C10 P(H0|y)C10 = 0.95C10. We have C01 = 10C10, so the posterior risk of accepting H0 is P(H1|y)C01 = (0.05)(10C10) = 0.5C10. Since P(H0|y)C10 ≥ P(H1|y)C01, we accept H0, so no alarm message needs to be sent. 9.1.9 Bayesian Interval Estimation Interval estimation has a very natural interpretation in Bayesian inference. Suppose that we would like to estimate the value of an unobserved random variable X, given that we have observed Y = y. After calculating the posterior density fX|Y (x|y), we can simply find an interval [a, b] for which we have P(a ≤ X ≤ b|Y = y) = 1 − α. Such an interval is said to be a (1 − α)100% credible interval for X. Bayesian Credible Intervals Given the observation Y = y, the interval [a, b] is said to be a (1 − α)100% credible interval for X, if the posterior probability of X being in [a, b] is equal to 1 − α. In other words, P(a ≤ X ≤ b|Y = y) = 1 − α. Example 9. 13 Let X and Y be jointly normal and X ∼ N(0, 1), Y ∼ N(1, 4), and ρ(X, Y ) = . Find a 95% credible interval for X, given Y = 2 is observed. Solution As we have seen before, if X and Y are jointly normal random variables with parameters μX, σ2 X, μY , σ2 Y , and ρ, then, given Y = y, X is normally distributed with E[X|Y = y] = μX + ρσX , Var(X|Y = y) = (1 − ρ 2)σ2 X. Therefore, X|Y = 2 is normal with 1 2 y − μY σY E[X|Y = y] = 0 + ⋅ = , Var(X|Y = y) = (1 − ) ⋅ 1 = . Here α = 0.05, so we need an interval [a, b] for which P(a ≤ X ≤ b|Y = 2) = 0.95 We usually choose a symmetric interval around the expected value E[X|Y = y] = . That is, we choose the interval in the form of [ − c, + c] . Thus, we need to have P ( − c ≤ X ≤ + c|Y = 2) = Φ ( ) − Φ ( ) = 2Φ ( ) − 1 = 0.95 Solving for c, we obtain c = √3/4Φ−1(0.975) ≈ 1.70 Therefore, the 95% credible interval for X is [ − c, + c] ≈ [−1.45, 1.95]. 1 2 2 − 1 2 1 4 1 4 3 4 1 4 1 4 1 4 1 4 1 4 c √3/4 −c √3/4 c √3/4 1 4 1 4 9.2.0 End of Chapter Problems Problem 1 Let X be a continuous random variable with the following PDF fX(x) = ⎧⎪ ⎨ ⎪⎩ 6x(1 − x) if 0 ≤ x ≤ 1 0 otherwise Suppose that we know Y | X = x ∼ Geometric(x). Find the posterior density of X given Y = 2, fX|Y (x|2). Problem 2 Let X be a continuous random variable with the following PDF fX(x) = ⎧⎪ ⎨ ⎪⎩ 3x 2 if 0 ≤ x ≤ 1 0 otherwise Also, suppose that Y | X = x ∼ Geometric(x). Find the MAP estimate of X given Y = 5. Problem 3 Let X and Y be two jointly continuous random variables with joint PDF fXY (x, y) = ⎧⎪ ⎪ ⎨ ⎪ ⎪⎩ x + y2 0 ≤ x, y ≤ 1 0 otherwise. 3 2 Find the MAP and the ML estimates of X given Y = y. Problem 4 Let X be a continuous random variable with the following PDF fX(x) = ⎧⎪ ⎪ ⎨ ⎪ ⎪⎩ 2x2 + if 0 ≤ x ≤ 1 0 otherwise We also know that fY |X(y|x) = ⎧⎪ ⎨ ⎪⎩ xy − + 1 if 0 ≤ y ≤ 1 0 otherwise Find the MMSE estimate of X, given Y = y is observed. Problem 5 Let X ∼ N(0, 1) and Y = 2X + W, where W ∼ N(0, 1) is independent of X. a . Find the MMSE estimator of X given Y , ( ^XM ). b . Find the MSE of this estimator , using M SE = E[(X − ^XM ) 2]. c. Check that E[X2] = E[ ^X2 M ] + E[ ~ X2]. Problem 6 Suppose X ∼ Uniform(0, 1), and given X = x, Y ∼ Exponential(λ = ). a . Find the linear MMSE estimate of X given Y . b . Find the MSE of this estimator . c. Check that E[ ~ XY ] = 0. 1 3 x 2 1 2x Problem 7 Suppose that the signal X ∼ N(0, σ2 X) is transmitted over a communication channel. Assume that the received signal is given by Y = X + W, where W ∼ N(0, σ2 W ) is independent of X. a . Find the MMSE estimator of X given Y , ( ^XM ). b . Find the MSE of this estimator . Problem 8 Let X be an unobserved random variable with EX = 0, Var(X) = 5. Assume that we have observed Y1 and Y2 given by Y1 = 2X + W1, Y2 = X + W2, where EW1 = EW2 = 0, Var(W1) = 2, and Var(W2) = 5. Assume that W1, W2 , and X are independent random variables. Find the linear MMSE estimator of X, given Y1 and Y2. Problem 9 Consider again Problem 8 , in which X is an unobserved random variable with EX = 0, Var(X) = 5. Assume that we have observed Y1 and Y2 given by Y1 = 2X + W1, Y2 = X + W2, where EW1 = EW2 = 0, Var(W1) = 2, and Var(W2) = 5. Assume that W1, W2 , and X are independent random variables. Find the linear MMSE estimator of X, given Y1 and Y2, using the vector formula ^XL = CXYCY−1(Y − E[Y]) + E[X]. Problem 10 Let X be an unobserved random variable with EX = 0, Var(X) = 5. Assume that we have observed Y1, Y2, and Y3 given by Y1 = 2X + W1, Y2 = X + W2, Y3 = X + 2W3, where EW1 = EW2 = EW3 = 0, Var(W1) = 2, Var(W2) = 5, and Var(W3) = 3. Assume that W1, W2, W3, and X are independent random variables. Find the linear MMSE estimator of X, given Y1, Y2, and Y3. Problem 1 1 Consider two random variables X and Y with the joint PMF given by the table below .   Y = 0 Y = 1 X = 0 X = 1 0 a . Find the linear MMSE estimator of X given Y , ( ^XL). b . Find the MMSE estimator of X given Y , ( ^XM ). c. Find the MSE of ^XM . Problem 12 Consider two random variables X and Y with the joint PMF given by the table below .   Y = 0 Y = 1 Y = 2 X = 0 0 X = 1 0 1 7 3 7 3 7 1 6 1 3 1 3 1 6 a . Find the linear MMSE estimator of X given Y , ( ^XL). b . Find the MSE of ^XL. c. Find the MMSE estimator of X given Y , ( ^XM ). d. Find the MSE of ^XM . Problem 13 Suppose that the random variable X is transmitted over a communication channel. Assume that the received signal is given by Y = 2X + W, where W ∼ N(0, σ2) is independent of X. Suppose that X = 1 with probability p, and X = −1 with probability 1 − p. The goal is to decide between X = −1 and X = 1 by observing the random variable Y . Find the MAP test for this problem. Problem 14 Find the average error probability in Problem 13 . Problem 15 A monitoring system is in charge of detecting malfunctioning machinery in a facility . There are two hypotheses to choose from: H0: There is not a malfunction, H1: There is a malfunction. The system notifies a maintenance team if it accepts H1. Suppose that, after processing the data, we obtain P(H1|y) = 0.10. Also, assume that the cost of missing a malfunction is 30 times the cost of a false alarm. Should the system alert a maintenance team (accept H1)? Problem 16 Let X and Y be jointly normal and X ∼ N(2, 1), Y ∼ N(1, 5), and ρ(X, Y ) = . Find a 90% credible interval for X, given Y = 1 is observed. Problem 17 When the choice of a prior distribution is subjective, it is often advantageous to choose a prior distribution that will result in a posterior distribution of the same distributional family . When the prior and posterior distributions share the same distributional family , they are called conjugate distributions , and the prior is called a conjugate prior . Conjugate priors are used out of ease because they always result in a closed form posterior distribution. One example of this is to use a gamma prior for Poisson distributed data. Assume our data Y given X is distributed Y | X = x ∼ Poisson(λ = x) and we chose the prior to be X ∼ Gamma(α, β). Then the PMF for our data is PY |X(y|x) = , for x > 0, y ∈ {0, 1, 2, …}, and the PDF of the prior is given by fX(x) = , for x > 0, α, β > 0. a . Show that the posterior distribution is Gamma(α + y, β + 1). ( Hint: Remove all the terms not containing x by putting them into some normalizing constant, c, and noting that fX|Y (x|y) ∝ PY |X(y|x)fX(x). ) b . Write out the PDF for the posterior distribution, fX|Y (x|y). c. Find mean and variance of the posterior distribution, E[X|Y ] and Var(X|Y ). Problem 18 Assume our data Y given X is distributed Y | X = x ∼ Binomial(n, p = x) and we chose the prior to be X ∼ Beta(α, β). Then the PMF for our data is 1 4 e −xx y y! βαx α−1e −βx Γ(α) PY |X(y|x) = ( )x y(1 − x) n−y, for x ∈ [0, 1], y ∈ {0, 1, … , n}, and the PDF of the prior is given by fX(x) = x α−1(1 − x) β−1, for 0 ≤ x ≤ 1, α > 0, β > 0. Note that, EX = and Var(X) = . a . Show that the posterior distribution is Beta(α + y, β + n − y). b . Write out the PDF for the posterior distribution, fX|Y (x|y). c. Find mean and variance of the posterior distribution, E[X|Y ] and Var(X|Y ). Problem 19 Assume our data Y given X is distributed Y | X = x ∼ Geometric(p = x) and we chose the prior to be X ∼ Beta(α, β). Refer to Problem 18 for the PDF and moments of the Beta distribution. a . Show that the posterior distribution is Beta(α + 1, β + y − 1). b . Write out the PDF for the posterior distribution, fX|Y (x|y). c. Find mean and variance of the posterior distribution, E[X|Y ] and Var(X|Y ). Problem 20 Assume our data Y = (y1, y2, … , yn) T given X is independently identically distributed, Y | X = x i.i.d. ∼ Exponential(λ = x), and we chose the prior to be X ∼ Gamma(α, β). a . Find the likelihood of the function, L(Y; X) = fY1,Y2,…,Yn|X(y1, y2, … , yn|x). b . Using the likelihood function of the data, show that the posterior distribution is Gamma(α + n, β + ∑n i=1 yi). c. Write out the PDF for the posterior distribution, fX|Y(x|y). d. Find mean and variance of the posterior distribution, E[X|Y] and Var(X|Y). n y Γ(α + β) Γ(α)Γ(β) α α+β αβ (α+β)2(α+β+1) 10.1.0 Basic Concepts In real-life applications, we are often interested in multiple observations of random values over a period of time. For example, suppose that you are observing the stock price of a company over the next few months. In particular , let S(t) be the stock price at time t ∈ [0, ∞). Here, we assume t = 0 refers to current time. Figure 10.1 shows a possible outcome of this random experiment from time t = 0 to time t = 1. Figure 10.1 - A possible realization of values of a stock observed as a function of time. Here, S(t) is an example of a random process. Note that at any fixed time t1 ∈ [0, ∞), S(t1) is a random variable. Based on your knowledge of finance and the historical data, you might be able to provide a PDF for S(t1). If you choose another time t2 ∈ [0, ∞), you obtain another random variable S(t2) that could potentially have a dif ferent PDF . When we consider the values of S(t) for t ∈ [0, ∞) collectively , we say S(t) is a random process or a stochastic process . We may show this process by {S(t), t ∈ [0, ∞)}. Therefore, a random process is a collection of random variables usually indexed by time (or sometimes by space). A random process is a collection of random variables usually indexed by time. The process S(t) mentioned here is an example of a continuous-time random process. In general, when we have a random process X(t) where t can take real values in an interval on the real line, then X(t) is a continuous-time random process. Here are a few more examples of continuous-time random processes: − Let N(t) be the number of customers who have visited a bank from t = 9 (when the bank opens at 9:00 am) until time t, on a given day , for t ∈ [9, 16]. Here, we measure t in hours, but t can take any real value between 9 and 16. We assume that N(9) = 0, and N(t) ∈ {0, 1, 2, . . . } for all t ∈ [9, 16]. Note that for any time t1, the random variable N(t1) is a discrete random variable. Thus, N(t) is a discrete-valued random process. However , since t can take any real value between 9 and 16, N(t) is a continuous-time random process. − Let W(t) be the thermal noise voltage generated across a resistor in an electric circuit at time t, for t ∈ [0, ∞). Here, W(t) can take real values. − Let T(t) be the temperature in New Y ork City at time t ∈ [0, ∞). We can assume here that t is measured in hours and t = 0 refers to the time we start measuring the temperature. In all of these examples, we are dealing with an uncountable number of random variables. For example, for any given t1 ∈ [9, 16], N(t1) is a random variable. Thus, the random process N(t) consists of an uncountable number of random variables. A random process can be defined on the entire real line, i.e., t ∈ (−∞, ∞). In fact, it is sometimes convenient to assume that the process starts at t = −∞ even if we are interested in X(t) only on a finite interval. For example, we can assume that the T(t) defined above is a random process defined for all t ∈ R although we get to observe only a finite portion of it. On the other hand, you can have a discrete-time random process. A discrete-time random process is a process {X(t), t ∈ J}, where J is a countable set. Since J is countable, we can write J = {t1, t2, ⋯}. We usually define X(tn) = X(n) or X(tn) = Xn, for n = 1, 2, ⋯, (the index values n could be from any countable set such as N or Z). Therefore, a discrete-time random process is just a sequence of random variables. For this reason, discrete-time random processes are sometimes referred to as random sequences . We can denote such a discrete-time process as {X(n), n = 0, 1, 2, …}  or  {Xn, n = 0, 1, 2, …}. Or, if the process is defined for all integers, then we may show the process by {X(n), n ∈ Z}  or  {Xn, n ∈ Z}. Here is an example of a discrete-time random process. Suppose that we are observing customers who visit a bank starting at a given time. Let Xn for n ∈ N be the amount of time the ith customer spends at the bank. This process consists of a countable number of random variables X1, X2, X3, . . . Thus, we say that the process {Xn, n = 1, 2, 3..} is a discrete-time random process. Discrete-time processes are sometimes obtained from continuous-time processes by discretizing time (sampling at specific times). For example, if you only record the temperature in New Y ork City once a day (let's say at noon), then you can define a process X1 = T(12) (temperature at noon on day 1, t = 12) X2 = T(36) (temperature at noon on day 2, t = 12 + 24) X3 = T(60) (temperature at noon on day 3, t = 12 + 24 + 24) . . . And, in general, Xn = T(tn) where tn = 24(n − 1) + 12 for n ∈ N. Here, Xn is a discrete-time random process. Figure 10.2 shows a possible realization of this random process. Figure 10.2 - Possible realization of the random process {Xn, n = 1, 2, 3, ⋯ } where Xn shows the temperature in New Y ork City at noon on day n. A continuous-time random process is a random process {X(t), t ∈ J}, where J is an interval on the real line such as [−1, 1], [0, ∞), (−∞, ∞), etc. A discrete-time random process (or a random sequence ) is a random process {X(n) = Xn, n ∈ J}, where J is a countable set such as N or Z. Ra n do m Pro c esses a s Ra n do m Fun c tio n s: Consider a random process {X(t), t ∈ J}. This random process is resulted from a random experiment, e.g., observing the stock prices of a company over a period of time. Remember that any random experiment is defined on a sample space S. After observing the values of X(t), we obtain a function of time such as the one showed in Figure 10.1 . The function shown in this figure is just one of the many possible outcomes of this random experiment. W e call each of these possible functions of X(t) a sample function or sample path . It is also called a realization of X(t). From this point of view , a random process can be thought of as a random function of time. You are familiar with the concept of functions. The dif ference here is that {X(t), t ∈ J} will be equal to one of many possible sample functions after we are done with our random experiment. In engineering applications, random processes are often referred to as random signals . A random process is a random function of time. Example 10. 1 You have 1000 dollars to put in an account with interest rate R, compounded annually . That is, if Xn is the value of the account at year n, then Xn = 1000(1 + R) n,  for n = 0, 1, 2, ⋯ . The value of R is a random variable that is determined when you put the money in the bank, but it does not not change after that. In particular , assume that R ∼ Uniform(0.04, 0.05). a . Find all possible sample functions for the random process {Xn, n = 0, 1, 2, . . . }. b . Find the expected value of your account at year three. That is, find E[X3]. Solution a . Here, the randomness in Xn comes from the random variable R. As soon as you know R, you know the entire sequence Xn for n = 0, 1, 2, ⋯. In particular , if R = r, then Xn = 1000(1 + r) n,  for all n ∈ {0, 1, 2, ⋯}. Thus, here sample functions are of the form f(n) = 1000(1 + r) n, n = 0, 1, 2, ⋯, where r ∈ [0.04, 0.05]. For any r ∈ [0.04, 0.05], you obtain a sample function for the random process Xn. b . The random variable X3 is given by X3 = 1000(1 + R) 3. If you let Y = 1 + R, then Y ∼ Uniform(1.04, 1.05), so fY (y) = { 100 1.04 ≤ y ≤ 1.05 0 otherwise To obtain E[X3], we can write E[X3] = 1000E[Y 3] = 1000 ∫ 1.05 1.04 100y3 dy (by LOTUS) = [y4] 1.05 1.04 = [(1.05) 4 − (1.04) 4] ≈ 1, 141.2 Example 10. 2 Let {X(t), t ∈ [0, ∞)} be defined as X(t) = A + Bt,  for all t ∈ [0, ∞), where A and B are independent normal N(1, 1) random variables. a . Find all possible sample functions for this random process. b . Define the random variable Y = X(1). Find the PDF of Y . c. Let also Z = X(2). Find E[Y Z]. Solution a . Here, we note that the randomness in X(t) comes from the two random variables A and B. The random variable A can take any real value a ∈ R. The random variable B can also take any real value b ∈ R. As soon as we know the values of A and B, the entire process X(t) is known. In particular , if A = a and B = b, then 105 4 105 4 X(t) = a + bt,  for all t ∈ [0, ∞). Thus, here, sample functions are of the form f(t) = a + bt, t ≥ 0, where a, b ∈ R. For any a, b ∈ R you obtain a sample function for the random process X(t). b . We have Y = X(1) = A + B. Since A and B are independent N(1, 1) random variables, Y = A + B is also normal with EY = E[A + B] = E[A] + E[B] = 1 + 1 = 2, Var(Y ) = Var(A + B) = Var(A) + Var(B) (since A and B are independent) = 1 + 1 = 2. Thus, we conclude that Y ∼ N(2, 2): fY (y) = e − . c. We have E[Y Z] = E[(A + B)(A + 2B)] = E[A2 + 3AB + 2B 2] = E[A2] + 3E[AB] + 2E[B 2] = 2 + 3E[A]E[B] + 2 ⋅ 2 (since A and B are independent) = 9. The random processes in the above examples were relatively simple in the sense that the randomness in the process originated from one or two random variables. W e will see more complicated examples later on. 1 √4π (y−2)2 4 10.1.1 PDFs and CDFs Consider the random process {X(t), t ∈ J}. For any t0 ∈ J, X(t0) is a random variable, so we can write its CDF FX(t0)(x) = P(X(t0) ≤ x). If t1, t2 ∈ J, then we can find the joint CDF of X(t1) and X(t2) by FX(t1)X(t2)(x1, x2) = P(X(t1) ≤ x1, X(t2) ≤ x2). More generally for t1, t2, ⋯ , tn ∈ J, we can write FX(t1)X(t2)⋯X(tn)(x1, x2, ⋯ , xn) = P(X(t1) ≤ x1, X(t2) ≤ x2, ⋯ , X(tn) ≤ xn). Similarly , we can write joint PDFs or PMFs depending on whether X(t) is continuous- valued (the X(ti)'s are continuous random variables) or discrete-valued (the X(ti)'s are discrete random variables). Example 10. 3 Consider the random process {Xn, n = 0, 1, 2, ⋯}, in which Xi's are i.i.d. standard normal random variables. 1 . Write down fXn(x) for n = 0, 1, 2, ⋯. 2 . Write down fXmXn(x1, x2) for m ≠ n. Solution 1 . Since Xn ∼ N(0, 1), we have fXn (x) = e − ,  for all x ∈ R. 2 . If m ≠ n, then Xm and Xn are independent (because of the i.i.d. assumption), so 1 √2π x2 2 fXmXn (x1, x2) = fXm(x1)fXn (x2) = e − ⋅ e − = exp{− },  for all x1, x2 ∈ R. 1 √2π x2 1 2 1 √2π x2 2 2 1 2π x 2 1 + x 2 2 2 10.1.2 Mean and Correlation Functions Since random processes are collections of random variables, you already possess the theoretical knowledge necessary to analyze random processes. From now on, we would like to discuss methods and tools that are useful in studying random processes. Remember that expectation and variance were among the important statistics that we considered for random variables. Here, we would like to extend those concepts to random processes. Mea n Fun c ti o n o f a Ra n do m Pro c ess: Mean Function of a Random Process For a random process {X(t), t ∈ J}, the mean function μX(t) : J → R, is defined as μX(t) = E[X(t)] The above definition is valid for both continuous-time and discrete-time random processes. In particular , if {Xn, n ∈ J} is a discrete-time random process, then μX(n) = E[Xn],  for all n ∈ J. Some books show the mean function by mX(t) or MX(t). Here, we chose μX(t) to avoid confusion with moment generating functions. The mean function gives us an idea about how the random process behaves on average as time evolves. For example, if X(t) is the temperature in a certain city , the mean function μX(t) might look like the function shown in Figure 10.3. As we see, the expected value of X(t) is lowest in the winter and highest in summer . Figure 10.3 - The mean function, μX(t), for the temperature in a certain city . Example 10. 4 Find the mean functions for the random processes given in Examples 10.1 and 10.2 . Solution For {Xn, n = 0, 1, 2, ⋯ } given in Example 10.1 , we have μX(n) = E[Xn] = 1000E[Y n] (where Y = 1 + R ∼ Uniform(1.04, 1.05)) = 1000 ∫ 1.05 1.04 100yn dy (by LOTUS) = [yn+1] 1.05 1.04 = [(1.05) n+1 − (1.04) n+1],  for all n ∈ {0, 1, 2, ⋯}. For {X(t), t ∈ [0, ∞)} given in Example 10.2 , we have μX(t) = E[X(t)] = E[A + Bt] = E[A] + E[B]t = 1 + t,  for all t ∈ [0, ∞). A uto c o rrel a ti o n a n d A uto c o va ri a n c e: 105 n + 1 105 n + 1 The mean function μX(t) gives us the expected value of X(t) at time t, but it does not give us any information about how X(t1) and X(t2) are related. T o get some insight on the relation between X(t1) and X(t2), we define correlation and covariance functions. For a random process {X(t), t ∈ J}, the autocorrelation function or , simply , the correlation function , RX(t1, t2), is defined by RX(t1, t2) = E[X(t1)X(t2)], for t1, t2 ∈ J. For a random process {X(t), t ∈ J}, the autocovariance function or , simply , the covariance function , CX(t1, t2), is defined by CX(t1, t2) = Cov(X(t1), X(t2)) = RX(t1, t2) − μX(t1)μX(t2), for t1, t2 ∈ J. Note that if we let t1 = t2 = t, we obtain RX(t, t) = E[X(t)X(t)] = E[X(t) 2], for t ∈ J; CX(t, t) = Cov(X(t), X(t)) = Var(X(t)), for t ∈ J. If t1 ≠ t2, then the covariance function CX(t1, t2) gives us some information about how X(t1) and X(t2) are statistically related. In particular , note that CX(t1, t2) = E[(X(t1) − E[X(t1)])(X(t2) − E[X(t2)])]. Intuitively , CX(t1, t2) shows how X(t1) and X(t2) move relative to each other . If large values of X(t1) tend to imply large values of X(t2), then (X(t1) − E[X(t1)])(X(t2) − E[X(t2)]) is positive on average. In this case, CX(t1, t2) is positive, and we say X(t1) and X(t2) are positively correlated. On the other hand, if large values of X(t1) imply small values of X(t2), then (X(t1) − E[X(t1)])(X(t2) − E[X(t2)]) is negative on average, and we say X(t1) and X(t2) are negatively correlated. If CX(t1, t2) = 0, then X(t1) and X(t2) are uncorrelated. Example 10. 5 Find the correlation functions and covariance functions for the random processes given in Examples 10.1 and 10.2 . Solution For {Xn, n = 0, 1, 2, ⋯ } given in Example 10.1 , we have RX(m, n) = E[XmXn] = 106E[Y mY n] (where Y = 1 + R ∼ Uniform(1.04, 1.05)) = 106 ∫ 1.05 1.04 100y(m+n) dy (by LOTUS) = [ym+n+1] 1.05 1.04 = [(1.05) m+n+1 − (1.04) m+n+1],  for all m, n ∈ {0, 1, 2, ⋯}. To find the covariance function, we write CX(m, n) = RX(m, n) − E[Xm]E[Xn] = [(1.05) m+n+1 − (1.04) m+n+1] − [(1.05) m+1 − (1.04) m+1][(1.05) n+1 − (1.04) n+1]. For {X(t), t ∈ [0, ∞)} given in Example 10.2 , we have RX(t1, t2) = E[X(t1)X(t2)] = E[(A + Bt1)(A + Bt2)] = E[A2] + E[AB](t1 + t2) + E[B 2]t1t2 = 2 + E[A]E[B](t1 + t2) + 2t1t2 (since A and B are independent) = 2 + t1 + t2 + 2t1t2,  for all t1, t2 ∈ [0, ∞). Finally , to find the covariance function for X(t), we can write CX(t1, t2) = RX(t1, t2) − E[X(t1)]E[X(t2)] = 2 + t1 + t2 + 2t1t2 − (1 + t1)(1 + t2) = 1 + t1t2,  for all t1, t2 ∈ [0, ∞). 108 m + n + 1 108 m + n + 1 108 m + n + 1 1010 (m + 1)(n + 1) 10.1.3 Multiple Random Processes We often need to study more than one random process. For example, when investing in the stock market you consider several dif ferent stocks and you are interested in how they are related. In particular , you might be interested in finding out whether two stocks are positively or negatively correlated. A useful idea in these situations is to look at cross-correlation and cross-covariance functions. For two random processes {X(t), t ∈ J} and {Y (t), t ∈ J}: − the cross-correlation function RXY (t1, t2), is defined by RXY (t1, t2) = E[X(t1)Y (t2)], for t1, t2 ∈ J; − the cross-covariance function CXY (t1, t2), is defined by CXY (t1, t2) = Cov(X(t1), Y (t2)) = RXY (t1, t2) − μX(t1)μY (t2), for t1, t2 ∈ J. To get an idea about these concepts suppose that X(t) is the price of oil (per gallon) and Y (t) is the price of gasoline (per gallon) at time t. Since gasoline is produced from oil, as oil prices increase, the gasoline prices tend to increase, too. Thus, we conclude that X(t) and Y (t) should be positively correlated (at least for the same t, i.e., CXY (t, t) > 0). Example 10. 6 Let A, B, and C be independent normal N(1, 1) random variables. Let {X(t), t ∈ [0, ∞)} be defined as X(t) = A + Bt,  for all t ∈ [0, ∞). Also, let {Y (t), t ∈ [0, ∞)} be defined as Y (t) = A + Ct,  for all t ∈ [0, ∞). Find RXY (t1, t2) and CXY (t1, t2), for t1, t2 ∈ [0, ∞). Solution First, note that μX(t) = E[X(t)] = EA + EB ⋅ t = 1 + t,  for all t ∈ [0, ∞). Similarly , μY (t) = E[Y (t)] = EA + EC ⋅ t = 1 + t,  for all t ∈ [0, ∞). To find RXY (t1, t2) for t1, t2 ∈ [0, ∞), we write RXY (t1, t2) = E[X(t1)Y (t2)] = E[(A + Bt1)(A + Ct2)] = E[A2 + ACt2 + BAt1 + BCt1t2] = E[A2] + E[AC]t2 + E[BA]t1 + E[BC]t1t2 = E[A2] + E[A]E[C]t2 + E[B]E[A]t1 + E[B]E[C]t1t2, (by independence) = 2 + t1 + t2 + t1t2. To find CXY (t1, t2) for t1, t2 ∈ [0, ∞), we write CXY (t1, t2) = RXY (t1, t2) − μX(t1)μY (t2) = (2 + t1 + t2 + t1t2) − (1 + t1)(1 + t2) = 1. In dep en den t Ra n do m Pro c esses: We have seen independence for random variables. In particular , remember that random variables X1, X2,..., Xn are independent if, for all (x1, x2, . . . , xn) ∈ Rn, we have FX1,X2,...,Xn (x1, x2, . . . , xn) = FX1 (x1)FX2 (x2). . . FXn (xn). Now , note that a random process is a collection of random variables. Thus, we can define the concept of independence for random processes, too. In particular , if for two random processes X(t) and Y (t), the random variables X(ti) are independent from the random variables Y (tj), we say that the two random processes are independent. More precisely , we have the following definition: Two random processes {X(t), t ∈ J} and {Y (t), t ∈ J ′} are said to be independent if, for all t1, t2, … , tm ∈ J and t′ 1, t′ 2, … , t′ n ∈ J ′, the set of random variables X(t1), X(t2), ⋯ , X(tm) are independent of the set of random variables Y (t′ 1), Y (t′ 2), ⋯ , Y (t′ n). The above definition implies that for all real numbers x1, x2, ⋯ , xm and y1, y2, ⋯ , yn, we have FX(t1),X(t2),⋯,X(tm),Y (t′ 1),Y (t′ 2),⋯,Y (t ′ n)(x1, x2, ⋯ , xm, y1, y2, ⋯ , yn) = FX(t1),X(t2),⋯,X(tm)(x1, x2, ⋯ , xm) ⋅ FY (t′ 1),Y (t′ 2),⋯,Y (t ′ n)(y1, y2, ⋯ , yn). The above equation might seem complicated; however , in many real-life applications we can often argue that two random processes are independent by looking at the problem structure. For example, in engineering we can reasonably assume that the thermal noise processes in two separate systems are independent. Note that if two random processes X(t) and Y (t) are independent, then their covariance function, CXY (t1, t2), for all t1 and t2 is given by CXY (t1, t2) = Cov(X(t1), Y (t2)) = 0 (since X(t1) and Y (t2) are independent). 10.1.4 Stationary Processes We can classify random processes based on many dif ferent criteria. One of the important questions that we can ask about a random process is whether it is a stationary process. Intuitively , a random process {X(t), t ∈ J} is stationary if its statistical properties do not change by time. For example, for a stationary process, X(t) and X(t + Δ) have the same probability distributions. In particular , we have FX(t)(x) = FX(t+Δ)(x),  for all t, t + Δ ∈ J. More generally , for a stationary process, the joint distribution of X(t1) and X(t2) is the same as the joint distribution of X(t1 + Δ) and X(t2 + Δ). For example, if you have a stationary process X(t), then P((X(t1), X(t2)) ∈ A) = P((X(t1 + Δ), X(t2 + Δ)) ∈ A), for any set A ∈ R2. In sum, a random process is stationary if a time shift does not change its statistical properties . Here is a formal definition of stationarity of continuous- time processes. A continuous-time random process {X(t), t ∈ R} is strict-sense stationary or simply stationary if, for all t1, t2, ⋯ , tr ∈ R and all Δ ∈ R, the joint CDF of X(t1), X(t2), ⋯ , X(tr) is the same as the joint CDF of X(t1 + Δ), X(t2 + Δ), ⋯ , X(tr + Δ). That is, for all real numbers x1, x2, ⋯ , xr, we have FX(t1)X(t2)⋯X(tr)(x1, x2, ⋯ , xr) = FX(t1+Δ)X(t2+Δ)⋯X(tr+Δ)(x1, x2, ⋯ , xr). We can provide similar definition for discrete-time processes. strict-sense stationary or simply stationary , if for all n1, n2, ⋯ , nr ∈ Z and all D ∈ Z, the joint CDF of X(n1), X(n2), ⋯ , X(nr) is the same as the joint CDF of X(n1 + D), X(n2 + D), ⋯ , X(nr + D). That is, for all real numbers x1, x2, ⋯ , xr, we have FX(n1)X(n2)⋯X(nr)(x1, x2, ⋯ , xn) = FX(n1+D)X(n2+D)⋯X(nr+D)(x1, x2, ⋯ , xr). 10.1.5 Gaussian Random Processes Here, we will briefly introduce normal (Gaussian) random processes. W e will discuss some examples of Gaussian processes in more detail later on. Many important practical random processes are subclasses of normal random processes. First, let us remember a few facts about Gaussian random vectors. As we saw before, random variables X1, X2,..., Xn are said to be jointly normal if, for all a1,a2,..., an ∈ R, the random variable a1X1 + a2X2+. . . +anXn is a normal random variable. Also, a random vector X = ⎡ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎣ X1 X2 . . . Xn ⎤ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎦ is said to be normal or Gaussian if the random variables X1, X2,..., Xn are jointly normal. An important property of jointly normal random variables is that their joint PDF is completely determined by their mean and covariance matrices. More specifically , for a normal random vector X with mean m and covariance matrix C, the PDF is given by fX(x) = exp{− (x − m) T C −1(x − m)}. Now , let us define Gaussian random processes. A random process {X(t), t ∈ J} is said to be a Gaussian (normal) random process if, for all t1, t2, … , tn ∈ J, the random variables X(t1), X(t2),..., X(tn) are jointly normal. 1 (2π) √det C n 2 1 2 Example 10. 12 Let X(t) be a zero-mean WSS Gaussian process with RX(τ) = e −τ 2 , for all τ ∈ R. 1 . Find P(X(1) < 1). 2 . Find P(X(1) + X(2) < 1). Solution 1 . X(1) is a normal random variable with mean E[X(1)] = 0 and variance Var(X(1)) = E[X(1) 2] = RX(0) = 1. Thus, P(X(1) < 1) = Φ ( ) = Φ(1) ≈ 0.84 2 . Let Y = X(1) + X(2). Then, Y is a normal random variable. W e have EY = E[X(1)] + E[X(2)] = 0; Var(Y ) = Var(X(1)) + Var(X(2)) + 2Cov(X(1), X(2)). Note that Var(X(1)) = E[X(1) 2] − E[X(1)] 2 = RX(0) − μ2 X = 1 − 0 = 1 = Var(X(2)); Cov(X(1), X(2)) = E[X(1)X(2)] − E[X(1)]E[X(2)] = RX(−1) − μ2 X = e −1 − 0 = . Therefore, Var(Y ) = 2 + . We conclude Y ∼ N(0, 2 + ). Thus, 1 − 0 1 1 e 2 e 2 e P(Y < 1) = Φ ⎛ ⎜ ⎜ ⎝ ⎞ ⎟ ⎟ ⎠ = Φ(0.6046) ≈ 0.73 An important property of normal random processes is that wide-sense stationarity and strict-sense stationarity are equivalent for these processes. More specifically , we can state the following theorem. Theorem 10. 1 Consider the Gaussian random processes {X(t), t ∈ R}. If X(t) is WSS, then X(t) is a stationary process. Proof We need to show that, for all t1, t2, ⋯ , tr ∈ R and all Δ ∈ R, the joint CDF of X(t1), X(t2), ⋯ , X(tr) is the same as the joint CDF of X(t1 + Δ), X(t2 + Δ), ⋯ , X(tr + Δ). Since these random variables are jointly Gaussian, it suf fices to show that the mean vectors and the covariance matrices are the same. T o see this, note that X(t) is a WSS process, so μX(ti) = μX(tj) = μX, for all i, j, and CX(ti + Δ, tj + Δ) = CX(ti, tj) = CX(ti − tj), for all i, j. From the above, we conclude that the mean vector and the covariance matrix of X(t1), X(t2), ⋯ , X(tr) is the same as the mean vector and the covariance matrix of X(t1 + Δ), X(t2 + Δ), ⋯ , X(tr + Δ). 1 − 0 √2 + 2 e Similarly , we can define jointly Gaussian random processes. Two random processes {X(t), t ∈ J} and {Y (t), t ∈ J ′} are said to be jointly Gaussian (normal) , if for all t1, t2, … , tm ∈ J and t′ 1, t′ 2, … , t′ n ∈ J ′, the random variables X(t1), X(t2), ⋯ , X(tm), Y (t′ 1), Y (t′ 2), ⋯ , Y (t′ n) are jointly normal. Note that from the properties of jointly normal random variables, we can conclude that if two jointly Gaussian random processes X(t) and Y (t) are uncorrelated, i.e., CXY (t1, t2) = 0, for all t1, t2, then X(t) and Y (t) are two independent random processes. 10.1.6 Solved Problems Problem 1 Let Y1, Y2, Y3, ⋯ be a sequence of i.i.d. random variables with mean EYi = 0 and Var(Yi) = 4. Define the discrete-time random process {X(n), n ∈ N} as X(n) = Y1 + Y2 + ⋯ + Yn, for all n ∈ N. Find μX(n) and RX(m, n), for all n, m ∈ N. Solution We have μX(n) = E[X(n)] = E[Y1 + Y2 + ⋯ + Yn] = E[Y1] + E[Y2] + ⋯ + E[Yn] = 0. Let m ≤ n, then RX(m, n) = E[X(m)X(n)] = E [X(m)(X(m) + Ym+1 + Ym+2 + ⋯ + Yn)] = E[X(m) 2] + E[X(m)]E[Ym+1 + Ym+2 + ⋯ + Yn] = E[X(m) 2] + 0 = Var(X(m)) = Var(Y1) + Var(Y2) + ⋯ + Var(Ym) = 4m. Similarly , for m ≥ n, we have RX(m, n) = E[X(m)X(n)] = 4n. We conclude RX(m, n) = 4 min(m, n). Problem 2 For any k ∈ Z, define the function gk(t) as gk(t) = ⎧⎪ ⎨ ⎪⎩ 1 k < t ≤ k + 1 0 otherwise Now , consider the continuous-time random process {X(t), t ∈ R} defined as X(t) = +∞ ∑ k=−∞ Akgk(t), where A1, A2, ⋯ are i.i.d. random variables with EAk = 1 and Var(Ak) = 1. Find μX(t) , RX(s, t), and CX(s, t) for all s, t ∈ R. Solution Note that, for any k ∈ Z, g(t) = 0 outside of the interval (k, k + 1]. Thus, if k < t ≤ k + 1, we can write X(t) = Ak. Thus, μX(t) = E[X(t)] = E[Ak] = 1. So, μX(t) = 1 for all t ∈ R. Now consider two real numbers s and t. If for some k ∈ Z, we have k < s, t ≤ k + 1, then RX(s, t) = E[X(s)X(t)] = E[A2 k] = 1 + 1 = 2. On the other hand, if s and t are in two dif ferent subintervals of R, that is if k < s ≤ k + 1, and l < t ≤ l + 1, where k and l are two dif ferent integers, then RX(s, t) = E[X(s)X(t)] = E[AkAl] = E[Ak]E[Al] = 1. To find CX(s, t), note that if \\ k < s, t ≤ k + 1, then CX(s, t) = RX(s, t) − E[X(s)]E[X(t)] = 2 − 1 ⋅ 1 = 1. On the other hand, if k < s ≤ k + 1, and l < t ≤ l + 1, where k and l are two dif ferent integers, then CX(s, t) = RX(s, t) − E[X(s)]E[X(t)] = 1 − 1 ⋅ 1 = 0. Problem 3 Let X(t) be a continuous-time WSS process with mean μX = 1 and RX(τ) = ⎧⎪ ⎨ ⎪⎩ 3 − |τ| −2 ≤ τ ≤ 2 1 otherwise a . Find the expected power in X(t). b . Find E [(X(1) + X(2) + X(3)) 2]. Solution a . The expected power in X(t) at time t is E[X(t) 2], which is given by RX(0) = 3. b . We have E [(X(1) + X(2) + X(3)) 2] = E[X(1) 2 + X(2) 2 + X(3) 2 + 2X(1)X(2) + 2X(1)X(3) + 2X(2)X(3)] = 3RX(0) + 2RX(−1) + 2RX(−2) + 2RX(−1) = 3 ⋅ 3 + 2 ⋅ 2 + 2 ⋅ 1 + 2 ⋅ 2 = 19. Problem 4 Let X(t) be a continuous-time WSS process with mean μX = 0 and RX(τ) = δ(τ), where δ(τ) is the Dirac delta function. We define the random process Y (t) as Y (t) = ∫ t t−2 X(u)du. a . Find μY (t) = E[Y (t)]. b . Find RXY (t1, t2). Solution a . We have μY (t) = E [∫ t t−2 X(u)du] = ∫ t t−2 E[X(u)] du = ∫ t t−2 0 du = 0. b . We have RXY (t1, t2) = E [X(t1) ∫ t2 t2−2 X(u)du] = E [∫ t2 t2−2 X(t1)X(u)du] = ∫ t2 t2−2 RX(t1 − u) du = ∫ t2 t2−2 δ(t1 − u) du = ⎧⎪ ⎨ ⎪⎩ 1 t2 − 2 < t1 < t2 0 otherwise Problem 5 Let X(t) be a Gaussian process with μX(t) = t, and RX(t1, t2) = 1 + 2t1t2, for all t, t1, t2 ∈ R. Find P(2X(1) + X(2) < 3). Solution Let Y = 2X(1) + X(2). Then, Y is a normal random variable. W e have EY = 2E[X(1)] + E[X(2)] = 2 ⋅ 1 + 2 = 4. Var(Y ) = 4Var(X(1)) + Var(X(2)) + 4Cov(X(1), X(2)). Note that Var(X(1)) = E[X(1) 2] − E[X(1)] 2 = RX(1, 1) − μX(1) 2 = 1 + 2 ⋅ 1 ⋅ 1 − 1 = 2. Var(X(2)) = E[X(2) 2] − E[X(2)] 2 = RX(2, 2) − μX(2) 2 = 1 + 2 ⋅ 2 ⋅ 2 − 4 = 5. Cov(X(1), X(2)) = E[X(1)X(2)] − E[X(1)]E[X(2)] = RX(1, 2) − μX(1)μX(2) = 1 + 2 ⋅ 1 ⋅ 2 − 1 ⋅ 2 = 3. Therefore, Var(Y ) = 4 ⋅ 2 + 5 + 4 ⋅ 3 = 25. We conclude Y ∼ N(4, 25). Thus, P(Y < 3) = Φ ( ) = Φ(−0.2) ≈ 0.42 3 − 4 5 10.2.0 Processing of Random Signals In this section, we will study what happens when a WSS random signal passes through a linear time-invariant (L TI) system. Such scenarios are encountered in many real-life systems, specifically in communications and signal processing. A main result of this section is that if the input to an L TI system is a WSS process, then the output is also a WSS process. Moreover , the input and output are jointly WSS. T o better understand these systems, we will discuss the study of random signals in the frequency domain. 10.2.1 Power Spectral Density So far , we have studied random processes in the time domain. It is often very useful to study random processes in the frequency domain as well. T o do this, we need to use the Fourier transform. Here, we will assume that you are familiar with the Fourier transform. A brief review of the Fourier transform and its properties is given in the appendix. Consider a WSS random process X(t) with autocorrelation function RX(τ). We define the Power Spectral Density (PSD) of X(t) as the Fourier transform of RX(τ). We show the PSD of X(t), by SX(f). More specifically , we can write SX(f) = F {RX(τ)} = ∫ ∞ −∞ RX(τ)e −2jπfτ dτ, where j = √−1. Power Spectral Density (PSD) SX(f) = F {RX(τ)} = ∫ ∞ −∞ RX(τ)e −2jπfτ dτ, where j = √−1. From this definition, we can conclude that RX(τ) can be obtained by the inverse Fourier transform of SX(f). That is RX(τ) = F −1{SX(f)} = ∫ ∞ −∞ SX(f)e 2jπfτ df. As we have seen before, if X(t) is a real-valued random process, then RX(τ) is an even, real-valued function of τ. From the properties of the Fourier transform, we conclude that SX(f) is also real-valued and an even function of f. Also, from what we will discuss later on, we can conclude that SX(f) is non-negative for all f. 1 . SX(−f) = SX(f), for all f; 2 . SX(f) ≥ 0, for all f. Before going any further , let's try to understand the idea behind the PSD. T o do so, let's choose τ = 0. We know that expected power in X(t) is given by E[X(t) 2] = RX(0) = ∫ ∞ −∞ SX(f)e 2jπf⋅0 df = ∫ ∞ −∞ SX(f) df. We conclude that the expected power in X(t) can be obtained by integrating the PSD of X(t). This fact helps us to understand why SX(f) is called the power spectral density . In fact, as we will see shortly , we can find the expected power of X(t) in a specific frequency range by integrating the PSD over that specific range. The expected power in X(t) can be obtained as E[X(t) 2] = RX(0) = ∫ ∞ −∞ SX(f) df. Example 10. 13 Consider a WSS random process X(t) with RX(τ) = e −a|τ|, where a is a positive real number . Find the PSD of X(t). Solution We need to find the Fourier transform of RX(τ). We can do this by looking at a Fourier transform table or by finding the Fourier transform directly as follows. SX(f) = F {RX(τ)} = ∫ ∞ −∞ e −a|τ|e −2jπfτ dτ = ∫ 0 −∞ e aτ e −2jπfτ dτ + ∫ ∞ 0 e −aτ e −2jπfτ dτ = + = . C ro ss S p e c tra l D e n si ty: For two jointly WSS random processes X(t) and Y (t), we define the cross spectral density SXY (f) as the Fourier transform of the cross-correlation function RXY (τ), SXY (f) = F {RXY (τ)} = ∫ ∞ −∞ RXY (τ)e −2jπfτ dτ. 1 a − j2πf 1 a + j2πf 2a a2 + 4π2f 2 10.2.2 Linear T ime-Invariant (L TI) Systems with Random Inputs Lin ea r Time- In va ria n t ( LTI) Systems: A linear time-invariant (L TI) system can be represented by its impulse response (Figure 10.6). More specifically , if X(t) is the input signal to the system, the output, Y (t), can be written as Y (t) = ∫ ∞ −∞ h(α)X(t − α) dα = ∫ ∞ −∞ X(α)h(t − α) dα. The above integral is called the convolution of h and X, and we write Y (t) = h(t) ∗ X(t) = X(t) ∗ h(t). Note that as the name suggests, the impulse response can be obtained if the input to the system is chosen to be the unit impulse function (delta function) x(t) = δ(t). For discrete-time systems, the output can be written as (Figure 10.6) Y (n) = h(n) ∗ X(n) = X(n) ∗ h(n) = ∞ ∑ k=−∞ h(k)X(n − k) = ∞ ∑ k=−∞ X(k)h(n − k). The discrete-time unit impulse function is defined as δ(n) = ⎧⎪ ⎨ ⎪⎩ 1 n = 0 0 otherwise For the rest of this chapter , we mainly focus on continuous-time signals. Figure 10.6 - L TI systems. LTI Systems with Ra n do m In p uts: Consider an L TI system with impulse response h(t). Let X(t) be a WSS random process. If X(t) is the input of the system, then the output, Y (t), is also a random process. More specifically , we can write Y (t) = h(t) ∗ X(t) = ∫ ∞ −∞ h(α)X(t − α) dα. Here, our goal is to show that X(t) and Y (t) are jointly WSS processes. Let's first start by calculating the mean function of Y (t), μY (t). We have μY (t) = E[Y (t)] = E [∫ ∞ −∞ h(α)X(t − α) dα] = ∫ ∞ −∞ h(α)E[X(t − α)] dα = ∫ ∞ −∞ h(α)μX dα = μX ∫ ∞ −∞ h(α) dα. We note that μY (t) is not a function of t, so we can write μY (t) = μY = μX ∫ ∞ −∞ h(α) dα. Let's next find the cross-correlation function, RXY (t1, t2). We have RXY (t1, t2) = E[X(t1)Y (t2)] = E [X(t1) ∫ ∞ −∞ h(α)X(t2 − α) dα] = E [∫ ∞ −∞ h(α)X(t1)X(t2 − α) dα] = ∫ ∞ −∞ h(α)E[X(t1)X(t2 − α)] dα = ∫ ∞ −∞ h(α)RX(t1, t2 − α) dα = ∫ ∞ −∞ h(α)RX(t1 − t2 + α) dα (since X(t) is WSS). We note that RXY (t1, t2) is only a function of τ = t1 − t2, so we may write RXY (τ) = ∫ ∞ −∞ h(α)RX(τ + α) dα = h(τ) ∗ RX(−τ) = h(−τ) ∗ RX(τ). Similarly , you can show that RY (τ) = h(τ) ∗ h(−τ) ∗ RX(τ). This has been shown in the Solved Problems section. From the above results we conclude that X(t) and Y (t) are jointly WSS. The following theorem summarizes the results. Theorem 10. 2 Let X(t) be a WSS random process and Y (t) be given by Y (t) = h(t) ∗ X(t), where h(t) is the impulse response of the system. Then X(t) and Y (t) are jointly WSS. Moreover , 1 . μY (t) = μY = μX ∫ ∞ −∞ h(α) dα; 2 . RXY (τ) = h(−τ) ∗ RX(τ) = ∫ ∞ −∞ h(−α)RX(t − α) dα; 3 . RY (τ) = h(τ) ∗ h(−τ) ∗ RX(τ). Freq uen c y D o ma in A n a lysis: Let's now rewrite the statement of Theorem 10.2 in the frequency domain. Let H(f) be the Fourier transform of h(t), H(f) = F {h(t)} = ∫ ∞ −∞ h(t)e −2jπft dt. H(f) is called the transfer function of the system. W e can rewrite μY = μX ∫ ∞ −∞ h(α) dα as μY = μXH(0) Since h(t) is assumed to be a real signal, we have F {h(−t)} = H(−f) = H ∗(f), where ∗ shows the complex conjugate. By taking the Fourier transform from both sides of RXY (τ) = RX(τ) ∗ h(−τ), we conclude SXY (f) = SX(f)H(−f) = SX(f)H ∗(f). Finally , by taking the Fourier transform from both sides of RY (τ) = h(τ) ∗ h(−τ) ∗ RX(τ) , we conclude SY (f) = SX(f)H ∗(f)H(f) = SX(f)|H(f)| 2. SY (f) = SX(f)|H(f)| 2 Example 10. 14 Let X(t) be a zero-mean WSS process with RX(τ) = e −|τ|. X(t) is input to an L TI system with |H(f)| = ⎧⎪ ⎨ ⎪⎩ √1 + 4π2f 2 |f| < 2 0 otherwise Let Y (t) be the output. a . Find μY (t) = E[Y (t)]. b . Find RY (τ). c. Find E[Y (t) 2]. Solution Note that since X(t) is WSS, X(t) and Y (t) are jointly WSS, and therefore Y (t) is WSS. a . To find μY (t), we can write μY = μXH(0) = 0 ⋅ 1 = 0. b . To find RY (τ), we first find SY (f). SY (f) = SX(f)|H(f)| 2. From Fourier transform tables, we can see that SX(f) = F {e −|τ|} = . Then, we can find SY (f) as SY (f) = SX(f)|H(f)| 2 = ⎧⎪ ⎨ ⎪⎩ 2 |f| < 2 0 otherwise We can now find RY (τ) by taking the inverse Fourier transform of SY (f). RY (τ) = 8sinc(4τ), 2 1 + (2πf)2 where sinc(f) = . c. We have E[Y (t) 2] = RY (0) = 8. sin(πf) πf 10.2.3 Power in a Frequency Band Here, we would like show that if you integrate SX(f) over a frequency range, you will obtain the expected power in X(t) in that frequency range. Let's first define what we mean by the expected power \"in a frequency range.\" Consider a WSS random process X(t) that goes through an L TI system with the following transfer function (Figure 10.7): H(f) = ⎧⎪ ⎨ ⎪⎩ 1 f1 < |f| < f2 0 otherwise Figure 10.7 - A bandpass filter . This is in fact a bandpass filter . This filter eliminates every frequency outside of the frequency band f1 < |f| < f2. Thus, the resulting random process Y (t) is a filtered version of X(t) in which frequency components in the frequency band f1 < |f| < f2 are preserved. The expected power in Y (t) is said to be the expected power in X(t) in the frequency range f1 < |f| < f2. Now , let's find the expected power in Y (t). We have SY (f) = SX(f)|H(f)| 2 = ⎧⎪ ⎨ ⎪⎩ SX(f) f1 < |f| < f2 0 otherwise Thus, the power in Y (t) is E[Y (t) 2] = ∫ ∞ −∞ SY (f) df = ∫ −f1 −f2 SX(f) df + ∫ f2 f1 SX(f) df = 2 ∫ f2 f1 SX(f) df b(since SX(−f) = SX(f)) Therefore, we conclude that, if we integrate SX(f) over the frequency range f1 < |f| < f2, we will obtain the expected power in X(t) in that frequency range. That is why SX(f) is called the power spectral density of X(t). Ga ussia n P ro c esses thro ugh LTI S ystems: Let X(t) be a stationary Gaussian random process that goes through an L TI system with impulse response h(t). Then, the output process is given by Y (t) = h(t) ∗ X(t) = ∫ ∞ −∞ h(α)X(t − α) dα. For each t, you can think of the above integral as a limit of a sum. Now , since the different sums of jointly normal random variables are also jointly normal, you can argue that Y (t) is also a Gaussian random process. Indeed, we can conclude that X(t) and Y (t) are jointly normal. Note that, for Gaussian processes, stationarity and wide- sense stationarity are equal. Let X(t) be a stationary Gaussian process. If X(t) is the input to an LTI system, then the output random process, Y (t), is also a stationary Gaussian process. Moreover , X(t) and Y (t) are jointly Gaussian. Example 10. 15 Let X(t) be a zero-mean Gaussian random process with RX(τ) = 8 sinc(4τ). Suppose that X(t) is input to an L TI system with transfer function H(f) = ⎧⎪ ⎪ ⎨ ⎪ ⎪⎩ |f| < 1 0 otherwise If Y (t) is the output, find P(Y (2) < 1|Y (1) = 1). Solution Since X(t) is a WSS Gaussian process, Y (t) is also a WSS Gaussian process. Thus, it suffices to find μY and RY (τ). Since μX = 0, we have μY = μXH(0) = 0. Also, note that SX(f) = F {RX(τ)} = ⎧⎪ ⎨ ⎪⎩ 2 |f| < 2 0 otherwise We can then find SY (f) as SY (f) = SX(f)|H(f)| 2 = ⎧⎪ ⎪ ⎨ ⎪ ⎪⎩ |f| < 1 0 otherwise Thus, RY (τ) is given by RY (τ) = F −1{SX(f)} = sinc(2τ). Therefore, E[Y (t) 2] = RY (0) = 1. We conclude that Y (t) ∼ N(0, 1), for all t. Since Y (1) and Y (2) are jointly Gaussian, to determine their joint PDF , it only remains to find their covariance. W e have E[Y (1)Y (2)] = RY (−1) = sinc(−2) = = 0. 1 2 1 2 sin(−2π) −2π Since E[Y (1)] = E[Y (2)] = 0, we conclude that Y (1) and Y (2) are uncorrelated. Since Y (1) and Y (2) are jointly normal, we conclude that they are independent, so P(Y (2) < 1|Y (1) = 1) = P(Y (2) < 1) = Φ(1) ≈ 0.84 10.2.4 White Noise A very commonly-used random process is white noise . White noise is often used to model the thermal noise in electronic systems. By definition, the random process X(t) is called white noise if SX(f) is constant for all frequencies. By convention, the constant is usually denoted by . The random process X(t) is called a white noise process if SX(f) = ,  for all f. Before going any further , let's calculate the expected power in X(t). We have E[X(t) 2] = ∫ ∞ −∞ SX(f) df = ∫ ∞ −∞ df = ∞. Thus, white noise, as defined above, has infinite power! In reality , white noise is in fact an approximation to the noise that is observed in real systems. T o better understand the idea, consider the PSDs shown in Figure 10.8. N0 2 N0 2 N0 2 Figure 10.8 - Part (a): PSD of thermal noise; Part (b) PSD of white noise. Part (a) in the figure shows what the real PSD of a thermal noise might look like. As we see, the PSD is not constant for all frequencies; however , it is approximately constant over the frequency range that we are interested in. In other words, real systems are bandlimited and work on a limited range of frequencies. For the frequency range that we are interested in, the two PSDs (the PSD in Part (a) and the PSD of the white noise, shown in Part (b)) are approximately the same. The thermal noise in electronic systems is usually modeled as a white Gaussian noise process. It is usually assumed that it has zero mean μX = 0 and is Gaussian. The random process X(t) is called a white Gaussian noise process if X(t) is a stationary Gaussian random process with zero mean, μX = 0, and flat power spectral density , SX(f) = ,  for all f. Since the PSD of a white noise process is given by SX(f) = , its autocorrelation function is given by N0 2 N0 2 RY (τ) = F −1 { } = δ(τ), where δ(τ) is the dirac delta function δ(x) = { ∞ x = 0 0 otherwise This again confirms that white noise has infinite power , E[X(t) 2] = RX(0). We also note that RX(τ) = 0 for any τ ≠ 0. This means that X(t1) and X(t2) are uncorrelated for any t1 ≠ t2. Therefore, for a white Gaussian noise, X(t1) and X(t2) are independent for any t1 ≠ t2. White Gaussian noise can be described as the \"derivative\" of Brownian motion . Brownian motion is an important random process that will be discussed in the next chapter . Example 10. 16 Let X(t) be a white Gaussian noise process that is input to an L TI system with transfer function |H(f)| = ⎧⎪ ⎨ ⎪⎩ 2 1 < |f| < 2 0 otherwise If Y (t) is the output, find P(Y (1) < √N0). Solution Since X(t) is a zero-mean Gaussian process, Y (t) is also a zero-mean Gaussian process. SY (f) is given by |H(f)| 2 = ⎧⎪ ⎨ ⎪⎩ 2N0 1 < |f| < 2 0 otherwise Therefore, E[Y (t) 2] = ∫ ∞ −∞ SY (f) df = 4N0. Thus, N0 2 N0 2 N0 2 Y (t) ∼ N(0, 4N0). To find P(Y (1) < √N0), we can write P(Y (1) < √N0) = Φ ( ) = Φ ( ) ≈ 0.69 √N0 √4N0 1 2 10.2.5 Solved Problems Problem 1 Consider a WSS random process X(t) with RX(τ) = ⎧⎪ ⎨ ⎪⎩ 1 − |τ| −1 ≤ τ ≤ 1 0 otherwise Find the PSD of X(t), and E[X(t) 2]. Solution First, we have E[X(t) 2] = RX(0) = 1. We can write triangular function, RX(τ) = Λ(τ), as RX(τ) = Π(τ) ∗ Π(τ), where Π(τ) = ⎧⎪ ⎪ ⎨ ⎪ ⎪⎩ 1 − ≤ τ ≤ 0 otherwise Thus, we conclude SX(f) = F {RX(τ)} = F {Π(τ) ∗ Π(τ)} = F {Π(τ)} ⋅ F {Π(τ)} = [sinc(f)]2. Problem 2 Let X(t) be a random process with mean function μX(t) and autocorrelation function RX(s, t) ( X(t) is not necessarily a WSS process). Let Y (t) be given by 1 2 1 2 Y (t) = h(t) ∗ X(t), where h(t) is the impulse response of the system. Show that a . μY (t) = μX(t) ∗ h(t). b . RXY (t1, t2) = h(t2) ∗ RX(t1, t2) = ∫ ∞ −∞ h(α)RX(t1, t2 − α) dα. Solution a . We have μY (t) = E[Y (t)] = E [∫ ∞ −∞ h(α)X(t − α) dα] = ∫ ∞ −∞ h(α)E[X(t − α)] dα = ∫ ∞ −∞ h(α)μX(t − α) dα = μX(t) ∗ h(t). b . We have RXY (t1, t2) = E[X(t1)Y (t2)] = E [X(t1) ∫ ∞ −∞ h(α)X(t2 − α) dα] = E [∫ ∞ −∞ h(α)X(t1)X(t2 − α) dα] = ∫ ∞ −∞ h(α)E[X(t1)X(t2 − α)] dα = ∫ ∞ −∞ h(α)RX(t1, t2 − α) dα. Problem 3 Prove the third part of Theorem 10.2 : Let X(t) be a WSS random process and Y (t) be given by Y (t) = h(t) ∗ X(t), where h(t) is the impulse response of the system. Show that RY (s, t) = RY (s − t) = ∫ ∞ −∞ ∫ ∞ −∞ h(α)h(β)RX(s − t − α + β) dαdβ. Also, show that we can rewrite the above integral as RY (τ) = h(τ) ∗ h(−τ) ∗ RX(τ). Solution RY (s, t) = E[X(s)Y (t)] = E [∫ ∞ −∞ h(α)X(s − α) dα ∫ ∞ −∞ h(β)X(s − β) dβ] = ∫ ∞ −∞ ∫ ∞ −∞ h(α)h(β)E[X(s − α)X(t − β)] dα dβ = ∫ ∞ −∞ ∫ ∞ −∞ h(α)h(β)RX(s − t − α + β) dα dβ. We now compute h(τ) ∗ h(−τ) ∗ RX(τ). First, let g(τ) = h(τ) ∗ h(−τ). Note that g(τ) = h(τ) ∗ h(−τ) = ∫ ∞ −∞ h(α)h(α − τ) dα. Thus, we have g(τ) ∗ RX(τ) = ∫ ∞ −∞ g(θ)RX(θ − τ) dθ = ∫ ∞ −∞ [∫ ∞ −∞ h(α)h(α − θ) dα] RX(θ − τ) dθ = ∫ ∞ −∞ ∫ ∞ −∞ h(α)h(α − θ)RX(θ − τ) dα dθ = ∫ ∞ −∞ ∫ ∞ −∞ h(α)h(β)RX(α − β − τ) dα dβ = ∫ ∞ −∞ ∫ ∞ −∞ h(α)h(β)RX(τ − α + β) dα dβ (since RX(−τ) = RX(τ)). Problem 4 Let X(t) be a WSS random process. Assuming that SX(f) is continuous at f1, show that SX(f1) ≥ 0. Solution Let f1 ∈ R. Suppose that X(t) goes through an L TI system with the following transfer function H(f) = ⎧⎪ ⎨ ⎪⎩ 1 f1 < |f| < f1 + Δ 0 otherwise where Δ is chosen to be very small. The PSD of Y (t) is given by SY (f) = SX(f)|H(f)| 2 = ⎧⎪ ⎨ ⎪⎩ SX(f) f1 < |f| < f1 + Δ 0 otherwise Thus, the power in Y (t) is E[Y (t) 2] = ∫ ∞ −∞ SY (f) df = 2 ∫ f1+Δ f1 SX(f) df ≈ 2ΔSX(f1). Since E[Y (t) 2] ≥ 0, we conclude that SX(f1) ≥ 0. Problem 5 Let X(t) be a white Gaussian noise with SX(f) = . Assume that X(t) is input to an LTI system with h(t) = e −tu(t). Let Y (t) be the output. a . Find SY (f). b . Find RY (τ). c. Find E[Y (t) 2]. Solution First, note that H(f) = F {h(t)} = . a . To find SY (f), we can write N0 2 1 1 + j2πf SY (f) = SX(f)|H(f)| 2 = . b . To find RY (τ), we can write RY (τ) = F −1{SY (f)} = e −|τ|. c. We have E[Y (t) 2] = RY (0) = . N0/2 1 + (2πf)2 N0 4 N0 4 10.3.0 End of Chapter Problems Problem 1 Let {Xn, n ∈ Z} be a discrete-time random process, defined as Xn = 2 cos( + Φ), where Φ ∼ Uniform(0, 2π). a . Find the mean function, μX(n). b . Find the correlation function RX(m, n). c. Is Xn a WSS process? Problem 2 Let {X(t), t ∈ R} be a continuous-time random process, defined as X(t) = A cos(2t + Φ), where A ∼ U(0, 1) and Φ ∼ U(0, 2π) are two independent random variables. a . Find the mean function μX(t). b . Find the correlation function RX(t1, t2). c. Is X(t) a WSS process? Problem 3 Let {X(n), n ∈ Z} be a WSS discrete-time random process with μX(n) = 1 and RX(m, n) = e −(m−n)2 . Define the random process Z(n) as Z(n) = X(n) + X(n − 1),  for all n ∈ Z. a . Find the mean function of Z(n), μZ(n). b . Find the autocorrelation function of Z(n), RZ(m, n). c. Is Z(n) a WSS random process? πn 8 Problem 4 Let g : R ↦ R be a periodic function with period T, i.e., g(t + T) = g(t),  for all t ∈ R. Define the random process {X(t), t ∈ R} as X(t) = g(t + U),  for all t ∈ R, where U ∼ Uniform(0, T). Show that X(t) is a WSS random process. Problem 5 Let {X(t), t ∈ R} and {Y (t), t ∈ R} be two independent random processes. Let Z(t) be defined as Z(t) = X(t)Y (t),  for all t ∈ R. Prove the following statements: a . μZ(t) = μX(t)μY (t), for all t ∈ R. b . RZ(t1, t2) = RX(t1, t2)RY (t1, t2), for all t ∈ R. c. If X(t) and Y (t) are WSS, then they are jointly WSS. d. If X(t) and Y (t) are WSS, then Z(t) is also WSS. e. If X(t) and Y (t) are WSS, then X(t) and Z(t) are jointly WSS. Problem 6 Let X(t) be a Gaussian process such that for all t > s ≥ 0 we have X(t) − X(s) ∼ N (0, t − s) . Show that X(t) is mean-square continuous at any time t ≥ 0. Problem 7 Let X(t) be a WSS Gaussian random process with μX(t) = 1 and RX(τ) = 1 + 4sinc(τ) . a . Find P(1 < X(1) < 2). b . Find P(1 < X(1) < 2, X(2) < 3). Problem 8 Let X(t) be a Gaussian random process with μX(t) = 0 and RX(t1, t2) = min(t1, t2). Find P(X(4) < 3|X(1) = 1). Problem 9 Let {X(t), t ∈ R} be a continuous-time random process, defined as X(t) = n ∑ k=0 Aktk, where A0, A1, ⋯, An are i.i.d. N(0, 1) random variables and n is a fixed positive integer . a . Find the mean function μX(t). b . Find the correlation function RX(t1, t2). c. Is X(t) a WSS process? d. Find P(X(1) < 1). Assume n = 10. e. Is X(t) a Gaussian process? Problem 10 (Complex Random Processes) In some applications, we need to work with complex- valued random processes. More specifically , a complex random process X(t) can be written as X(t) = Xr(t) + jXi(t), where Xr(t) and Xi(t) are two real-valued random processes and j = √−1. W e define the mean function and the autocorrelation function as μX(t) = E[X(t)] = E[Xr(t)] + jE[Xi(t)] = μXr(t) + jμXi(t); RX(t1, t2) = E[X(t1)X∗(t2)] = E [(Xr(t1) + jXi(t1))(Xr(t2) − jXi(t2))] . Let X(t) be a complex-valued random process defined as X(t) = Ae j(ωt+Φ), where Φ ∼ Uniform(0, 2π), and A is a random variable independent of Φ with EA = μ and Var(A) = σ2. a . Find the mean function of X(t), μX(t). b . Find the autocorrelation function of X(t), RX(t1, t2). Problem 1 1 (T ime Averages) Let {X(t), t ∈ R} be a continuous-time random process. The time average mean of X(t) is defined as (assuming that the limit exists in mean-square sense) ⟨X(t)⟩ = lim T →∞ [ ∫ T −T X(t)dt] . Consider the random process {X(t), t ∈ R} defined as X(t) = cos(t + U), where U ∼ Uniform(0, 2π). Find ⟨X(t)⟩. Problem 12 (Ergodicity) Let X(t) be a WSS process. W e say that X(t) is mean ergodic if ⟨X(t)⟩ (defined above) is equal to μX. Let A0, A1, A−1, A2, A−2, ⋯ be a sequence of i.i.d. random variables with mean EAi = μ < ∞. Define the random process {X(t), t ∈ R} as 1 2T X(t) = ∞ ∑ k=−∞ Akg(t − k), where, g(t) is given by g(t) = ⎧⎪ ⎨ ⎪⎩ 1 0 ≤ t < 1 0 otherwise Show that X(t) is mean ergodic. Problem 13 Let {X(t), t ∈ R} be a WSS random process. Show that for any α > 0, we have P(|X(t + τ) − X(t)| > α) ≤ . Problem 14 Let {X(t), t ∈ R} be a WSS random process. Suppose that RX(τ) = RX(0) for some τ > 0. Show that, for any t, we have X(t + τ) = X(t), with probability one. Problem 15 Let X(t) be a real-valued WSS random process with autocorrelation function RX(τ). Show that the Power Spectral Density (PSD) of X(t) is given by SX(f) = ∫ ∞ −∞ RX(τ) cos(2πfτ) dτ. Problem 16 Let X(t) and Y (t) be real-valued jointly WSS random processes. Show that 2RX(0) − 2RX(τ) α2 SY X(f) = S ∗ XY (f), where, ∗ shows the complex conjugate. Problem 17 Let X(t) be a WSS process with autocorrelation function RX(τ) = . Assume that X(t) is input to a low-pass filter with frequency response H(f) = ⎧⎪ ⎨ ⎪⎩ 3 |f| < 2 0 otherwise Let Y (t) be the output. a . Find SX(f). b . Find SXY (f). c. Find SY (f). d. Find E[Y (t) 2]. Problem 18 Let X(t) be a WSS process with autocorrelation function RX(τ) = 1 + δ(τ). Assume that X(t) is input to an L TI system with impulse response h(t) = e −tu(t). Let Y (t) be the output. a . Find SX(f). b . Find SXY (f). c. Find RXY (τ). d. Find SY (f). e. Find RY (τ). 1 1 + π2τ 2 f. Find E[Y (t) 2]. Problem 19 Let X(t) be a zero-mean WSS Gaussian random process with RX(τ) = e −πτ 2 . Suppose that X(t) is input to an L TI system with transfer function |H(f)| = e − πf 2 . Let Y (t) be the output. a . Find μY . b . Find RY (τ) and Var(Y (t)). c. Find E[Y (3)|Y (1) = −1]. d. Find Var(Y (3)|Y (1) = −1). e. Find P(Y (3) < 0|Y (1) = −1). Problem 20 Let X(t) be a white Gaussian noise with SX(f) = . Assume that X(t) is input to a bandpass filter with frequency response H(f) = ⎧⎪ ⎨ ⎪⎩ 2 1 < |f| < 3 0 otherwise Let Y (t) be the output. a . Find SY (f). b . Find RY (τ). c. Find E[Y (t) 2]. 3 2 N0 2 1 1.0.0 Introduction In the previous chapter , we discussed a general theory of random processes. In this chapter , we will focus on some specific random processes that are used frequently in applications. More specifically , we will discuss the Poisson process , Markov chains , and Brownian Motion ( the W iener process ). Most of the discussion in this chapter is self-contained in the sense that it depends very lightly on the material of the previous chapter . 1 1.1.1 Counting Processes In some problems, we count the occurrences of some types of events. In such scenarios, we are dealing with a counting process . For example, you might have a random process N(t) that shows the number of customers who arrive at a supermarket by time t starting from time 0. For such a processes, we usually assume N(0) = 0, so as time passes and customers arrive, N(t) takes positive integer values. Definition 1 1. 1 A random process {N(t), t ∈ [0, ∞)} is said to be a counting process if N(t) is the number of events occurred from time 0 up to and including time t. For a counting process, we assume 1 . N(0) = 0; 2 . N(t) ∈ {0, 1, 2, ⋯}, for all t ∈ [0, ∞); 3 . for 0 ≤ s < t, N(t) − N(s) shows the number of events that occur in the interval (s, t]. Since counting processes have been used to model arrivals (such as the supermarket example above), we usually refer to the occurrence of each event as an \"arrival\". For example, if N(t) is the number of accidents in a city up to time t, we still refer to each accident as an arrival. Figure 1 1.1 shows a possible realization and the corresponding sample function of a counting process. Figure 1 1.1 - A possible realization and the corresponding sample path of a counting process.. By the above definition, the only sources of randomness are the arrival times Ti. Before introducing the Poisson process, we would like to provide two definitions. Definition 1 1. 2 Let {X(t), t ∈ [0, ∞)} be a continuous-time random process. W e say that X(t) has independent increments if, for all 0 ≤ t1 < t2 < t3 ⋯ < tn, the random variables X(t2) − X(t1), X(t3) − X(t2), ⋯ , X(tn) − X(tn−1) are independent. Note that for a counting process, N(ti) − N(ti−1) is the number of arrivals in the interval (ti−1, ti]. Thus, a counting process has independent increments if the numbers of arrivals in non-overlapping (disjoint) intervals (t1, t2], (t2, t3], ⋯ , (tn−1, tn] are independent. Having independent increments simplifies analysis of a counting process. For example, suppose that we would like to find the probability of having 2 arrivals in the interval (1, 2], and 3 arrivals in the interval (3, 5]. Since the two intervals (1, 2] and (3, 5] are disjoint, we can write P(2 arrivals in (1, 2]   and   3 arrivals in (3, 5]) = P(2 arrivals in (1, 2]) ⋅ P(3 arrivals in (3, 5]). Here is another useful definition. Definition 1 1. 3 Let {X(t), t ∈ [0, ∞)} be a continuous-time random process. W e say that X(t) has stationary increments if, for all t2 > t1 ≥ 0, and all r > 0, the two random variables X(t2) − X(t1) and X(t2 + r) − X(t1 + r) have the same distributions. In other words, the distribution of the dif ference depends only on the length of the interval (t1, t2], and not on the exact location of the interval on the real line. Note that for a counting process N(t), N(t2) − N(t1) is the number of arrivals in the interval (t1, t2]. We also assume N(0) = 0. Therefore, a counting process has stationary increments if for all t2 > t1 ≥ 0, N(t2) − N(t1) has the same distribution as N(t2 − t1). This means that the distribution of the number of arrivals in any interval depends only on the length of the interval, and not on the exact location of the interval on the real line. A counting process has independent increments if the numbers of arrivals in non-overlapping (disjoint) intervals are independent. A counting process has stationary increments if, for all t2 > t1 ≥ 0 , N(t2) − N(t1) has the same distribution as N(t2 − t1). 1 1.1.2 Basic Concepts of the Poisson Process The Poisson process is one of the most widely-used counting processes. It is usually used in scenarios where we are counting the occurrences of certain events that appear to happen at a certain rate, but completely at random (without a certain structure). For example, suppose that from historical data, we know that earthquakes occur in a certain area with a rate of 2 per month. Other than this information, the timings of earthquakes seem to be completely random. Thus, we conclude that the Poisson process might be a good model for earthquakes. In practice, the Poisson process or its extensions have been used to model [24] − the number of car accidents at a site or in an area; − the location of users in a wireless network; − the requests for individual documents on a web server; − the outbreak of wars; − photons landing on a photodiode. Poisson random variable: Here, we briefly review some properties of the Poisson random variable that we have discussed in the previous chapters. Remember that a discrete random variable X is said to be a Poisson random variable with parameter μ, shown as X ∼ Poisson(μ), if its range is RX = {0, 1, 2, 3, . . . }, and its PMF is given by PX(k) = { for k ∈ RX 0  otherwise Here are some useful facts that we have seen before: 1 . If X ∼ Poisson(μ), then EX = μ, and Var(X) = μ. 2 . If Xi ∼ Poisson(μi), for i = 1, 2, ⋯ , n, and the Xi's are independent, then X1 + X2 + ⋯ + Xn ∼ Poisson(μ1 + μ2 + ⋯ + μn). 3 . The Poisson distribution can be viewed as the limit of binomial distribution. e−μμk k! Theorem 1 1. 1 Let Yn ∼ Binomial(n, p = p(n)). Let μ > 0 be a fixed real number , and limn→∞ np = μ. Then, the PMF of Yn converges to a Poisson(μ) PMF, as n → ∞. That is, for any k ∈ {0, 1, 2, . . . }, we have lim n→∞ PYn (k) = . P o isso n P ro cess as the Limit o f a B ern o ulli P ro cess: Suppose that we would like to model the arrival of events that happen completely at random at a rate λ per unit time. Here is one way to do this. At time t = 0, we have no arrivals yet, so N(0) = 0. We now divide the half-line [0, ∞) to tiny subintervals of length δ as shown in Figure 1 1.2. Figure 1 1.2 - Dividing the half-line [0, ∞) to tiny subintervals of length δ. Each subinterval corresponds to a time slot of length δ. Thus, the intervals are (0, δ], (δ, 2δ], (2δ, 3δ], ⋯. More generally , the kth interval is ((k − 1)δ, kδ]. We assume that in each time slot, we toss a coin for which P(H) = p = λδ. If the coin lands heads up, we say that we have an arrival in that subinterval. Otherwise, we say that we have no arrival in that interval. Figure 1 1.3 shows this process. Here, we have an arrival at time t = kδ, if the kth coin flip results in a heads. Figure 1 1.3 - Poisson process as a limit of a Bernoulli process. Now , let N(t) be defined as the number of arrivals (number of heads) from time 0 to time t. There are n ≈ time slots in the interval (0, t]. Thus, N(t) is the number of e −μμk k! t δ heads in n coin flips. W e conclude that N(t) ∼ Binomial(n, p). Note that here p = λδ, so np = nλδ = ⋅ λδ = λt. Thus, by Theorem 1 1.1 , as δ → 0, the PMF of N(t) converges to a Poisson distribution with rate λt. More generally , we can argue that the number of arrivals in any interval of length τ follows a Poisson(λτ) distribution as δ → 0. Consider several non-overlapping intervals. The number of arrivals in each interval is determined by the results of the coin flips for that interval. Since dif ferent coin flips are independent, we conclude that the above counting process has independent increments. D efin itio n o f the P o isso n P ro cess: The above construction can be made mathematically rigorous. The resulting random process is called a Poisson process with rate (or intensity) λ. Here is a formal definition of the Poisson process. The Poisson Process Let λ > 0 be fixed. The counting process {N(t), t ∈ [0, ∞)} is called a Poisson process with rates λ if all the following conditions hold: 1 . N(0) = 0; 2 . N(t) has independent increments; 3 . the number of arrivals in any interval of length τ > 0 has Poisson(λτ) distribution. Note that from the above definition, we conclude that in a Poisson process, the distribution of the number of arrivals in any interval depends only on the length of the interval, and not on the exact location of the interval on the real line. Therefore the Poisson process has stationary increments . t δ Example 1 1. 1 The number of customers arriving at a grocery store can be modeled by a Poisson process with intensity λ = 10 customers per hour . 1 . Find the probability that there are 2 customers between 10:00 and 10:20. 2 . Find the probability that there are 3 customers between 10:00 and 10:20 and 7 customers between 10:20 and 1 1. Solution 1 . Here, λ = 10 and the interval between 10:00 and 10:20 has length τ = hours. Thus, if X is the number of arrivals in that interval, we can write X ∼ Poisson(10/3). Therefore, P(X = 2) = ≈ 0.2 2 . Here, we have two non-overlapping intervals I1 =(10:00 a.m., 10:20 a.m.] and I2 = (10:20 a.m., 1 1 a.m.]. Thus, we can write P(3 arrivals in I1   and   7 arrivals in I2) = P(3 arrivals in I1) ⋅ P(7 arrivals in I2). Since the lengths of the intervals are τ1 = 1/3 and τ2 = 2/3 respectively , we obtain λτ1 = 10/3 and λτ2 = 20/3. Thus, we have P(3 arrivals in I1   and   7 arrivals in I2) = ⋅ ≈ 0.0325 S eco n d D efin itio n o f the P o isso n P ro cess: Let N(t) be a Poisson process with rate λ. Consider a very short interval of length Δ. Then, the number of arrivals in this interval has the same distribution as N(Δ). In 1 3 e − ( )210 3 10 3 2! e − ( )310 3 10 3 3! e − ( )720 3 20 3 7! particular , we can write P(N(Δ) = 0) = e −λΔ = 1 − λΔ + Δ 2 − ⋯ (Taylor Series). Note that if Δ is small, the terms that include second or higher powers of Δ are negligible compared to Δ. We write this as P(N(Δ) = 0) = 1 − λΔ + o(Δ) (11.1) Here o(Δ) shows a function that is negligible compared to Δ, as Δ → 0. More precisely , g(Δ) = o(Δ) means that lim Δ→0 = 0. Now , let us look at the probability of having one arrival in an interval of length Δ. P(N(Δ) = 1) = e −λΔλΔ = λΔ (1 − λΔ + Δ 2 − ⋯) (Taylor Series) = λΔ + (−λ 2Δ 2 + Δ 3 ⋯) = λΔ + o(Δ). We conclude that P(N(Δ) = 1) = λΔ + o(Δ) (11.2) Similarly , we can show that P(N(Δ) ≥ 2) = o(Δ) (11.3) In fact, equations 1 1.1 , 1 1.2 , and 1 1.3 give us another way to define a Poisson process. λ 2 2 g(Δ) Δ λ 2 2 λ 3 2 The Second Definition of the Poisson Process Let λ > 0 be fixed. The counting process {N(t), t ∈ [0, ∞)} is called a Poisson process with rate λ if all the following conditions hold: 1 . N(0) = 0; 2 . N(t) has independent and stationary increments 3 . we have P(N(Δ) = 0) = 1 − λΔ + o(Δ), P(N(Δ) = 1) = λΔ + o(Δ), P(N(Δ) ≥ 2) = o(Δ). We have already shown that any Poisson process satisfies the above definition. T o show that the above definition is equivalent to our original definition, we also need to show that any process that satisfies the above definition also satisfies the original definition. A method to show this is outlined in the End of Chapter Problems. A rrival an d In terarrival Times: Let N(t) be a Poisson process with rate λ. Let X1 be the time of the first arrival. Then, P(X1 > t) = P(no arrival in (0, t]) = e −λt. We conclude that FX1 (t) = ⎧⎪ ⎨ ⎪⎩ 1 − e −λt t > 0 0 otherwise Therefore, X1 ∼ Exponential(λ). Let X2 be the time elapsed between the first and the second arrival (Figure 1 1.4). Figure 1 1.4 - The random variables X1, X2, ⋯ are called the interarrival times of the counting process N(t). Let s > 0 and t > 0. Note that the two intervals (0, s] and [s, s + t] are independent. W e can write P(X2 > t|X1 = s) = P(no arrival in (s, s + t]|X1 = s) = P(no arrivals in (s, s + t]) (independent increments) = e −λt. We conclude that X2 ∼ Exponential(λ), and that X1 and X2 are independent. The random variables X1, X2, ⋯ are called the interarrival times of the counting process N(t). Similarly , we can argue that all Xi's are independent and Xi ∼ Exponential(λ) for i = 1, 2, 3, ⋯. Interarrival T imes for Poisson Processes If N(t) is a Poisson process with rate λ, then the interarrival times X1, X2, ⋯ are independent and Xi ∼ Exponential(λ),  for i = 1, 2, 3, ⋯ . Remember that if X is exponential with parameter λ > 0, then X is a memoryless random variable, that is P(X > x + a|X > a) = P(X > x),  for a, x ≥ 0. Thinking of the Poisson process, the memoryless property of the interarrival times is consistent with the independent increment property of the Poisson distribution. In some sense, both are implying that the number of arrivals in non-overlapping intervals are independent. T o better understand this issue, let's look at an example. Example 1 1. 2 Let N(t) be a Poisson process with intensity λ = 2, and let X1, X2, ⋯ be the corresponding interarrival times. a . Find the probability that the first arrival occurs after t = 0.5, i.e., P(X1 > 0.5). b . Given that we have had no arrivals before t = 1, find P(X1 > 3). c. Given that the third arrival occurred at time t = 2, find the probability that the fourth arrival occurs after t = 4. d. I start watching the process at time t = 10. Let T be the time of the first arrival that I see. In other words, T is the first arrival after t = 10. Find ET and Var(T). e. I start watching the process at time t = 10. Let T be the time of the first arrival that I see. Find the conditional expectation and the conditional variance of T given that I am informed that the last arrival occurred at time t = 9. Solution a . Since X1 ∼ Exponential(2), we can write P(X1 > 0.5) = e −(2×0.5) ≈ 0.37 Another way to solve this is to note that P(X1 > 0.5) = P(no arrivals in (0, 0.5]) = e −(2×0.5) ≈ 0.37 b . We can write P(X1 > 3|X1 > 1) = P(X1 > 2) (memoryless property) = e −2×2 ≈ 0.0183 Another way to solve this is to note that the number of arrivals in (1, 3] is independent of the arrivals before t = 1. Thus, P(X1 > 3|X1 > 1) = P(no arrivals in (1, 3] | no arrivals in (0, 1]) = P(no arrivals in (1, 3]) (independent increments) = e −2×2 ≈ 0.0183 c. The time between the third and the fourth arrival is X4 ∼ Exponential(2). Thus, the desired conditional probability is equal to P(X4 > 2|X1 + X2 + X3 = 2) = P(X4 > 2) (independence of the Xi's) = e −2×2 ≈ 0.0183 d. When I start watching the process at time t = 10, I will see a Poisson process. Thus, the time of the first arrival from t = 10 is Exponential(2). In other words, we can write T = 10 + X, where X ∼ Exponential(2). Thus, ET = 10 + EX = 10 + = , Var(T) = Var(X) = . e. Arrivals before t = 10 are independent of arrivals after t = 10. Thus, knowing that the last arrival occurred at time t = 9 does not impact the distribution of the first arrival after t = 10. Thus, if A is the event that the last arrival occurred at t = 9, we can write E[T|A] = E[T] = , Var(T|A) = Var(T) = . Now that we know the distribution of the interarrival times, we can find the distribution of arrival times T1 = X1, T2 = X1 + X2, T3 = X1 + X2 + X3, ⋮ More specifically , Tn is the sum of n independent Exponential(λ) random variables. In previous chapters we have seen that if Tn = X1 + X2 + ⋯ + Xn, where the Xi's are independent Exponential(λ) random variables, then Tn ∼ Gamma(n, λ). This has been shown using MGFs. Note that here n ∈ N. The Gamma(n, λ) is also called Erlang distribution, i.e, we can write Tn ∼ Erlang(n, λ) = Gamma(n, λ),  for n = 1, 2, 3, ⋯ . The PDF of Tn, for n = 1, 2, 3, ⋯, is given by fTn (t) = ,  for t > 0. 1 2 21 2 1 4 21 2 1 4 λ ntn−1e −λt (n − 1)! Remember that if X ∼ Exponential(λ), then E[X] = , Var(X) = . Since Tn = X1 + X2 + ⋯ + Xn, we conclude that E[Tn] = nEX1 = , Var(Tn) = nVar(Xn) = . Note that the arrival times are not independent. In particular , we must have T1 ≤ T2 ≤ T3 ≤ ⋯. Arrival T imes for Poisson Processes If N(t) is a Poisson process with rate λ, then the arrival times T1, T2 , ⋯ have Gamma(n, λ) distribution. In particular , for n = 1, 2, 3, ⋯, we have E[Tn] = , and Var(Tn) = . The above discussion suggests a way to simulate (generate) a Poisson process with rate λ. We first generate i.i.d. random variables X1, X2, X3, ⋯, where Xi ∼ Exponential(λ). Then the arrival times are given by T1 = X1, T2 = X1 + X2, T3 = X1 + X2 + X3, ⋮ 1 λ 1 λ2 n λ n λ2 n λ n λ2 1 1.1.3 Merging and Splitting Poisson Processes Mergin g In dep en den t P o isso n P ro c esses: Let N1(t) and N2(t) be two independent Poisson processes with rates λ1 and λ2 respectively . Let us define N(t) = N1(t) + N2(t). That is, the random process N(t) is obtained by combining the arrivals in N1(t) and N2(t) (Figure 1 1.5). W e claim that N(t) is a Poisson process with rate λ = λ1 + λ2. To see this, first note that N(0) = N1(0) + N2(0) = 0 + 0 = 0. Figure 1 1.5 - Merging two Poisson processes N1(t) and N2(t). Next, since N1(t) and N2(t) are independent and both have independent increments, we conclude that N(t) also has independent increments. Finally , consider an interval of length τ, i.e, I = (t, t + τ]. Then the numbers of arrivals in I associated with N1(t) and N2(t) are Poisson(λ1τ) and Poisson(λ2τ) and they are independent. Therefore, the number of arrivals in I associated with N(t) is Poisson((λ1 + λ2)τ) (sum of two independent Poisson random variables). Merging Independent Poisson Processes Let N1(t), N2(t), ⋯, Nm(t) be m independent Poisson processes with rates λ1, λ2, ⋯, λm. Let also N(t) = N1(t) + N2(t) + ⋯ + Nm(t), for all t ∈ [0, ∞). Then, N(t) is a Poisson process with rate λ1 + λ2 + ⋯ + λm. S p littin g ( Thin n in g) o f P o isso n P ro c esses: Here, we will talk about splitting a Poisson process into two independent Poisson processes. The idea will be better understood if we look at a concrete example. Example 1 1. 3 Suppose that the number of customers visiting a fast food restaurant in a given time interval I is N ∼ Poisson(μ). Assume that each customer purchases a drink with probability p, independently from other customers, and independently from the value of N. Let X be the number of customers who purchase drinks in that time interval. Also, let Y be the number of customers that do not purchase drinks; so X + Y = N. a . Find the marginal PMFs of X and Y . b . Find the joint PMF of X and Y . c. Are X and Y independent? Solution a . First, note that RX = RY = {0, 1, 2, . . . }. Also, given N = n, X is a sum of n independent Bernoulli(p) random variables. Thus, given N = n, X has a binomial distribution with parameters n and p, so X|N = n ∼ Binomial(n, p), Y |N = n ∼ Binomial(n, q = 1 − p). We have PX(k) = ∞ ∑ n=0 P(X = k|N = n)PN (n) (law of total probability) = ∞ ∑ n=k ( )p kqn−ke −μ = ∞ ∑ n=k = ∞ ∑ n=k = e μq (Taylor series for e x) = ,  for k = 0, 1, 2, . . . Thus, we conclude that X ∼ Poisson(μp). Similarly , we obtain Y ∼ Poisson(μq). b . To find the joint PMF of X and Y , we can also use the law of total probability: PXY (i, j) = ∞ ∑ n=0 P(X = i, Y = j|N = n)PN (n) (law of total probability). However , note that P(X = i, Y = j|N = n) = 0 if N ≠ i + j, thus PXY (i, j) = P(X = i, Y = j|N = i + j)PN (i + j) = P(X = i|N = i + j)PN (i + j) = ( )p iqje −μ = = . = PX(i)PY (j). c. X and Y are independent since, as we saw above, PXY (i, j) = PX(i)PY (j). n k μn n! p kqn−ke −μμn k!(n − k)! e −μ(μp) k k! (μq) n−k (n − k)! e −μ(μp) k k! e −μp(μp) k k! i + j i μi+j (i + j)! e −μ(μp) i(μq) j i!j! e −μp(μp) i i! e −μq(μq) j j! The above example was given for a specific interval I, in which a Poisson random variable N was split to two independent Poisson random variables X and Y . However , the argument can be used to show the same result for splitting a Poisson process to two independent Poisson processes. More specifically , we have the following result. Splitting a Poisson Processes Let N(t) be a Poisson process with rate λ. Here, we divide N(t) to two processes N1(t) and N2(t) in the following way (Figure 1 1.6). For each arrival, a coin with P(H) = p is tossed. If the coin lands heads up, the arrival is sent to the first process ( N1(t)), otherwise it is sent to the second process. The coin tosses are independent of each other and are independent of N(t). Then, 1 . N1(t) is a Poisson process with rate λp; 2 . N2(t) is a Poisson process with rate λ(1 − p); 3 . N1(t) and N2(t) are independent. Figure 1 1.6 - Splitting a Poisson process to two independent Poisson processes. 1 1.1.4 Nonhomogeneous Poisson Processes Let N(t) be the number of customers arriving at a fast food restaurant by time t. W e think that the customers arrive somewhat randomly , so we might want to model N(t) as a Poisson process. However , we notice that this process does not have stationary increments. For example, we note that the arrival rate of customers is larger during lunch time compared to, say , 4 p.m. In such scenarios, we might model N(t) as a nonhomogeneous Poisson process . Such a process has all the properties of a Poisson process, except for the fact that its rate is a function of time, i.e., λ = λ(t). Nonhomogeneous Poisson Process Let λ(t) : [0, ∞) ↦ [0, ∞) be an integrable function. The counting process {N(t), t ∈ [0, ∞)} is called a nonhomogeneous Poisson process with rate λ(t) if all the following conditions hold. 1 . N(0) = 0; 2 . N(t) has independent increments; 3 . for any t ∈ [0, ∞), we have P(N(t + Δ) − N(t) = 0) = 1 − λ(t)Δ + o(Δ), P(N(t + Δ) − N(t) = 1) = λ(t)Δ + o(Δ), P(N(t + Δ) − N(t) ≥ 2) = o(Δ). For a nonhomogeneous Poisson process with rate λ(t), the number of arrivals in any interval is a Poisson random variable; however , its parameter can depend on the location of the interval. More specifically , we can write N(t + s) − N(t) ∼ Poisson (∫ t+s t λ(α)dα) . 1 1.1.5 Solved Problems Problem 1 Let {N(t), t ∈ [0, ∞)} be a Poisson process with rate λ = 0.5. a . Find the probability of no arrivals in (3, 5]. b . Find the probability that there is exactly one arrival in each of the following intervals: (0, 1], (1, 2], (2, 3], and (3, 4]. Solution a . If Y is the number arrivals in (3, 5], then Y ∼ Poisson(μ = 0.5 × 2). Therefore, P(Y = 0) = e −1 = 0.37 b . Let Y1, Y2, Y3 and Y4 be the numbers of arrivals in the intervals (0, 1], (1, 2], (2, 3], and (3, 4]. Then Yi ∼ Poisson(0.5) and Yi's are independent, so P(Y1 = 1, Y2 = 1, Y3 = 1, Y4 = 1) = P(Y1 = 1) ⋅ P(Y2 = 1) ⋅ P(Y3 = 1) ⋅ P(Y4 = 1) = [0.5e −0.5] 4 ≈ 8.5 × 10−3. Problem 2 Let {N(t), t ∈ [0, ∞)} be a Poisson process with rate λ. Find the probability that there are two arrivals in (0, 2] and three arrivals in (1, 4]. Solution Note that the two intervals (0, 2] and (1, 4] are not disjoint. Thus, we cannot multiply the probabilities for each interval to obtain the desired probability . In particular , (0, 2] ∩ (1, 4] = (1, 2]. Let X, Y , and Z be the numbers of arrivals in (0, 1], (1, 2], and (2, 4] respectively . Then X, Y , and Z are independent, and X ∼ Poisson(λ ⋅ 1), Y ∼ Poisson(λ ⋅ 1), Z ∼ Poisson(λ ⋅ 2). Let A be the event that there are two arrivals in (0, 2] and three arrivals in (1, 4]. W e can use the law of total probability to obtain P(A). In particular , P(A) = P(X + Y = 2 and Y + Z = 3) = ∞ ∑ k=0 P(X + Y = 2 and Y + Z = 3|Y = k)P(Y = k) = P(X = 2, Z = 3|Y = 0)P(Y = 0) + P(X = 1, Z = 2|Y = 1)P(Y = 1)+ + P(X = 0, Z = 1|Y = 2)P(Y = 2) = P(X = 2, Z = 3)P(Y = 0) + P(X = 1, Z = 2)P(Y = 1)+ P(X = 0, Z = 1)P(Y = 2) = P(X = 2)P(Z = 3)P(Y = 0) + P(X = 1)P(Z = 2)P(Y = 1)+ P(X = 0)P(Z = 1)P(Y = 2) = ( ) ⋅ ( ) ⋅ (e −λ) + (λe −λ) ⋅ ( ) ⋅ (λe −λ) + (e −λ) ⋅ (e −2λ(2λ)) ⋅ ( ) . Problem 3 Let {N(t), t ∈ [0, ∞)} be a Poisson Process with rate λ. Find its covariance function CN (t1, t2) = Cov(N(t1), N(t2)), for t1, t2 ∈ [0, ∞) Solution Let's assume t1 ≥ t2 ≥ 0. Then, by the independent increment property of the Poisson process, the two random variables N(t1) − N(t2) and N(t2) are independent. W e can write e −λλ 2 2 e −2λ(2λ) 3 6 e −2λ(2λ) 2 2 e −λλ 2 2 CN (t1, t2) = Cov(N(t1), N(t2)) = Cov(N(t1) − N(t2) + N(t2), N(t2)) = Cov(N(t1) − N(t2), N(t2)) + Cov(N(t2), N(t2)) = Cov(N(t2), N(t2)) = Var(N(t2)) = λt2, since N(t2) ∼ Poisson(λt2). Similarly , if t2 ≥ t1 ≥ 0, we conclude CN (t1, t2) = λt1. Therefore, we can write CN (t1, t2) = λ min(t1, t2), for t1, t2 ∈ [0, ∞). Problem 4 Let {N(t), t ∈ [0, ∞)} be a Poisson process with rate λ, and X1 be its first arrival time. Show that given N(t) = 1, then X1 is uniformly distributed in (0, t]. That is, show that P(X1 ≤ x|N(t) = 1) = , for 0 ≤ x ≤ t. Solution For 0 ≤ x ≤ t, we can write P(X1 ≤ x|N(t) = 1) = . We know that P(N(t) = 1) = λte −λt, and P(X1 ≤ x, N(t) = 1) = P(one arrival in (0, x]   and   no arrivals in (x, t]) = [λxe −λx] ⋅ [e −λ(t−x)] = λxe −λt. Thus, x t P(X1 ≤ x, N(t) = 1) P(N(t) = 1) P(X1 ≤ x|N(t) = 1) = , for 0 ≤ x ≤ t. Note: The above result can be generalized for n arrivals. That is, given that N(t) = n, the n arrival times have the same joint CDF as the order statistics of n independent Uniform(0, t) random variables. This fact is discussed more in detail in the End of Chapter Problems. Problem 5 Let N1(t) and N2(t) be two independent Poisson processes with rates λ1 = 1 and λ2 = 2, respectively . Let N(t) be the merged process N(t) = N1(t) + N2(t). a . Find the probability that N(1) = 2 and N(2) = 5. b . Given that N(1) = 2, find the probability that N1(1) = 1. Solution N(t) is a Poisson process with rate λ = 1 + 2 = 3. a . We have P(N(1) = 2, N(2) = 5) = P(two–––– arrivals in (0, 1] and three–––––– arrivals in (1, 2]) = [ ] ⋅ [ ] ≈ .05 b . P(N1(1) = 1|N(1) = 2) = = = = [e −1 ⋅ 2e −2] / [ ] = . x t e −332 2! e −333 3! P(N1(1) = 1, N(1) = 2) P(N(1) = 2) P(N1(1) = 1, N2(1) = 1) P(N(1) = 2) P(N1(1) = 1) ⋅ P(N2(1) = 1) P(N(1) = 2) e −332 2! 4 9 Problem 6 Let N1(t) and N2(t) be two independent Poisson processes with rates λ1 = 1 and λ2 = 2, respectively . Find the probability that the second arrival in N1(t) occurs before the third arrival in N2(t). Hint: One way to solve this problem is to think of N1(t) and N2(t) as two processes obtained from splitting a Poisson process. Solution Let N(t) be a Poisson process with rate λ = 1 + 2 = 3. W e split N(t) into two processes N1(t) and N2(t) in the following way . For each arrival, a coin with P(H) = is tossed. If the coin lands heads up, the arrival is sent to the first process ( N1(t)), otherwise it is sent to the second process. The coin tosses are independent of each other and are independent of N(t). Then a . N1(t) is a Poisson process with rate λp = 1; b . N2(t) is a Poisson process with rate λ(1 − p) = 2; c. N1(t) and N2(t) are independent. Thus, N1(t) and N2(t) have the same probabilistic properties as the ones stated in the problem. W e can now restate the probability that the second arrival in N1(t) occurs before the third arrival in N2(t) as the probability of observing at least two heads in four coin tosses, which is 4 ∑ k=2 ( )( ) k( ) 4−k. 1 3 4 k 1 3 2 3 1 1.2.1 Introduction Consider a discrete-time random process {Xm, m = 0, 1, 2, …}. In the very simple case where the Xm's are independent, the analysis of this process is relatively straightforward. In this case, there is no \"memory\" in the system, so each Xm can be looked at independently from previous ones. However , for many real-life processes, the independence assumption is not valid. For example, if Xm is the stock price of a company at time m ∈ {0, 1, 2, . . }, then it is reasonable to assume that the Xm's are dependent. Therefore, we need to develop models where the value of Xm depends on the previous values. In a Markov chain, Xm+1 depends on Xm, but given Xm, it does not depend on the other previous values X0, X1, ⋯, Xm−1. That is, conditioned on Xm, the random variable Xm+1 is independent of the random variables X0, X1, ⋯, Xm−1. Markov chains are usually used to model the evolution of \"states\" in probabilistic systems. More specifically , consider a system with the set of possible states S = {s1, s2, . . . }. Without loss of generality , the states are usually chosen to be 0, 1, 2, ⋯, or 1, 2, 3, ⋯, depending on which one is more convenient for a particular problem. If Xn = i, we say that the system is in state i at time n. The idea behind Markov chains is usually summarized as follows: \"conditioned on the current state, the past and the future states are independent.\" For example, suppose that we are modeling a queue at a bank. The number of people in the queue is a non-negative integer . Here, the state of the system can be defined as the number of people in the queue. More specifically , if Xn shows the number of people in the queue at time n, then Xn ∈ S = {0, 1, 2, . . . }. The set S is called the state space of the Markov chain. Let us now provide a formal definition for discrete-time Markov chains. Discrete-T ime Markov Chains Consider the random process {Xn, n = 0, 1, 2, ⋯}, where RXi = S ⊂ {0, 1, 2, ⋯}. W e say that this process is a Markov chain if P(Xm+1 = j|Xm = i, Xm−1 = im−1, ⋯ , X0 = i0) = P(Xm+1 = j|Xm = i), for all m, j, i, i0, i1, ⋯ im−1. If the number of states is finite, e.g., S = {0, 1, 2, ⋯ , r}, we call it a finite Markov chain. If Xn = j, we say that the process is in state j. The numbers P(Xm+1 = j|Xm = i) are called the transition probabilities . W e assume that the transition probabilities do not depend on time. That is, P(Xm+1 = j|Xm = i) does not depend on m. Thus, we can define pij = P(Xm+1 = j|Xm = i). In particular , we have pij = P(X1 = j|X0 = i) = P(X2 = j|X1 = i) = P(X3 = j|X2 = i) = ⋯ . In other words, if the process is in state i, it will next make a transition to state j with probability pij. 1 1.2.2 State T ransition Matrix and Diagram We often list the transition probabilities in a matrix. The matrix is called the state transition matrix or transition probability matrix and is usually shown by P. Assuming the states are 1, 2, ⋯, r, then the state transition matrix is given by P = ⎡ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎣ p11 p12 . . . p1r p21 p22 . . . p2r . . . . . . . . . . . . pr1 pr2 . . . prr ⎤ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎦ . Note that pij ≥ 0, and for all i, we have r ∑ k=1 pik = r ∑ k=1 P(Xm+1 = k|Xm = i) = 1. This is because, given that we are in state i, the next state must be one of the possible states. Thus, when we sum over all the possible values of k, we should get one. That is, the rows of any state transition matrix must sum to one. State Tran si ti o n Di agram: A Markov chain is usually shown by a state transition diagram . Consider a Markov chain with three possible states 1, 2, and 3 and the following transition probabilities P = ⎡ ⎢ ⎢ ⎢ ⎢ ⎢ ⎣ 0 0 ⎤ ⎥ ⎥ ⎥ ⎥ ⎥ ⎦ . Figure 1 1.7 shows the state transition diagram for the above Markov chain. In this diagram, there are three possible states 1, 2, and 3, and the arrows from each state to other states show the transition probabilities pij. When there is no arrow from state i to state j, it means that pij = 0. 1 4 1 2 1 4 1 3 2 3 1 2 1 2 Figure 1 1.7 - A state transition diagram. Example 1 1. 4 Consider the Markov chain shown in Figure 1 1.7. a . Find P(X4 = 3|X3 = 2). b . Find P(X3 = 1|X2 = 1). c. If we know P(X0 = 1) = , find P(X0 = 1, X1 = 2). d. If we know P(X0 = 1) = , find P(X0 = 1, X1 = 2, X2 = 3). Solution a . By definition P(X4 = 3|X3 = 2) = p23 = . b . By definition P(X3 = 1|X2 = 1) = p11 = . c. We can write P(X0 = 1, X1 = 2) = P(X0 = 1)P(X1 = 2|X0 = 1) = ⋅  p12 = ⋅ = . d. We can write 1 3 1 3 2 3 1 4 1 3 1 3 1 2 1 6 P(X0 = 1, X1 = 2, X2 = 3) = P(X0 = 1)P(X1 = 2|X0 = 1)P(X2 = 3|X1 = 2, X0 = 1) = P(X0 = 1)P(X1 = 2|X0 = 1)P(X2 = 3|X1 = 2) (by Markov property) = ⋅  p12 ⋅ p23 = ⋅ ⋅ = . 1 3 1 3 1 2 2 3 1 9 1 1.2.3 Probability Distributions S tate P ro bability D istributio n s: Consider a Markov chain {Xn, n = 0, 1, 2, . . . }, where Xn ∈ S = {1, 2, ⋯ , r}. Suppose that we know the probability distribution of X0. More specifically , define the row vector π(0) as π(0) = [ P(X0 = 1) P(X0 = 2) ⋯ P(X0 = r) ] . How can we obtain the probability distribution of X1, X2, ⋯? W e can use the law of total probability . More specifically , for any j ∈ S, we can write P(X1 = j) = r ∑ k=1 P(X1 = j|X0 = k)P(X0 = k) = r ∑ k=1 pkjP(X0 = k). If we generally define π(n) = [ P(Xn = 1) P(Xn = 2) ⋯ P(Xn = r) ] , we can rewrite the above result in the form of matrix multiplication π(1) = π(0)P, where P is the state transition matrix. Similarly , we can write π(2) = π(1)P = π(0)P 2. More generally , we can write π(n+1) = π(n)P,  for n = 0, 1, 2, ⋯ ; π(n) = π(0)P n,  for n = 0, 1, 2, ⋯ . Example 1 1. 5 Consider a system that can be in one of two possible states, S = {0, 1}. In particular , suppose that the transition matrix is given by P = ⎡ ⎢ ⎣ ⎤ ⎥ ⎦ . Suppose that the system is in state 0 at time n = 0, i.e., X0 = 0. a . Draw the state transition diagram. b . Find the probability that the system is in state 1 at time n = 3. Solution a . The state transition diagram is shown in Figure 1 1.8. Figure 1 1.8 - A state transition diagram. b . Here, we know π(0) = [ P(X0 = 0) P(X0 = 1) ] = [ 1 0 ] . Thus, π(3) = π(0)P 3 = [ 1 0 ] ⎡ ⎢ ⎣ ⎤ ⎥ ⎦ 3 = [ ] . Thus, the probability that the system is in state 1 at time n = 3 is . 1 2 1 2 1 3 2 3 1 2 1 2 1 3 2 3 29 72 43 72 43 72 n-S tep Tran sitio n P ro babilities: Consider a Markov chain {Xn, n = 0, 1, 2, . . . }, where Xn ∈ S. If X0 = i, then X1 = j with probability pij. That is, pij gives us the probability of going from state i to state j in one step. Now suppose that we are interested in finding the probability of going from state i to state j in two steps, i.e., p (2) ij = P(X2 = j|X0 = i). We can find this probability by applying the law of total probability . In particular , we argue that X1 can take one of the possible values in S. Thus, we can write p (2) ij = P(X2 = j|X0 = i) = ∑ k∈S P(X2 = j|X1 = k, X0 = i)P(X1 = k|X0 = i) = ∑ k∈S P(X2 = j|X1 = k)P(X1 = k|X0 = i) (by Markov property) = ∑ k∈S pkjpik. We conclude p (2) ij = P(X2 = j|X0 = i) = ∑ k∈S pikpkj (11.4) We can explain the above formula as follows. In order to get to state j, we need to pass through some intermediate state k. The probability of this event is pikpkj. T o obtain p (2) ij , we sum over all possible intermediate states. Accordingly , we can define the two-step transition matrix as follows: P (2) = ⎡ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎣ p (2) 11 p (2) 12 . . . p (2) 1r p (2) 21 p (2) 22 . . . p (2) 2r . . . . . . . . . . . . p (2) r1 p (2) r2 . . . p (2) rr ⎤ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎦ . Looking at Equation 1 1.4 , we notice that p (2) ij is in fact the element in the ith row and j th column of the matix P 2 = ⎡ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎣ p11 p12 . . . p1r p21 p22 . . . p2r . . . . . . . . . . . . pr1 pr2 . . . prr ⎤ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎦ ⋅ ⎡ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎣ p11 p12 . . . p1r p21 p22 . . . p2r . . . . . . . . . . . . pr1 pr2 . . . prr ⎤ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎦ . Thus, we conclude that the two-step transition matrix can be obtained by squaring the state transition matrix, i.e., P (2) = P 2. More generally , we can define the n-step transition probabilities p (n) ij as p (n) ij = P(Xn = j|X0 = i), for n = 0, 1, 2, ⋯ , (11.5) and the n-step transition matrix, P (n), as P (n) = ⎡ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎣ p (n) 11 p (n) 12 . . . p (n) 1r p (n) 21 p (n) 22 . . . p (n) 2r . . . . . . . . . . . . p (n) r1 p (n) r2 . . . p (n) rr ⎤ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎦ . We can now generalize Equation 1 1.5 . Let m and n be two positive integers and assume X0 = i. In order to get to state j in (m + n) steps, the chain will be at some intermediate state k after m steps. T o obtain p (m+n) ij , we sum over all possible intermediate states: p (m+n) ij = P(Xm+n = j|X0 = i) = ∑ k∈S p (m) ik p (n) kj . The above equation is called the Chapman-Kolmogorov equation . Similar to the case of two-step transition probabilities, we can show that P (n) = P n, for n = 1, 2, 3, ⋯. The Chapman-Kolmogorov equation can be written as p (m+n) ij = P(Xm+n = j|X0 = i) = ∑ k∈S p (m) ik p (n) kj . The n-step transition matrix is given by P (n) = P n,  for n = 1, 2, 3, ⋯ . 1 1.2.4 Classification of States To better understand Markov chains, we need to introduce some definitions. The first definition concerns the accessibility of states from each other: If it is possible to go from state i to state j, we say that state j is accessible from state i. In particular , we can provide the following definitions. We say that state j is accessible from state i, written as i → j, if p (n) ij > 0 for some n. We assume every state is accessible from itself since p (0) ii = 1. Two states i and j are said to communicate , written as i ↔ j, if they are accessible from each other . In other words, i ↔ j  means  i → j and j → i. Communication is an equivalence relation. That means that −every state communicates with itself, i ↔ i; −if i ↔ j, then j ↔ i; −if i ↔ j and j ↔ k, then i ↔ k. Therefore, the states of a Markov chain can be partitioned into communicating classes such that only members of the same class communicate with each other . That is, two states i and j belong to the same class if and only if i ↔ j. Example 1 1. 6 Consider the Markov chain shown in Figure 1 1.9. It is assumed that when there is an arrow from state i to state j, then pij > 0. Find the equivalence classes for this Markov chain. Figure 1 1.9 - A state transition diagram. Solution There are four communicating classes in this Markov chain. Looking at Figure 1 1.10, we notice that states 1 and 2 communicate with each other , but they do not communicate with any other nodes in the graph. Similarly , nodes 3 and 4 communicate with each other , but they do not communicate with any other nodes in the graph. State 5 does not communicate with any other states, so it by itself is a class. Finally , states 6 , 7, and 8 construct another class. Thus, here are the classes: Class 1 = {state 1, state 2}, Class 2 = {state 3, state 4}, Class 3 = {state 5}, Class 4 = {state 6, state 7, state 8}. Figure 1 1.10 - Equivalence classes. A Markov chain is said to be irreducible if it has only one communicating class. As we will see shortly , irreducibility is a desirable property in the sense that it can simplify analysis of the limiting behavior . A Markov chain is said to be irreducible if all states communicate with each other . Looking at Figure 1 1.10, we notice that there are two kinds of classes. In particular , if at any time the Markov chain enters Class 4, it will always stay in that class. On the other hand, for other classes this is not true. For example, if X0 = 1, then the Markov chain might stay in Class 1 for a while, but at some point, it will leave that class and it will never return to that class again. The states in Class 4 are called recurrent states, while the other states in this chain are called transient . In general, a state is said to be recurrent if, any time that we leave that state, we will return to that state in the future with probability one. On the other hand, if the probability of returning is less than one, the state is called transient. Here, we provide a formal definition: For any state i, we define fii = P(Xn = i,  for some n ≥ 1|X0 = i). State i is recurrent if fii = 1, and it is transient if fii < 1. It is relatively easy to show that if two states are in the same class, either both of them are recurrent, or both of them are transient. Thus, we can extend the above definitions to classes. A class is said to be recurrent if the states in that class are recurrent. If, on the other hand, the states are transient, the class is called transient. In general, a Markov chain might consist of several transient classes as well as several recurrent classes. Consider a Markov chain and assume X0 = i. If i is a recurrent state, then the chain will return to state i any time it leaves that state. Therefore, the chain will visit state i an infinite number of times. On the other hand, if i is a transient state, the chain will return to state i with probability fii < 1. Thus, in that case, the total number of visits to state i will be a Geometric random variable with parameter 1 − fii. Consider a discrete-time Markov chain. Let V be the total number of visits to state i. a . If i is a recurrent state, then P(V = ∞|X0 = i) = 1. b . If i is a transient state, then V |X0 = i ∼ Geometric(1 − fii). Example 1 1. 7 Show that in a finite Markov chain, there is at least one recurrent class. Solution Consider a finite Markov chain with r states, S = {1, 2, ⋯ , r}. Suppose that all states are transient. Then, starting from time 0, the chain might visit state 1 several times, but at some point the chain will leave state 1 and will never return to it. That is, there exists an integer M1 > 0 such that Xn ≠ 1, for all n ≥ M1. Similarly , there exists an integer M2 > 0 such that Xn ≠ 2, for all n ≥ M2, and so on. Now , if you choose n ≥ max{M1, M2, ⋯ , Mr}, then Xn cannot be equal to any of the states 1, 2, ⋯ , r. This is a contradiction, so we conclude that there must be at least one recurrent state, which means that there must be at least one recurrent class. P eri o di ci ty: Consider the Markov chain shown in Figure 1 1.1 1. There is a periodic pattern in this chain. Starting from state 0, we only return to 0 at times n = 3, 6, ⋯. In other words, p (n) 00 = 0, if n is not divisible by 3. Such a state is called a periodic state with period d(0) = 3. Figure 1 1.1 1 - A state transition diagram. The period of a state i is the largest integer d satisfying the following property: p (n) ii = 0, whenever n is not divisible by d. The period of i is shown by d(i). If p (n) ii = 0, for all n > 0, then we let d(i) = ∞. −If d(i) > 1, we say that state i is periodic . −If d(i) = 1, we say that state i is aperiodic . You can show that all states in the same communicating class have the same period. A class is said to be periodic if its states are periodic. Similarly , a class is said to be aperiodic if its states are aperiodic. Finally , a Markov chain is said to be aperiodic if all of its states are aperiodic. If i ↔ j, then d(i) = d(j). Why is periodicity important? As we will see shortly , it plays a roll when we discuss limiting distributions. It turns out that in a typical problem, we are given an irreducible Markov chain, and we need to check if it is aperiodic. How do we check that a Markov chain is aperiodic? Here is a useful method. Remember that two numbers m and l are said to be co-prime if their greatest common divisor (gcd) is 1, i.e., gcd(l, m) = 1. Now , suppose that we can find two co-prime numbers l and m such that p (l) ii > 0 and p (m) ii > 0. That is, we can go from state i to itself in l steps, and also in m steps. Then, we can conclude state i is aperiodic. If we have an irreducible Markov chain, this means that the chain is aperiodic. Since the number 1 is co-prime to every integer , any state with a self-transition is aperiodic. Consider a finite irreducible Markov chain Xn: a . If there is a self-transition in the chain ( pii > 0 for some i), then the chain is aperiodic. b . Suppose that you can go from state i to state i in l steps, i.e., p (l) ii > 0. Also suppose that p (m) ii > 0. If gcd(l, m) = 1, then state i is aperiodic. c. The chain is aperiodic if and only if there exists a positive integer n such that all elements of the matrix P n are strictly positive, i.e., p (n) ij > 0,  for all i, j ∈ S. Example 1 1. 8 Consider the Markov chain in Example 1 1.6 . a . Is Class 1 = {state 1, state 2} aperiodic? b . Is Class 2 = {state 3, state 4} aperiodic? c. Is Class 4 = {state 6, state 7, state 8} aperiodic? Solution a . Class 1 = {state 1, state 2} is aperiodic since it has a self-transition, p22 > 0. b . Class 2 = {state 3, state 4} is periodic with period 2. c. Class 4 = {state 6, state 7, state 8} is aperiodic . For example, note that we can go from state 6 to state 6 in two steps ( 6 − 7 − 6) and in three steps ( 6 − 7 − 8 − 6). Since gcd(2, 3) = 1, we conclude state 6 and its class are aperiodic. 1 1.2.5 Using the Law of T otal Probability with Recursion A very useful technique in the analysis of Markov chains is using law of total probability . In fact, we have already used this when finding n-step transition probabilities. In this section, we will use this technique to find absorption probabilities , mean hitting times , and mean return times . We will introduce this technique by looking at an example. W e will then provide the general formulas. Y ou should try to understand the main idea here. This way , you do not need to memorize any formulas. Let's consider the Markov chain shown in Figure 1 1.12. Figure 1 1.12 - A state transition diagram. The state transition matrix of this Markov chain is given by the following matrix. P = ⎡ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎣ 1 0 0 0 0 0 0 0 0 0 0 1 ⎤ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎦ . Before going any further , let's identify the classes in this Markov chain. Example 1 1. 9 For the Markov chain given in Figure 1 1.12, answer the following questions: How many classes are there? For each class, mention if it is recurrent or transient. Solution There are three classes: Class 1 consists of one state, state 0, which is a recurrent state. Class two consists of two states, states 1 and 2, both of which are transient. Finally , class three consists of one state, state 3, which is a recurrent state. 1 3 2 3 1 2 1 2 Note that states 0 and 3 have the following property: once you enter those states, you never leave them. For this reason, we call them absorbing states. For our example here, there are two absorbing states. The process will eventually get absorbed in one of them. The first question that we would like to address deals with finding absorption probabilities. Abso rp tio n Pro babilities: Consider the Markov chain in Figure 1 1.12. Let's define ai as the absorption probability in state 0 if we start from state i. More specifically , a0 = P(absorption in 0|X0 = 0), a1 = P(absorption in 0|X0 = 1), a2 = P(absorption in 0|X0 = 2), a3 = P(absorption in 0|X0 = 3). By the above definition, we have a0 = 1 and a3 = 0. To find the values of a1 and a2, we apply the law of total probability with recursion. The main idea is the following: if Xn = i, then the next state will be Xn+1 = k with probability pik. Thus, we can write ai = ∑ k akpik,  for i = 0, 1, 2, 3 (11.6) Solving the above equations will give us the values of a1 and a2. More specifically , using Equation 1 1.6, we obtain a0 = a0, a1 = a0 + a2, a2 = a1 + a3, a3 = a3. We also know a0 = 1 and a3 = 0. Solving for a1 and a2, we obtain a1 = , a2 = . Let's now define bi as the absorption probability in state 3 if we start from state i. Since ai + bi = 1, we conclude 1 3 2 3 1 2 1 2 1 2 1 4 b0 = 0, b1 = , b2 = , b3 = 1. Nevertheless, for practice, let's find the bi's directly . Example 1 1. 10 Consider the Markov chain in Figure 1 1.12. Let's define bi as the absorption probability in state 3 if we start from state i. Use the above procedure to obtain bi for i = 0, 1, 2, 3. Solution From the definition of bi and the Markov chain graph, we have b0 = 0 and b3 = 1. Writing Equation 1 1.6 for i = 1, 2, we obtain b1 = b0 + b2 = b2, b2 = b1 + b3 = b1 + . Solving the above equations, we obtain b1 = , b2 = . 1 2 3 4 1 3 2 3 2 3 1 2 1 2 1 2 1 2 1 2 3 4 Absorption Probabilities Consider a finite Markov chain {Xn, n = 0, 1, 2, ⋯} with state space S = {0, 1, 2, ⋯ , r}. Suppose that all states are either absorbing or transient. Let l ∈ S be an absorbing state. Define ai = P(absorption in l|X0 = i),  for all i ∈ S. By the above definition, we have al = 1, and aj = 0 if j is any other absorbing state. T o find the unknown values of ai's, we can use the following equations ai = ∑ k akpik,  for i ∈ S. In general, a finite Markov chain might have several transient as well as several recurrent classes. As n increases, the chain will get absorbed in one of the recurrent classes and it will stay there forever . We can use the above procedure to find the probability that the chain will get absorbed in each of the recurrent classes. In particular , we can replace each recurrent class with one absorbing state. Then, the resulting chain consists of only transient and absorbing states. W e can then follow the above procedure to find absorption probabilities. An example of this procedure is provided in the Solved Problems Section (See Problem 2 in Section 1 1.2.7 ). Mean Hittin g Times: We now would like to study the expected time until the process hits a certain set of states for the first time. Again, consider the Markov chain in Figure 1 1.12. Let's define ti as the number of steps needed until the chain hits state 0 or state 3, given that X0 = i. In other words, ti is the expected time (number of steps) until the chain is absorbed in 0 or 3, given that X0 = i. By this definition, we have t0 = t3 = 0. To find t1 and t2, we use the law of total probability with recursion as before. For example, if X0 = 1, then after one step, we have X1 = 0 or X1 = 2. Thus, we can write t1 = 1 + t0 + t2 = 1 + t2. 1 3 2 3 2 3 Similarly , we can write t2 = 1 + t1 + t3 = 1 + t1. Solving the above equations, we obtain t1 = , t2 = . Generally , let A ⊂ S be a set of states. The above procedure can be used to find the expected time until the chain first hits one of the states in the set A. Mean Hitting T imes Consider a finite Markov chain {Xn, n = 0, 1, 2, ⋯} with state space S = {0, 1, 2, ⋯ , r}. Let A ⊂ S be a set of states. Let T be the first time the chain visits a state in A. For all i ∈ S, define ti = E[T|X0 = i]. By the above definition, we have tj = 0, for all j ∈ A. To find the unknown values of ti's, we can use the following equations ti = 1 + ∑ k tkpik,  for i ∈ S − A. Mean Return Times: Another interesting random variable is the first return time. In particular , assuming the chain is in state l, we consider the expected time (number of steps) needed until the chain returns to state l. For example, consider a Markov chain for which X0 = 2. If the chain gets the values X0 = 2, X1 = 1, X2 = 4, X3 = 3, X4 = 2, X5 = 3, X6 = 2, X7 = 3, ⋯ , then the first return to state 2 occurs at time n = 4. Thus, the first return time to state 2 is equal to 4 for this example. Here, we are interested in the expected value of the first return time. In particular , assuming X0 = l, let's define rl as the expected number of 1 2 1 2 1 2 5 2 9 4 steps needed until the chain returns to state l. To make the definition more precise, let's define Rl = min{n ≥ 1 : Xn = l}. Then, rl = E[Rl|X0 = l]. Note that by definition, Rl ≥ 1, so we conclude rl ≥ 1. In fact, rl = 1 if and only if l is an absorbing state (i.e., pll = 1). As before, we can apply the law of total probability to obtain rl. Again, let's define tk as the expected time until the chain hits state l for the first time, given that X0 = k. We have already seen how to find tk's (mean hitting times). Using the law of total probability , we can write rl = 1 + ∑ k plktk. Let's look at an example to see how we can find the mean return time. Example 1 1. 1 1 Consider the Markov chain shown in Figure 1 1.13. Let tk be the expected number of steps until the chain hits state 1 for the first time, given that X0 = k. Clearly , t1 = 0. Also, let r1 be the mean return time to state 1. 1 . Find t2 and t3. 2 . Find r1. Figure 1 1.13 - A state transition diagram. Solution 1 . To find t2 and t3, we use the law of total probability with recursion as before. For example, if X0 = 2, then after one step, we have X1 = 2 or X1 = 3. Thus, we can write t2 = 1 + t2 + t3. Similarly , we can write t3 = 1 + t1 + t2 = 1 + t2. Solving the above equations, we obtain t2 = 5, t3 = . 2 . To find r1, we note that if X0 = 1, then X1 = 1 or X1 = 2. We can write r1 = 1 + ⋅ t1 + t2 = 1 + ⋅ 0 + ⋅ 5 = . Here, we summarize the formulas for finding the mean return times. As we mentioned before, there is no need to memorize these formulas once you understand how they are derived. 1 3 2 3 1 2 1 2 1 2 7 2 1 2 1 2 1 2 1 2 7 2 Mean Return T imes Consider a finite irreducible Markov chain {Xn, n = 0, 1, 2, ⋯} with state space S = {0, 1, 2, ⋯ , r}. Let l ∈ S be a state. Let rl be the mean return time to state l. Then rl = 1 + ∑ k tkplk, where tk is the expected time until the chain hits state l given X0 = k. Specifically , tl = 0, tk = 1 + ∑ j tjpkj,  for k ≠ l. 1 1.2.6 Stationary and Limiting Distributions Here, we would like to discuss long-term behavior of Markov chains. In particular , we would like to know the fraction of times that the Markov chain spends in each state as n becomes large. More specifically , we would like to study the distributions π(n) = [ P(Xn = 0) P(Xn = 1) ⋯ ] as n → ∞. To better understand the subject, we will first look at an example and then provide a general analysis. Example 1 1. 12 Consider a Markov chain with two possible states, S = {0, 1}. In particular , suppose that the transition matrix is given by P = [ 1 − a a b 1 − b ] , where a and b are two real numbers in the interval [0, 1] such that 0 < a + b < 2. Suppose that the system is in state 0 at time n = 0 with probability α, i.e., π(0) = [ P(X0 = 0) P(X0 = 1) ] = [ α 1 − α ] , where α ∈ [0, 1]. a . Using induction (or any other method), show that P n = [ b a b a ] + [ a −a −b b ] . b . Show that lim n→∞ P n = [ b a b a ] . c. Show that lim n→∞ π(n) = [ ] . 1 a + b (1 − a − b) n a + b 1 a + b b a+b a a+b Solution a . For n = 1, we have P 1 = [ 1 − a a b 1 − b ] = [ b a b a ] + [ a −a −b b ] . Assuming that the statement of the problem is true for n, we can write P n+1 as P n+1 = P nP = ([ b a b a ] + (1 − a − b) n [ a −a −b b ]) ⋅ [ 1 − a a b 1 − b ] = [ b a b a ] + [ a −a −b b ] , which completes the proof. b . By assumption 0 < a + b < 2, which implies −1 < 1 − a − b < 1. Thus, lim n→∞(1 − a − b) n = 0. Therefore, lim n→∞ P n = [ b a b a ] . c. We have lim n→∞ π(n) = lim n→∞ [π(0)P n] = π(0) lim n→∞ P n = [ α 1 − α ] ⋅ [ b a b a ] = [ ] . In the above example, the vector lim n→∞ π(n) = [ ] 1 a + b 1 − a − b a + b 1 a + b 1 a + b (1 − a − b) n+1 a + b 1 a + b 1 a + b b a+b a a+b b a+b a a+b is called the limiting distribution of the Markov chain. Note that the limiting distribution does not depend on the initial probabilities α and 1 − α. In other words, the initial state (X0) does not matter as n becomes large. Thus, for i = 1, 2, we can write lim n→∞ P(Xn = 0|X0 = i) = , lim n→∞ P(Xn = 1|X0 = i) = . Remember that we show P(Xn = j|X0 = i) by P (n) ij , which is the entry in the ith row and jth column of P n. Limiting Distributions The probability distribution π = [π0, π1, π2, ⋯] is called the limiting distribution of the Markov chain Xn if πj = lim n→∞ P(Xn = j|X0 = i) for all i, j ∈ S, and we have ∑ j∈S πj = 1. By the above definition, when a limiting distribution exists, it does not depend on the initial state ( X0 = i), so we can write πj = lim n→∞ P(Xn = j), for all j ∈ S. So far we have shown that the Markov chain in Example 1 1.12 has the following limiting distribution: π = [ π0 π1 ] = [ ] . Let's now look at mean return times for this Markov chain. Example 1 1. 13 b a + b a a + b b a+b a a+b Consider a Markov chain in Example 1 1.12 : a Markov chain with two possible states, S = {0, 1}, and the transition matrix P = [ 1 − a a b 1 − b ] , where a and b are two real numbers in the interval [0, 1] such that 0 < a + b < 2. Find the mean return times, r0 and r1, for this Markov chain. Solution We can use the method of the law of total probability that we explained before to find the mean return times ( Example 1 1.1 1 ). We can also find r0 and r1 directly as follows: Let R be the first return time to state 0, i.e., r0 = E[R|X0 = 0]. If X0 = 0, then X1 = 0 with probability 1 − a, and X1 = 1 with probability a. Thus, using the law of total probability , and assuming X0 = 0, we can write r0 = E[R|X1 = 0]P(X1 = 0) + E[R|X1 = 1]P(X1 = 1) = E[R|X1 = 0] ⋅ (1 − a) + E[R|X1 = 1] ⋅ a. If X1 = 0, then R = 1, so E[R|X1 = 0] = 1. If X1 = 1, then R ∼ 1 + Geometric(b), so E[R|X1 = 1] = 1 + E[Geometric(b)] = 1 + . We conclude r0 = E[R|X1 = 0]P(X1 = 0) + E[R|X1 = 1]P(X1 = 1) = 1 ⋅ (1 − a) + (1 + ) ⋅ a = . Similarly , we can obtain the mean return time to state 1: r1 = . 1 b 1 b a + b b a + b a We notice that for this example, the mean return times are given by the inverse of the limiting probabilities. In particular , we have r0 = , r1 = . As we will see shortly , this is not a coincidence. In fact, we can explain this intuitively . The larger the πi is, the smaller the ri will be. For example, if πi = , we conclude that the chain is in state i one-fourth of the time. In this case, ri = 4, which means that on average it takes the chain four time units to go back to state i. The two-state Markov chain discussed above is a \"nice\" one in the sense that it has a well-defined limiting behavior that does not depend on the initial probability distribution (PMF of X0). However , not all Markov chains are like that. For example, consider the same Markov chain; however , choose a = b = 1. In this case, the chain has a periodic behavior , i.e., Xn+2 = Xn,  for all n. In particular , Xn = ⎧⎪ ⎨ ⎪⎩ X0 if n is even X1 if n is odd In this case, the distribution of Xn does not converge to a single PMF . Also, the distribution of Xn depends on the initial distribution. As another example, if we choose a = b = 0, the chain will consist of two disconnected nodes. In this case, Xn = X0,  for all n. Here again, the PMF of Xn depends on the initial distribution. Now , the question that arises here is: when does a Markov chain have a limiting distribution (that does not depend on the initial PMF)? W e will next discuss this question. W e will first consider finite Markov chains and then discuss infinite Markov chains. Fin ite Marko v C hain s: Here, we consider Markov chains with a finite number of states. In general, a finite Markov chain can consist of several transient as well as recurrent states. As n becomes large the chain will enter a recurrent class and it will stay there forever . Therefore, when studying long-run behaviors we focus only on the recurrent classes. 1 π0 1 π1 1 4 If a finite Markov chain has more than one recurrent class, then the chain will get absorbed in one of the recurrent classes. Thus, the first question is: in which recurrent class does the chain get absorbed? W e have already seen how to address this when we discussed absorption probabilities (see Section 1 1.2.5 , and Problem 2 of in Section 1 1.2.7 ). Thus, we can limit our attention to the case where our Markov chain consists of one recurrent class. In other words, we have an irreducible Markov chain. Note that as we showed in Example 1 1.7 , in any finite Markov chain, there is at least one recurrent class. Therefore, in finite irreducible chains, all states are recurrent. It turns out that in this case the Markov chain has a well-defined limiting behavior if it is aperiodic (states have period 1). How do we find the limiting distribution? The trick is to find a stationary distribution . Here is the idea: If π = [π1, π2, ⋯] is a limiting distribution for a Markov chain, then we have π = lim n→∞ π(n) = lim n→∞ [π(0)P n]. Similarly , we can write π = lim n→∞ π(n+1) = lim n→∞ [π(0)P n+1] = lim n→∞ [π(0)P nP] = [ lim n→∞ π(0)P n]P = πP. We can explain the equation π = πP intuitively: Suppose that Xn has distribution π. As we saw before, πP gives the probability distribution of Xn+1. If we have π = πP, we conclude that Xn and Xn+1 have the same distribution. In other words, the chain has reached its steady-state (limiting) distribution. W e can equivalently write π = πP as πj = ∑ k∈S πkPkj,  for all j ∈ S. The righthand side gives the probability of going to state j in the next step. When we equate both sides, we are implying that the probability of being in state j in the next step is the same as the probability of being in state j now . Example 1 1. 14 Consider a Markov chain in Example 1 1.12 : a Markov chain with two possible states, S = {0, 1}, and the transition matrix P = [ 1 − a a b 1 − b ] , where a and b are two real numbers in the interval [0, 1] such that 0 < a + b < 2. Using π = πP, find the limiting distribution of this Markov chain. Solution Let π = [π0, π1]. Then, we can write [π0, π1] = πP = [π0, π1] [ 1 − a a b 1 − b ] = [ π0(1 − a) + π1b π0a + π1(1 − b) ] . We obtain two equations; however , they both simplify to π0a = π1b. We remember that π must be a valid probability distribution, i.e., π0 + π1 = 1. Thus, we can obtain a unique solution, i.e., π = [ π0 π1 ] = [ ] which is the same answer that we obtained previously . We now summarize the above discussion in the following theorem. b a+b a a+b Consider a finite Markov chain {Xn, n = 0, 1, 2, . . . } where Xn ∈ S = {0, 1, 2, ⋯ , r}. Assume that the chain is irreducible and aperiodic . Then, 1 . The set of equations π = πP, ∑ j∈S πj = 1 has a unique solution. 2 . The unique solution to the above equations is the limiting distribution of the Markov chain, i.e., πj = lim n→∞ P(Xn = j|X0 = i), for all i, j ∈ S. 3 . We have rj = , for all j ∈ S, where rj is the mean return time to state j. In practice, if we are given a finite irreducible Markov chain with states 0, 1, 2, ⋯ , r, we first find a stationary distribution. That is, we find a probability distribution π that satisfies πj = ∑ k∈S πkPkj,  for all j ∈ S, ∑ j∈S πj = 1. In this case, if the chain is also aperiodic, we conclude that the stationary distribution is a limiting distribution. Example 1 1. 15 Consider the Markov chain shown in Figure 1 1.14. 1 πj Figure 1 1.14- A state transition diagram. a . Is this chain irreducible? b . Is this chain aperiodic? c. Find the stationary distribution for this chain. d. Is the stationary distribution a limiting distribution for the chain? Solution a . The chain is irreducible since we can go from any state to any other states in a finite number of steps. b . Since there is a self-transition, i.e., p11 > 0, we conclude that the chain is aperiodic. c. To find the stationary distribution, we need to solve π1 = π1 + π2 + π3, π2 = π1, π3 = π1 + π2 + π3, π1 + π2 + π3 = 1. We find π1 = , π2 = , π3 = . d. Since the chain is irreducible and aperiodic, we conclude that the above stationary distribution is a limiting distribution. 1 4 1 3 1 2 1 2 1 4 2 3 1 2 3 8 3 16 7 16 C o un tably In fin ite Marko v C hain s: When a Markov chain has an infinite (but countable) number of states, we need to distinguish between two types of recurrent states: positive recurrent and null recurrent states. Remember that if state i is recurrent, then that state will be visited an infinite number of times (any time that we visit that state, we will return to it with probability one in the future). We previously defined ri as the expected number of transitions between visits to state i. Consider a recurrent state i. If ri < ∞, then state i is a positive recurrent state. Otherwise, it is called null recurrent. Let i be a recurrent state. Assuming X0 = i, let Ri be the number of transitions needed to return to state i, i.e., Ri = min{n ≥ 1 : Xn = i}. If ri = E[Ri|X0 = i] < ∞, then i is said to be positive recurrent . If E[Ri|X0 = i] = ∞, then i is said to be null recurrent . Theorem 1 1. 2 Consider an infinite Markov chain {Xn, n = 0, 1, 2, . . . } where Xn ∈ S = {0, 1, 2, ⋯}. Assume that the chain is irreducible and aperiodic . Then, one of the following cases can occur: 1 . All states are transient , and lim n→∞ P(Xn = j|X0 = i) = 0,  for all i, j. 2 . All states are null recurrent , and lim n→∞ P(Xn = j|X0 = i) = 0,  for all i, j. 3 . All states are positive recurrent . In this case, there exists a limiting distribution, π = [π0, π1, ⋯], where πj = lim n→∞ P(Xn = j|X0 = i) > 0, for all i, j ∈ S. The limiting distribution is the unique solution to the equations πj = ∞ ∑ k=0 πkPkj,  for j = 0, 1, 2, ⋯ , ∞ ∑ j=0 πj = 1. We also have rj = , for all j = 0, 1, 2, ⋯, where rj is the mean return time to state j. How do we use the above theorem? Consider an infinite Markov chain {Xn, n = 0, 1, 2, . . . }, where Xn ∈ S = {0, 1, 2, ⋯}. Assume that the chain is irreducible and aperiodic. W e first try to find a stationary distribution π by solving the equations 1 πj πj = ∞ ∑ k=0 πkPkj,  for j = 0, 1, 2, ⋯ , ∞ ∑ j=0 πj = 1. If the above equations have a unique solution, we conclude that the chain is positive recurrent and the stationary distribution is the limiting distribution of this chain. On the other hand, if no stationary solution exists, we conclude that the chain is either transient or null recurrent, so lim n→∞ P(Xn = j|X0 = i) = 0,  for all i, j. Example 1 1. 16 Consider the Markov chain shown in Figure 1 1.15. Assume that 0 < p < . Does this chain have a limiting distribution? Figure 1 1.15 - A state transition diagram. Solution This chain is irreducible since all states communicate with each other . It is also aperiodic since it includes a self-transition, P00 > 0. Let's write the equations for a stationary distribution. For state 0, we can write π0 = (1 − p)π0 + (1 − p)π1, which results in π1 = π0. For state 1, we can write π1 = pπ0 + (1 − p)π2 = (1 − p)π1 + (1 − p)π2, which results in 1 2 p 1 − p π2 = π1. Similarly , for any j ∈ {1, 2, ⋯}, we obtain πj = απj−1, where α = . Note that since 0 < p < , we conclude that 0 < α < 1. We obtain πj = α jπ0,  for j = 1, 2, ⋯ . Finally , we must have 1 = ∞ ∑ j=0 πj = ∞ ∑ j=0 α jπ0, (where 0 < α < 1) = π0 (geometric series). Thus, π0 = 1 − α. Therefore, the stationary distribution is given by πj = (1 − α)α j,  for j = 0, 1, 2, ⋯ . Since this chain is irreducible and aperiodic and we have found a stationary distribution, we conclude that all states are positive recurrent and π = [π0, π1, ⋯] is the limiting distribution. p 1 − p p 1−p 1 2 1 1 − α 1 1.2.7 Solved Problems Problem 1 Consider the Markov chain with three states, S = {1, 2, 3}, that has the following transition matrix P = ⎡ ⎢ ⎢ ⎢ ⎢ ⎢ ⎣ 0 0 ⎤ ⎥ ⎥ ⎥ ⎥ ⎥ ⎦ . a . Draw the state transition diagram for this chain. b . If we know P(X1 = 1) = P(X1 = 2) = , find P(X1 = 3, X2 = 2, X3 = 1). Solution a . The state transition diagram is shown in Figure 1 1.6 Figure 1 1.6 - A state transition diagram. b . First, we obtain P(X1 = 3) = 1 − P(X1 = 1) − P(X1 = 2) = 1 − − = . 1 2 1 4 1 4 1 3 2 3 1 2 1 2 1 4 1 4 1 4 1 2 We can now write P(X1 = 3, X2 = 2, X3 = 1) = P(X1 = 3) ⋅ p32 ⋅ p21 = ⋅ ⋅ = . Problem 2 Consider the Markov chain in Figure 1 1.17. There are two recurrent classes, R1 = {1, 2}, and R2 = {5, 6, 7}. Assuming X0 = 3, find the probability that the chain gets absorbed in R1. Figure 1 1.17 - A state transition diagram. Solution Here, we can replace each recurrent class with one absorbing state. The resulting state diagram is shown in Figure 1 1.18 1 2 1 2 1 3 1 12 Figure 1 1.18 - The state transition diagram in which we have replaced each recurrent class with one absorbing state. Now we can apply our standard methodology to find probability of absorption in state R1. In particular , define ai = P(absorption in R1|X0 = i),  for all i ∈ S. By the above definition, we have aR1 = 1, and aR2 = 0. To find the unknown values of ai's, we can use the following equations ai = ∑ k akpik,  for i ∈ S. We obtain a3 = aR1 + a4 = + a4, a4 = aR1 + a3 + aR2 = + a3. Solving the above equations, we obtain a3 = , a4 = . Therefore, if X0 = 3, the chain will end up in class R1 with probability a3 = . Problem 3 1 2 1 2 1 2 1 2 1 4 1 4 1 2 1 4 1 4 5 7 3 7 5 7 Consider the Markov chain of Example 2 . Again assume X0 = 3. We would like to find the expected time (number of steps) until the chain gets absorbed in R1 or R2. More specifically , let T be the absorption time, i.e., the first time the chain visits a state in R1 or R2. We would like to find E[T|X0 = 3]. Solution Here we follow our standard procedure for finding mean hitting times. Consider Figure 1 1.18. Let T be the first time the chain visits R1 or R2. For all i ∈ S, define ti = E[T|X0 = i]. By the above definition, we have tR1 = tR2 = 0. To find t3 and t4, we can use the following equations ti = 1 + ∑ k tkpik,  for i = 3, 4. Specifically , we obtain t3 = 1 + tR1 + t4 = 1 + t4, t4 = 1 + tR1 + t3 + tR2 = 1 + t3. Solving the above equations, we obtain t3 = , t4 = . Therefore, if X0 = 3, it will take on average steps until the chain gets absorbed in R1 or R2. Problem 4 Consider the Markov chain shown in Figure 1 1.19. Assume X0 = 1, and let R be the first time that the chain returns to state 1, i.e., R = min{n ≥ 1 : Xn = 1}. 1 2 1 2 1 2 1 4 1 4 1 2 1 4 12 7 10 7 12 7 Find E[R|X0 = 1]. Figure 1 1.19 - A state transition diagram. Solution In this question, we are asked to find the mean return time to state 1. Let r1 be the mean return time to state 1, i.e., r1 = E[R|X0 = 1]. Then r1 = 1 + ∑ k tkp1k, where tk is the expected time until the chain hits state 1 given X0 = k. Specifically , t1 = 0, tk = 1 + ∑ j tjpkj,  for k ≠ 1. So, let's first find tk's. We obtain t2 = 1 + t1 + t3 = 1 + t3, t3 = 1 + t3 + t1 = 1 + t3. 1 3 2 3 2 3 1 2 1 2 1 2 Solving the above equations, we obtain t3 = 2, t2 = . Now , we can write r1 = 1 + t1 + t2 + t3 = 1 + ⋅ 0 + ⋅ + ⋅ 2 = . Problem 5 Consider the Markov chain shown in Figure 1 1.20. Figure 1 1.20 - A state transition diagram. a . Is this chain irreducible? b . Is this chain aperiodic? c. Find the stationary distribution for this chain. d. Is the stationary distribution a limiting distribution for the chain? Solution 7 3 1 4 1 2 1 4 1 4 1 2 7 3 1 4 8 3 a . The chain is irreducible since we can go from any state to any other states in a finite number of steps. b . The chain is aperiodic since there is a self-transition, i.e., p11 > 0. c. To find the stationary distribution, we need to solve π1 = π1 + π2 + π3, π2 = π1 + π3, π3 = π1 + π2, π1 + π2 + π3 = 1. We find π1 ≈ 0.457, π2 ≈ 0.257, π3 ≈ 0.286 d. The above stationary distribution is a limiting distribution for the chain because the chain is irreducible and aperiodic. Problem 6 Consider the Markov chain shown in Figure 1 1.21. Assume that < p < 1. Does this chain have a limiting distribution? For all i, j ∈ {0, 1, 2, ⋯}, find lim n→∞ P(Xn = j|X0 = i). Figure 1 1.21 - A state transition diagram. Solution This chain is irreducible since all states communicate with each other . It is also aperiodic since it includes a self-transition, P00 > 0. Let's write the equations for a stationary distribution. For state 0, we can write π0 = (1 − p)π0 + (1 − p)π1, which results in 1 2 1 3 1 2 1 4 1 2 1 4 2 3 1 2 π1 = π0. For state 1, we can write π1 = pπ0 + (1 − p)π2 = (1 − p)π1 + (1 − p)π2, which results in π2 = π1. Similarly , for any j ∈ {1, 2, ⋯}, we obtain πj = απj−1, where α = . Note that since < p < 1, we conclude that α > 1. We obtain πj = α jπ0,  for j = 1, 2, ⋯ . Finally , we must have 1 = ∞ ∑ j=0 πj = ∞ ∑ j=0 α jπ0, (where α > 1) = ∞π0. Therefore, the above equation cannot be satisfied if π0 > 0. If π0 = 0, then all πj's must be zero, so they cannot sum to 1. We conclude that there is no stationary distribution. This means that either all states are transient, or all states are null recurrent. In either case, we have lim n→∞ P(Xn = j|X0 = i) = 0,  for all i, j. We will see how to figure out if the states are transient or null recurrent in the End of Chapter Problems (see Problem 15 in Section 1 1.5 ). p 1 − p p 1 − p p 1−p 1 2 1 1.3.1 Introduction So far , we have discussed discrete-time Markov chains in which the chain jumps from the current state to the next state after one unit time. That is, the time that the chain spends in each state is a positive integer . It is equal to 1 if the state does not have a self-transition ( pii = 0), or it is a Geometric(1 − pii) random variable if pii > 0. Here, we would like to discuss continuous-time Markov chains where the time spent in each state is a continuous random variable. More specifically , we will consider a random process {X(t), t ∈ [0, ∞)}. Again, we assume that we have a countable state space S ⊂ {0, 1, 2, ⋯}. If X(0) = i, then X(t) stays in state i for a random amount of time, say T1, where T1 is a continuous random variable. At time T1, the process jumps to a new state j and will spend a random amount of time T2 in that state, and so on. As it will be clear shortly , the random variables T1, T2, ⋯ have exponential distribution. The probability of going from state i to state j is shown by pij. Similar to discrete-time Markov chains, we would like to have the Markov property , i.e., conditioned on the current value of X(t), the past and the future values of the process must be independent. W e can express the Markov property as follows: for all 0 ≤ t1 < t2 < ⋯ < tn < tn+1, we must have P(X(tn+1) = j|X(tn) = i, X(tn−1) = in−1, ⋯ , X(t1) = i1) = P(X(tn+1) = j|X(tn) = i). In particular , suppose that at time t, we know that X(t) = i. To make any prediction about the future, it should not matter how long the process has been in state i. Thus, the time that the process spends in each state must have a \"memoryless\" property . As it has been discussed in previous chapters (e.g, Chapter 4 ), the exponential distribution is the only continuous distribution with that property . Thus, the time that a continuous-time Markov chain spends in state i (called the holding time ) will have Exponential(λi) distribution, where λi is a nonnegative real number . We further assume that the λi's are bounded, i.e., there exists a real number M < ∞ such that λi < M , for all i ∈ S. Thus, a continuous Markov chain has two components. First, we have a discrete-time Markov chain, called the jump chain or the the embedded Markov chain , that gives us the transition probabilities pij. Second, for each state we have a holding time parameter λi that controls the amount of time spent in each state. Note that if i is not an absorbing state, we can assume that i does not have a self- transition, i.e., pii = 0. The reason is that if we go from state i to state i, it's as if we never left that state. On the other hand, if i is an absorbing state, we have pii = 1, and pij = 0, for all i ≠ j. In this case, we have λi = 0, which means that the chain will spend an infinite amount of time in the absorbing state i. Continuous-T ime Markov Chains A continuous-time Markov chain X(t) is defined by two components: a jump chain , and a set of holding time parameters λi. The jump chain consists of a countable set of states S ⊂ {0, 1, 2, ⋯} along with transition probabilities pij. We assume pii = 0, for all non-absorbing states i ∈ S. We assume 1 . if X(t) = i, the time until the state changes has Exponential(λi) distribution; 2 . if X(t) = i, the next state will be j with probability pij. The process satisfies the Markov property . That is, for all 0 ≤ t1 < t2 < ⋯ < tn < tn+1, we have P(X(tn+1) = j∣∣∣X(tn) = i, X(tn−1) = in−1, ⋯ , X(t1) = i1) = P(X(tn+1) = j∣∣X(tn) = i). Let's define the transition probability Pij(t) as Pij(t) = P(X(t + s) = j|X(s) = i) = P(X(t) = j|X(0) = i),  for all s, t ∈ [0, ∞). We can then define the transition matrix , P(t). Assuming the states are 1, 2, ⋯, r, then the state transition matrix for any t ≥ 0 is given by P(t) = ⎡ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎣ p11(t) p12(t) . . . p1r(t) p21(t) p22(t) . . . p2r(t) . . . . . . . . . . . . pr1(t) pr2(t) . . . prr(t) ⎤ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎦ . Let's look at an example. Example 1 1. 17 Consider a continuous Markov chain with two states S = {0, 1}. Assume the holding time parameters are given by λ0 = λ1 = λ > 0. That is, the time that the chain spends in each state before going to the other state has an Exponential(λ) distribution. a . Draw the state diagram of the embedded (jump) chain. b . Find the transition matrix P(t). Hint: You might want to use the following identities sinh(x) = = ∞ ∑ n=0 , cosh(x) = = ∞ ∑ n=0 . Solution a . There are two states in the chain and none of them are absorbing (since λi > 0). Since we do not allow self-transitions, the jump chain must have the following transition matrix: P = [ 0 1 1 0 ] . The state transition diagram of the jump chain is shown in Figure 1 1.22. Figure 1 1.22 - The jump chain of the continuous-time Markov chain in Example 1 1.17. e x − e −x 2 x 2n+1 (2n + 1)! e x + e −x 2 x 2n (2n)! b . This Markov chain has a simple structure. Let's find P00(t). By definition P00(t) = P(X(t) = 0|X(0) = 0),  for all t ∈ [0, ∞). Assuming that X(0) = 0, X(t) will be 0 if and only if we have an even number of transitions in the time interval [0, t]. The time between each transition is an Exponential(λ) random variable. Thus, the transitions occur according to a Poisson process with parameter λ. We have P00(t) = P(X(t) = 0|X(0) = 0) = P(an even number of arrivals in [0, t]) = ∞ ∑ n=0 e −λt = e −λt ∞ ∑ n=0 = e −λt [ ] (by the hint) = + e −2λt. Next, we obtain P01(t) = 1 − P00(t) = − e −2λt. Finally , because of the symmetry in the problem, we have P11(t) = P00(t) = + e −2λt, P10(t) = P01(t) = − e −2λt. Thus, the transition matrix for any t ≥ 0 is given by P(t) = ⎡ ⎢ ⎣ + e −2λt − e −2λt − e −2λt + e −2λt ⎤ ⎥ ⎦ . If we let t go to infinity in the above example, the transition matrix becomes (λt) 2n (2n)! (λt) 2n (2n)! e λt + e −λt 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 1 lim t→∞ P(t) = ⎡ ⎢ ⎣ ⎤ ⎥ ⎦ , which means that for i = 0, 1, we have lim t→∞ P(X(t) = 0|X(0) = i) = , lim t→∞ P(X(t) = 1|X(0) = i) = . In fact, π = [ , ] is a limiting distribution of this chain. W e will discuss limiting distributions shortly . Before doing that let's talk about some properties of the transition matrices. First, similar to the discrete-time analysis, the rows of the transition matrix must sum to 1: ∑ j∈S pij(t) = 1,  for all t ≥ 0. Next, we note that by definition Pii(0) = P(X(0) = i|X(0) = i) = 1,  for all i. Pij(0) = P(X(0) = j|X(0) = i) = 0,  for all i ≠ j. We conclude that P(0) is equal to the identity matrix, P(0) = I. Finally , we can obtain the Chapman-Kolmogorov equation by applying the law of total probability and by using the Markov property: Pij(s + t) = P(X(s + t) = j|X(0) = i) = ∑ k∈S P(X(s) = k|X(0) = i)P(X(s + t) = j|X(s) = k) = ∑ k∈S Pik(s)Pkj(t),  for all s, t ≥ 0. The above equation can be written in the matrix form as follows P(s + t) = P(s)P(t),  for all s, t ≥ 0. 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 Transition Matrix For a continuous-time Markov chain, we define the transition matrixP(t). The (i, j)th entry of the transition matrix is given by Pij(t) = P(X(t) = j|X(0) = i). The transition matrix satisfies the following properties: 1 . P(0) is equal to the identity matrix, P(0) = I; 2 . the rows of the transition matrix must sum to 1, ∑ j∈S pij(t) = 1,  for all t ≥ 0; 3 . for all s, t ≥ 0, we have P(s + t) = P(s)P(t). 1 1.3.2 Stationary and Limiting Distributions Here we introduce stationary distributions for continuous Markov chains. As in the case of discrete-time Markov chains, for \"nice\" chains, a unique stationary distribution exists and it is equal to the limiting distribution . Remember that for discrete-time Markov chains, stationary distributions are obtained by solving π = πP. We have a similar definition for continuous-time Markov chains. Let X(t) be a continuous-time Markov chain with transition matrix P(t) and state space S = {0, 1, 2, ⋯}. A probability distribution π on S, i.e, a vector π = [π0, π1, π2, ⋯], where πi ∈ [0, 1] and ∑ i∈S πi = 1, is said to be a stationary distribution for X(t) if π = πP(t),  for all t ≥ 0. The intuition here is exactly the same as in the case of discrete-time chains. If the probability distribution of X(0) is π, then the distribution of X(t) is also given by π, for any t ≥ 0. Example 1 1. 18 Consider the continuous Markov chain of Example 1 1.17 : A chain with two states S = {0, 1} and λ0 = λ1 = λ > 0. In that example, we found that the transition matrix for any t ≥ 0 is given by P(t) = ⎡ ⎢ ⎣ + e−2λt − e−2λt − e−2λt + e−2λt ⎤ ⎥ ⎦ . Find the stationary distribution π for this chain. 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 Solution For π = [π0, π1], we obtain πP(t) = [π0, π1] ⎡ ⎢ ⎣ + e −2λt − e −2λt − e −2λt + e −2λt ⎤ ⎥ ⎦ = [π0, π1]. We also need π0 + π1 = 1. Solving the above equations, we obtain π0 = π1 = . Similar to the case of discrete-time Markov chains, we are interested in limiting distributions for continuous-time Markov chains. Limiting Distributions The probability distribution π = [π0, π1, π2, ⋯] is called the limiting distribution of the continuous-time Markov chain X(t) if πj = lim t→∞ P(X(t) = j|X(0) = i) for all i, j ∈ S, and we have ∑ j∈S πj = 1. As we will see shortly , for \"nice\" chains, there exists a unique stationary distribution which will be equal to the limiting distribution. In theory , we can find the stationary (and limiting) distribution by solving πP(t) = π, or by finding limt→∞ P(t). However , in practice, finding P(t) itself is usually very dif ficult. It is easier if we think in terms of the jump (embedded) chain. The following intuitive argument gives us the idea of how to 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 obtain the limiting distribution of a continuous Markov chain from the limiting distribution of the corresponding jump chain. Suppose that ~π = [~π0, ~π1, ~π2, ⋯ ] is the limiting distribution of the jump chain. That is, the discrete-time Markov chain associated with the jump chain will spend a fraction ~πj of time in state j in the long run. Note that, for the corresponding continuous-time Markov chain, any time that the chain visits state j, it spends on average time units in that state. Thus, we can obtain the limiting distribution of the continuous-time Markov chain by multiplying each ~πj by . We also need to normalize (divide by ∑ ) to get a valid probability distribution. The following theorem states this result more accurately . (It is worth noting that in the discrete-time case, we worried about periodicity . However , for continuous-time Markov chains, this is not an issue. This is because the times could any take positive real values and will not be multiples of a specific period.) Theorem 1 1. 3 Let {X(t), t ≥ 0} be a continuous-time Markov chain with an irreducible positive recurrent jump chain. Suppose that the unique stationary distribution of the jump chain is given by ~π = [~π0, ~π1, ~π2, ⋯ ]. Further assume that 0 < ∑ k∈S < ∞. Then, πj = lim t→∞ P(X(t) = j|X(0) = i) = . for all i, j ∈ S. That is, π = [π0, π1, π2, ⋯] is the limiting distribution of X(t). 1 λj 1 λj ~πk λk ~πk λk ~πj λj ∑k∈S ~πk λk Example 1 1. 19 Consider a continuous-time Markov chain X(t) that has the jump chain shown in Figure 1 1.23. Assume the holding time parameters are given by λ1 = 2, λ2 = 1, and λ3 = 3. Find the limiting distribution for X(t). Figure 1 1.23 - The jump chain for the Markov chain of Example 1 1.19. Solution We first note that the jump chain is irreducible. In particular , the transition matrix of the jump chain is given by P = ⎡ ⎢ ⎢ ⎢ ⎣ 0 1 0 0 0 1 0 ⎤ ⎥ ⎥ ⎥ ⎦ . The next step is to find the stationary distribution for the jump chain by solving ~πP = ~π. We obtain ~π = [1, 2, 2]. Finally , we can obtain the limiting distribution of X(t) using πj = . 1 2 1 2 1 5 ~πj λj ∑k∈S ~πk λk We obtain π1 = = = . π2 = = = . π3 = = = . Thus, we conclude that π = [3, 12, 4] is the limiting distribution of X(t). ~π1 λ1 + + ~π1 λ1 ~π2 λ2 ~π3 λ3 1 2 + + 1 2 2 1 2 3 3 19 ~π2 λ2 + + ~π1 λ1 ~π2 λ2 ~π3 λ3 2 1 + + 1 2 2 1 2 3 12 19 ~π3 λ3 + + ~π1 λ1 ~π2 λ2 ~π3 λ3 2 3 + + 1 2 2 1 2 3 4 19 1 19 1 1.3.3 The Generator Matrix Here, we introduce the generator matrix . The generator matrix, usually shown by G, gives us an alternative way of analyzing continuous-time Markov chains. Consider a continuous-time Markov chain X(t). Assume X(0) = i. The chain will jump to the next state at time T1, where T1 ∼ Exponential(λi). In particular , for a very small δ > 0, we can write P(T1 < δ) = 1 − e −λiδ ≈ 1 − (1 − λiδ) = λiδ. Thus, in a short interval of length δ, the probability of leaving state i is approximately λiδ. For this reason, λi is often called the transition rate out of state i. Formally , we can write λi = lim δ→0+ [ ] (11.7) Since we go from state i to state j with probability pij, we call the quantity gij = λipij, the transition rate from state i to state j. Here, we introduce the generator matrix, G , whose (i, j)th element is gij, when i ≠ j. We choose the diagonal elements ( gii) of G such that the rows of G sum to 0. That is, we let gii = − ∑ j≠i gij = − ∑ j≠i λipij = −λi ∑ j≠i pij = −λi. The last equality resulted as follows: If λi = 0, then clearly λi ∑ j≠i pij = λi = 0. If λi ≠ 0, then pii = 0 (no self-transitions), so P(X(δ) ≠ i|X(0) = i) δ ∑ j≠i pij = 1. It turns out the generator matrix is useful in analyzing continuous-time Markov chains. Example 1 1. 20 Explain why the following approximations hold: a . pjj(δ) ≈ 1 + gjjδ, for all j ∈ S. b . pkj(δ) ≈ δgkj, for k ≠ j. Solution Let δ be small. Equation 1 1.7 can be written as pjj(δ) ≈ 1 − λjδ = 1 + gjjδ. Also, we can approximate pkj(δ) = P(X(δ) = j|X(0) = k) as follows. This probability is approximately equal to the probability that we have a single transition from state k to state j in the interval [0, δ]. Note that the probability of more than one transition is negligible if δ is small (refer to the Poisson process section). Thus, we can write pkj(δ) = P(X(δ) = j|X(0) = k) ≈ P(X(δ) ≠ k|X(0) = k)pkj ≈ λkδpkj = δgkj,  for k ≠ j. We can state the above approximations more precisely as a . gjj = − limδ→0+ [ ] ,  for all j ∈ S; b . gkj = limδ→0+ [ ], for k ≠ j. 1−pjj(δ) δ pkj(δ) δ The Generator Matrix For a continuous-time Markov chain, we define the generator matrix G. The (i, j)th entry of the transition matrix is given by gij = ⎧⎪ ⎨ ⎪⎩ λipij  if i ≠ j −λi  if i = j Example 1 1. 21 Consider the continuous Markov chain of Example 1 1.17 : A chain with two states S = {0, 1} and λ0 = λ1 = λ > 0. In that example, we found that the transition matrix for any t ≥ 0 is given by P(t) = ⎡ ⎢ ⎣ + e −2λt − e −2λt − e −2λt + e −2λt ⎤ ⎥ ⎦ . a . Find the generator matrix G. b . Show that for any t ≥ 0, we have P ′(t) = P(t)G = GP(t), where P ′(t) is the derivative of P(t). Solution a . First, we have g00 = −λ0 = −λ, g11 = −λ1 = −λ. The transition matrix for the corresponding jump chain is given by P = [ p00 p01 p10 p11 ] = [ 0 1 1 0 ] . 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 Therefore, we have g01 = λ0p01 = λ, g10 = λ1p10 = λ. Thus, the generator matrix is given by G = [ −λ λ λ −λ ] . b . We have P ′(t) = [ −λe −2λt λe −2λt λe −2λt −λe −2λt ] , where P ′(t) is the derivative of P(t). We also have P(t)G = ⎡ ⎢ ⎣ + e −2λt − e −2λt − e −2λt + e −2λt ⎤ ⎥ ⎦ [ −λ λ λ −λ ] = [ −λe −2λt λe −2λt λe −2λt −λe −2λt ] , GP(t) = [ −λ λ λ −λ ] ⎡ ⎢ ⎣ + e −2λt − e −2λt − e −2λt + e −2λt ⎤ ⎥ ⎦ = [ −λe −2λt λe −2λt λe −2λt −λe −2λt ] . We conclude P ′(t) = P(t)G = GP(t). The equation P ′(t) = P(t)G = GP(t) (in the above example) is in fact true in general. To see the proof idea, we can argue as follows. Let δ be small. By Example 1 1.20 , we have pjj(δ) ≈ 1 + gjjδ, pkj(δ) ≈ δgkj, for k ≠ j. Using the Chapman-Kolmogorov equation, we can write 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 Pij(t + δ) = ∑ k∈S Pik(t)pkj(δ) = pij(t)pjj(δ) + ∑ k≠j Pik(t)pkj(δ) ≈ pij(t)(1 + gjjδ) + ∑ k≠j Pik(t)δgkj = pij(t) + δpij(t)gjj + δ ∑ k≠j Pik(t)gkj = pij(t) + δ ∑ k∈S Pik(t)gkj. Thus, ≈ ∑ k∈S Pik(t)gkj, which is the (i, j)th element of P(t)G. The above argument can be made rigorous. Forward and Backward Equations The forward equations state that P ′(t) = P(t)G, which is equivalent to p ′ ij(t) = ∑ k∈S pik(t)gkj,  for all i, j ∈ S. The backward equations state that P ′(t) = GP(t), which is equivalent to p ′ ij(t) = ∑ k∈S gikpkj(t),  for all i, j ∈ S. Pij(t + δ) − pij(t) δ One of the main uses of the generator matrix is finding the stationary distribution. So far , we have seen how to find the stationary distribution using the jump chain. The following result tells us how to find the stationary matrix using the generator matrix. Consider a continuous Markov chain X(t) with the state space S and the generator Matrix G. The probability distribution π on S is a stationary distribution for X(t) if and only if it satisfies πG = 0. Proof: For simplicity , let's assume that S is finite, i.e., π = [π0, π1, ⋯ , πr], for some r ∈ N. If π is a stationary distribution, then π = πP(t). Dif ferentiating both sides, we obtain 0 = [πP(t)] = πP ′(t) = πGP(t) (backward equations) Now , let t = 0 and remember that P(0) = I, the identity matrix. W e obtain 0 = πGP(0) = πG. Next, let π be a probability distribution on S that satisfies πG = 0. Then, by backward equations, P ′(t) = GP(t). Multiplying both sides by π, we obtain πP ′(t) = πGP(t) = 0. Note that πP ′(t) is the derivative of πP(t). Thus, we conclude πP(t) does not depend on t. In particular , for any t ≥ 0, we have πP(t) = πP(0) = π. Therefore, π is a stationary distribution. Example 1 1. 22 d dt The generator matrix for the continuous Markov chain of Example 1 1.17 is given by G = [ −λ λ λ −λ ] . Find the stationary distribution for this chain by solving πG = 0. Solution We obtain πG = [π0, π1] [ −λ λ λ −λ ] = 0. which results in π0 = π1. We also need π0 + π1 = 1. Solving the above equations, we obtain π0 = π1 = . Tran sitio n Rate Diagram: A continuous-time Markov chain can be shown by its transition rate diagram . In this diagram, the values gij are shown on the edges. The values of gii's are not usually shown because they are implied by the other values, i.e., gii = − ∑ j≠i gij. For example, Figure 1 1.24 shows the transition rate diagram for the following generator matrix 1 2 G = ⎡ ⎢ ⎢ ⎢ ⎣ −5 5 0 1 −2 1 3 1 −4 ⎤ ⎥ ⎥ ⎥ ⎦ , (11.8) Figure 1 1.24 - The transition rate diagram for the continuous-time Markov chain defined by Equation 1 1.8. 1 1.3.4 Solved Problems Problem 1 Consider a continuous-time Markov chain X(t) with the jump chain shown in Figure 1 1.25. Assume λ1 = 2, λ2 = 3, and λ3 = 4. Figure 1 1.25 - The jump chain for the Markov chain of Problem 1 a . Find the stationary distribution of the jump chain ~π = [~π1, ~π2, ~π3]. b . Using ~π, find the stationary distribution for X(t). Note: Although this jump chain has self-transitions, you can still use the discussed methods. In fact, X(t) can be equivalently shown by a jump chain with no self- transitions along with appropriate holding times. Solution a . To find the stationary distribution of the jump chain, ~π = [~π1, ~π2, ~π3], we need to solve ~π1 = ~π3, ~π2 = ~π1 + ~π2 ~π3 = ~π1 + ~π2 + ~π3, ~π1 + ~π2 + ~π3 = 1. We find ~π1 = , ~π2 = , ~π3 = . b . We have obtained ~π = [4, 3, 8]. We can find the limiting distribution of X(t) using πj = . We obtain π1 = = = . π2 = = = . 1 2 1 2 1 3 1 2 2 3 1 2 4 15 3 15 8 15 1 15 ~πj λj ∑k∈S ~πk λk ~π1 λ1 + + ~π1 λ1 ~π2 λ2 ~π3 λ3 4 2 + + 4 2 3 3 8 4 2 5 ~π2 λ2 + + ~π1 λ1 ~π2 λ2 ~π3 λ3 3 3 + + 4 2 3 3 8 4 1 5 ~ π3 = = = . Thus, we conclude that π = [2, 1, 2] is the limiting distribution of X(t). Problem 2 Consider a continuous-time Markov chain X(t) that has the jump chain shown in Figure 1 1.26 (this is the same Markov chain given in Example 1 1.19). Assume λ1 = 2, λ2 = 1, and λ3 = 3. a . Find the generator matrix for this chain. b . Find the limiting distribution for X(t) by solving πG = 0. Figure 1 1.26 - The jump chain for the Markov chain of Problem 2 Solution The jump chain is irreducible and the transition matrix of the jump chain is given by ~π3 λ3 + + ~π1 λ1 ~π2 λ2 ~π3 λ3 8 4 + + 4 2 3 3 8 4 2 5 1 5 P = ⎡ ⎢ ⎢ ⎢ ⎣ 0 1 0 0 0 1 0 ⎤ ⎥ ⎥ ⎥ ⎦ . The generator matrix can be obtained using gij = ⎧⎪ ⎨ ⎪⎩ λipij  if i ≠ j −λi  if i = j We obtain G = ⎡ ⎢ ⎢ ⎢ ⎢ ⎣ −2 2 0 0 −1 1 −3 ⎤ ⎥ ⎥ ⎥ ⎥ ⎦ . Solving πG = 0, and π1 + π2 + π3 = 1 we obtain π = [3, 12, 4], which is the same answer that we obtained in Example 1 1.19. Problem 3 (A queuing system ) Suppose that customers arrive according to a Poisson process with rate λ at a service center that has a single server . Customers are served one at a time in order of arrival. Service times are assumed to be i.i.d. Exponential(μ) random variables and independent of the arrival process. Customers leave the system after being served. Our goal in this problem is to model the above system as a continuous- time Markov chain. Let X(t) be the number of customers in the system at time t, so the state space is S = {0, 1, 2, ⋯}. Assume i > 0. If the system is in state i at time t, then the next state would either be i + 1 (if a new customers arrive) or state i − 1 (if a customer leaves). a . Suppose that the system is in state 0, so there are no customers in the system and the next transition will be to state 1. Let T0 be the time until the next transition. Show that T0 ∼ Exponential(λ). b . Suppose that the system is currently in state i, where i > 0. Let Ti be the time until the next transition. Show that Ti ∼ Exponential(λ + μ). 1 2 1 2 3 2 3 2 1 19 c. Suppose that the system is at state i. Find the probability that the next transition will be to state i + 1. d. Draw the jump chain, and provide the holding time parameters λi. e. Find the Generator matrix. f. Draw the transition rate diagram. Solution Note that to solve this problem, we use several results from the Poisson process section. In particular , you might want to review merging and splitting of Poisson processes before reading the solution to this problem. a . If there are no customers in the system, the next transition occurs when a new customer arrives. Since the customers arrive according to a Poisson process, and the interarrival times in the Poisson process have Exponential(λ) distribution, we conclude T0 ∼ Exponential(λ). b . Suppose that the system is in state i, where i > 0. Thus, there is a customer being served. W e assume the service times have Exponential(μ) distribution. The next transition occurs either when a new customer arrives, or when the service time of the current customer is ended. Thus, we can express Ti as Ti = min(X, Y ), where X ∼ Exponential(λ) and Y ∼ Exponential(μ), and X and Y are independent. W e claim that Ti ∼ Exponential(λ + μ). One way to see this is as follows. Here, you can imagine two independent Poisson processes. The first one is the customer arrival process. The second one is the process that has interarrival times equal to the service times. Now , the merged process has rate λ + μ. Since Ti can be thought of the first arrival in the merged process, we conclude that Ti ∼ Exponential(λ + μ). c. Suppose that the system is ate state i. We would like to find the probability that the next transition will be to state i + 1, shown by pi,i+1. Again consider the two Poisson processes defined above. W e can model this system as follows. Each arrival in the merged process is of type 1 (customer arrival) with probability , or of type 2 (customer departure) with probability . The probability pi,i+1 is the probability that the first arrival in the merged process is of type 1. This happens with probability , so we conclude λ λ+μ μ λ+μ λ λ+μ Pi,i+1 = , pi,i−1 = 1 − pi,i+1 = . d. From the above, we can draw the jump chain as in Figure 1 1.27 Figure 1 1.27 - The jump chain for the above queuing system. The holding time parameters, λi's, are given by λ0 = λ, λi = λ + μ,  for i = 1, 2, ⋯ . e. The generator matrix can be obtained using gij = ⎧⎪ ⎨ ⎪⎩ λipij  if i ≠ j −λi  if i = j We obtain G = ⎡ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎣ −λ λ 0 0 ⋯ μ −(μ + λ) λ 0 ⋯ 0 μ −(μ + λ) λ ⋯ ⋮ ⋮ ⋮ ⋮ ⎤ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎦ . f. Remember that in the transition rate diagram, the values gij are shown on the edges (the values of gii's are not usually shown). The transition rate diagram for this chain is shown in Figure 1 1.28. Figure 1 1.28 - The transition rate diagram for the above queuing system. λ λ + μ μ λ + μ 1 1.4.0 Brownian Motion (W iener Process) Brownian motion is another widely-used random process. It has been used in engineering, finance, and physical sciences. It is a Gaussian random process and it has been used to model motion of particles suspended in a fluid, percentage changes in the stock prices, integrated white noise, etc. Figure 1 1.29 shows a sample path of Brownain motion. Figure 1 1.29 - A possible realization of Brownian motion. In this section, we provide a very brief introduction to Brownian motion. It is worth noting that in order to have a deep understanding of Brownian motion, one needs to understand It ¯¯¯o calculus, a topic that is beyond the scope of this book. A good place to start learning It ¯¯¯o calculus is [25] . 1 1.4.1 Brownian Motion as the Limit of a Symmetric Random W alk Here, we introduce a construction of Brownian motion from a symmetric random walk. Divide the half-line [0, ∞) to tiny subintervals of length δ as shown in Figure 1 1.30. Figure 1 1.30 - Dividing the half-line [0, ∞) to tiny subintervals of length δ. Each subinterval corresponds to a time slot of length δ. Thus, the intervals are (0, δ], (δ, 2δ], (2δ, 3δ], ⋯. More generally , the kth interval is ((k − 1)δ, kδ]. We assume that in each time slot, we toss a fair coin. W e define the random variables Xi as follows. Xi = √δ if the kth coin toss results in heads, and Xi = −√δ if the kth coin toss results in tails. Thus, Xi = ⎧⎪ ⎪ ⎨ ⎪ ⎪⎩ √δ with probability  −√δ with probability  Moreover , the Xi's are independent. Note that E[Xi] = 0, Var(Xi) = δ. Now , we would like to define the process W(t) as follows. W e let W(0) = 0. At time t = nδ, the value of W(t) is given by W(t) = W(nδ) = n ∑ i=1 Xi. Since W(t) is the sum of n i.i.d. random variables, we know how to find E[W(t)] and Var(W(t)). In particular , 1 2 1 2 E[W(t)] = n ∑ i=1 E[Xi] = 0, Var(W(t)) = n ∑ i=1 Var(Xi) = nVar(X1) = nδ = t. For any t ∈ (0, ∞), as n goes to ∞, δ goes to 0. By the central limit theorem, W(t) will become a normal random variable, W(t) ∼ N(0, t). Since the coin tosses are independent, we conclude that W(t) has independent increments . That is, for all 0 ≤ t1 < t2 < t3 ⋯ < tn, the random variables W(t2) − W(t1), W(t3) − W(t2), ⋯ , W(tn) − W(tn−1) are independent. Remember that we say that a random process X(t) has stationary increments if, for all t2 > t1 ≥ 0, and all r > 0, the two random variables X(t2) − X(t1) and X(t2 + r) − X(t1 + r) have the same distributions. In other words, the distribution of the dif ference depends only on the length of the interval (t1, t2], and not on the exact location of the interval on the real line. W e now claim that the random process W(t), defined above, has stationary increments. T o see this, we argue as follows. For 0 ≤ t1 < t2, if we have t1 = n1δ and t2 = n2δ, we obtain W(t1) = W(n1δ) = n1 ∑ i=1 Xi, W(t2) = W(n2δ) = n2 ∑ i=1 Xi. Then, we can write W(t2) − W(t1) = n2 ∑ i=n1+1 Xi. Therefore, we conclude E[W(t2) − W(t1)] = n2 ∑ i=n1+1 E[Xi] = 0, Var(W(t2) − W(t1)) = n2 ∑ i=n1+1 Var(Xi) = (n2 − n1)Var(X1) = (n2 − n1)δ = t2 − t1. Therefore, for any 0 ≤ t1 < t2, the distribution of W(t2) − W(t1) only depends on the lengths of the interval [t1, t2], i.e., how many coin tosses are in that interval. In particular , for any 0 ≤ t1 < t2, the distribution of W(t2) − W(t1) converges to N(0, t2 − t1). Therefore, we conclude that W(t) has stationary increments . The above construction can be made more rigorous. The random process W(t) is called the standard Brownian motion or the standard Wiener process . Brownian motion has continuous sample paths, i.e., W(t) is a continuous function of t (See Figure 1 1.29). However , it can be shown that it is nowhere dif ferentiable. 1 1.4.2 Definition and Some Properties Here, we provide a more formal definition for Brownian Motion. Standard Brownian Motion A Gaussian random process {W(t), t ∈ [0, ∞)} is called a (standard) Brownian motion or a (standard) Wiener process if 1 . W(0)=0; 2 . for all 0 ≤ t1 < t2, W(t2) − W(t1) ∼ N(0, t2 − t1); 3 . W(t) has independent increments. That is, for all 0 ≤ t1 < t2 < t3 ⋯ < tn, the random variables W(t2) − W(t1), W(t3) − W(t2), ⋯ , W(tn) − W(tn−1) are independent; 4 . W(t) has continuous sample paths. A more general process is obtained if we define X(t) = μ + σW(t). In this case, X(t) is a Brownian motion with E[X(t)] = μ, Var(X(t)) = σ2t. Nevertheless, since X(t) is obtained by simply shifting and scaling W(t), it suf fices to study properties of the standard Brownian motion, W(t). Example 1 1. 23 Let W(t) be a standard Brownian motion. For all s, t ∈ [0, ∞), find CW (s, t) = Cov(W(s), W(t)). Solution Let's assume s ≤ t. Then, we have Cov(W(s), W(t)) = Cov(W(s), W(s) + W(t) − W(s)) = Cov(W(s), W(s)) + Cov(W(s), W(t) − W(s)) = Var(W(s)) + Cov(W(s), W(t) − W(s)) = s + Cov(W(s), W(t) − W(s)). Brownian motion has independent increments, so the two random variables W(s) = W(s) − W(0) and W(t) − W(s) are independent. Therefore, Cov(W(s), W(t) − W(s)) = 0. W e conclude Cov(W(s), W(t)) = s. Similarly , if t ≤ s, we obtain Cov(W(s), W(t)) = t. We conclude Cov(W(s), W(t)) = min(s, t),  for all s, t. If W(t) is a standard Brownian motion, we have Cov(W(s), W(t)) = min(s, t),  for all s, t. Example 1 1. 24 Let W(t) be a standard Brownian motion. a . Find P(1 < W(1) < 2). b . Find P(W(2) < 3|W(1) = 1). Solution a . We have W(1) ∼ N(0, 1). Thus, P(1 < W(1) < 2) = Φ(2) − Φ(1) ≈ 0.136 b . Note that W(2) = W(1) + W(2) − W(1). Also, note that W(1) and W(2) − W(1) are independent, and W(2) − W(1) ∼ N(0, 1). We conclude that W(2)|W(1) = 1 ∼ N(1, 1). Thus, P(W(2) < 3|W(1) = 1) = Φ ( ) = Φ(2) ≈ 0.98 3 − 1 1 1 1.4.3 Solved Problems Problem 1 Let W(t) be a standard Brownian motion. Find P(W(1) + W(2) > 2). Solution Let X = W(1) + W(2). Since W(t) is a Gaussian process, X is a normal random variable. EX = E[W(1)] + E[W(2)] = 0, Var(X) = Var(W(1)) + Var(W(2)) + 2Cov(W(1), W(2)) = 1 + 2 + 2 ⋅ 1 = 5. We conclude X ∼ N(0, 5). Thus, P(X > 2) = 1 − Φ ( ) ≈ 0.186 Problem 2 Let W(t) be a standard Brownian motion, and 0 ≤ s < t. Find the conditional PDF of W(s) given W(t) = a. Solution It is useful to remember the following result from the previous chapters: Suppose X and Y are jointly normal random variables with parameters μX, σ2 X, μY , σ2 Y , and ρ. Then, given X = x, Y is normally distributed with 2 − 0 √5 E[Y |X = x] = μY + ρσY , Var(Y |X = x) = (1 − ρ 2)σ2 Y . Now , if we let X = W(t) and Y = W(s), we have X ∼ N(0, t) and Y ∼ N(0, s) and ρ = = = = √ . We conclude that E[Y |X = a] = a, Var(Y |X = a) = s (1 − ) . Therefore, W(s)|W(t) = a ∼ N ( a, s (1 − )) . Problem 3 ( Geometric Brownian Motion ) Let W(t) be a standard Brownian motion. Define X(t) = exp{W(t)}, for all t  ∈ [0, ∞). a . Find E[X(t)], for all t ∈ [0, ∞). b . Find Var(X(t)), for all t ∈ [0, ∞). c. Let 0 ≤ s ≤ t. Find Cov(X(s), X(t)). Solution It is useful to remember the MGF of the normal distribution. In particular , if X ∼ N(μ, σ) , then MX(s) = E[e sX] = exp{sμ + }, for all s ∈ R. x − μX σX Cov(X, Y ) σxσY min(s, t) √t√s s √t√s s t s t s t s t s t σ2s2 2 a . We have E[X(t)] = E[e W (t)], (where W(t) ∼ N(0, t)) = exp{ }. b . We have E[X2(t)] = E[e 2W (t)], (where W(t) ∼ N(0, t)) = exp{2t}. Thus, Var(X(t)) = E[X2(t)] − E[X(t)] 2 = exp{2t} − exp{t}. c. Let 0 ≤ s ≤ t. Then, we have Cov(X(s), X(t)) = E[X(s)X(t)] − E[X(s)]E[X(t)] = E[X(s)X(t)] − exp{ }. To find E[X(s)X(t)], we can write E[X(s)X(t)] = E[ exp{W(s)} exp{W(t)}] = E[ exp{W(s)} exp{W(s) + W(t) − W(s)}] = E[ exp{2W(s)} exp{W(t) − W(s)}] = E[ exp{2W(s)}]E[ exp{W(t) − W(s)}] = exp{2s} exp{ } = exp{ }. We conclude, for 0 ≤ s ≤ t, Cov(X(s), X(t)) = exp{ } − exp{ }. t 2 s + t 2 t − s 2 3s + t 2 3s + t 2 s + t 2 1 1.5.0 End of Chapter Problems Problem 1 The number of orders arriving at a service facility can be modeled by a Poisson process with intensity λ = 10 orders per hour . a . Find the probability that there are no orders between 10:30 and 1 1. b . Find the probability that there are 3 orders between 10:30 and 1 1 and 7 orders between 1 1:30 and 12. Problem 2 Let {N(t), t ∈ [0, ∞)} be a Poisson process with rate λ. Find the probability that there are two arrivals in (0, 2] or three arrivals in (4, 7]. Problem 3 Let X ∼ Poisson(μ1) and Y ∼ Poisson(μ2) be two independent random variables. Define Z = X + Y . Show that X|Z = n ∼ Binomial (n, ) . Problem 4 Let N(t) be a Poisson process with rate λ. Let 0 < s < t. Show that given N(t) = n, N(s) is a binomial random variable with parameters n and p = . Problem 5 μ1 μ1 + μ2 s t Let N1(t) and N2(t) be two independent Poisson processes with rate λ1 and λ2 respectively . Let N(t) = N1(t) + N2(t) be the merged process. Show that given N(t) = n, N1(t) ∼ Binomial (n, ). Note: We can interpret this result as follows: Any arrival in the merged process belongs to N1(t) with probability and belongs to N2(t) with probability independent of other arrivals. Problem 6 In this problem, our goal is to complete the proof of the equivalence of the first and the second definitions of the Poisson process. More specifically , suppose that the counting process {N(t), t ∈ [0, ∞)} satisfies all the following conditions: 1 . N(0) = 0. 2 . N(t) has independent –––––––––––––– and stationary –––––––––––– increments. 3 . We have P(N(Δ) = 0) = 1 − λΔ + o(Δ), P(N(Δ) = 1) = λΔ + o(Δ), P(N(Δ) ≥ 2) = o(Δ). We would like to show that N(t) ∼ Poisson(λt). To this, for any k ∈ {0, 1, 2, ⋯}, define the function gk(t) = P(N(t) = k). a . Show that for any Δ > 0, we have g0(t + Δ) = g0(t)[1 − λΔ + o(Δ)]. b . Using Part (a), show that = −λ. c. By solving the above dif ferential equation and using the fact that g0(0) = 1, conclude that g0(t) = e −λt. d. For k ≥ 1, show that gk(t + Δ) = gk(t)(1 − λΔ) + gk−1(t)λΔ + o(Δ). λ1 λ1+λ2 λ1 λ1+λ2 λ2 λ1+λ2 g ′ 0(t) g0(t) e. Using the previous part show that g ′ k(t) = −λgk(t) + λgk−1(t), which is equivalent to [e λtgk(t)] = λe λtgk−1(t). f. Check that the function gk(t) = satisfies the above dif ferential equation for any k ≥ 1. In fact, this is the only solution that satisfies g0(t) = e −λt, and gk(0) = 0 for k ≥ 1. Problem 7 Let {N(t), t ∈ [0, ∞)} be a Poisson process with rate λ. Let T1, T2, ⋯ be the arrival times for this process. Show that fT1,T2,...,Tn (t1, t2, ⋯ , tn) = λ ne −λtn ,  for 0 < t1 < t2 < ⋯ < tn. Hint: One way to show the above result is to show that for suf ficiently small Δi, we have P(t1 ≤ T1 < t1 + Δ1, t2 ≤ T2 < t2 + Δ2, . . . , tn ≤ Tn < tn + Δn) ≈ λ ne −λtn Δ1Δ2 ⋯ Δn,  for 0 < t1 < t2 < ⋯ < tn. Problem 8 Let {N(t), t ∈ [0, ∞)} be a Poisson process with rate λ. Show the following: given that N(t) = n, the n arrival times have the same joint CDF as the order statistics of n independent Uniform(0, t) random variables. T o show this you can show that fT1,T2,...,Tn|N(t)=n(t1, t2, ⋯ , tn) = ,  for 0 < t1 < t2 < ⋯ < tn < t. d dt e −λt(λt) k k! n! tn Problem 9 Let {N(t), t ∈ [0, ∞)} be a Poisson process with rate λ. Let T1, T2, ⋯ be the arrival times for this process. Find E[T1 + T2 + ⋯ + T10|N(4) = 10]. Hint: Use the result of Problem 8 . Problem 10 Two teams A and B play a soccer match. The number of goals scored by T eam A is modeled by a Poisson process N1(t) with rate λ1 = 0.02 goals per minute, and the number of goals scored by T eam B is modeled by a Poisson process N2(t) with rate λ2 = 0.03 goals per minute. The two processes are assumed to be independent. Let N(t) be the total number of goals in the game up to and including time t. The game lasts for 90 minutes. a . Find the probability that no goals are scored, i.e., the game ends with a 0-0 draw . b . Find the probability that at least two goals are scored in the game. c. Find the probability of the final score being Team A : 1, Team B : 2 d. Find the probability that they draw . Problem 1 1 In Problem 10 , find the probability that T eam B scores the first goal. That is, find the probability that at least one goal is scored in the game and the first goal is scored by Team B. Problem 12 Let {N(t), t ∈ [0, ∞)} be a Poisson process with rate λ. Let p : [0, ∞) ↦ [0, 1] be a function. Here we divide N(t) to two processes N1(t) and N2(t) in the following way . For each arrival, a coin with P(H) = p(t) is tossed. If the coin lands heads up, the arrival is sent to the first process ( N1(t)), otherwise it is sent to the second process. The coin tosses are independent of each other and are independent of N(t). Show that N1(t) is a nonhomogeneous Poisson process with rate λ(t) = λp(t). Problem 13 Consider the Markov chain with three states S = {1, 2, 3}, that has the state transition diagram is shown in Figure 1 1.31. Figure 1 1.31 - A state transition diagram. Suppose P(X1 = 1) = and P(X1 = 2) = . a . Find the state transition matrix for this chain. b . Find P(X1 = 3, X2 = 2, X3 = 1). c. Find P(X1 = 3, X3 = 1). Problem 14 Let α0, α1, ⋯ be a sequence of nonnegative numbers such that ∞ ∑ j=0 αj = 1. Consider a Markov chain X0, X1, X2, ⋯ with the state space S = {0, 1, 2, ⋯} such that pij = αj,  for all j ∈ S. 1 2 1 4 Show that X1, X2, ⋯ is a sequence of i.i.d random variables. Problem 15 et Xn be a discrete-time Markov chain. Remember that, by definition, p (n) ii = P(Xn = i|X0 = i). Show that state i is recurrent if and only if ∞ ∑ n=1 p (n) ii = ∞. Problem 16 Consider the Markov chain in Figure 1 1.32. There are two recurrent classes, R1 = {1, 2}, and R2 = {5, 6, 7}. Assuming X0 = 4, find the probability that the chain gets absorbed to R1. Figure 1 1.32 - A state transition diagram. Problem 17 Consider the Markov chain of Problem 16 . Again assume X0 = 4. We would like to find the expected time (number of steps) until the chain gets absorbed in R1 or R2. More specifically , let T be the absorption time, i.e., the first time the chain visits a state in R1 or R2. We would like to find E[T|X0 = 4]. Problem 18 Consider the Markov chain shown in Figure 1 1.33. Assume X0 = 2, and let N be the first time that the chain returns to state 2, i.e., N = min{n ≥ 1 : Xn = 2}. Find E[N|X0 = 2]. Figure 1 1.33 - A state transition diagram. Problem 19 Consider the Markov chain shown in Figure 1 1.34. Figure 1 1.34 - A state transition diagram. a . Is this chain irreducible? b . Is this chain aperiodic? c. Find the stationary distribution for this chain. d. Is the stationary distribution a limiting distribution for the chain? Problem 20 (Random W alk) Consider the Markov chain shown in Figure 1 1.35. Figure 1 1.35 - Simple random walk. This is known as the simple random walk . Show that p (2n) 00 = ( )p n(1 − p) n, p (2n+1) 00 = 0. Note: Using Stirling's formula, it can be shown that ∞ ∑ k=1 p (k) 00 = ∞ ∑ n=1 ( )p n(1 − p) n is finite if and only if p ≠ . Thus, we conclude that the simple random walk is recurrent if p = and is transient if p ≠ (see Problem 15 ). Problem 21 Consider the Markov chain shown in Figure 1 1.36. Assume that 0 < p < q. Does this chain have a limiting distribution? For all i, j ∈ {0, 1, 2, ⋯}, find lim n→∞ P(Xn = j|X0 = i). Figure 1 1.36 - A state transition diagram. 2n n 2n n 1 2 1 2 1 2 Problem 22 Consider the Markov chain shown in Figure 1 1.37. Assume that p > q > 0. Does this chain have a limiting distribution? For all i, j ∈ {0, 1, 2, ⋯}, find lim n→∞ P(Xn = j|X0 = i). Figure 1 1.37 - A state transition diagram. Problem 23 (Gambler's Ruin Problem ) Two gamblers, call them Gambler A and Gambler B, play repeatedly . In each round, A wins 1 dollar with probability p or loses 1 dollar with probability q = 1 − p (thus, equivalently , in each round B wins 1 dollar with probability q = 1 − p and loses 1 dollar with probability p). We assume dif ferent rounds are independent. Suppose that initially A has i dollars and B has N − i dollars. The game ends when one of the gamblers runs out of money (in which case the other gambler will have N dollars). Our goal is to find pi, the probability that A wins the game given that he has initially i dollars. a . Define a Markov chain as follows: The chain is in state i if the Gambler A has i dollars. Here, the state space is S = {0, 1, ⋯ , N}. Draw the state transition diagram of this chain. b . Let ai be the probability of absorption to state N (the probability that A wins) given that X0 = i. Show that a0 = 0, aN = 1, ai+1 − ai = (ai − ai−1),  for i = 1, 2, ⋯ , N − 1. c. Show that ai = [1 + + ( ) 2 + ⋯ + ( ) i−1] a1,  for i = 1, 2, ⋯ , N. d. Find ai for any i ∈ {0, 1, 2, ⋯ , N}. Consider two cases: p = and p ≠ . q p q p q p q p 1 2 1 2 Problem 24 Let N = 4 and i = 2 in the gambler's ruin problem ( Problem 23 ). Find the expected number of rounds the gamblers play until one of them wins the game. Problem 25 The Poisson process is a continuous-time Markov chain. Specifically , let N(t) be a Poisson process with rate λ. a . Draw the state transition diagram of the corresponding jump chain. b . What are the rates λi for this chain? Problem 26 Consider a continuous-time Markov chain X(t) that has the jump chain shown in Figure 1 1.38. Assume λ1 = λ2 = λ3, and λ4 = 2λ1. Figure 1 1.38 - The jump chain for the Markov chain of Problem 26. a . Find the stationary distribution of the jump chain ~π = [~π1, ~π2, ~π3, ~π4]. b . Using ~π, find the stationary distribution for X(t). Problem 27 Consider a continuous-time Markov chain X(t) that has the jump chain shown in Figure 1 1.39. Assume λ1 = 1, λ2 = 2, and λ3 = 4. a . Find the generator matrix for this chain. b . Find the limiting distribution for X(t) by solving πG = 0. Figure 1 1.39 - The jump chain for the Markov chain of Problem 27. Problem 28 Consider the queuing system of Problem 3 in the Solved Problems Section ( Section 3.4 ). Specifically , in that problem we found the following generator matrix and transition rate diagram: G = ⎡ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎣ −λ λ 0 0 ⋯ μ −(μ + λ) λ 0 ⋯ 0 μ −(μ + λ) λ ⋯ ⋮ ⋮ ⋮ ⋮ ⎤ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎦ . The transition rate diagram is shown in Figure 1 1.40 Figure 1 1.40 - The transition rate diagram for the above queuing system. Assume that 0 < λ < μ. Find the stationary distribution for this queueing system. Problem 29 Let W(t) be the standard Brownian motion. a . Find P(−1 < W(1) < 1). b . Find P(1 < W(2) + W(3) < 2). c. Find P(W(1) > 2|W(2) = 1). Problem 30 Let W(t) be a standard Brownian motion. Find P(0 < W(1) + W(2) < 2, 3W(1) − 2W(2) > 0). Problem 31 (Brownian Bridge) Let W(t) be a standard Brownian motion. Define X(t) = W(t) − tW(1),  for all t ∈ [0, ∞). Note that X(0) = X(1) = 0. Find Cov(X(s), X(t)), for 0 ≤ s ≤ t ≤ 1. Problem 32 (Correlated Brownian Motions) Let W(t) and U(t) be two independent standard Brownian motions. Let −1 ≤ ρ ≤ 1. Define the random process X(t) as X(t) = ρW(t) + √1 − ρ 2U(t),  for all t ∈ [0, ∞). a . Show that X(t) is a standard Brownian motion. b . Find the covariance and correlation coef ficient of X(t) and W(t). That is, find Cov(X(t), W(t)) and ρ(X(t), W(t)). Problem 33 (Hitting T imes for Brownian Motion) Let W(t) be a standard Brownian motion. Let a > 0. Define Ta as the first time that W(t) = a. That is Ta = min{t : W(t) = a}. a . Show that for any t ≥ 0, we have P(W(t) ≥ a) = P(W(t) ≥ a|Ta ≤ t)P(Ta ≤ t). b . Using Part (a), show that P(Ta ≤ t) = 2 [1 − Φ ( )] . c. Using Part (b), show that the PDF of Ta is given by fTa(t) = exp{− }. Note: By symmetry of Brownian motion, we conclude that for any a ≠ 0, we have fTa(t) = exp{− }. a √t a t√2πt a 2 2t |a| t√2πt a 2 2t Chapter 12 Introduction to Simulation Using MATLAB A. Rakhshan and H. Pishro-Nik 12.1 Analysis versus Computer Simulation A computer simulation is a computer program which attempts to represent the real world based on a model. The accuracy of the simulation depends on the precision of the model. Suppose that the probability of heads in a coin toss experiment is unknown. We can perform the experiment of tossing the coin n times repetitively to approximate the probability of heads. P (H) = Number of times heads observed Number of times the experiment executed However, for many practical problems it is not possible to determine the probabilities by exe- cuting experiments a large number of times. With today’s computers processing capabilities, we only need a high-level language, such as MATLAB, which can generate random numbers, to deal with these problems. In this chapter, we present basic methods of generating random variables and simulate prob- abilistic systems. The provided algorithms are general and can be implemented in any computer language. However, to have concrete examples, we provide the actual codes in MATLAB. If you are unfamiliar with MATLAB, you should still be able to understand the algorithms. 12.2 Introduction: What is MATLAB? MATLAB is a high-level language that helps engineers and scientists ﬁnd solutions for given problems with fewer lines of codes than traditional programming languages, such as C/C++ or Java, by utilizing built-in math functions. You can use MATLAB for many applications including signal processing and communications, ﬁnance, and biology. Arrays are the basic data structure in MATLAB. Therefore, a basic knowledge of linear algebra is useful to use MATLAB in an eﬀective way. Here we assume you are familiar with basic commands of MATLAB. We can use the built-in commands to generate probability distributions in MATLAB, but in this chapter we will also learn how to generate these distributions from the uniform distribution. 1 2CHAPTER 12. INTRODUCTION TO SIMULATION USING MATLABA. RAKHSHAN AND H. PISHRO-NIK 12.3 Discrete and Continuous Random Number Generators Most of the programming languages can deliver samples from the uniform distribution to us (In reality, the given values are pseudo-random instead of being completely random.) The rest of this section shows how to convert uniform random variables to any other desired random variable. The MATLAB code for generating uniform random variables is: U = rand; which returns a pseudorandom value drawn from the standard uniform distribution on the open interval (0,1). Also, U = rand(m, n); returns an m-by-n matrix containing independent pseudorandom values drawn from the standard uniform distribution on the open interval (0,1). 12.3.1 Generating Discrete Probability Distributions from Uniform Distri- bution Let’s see a few examples of generating certain simple distributions: Example 1. (Bernoulli) Simulate tossing a coin with probability of heads p. Solution: Let U be a Uniform(0,1) random variable. We can write Bernoulli random variable X as: X = { 1 U < p 0 U ≥ p Thus, P (H) = P (X = 1) = P (U < p) = p Therefore, X has Bernoulli(p) distribution. The MATLAB code for Bernoulli(0.5) is: p = 0.5; U = rand; X = (U < p); Since the “rand” command returns a number between 0 and 1, we divided the interval [0, 1] into two parts, p and 1 − p in length. Then, the value of X is determined based on where the number generated from uniform distribution fell. 12.3. DISCRETE AND CONTINUOUS RANDOM NUMBER GENERATORS 3 Example 2. (Coin Toss Simulation) Write codes to simulate tossing a fair coin to see how the law of large numbers works. Solution: You can write: n = 1000; U = rand(1, n); toss = (U < 0.5); a = zeros(n + 1); avg = zeros(n); f or i = 2 : n + 1 a(i) = a(i − 1) + toss(i − 1); avg(i − 1) = a(i)/(i − 1); end plot(avg) If you run the above codes to compute the proportion of ones in the variable “toss,” the result will look like Figure 12.5. You can also assume the coin is unbiased with probability of heads equal to 0.6 by replacing the third line of the previous code with: toss = (U < 0.6); Figure 12.1: MATLAB coin toss simualtion Example 3. (Binomial) Generate a Binomial(50, 0.2) random variable. Solution: To solve this problem, we can use the following lemma: 4CHAPTER 12. INTRODUCTION TO SIMULATION USING MATLABA. RAKHSHAN AND H. PISHRO-NIK Lemma 1. If X1, X2, ..., Xn are independent Bernoulli(p) random variables, then the random variable X deﬁned by X = X1 + X2 + ... + Xn has a Binomial(n, p) distribution. To generate a random variable X ∼ Binomial(n, p), we can toss a coin n times and count the number of heads. Counting the number of heads is exactly the same as ﬁnding X1 +X2 +...+Xn, where each Xi is equal to one if the corresponding coin toss results in heads and zero otherwise. Since we know how to generate Bernoulli random variables, we can generate a Binomial(n, p) by adding n independent Bernoulli(p) random variables. p = 0.2; n = 50; U = rand(n, 1); X = sum(U < p); Generating Arbitrary Discrete Distributions In general, we can generate any discrete random variables similar to the above examples using the following algorithm. Suppose we would like to simulate the discrete random variable X with range RX = {x1, x2, ..., xn} and P (X = xj) = pj, so ∑ j pj = 1. To achieve this, ﬁrst we generate a random number U (i.e., U ∼ U nif orm(0, 1)). Next, we divide the interval [0, 1] into subintervals such that the jth subinterval has length pj (Figure 12.2). Assume X =    x0 if (U < p0) x1 if (p0 ≤ U < p0 + p1) ... xj if (∑j−1 k=0 pk ≤ U < ∑j k=0 pk) ... In other words X = xj if F (xj−1) ≤ U < F (xj), where F (x) is the desired CDF. We have P (X = xj) = P (j−1∑ k=0 pk ≤ U < j∑ k=0 pk ) = pj 0 1 p0 p1 p2 p3 · · · pj Figure 12.2: Generating discrete random variables 12.3. DISCRETE AND CONTINUOUS RANDOM NUMBER GENERATORS 5 Example 4. Give an algorithm to simulate the value of a random variable X such that P (X = 1) = 0.35 P (X = 2) = 0.15 P (X = 3) = 0.4 P (X = 4) = 0.1 Solution: We divide the interval [0, 1] into subintervals as follows: A0 = [0, 0.35) A1 = [0.35, 0.5) A2 = [0.5, 0.9) A3 = [0.9, 1) Subinterval Ai has length pi. We obtain a uniform number U . If U belongs to Ai, then X = xi. P (X = xi) = P (U ∈ Ai) = pi P = [0.35, 0.5, 0.9, 1]; X = [1, 2, 3, 4]; counter = 1; r = rand; while(r > P (counter)) counter = counter + 1; end X(counter) 12.3.2 Generating Continuous Probability Distributions from the Uniform Distribution- Inverse Transformation Method At least in principle, there is a way to convert a uniform distribution to any other distribution. Let’s see how we can do this. Let U ∼ U nif orm(0, 1) and F be a CDF. Also, assume F is continuous and strictly increasing as a function. Theorem 1. Let U ∼ U nif orm(0, 1) and F be a CDF which is strictly increasing. Also, consider a random variable X deﬁned as X = F −1(U ). Then, X ∼ F (The CDF of X is F ) 6CHAPTER 12. INTRODUCTION TO SIMULATION USING MATLABA. RAKHSHAN AND H. PISHRO-NIK Proof: P (X ≤ x) = P (F −1(U ) ≤ x) = P (U ≤ F (x)) (increasing function) = F (x) Now, let’s see some examples. Note that to generate any continuous random variable X with the continuous cdf F , F −1(U ) has to be computed. Example 5. (Exponential) Generate an Exponential(1) random variable. Solution: To generate an Exponential random variable with parameter λ = 1, we proceed as follows F (x) = 1 − e −x x > 0 U ∼ U nif orm(0, 1) X = F −1(U ) = − ln(1 − U ) X ∼ F This formula can be simpliﬁed since 1 − U ∼ U nif orm(0, 1) 0 1 U 1 − U Figure 12.3: Symmetry of Uniform Hence we can simulate X using X = − ln(U ) U = rand; X = −log(U ); Example 6. (Gamma) Generate a Gamma(20,1) random variable. Solution: For this example, F −1 is even more complicated than the complicated gamma cdf F itself. Instead of inverting the CDF, we generate a gamma random variable as a sum of n independent exponential variables. Theorem 2. Let X1, X2, · · · , Xn be independent random variables with Xi ∼ Exponential(λ). Deﬁne Y = X1 + X2 + · · · + Xn 12.3. DISCRETE AND CONTINUOUS RANDOM NUMBER GENERATORS 7 By the moment generating function method, you can show that Y has a gamma distribution with parameters n and λ, i.e., Y ∼ Gamma(n, λ). Having this theorem in mind, we can write: n = 20; lambda = 1; X = (−1/lambda) ∗ sum(log(rand(n, 1))); Example 7. (Poisson) Generate a Poisson random variable. Hint: In this example, use the fact that the number of events in the interval [0, t] has Poisson distribution when the elapsed times between the events are Exponential. Solution: We want to employ the deﬁnition of Poisson processes. Assume N represents the number of events (arrivals) in [0,t]. If the interarrival times are distributed exponentially (with parameter λ) and independently, then the number of arrivals occurred in [0,t], N , has Poisson distribution with parameter λt (Figure 12.4). Therefore, to solve this problem, we can repeat generating Exponential(λ) random variables while their sum is not larger than 1 (choosing t = 1). More speciﬁcally, we generate Exponential(λ) random variables Ti = −1 λ ln(Ui) by ﬁrst generating uniform random variables Ui’s. Then we deﬁne X = max {j : T1 + · · · + Tj ≤ 1} The algorithm can be simpliﬁed: X = max {j : −1 λ ln(U1 · · · Uj) ≤ 1} 8CHAPTER 12. INTRODUCTION TO SIMULATION USING MATLABA. RAKHSHAN AND H. PISHRO-NIK Lambda = 2; i = 0; U = rand; Y = −(1/Lambda) ∗ log(U ); sum = Y ; while(sum <= 1) U = rand; Y = −(1/Lambda) ∗ log(U ); sum = sum + Y ; i = i + 1; end X = i; 0 1 Exp(λ) Exp(λ) Exp(λ) Exp(λ) X= Maximum number of exponential random variables Figure 12.4: Poisson Random Variable To ﬁnish this section, let’s see how to convert uniform numbers to normal random variables. Normal distribution is extremely important in science because it is very commonly occuring. Theorem 3. (Box-Muller transformation) We can generate a pair of independent normal vari- ables (Z1, Z2) by transforming a pair of independent U nif orm(0, 1) random variables (U1, U2) [1]. { Z1 = √−2 ln U1 cos(2πU2) Z2 = √−2 ln U1 sin(2πU2) Example 8. (Box-Muller) Generate 5000 pairs of normal random variables and plot both histograms. Solution: We display the pairs in Matrix form. r = rand(5000, 2); n = sqrt(−2 ∗ log(r(:, 1))) ∗ [1, 1]. ∗ [cos(2 ∗ pi ∗ r(:, 2)), sin(2 ∗ pi ∗ r(:, 2))]; hist(n) 12.4. MATLAB COMMANDS FOR SPECIAL DISTRIBUTIONS 9 Figure 12.5: Histogram of a pair of normal random variables generated by Box-Muller transfor- mation 12.4 MATLAB Commands for Special Distributions In this section, we will see some useful commands for commonly employed distributions. To be as precise as possible, we repeat the description of the commands from MATLAB help [2]. 12.4.1 Discrete Distributions - Binomial Distribution: Y = binopdf(X,N,P) computes the binomial pdf at each of the values in X (vector) using the corresponding number of trials in N and probability of success for each trial in P. Y, N, and P can be vectors, matrices, or multidimensional arrays that all have the same size. A scalar input is expanded to a constant array with the same dimensions of the other inputs. Y = binocdf(X,N,P) computes a binomial cdf at each of the values in X using the corresponding number of trials in N and probability of success for each trial in P. X, N, and P can be vectors, ma- trices, or multidimensional arrays that are all the same size. A scalar input is expanded to a constant array with the same dimensions of the other inputs. The values in N must all be positive integers, the values in X must lie on the interval [0,N], and the values in P must lie on the interval [0, 1]. R = binornd(N,P) generates random numbers from the binomial distribution with parameters speciﬁed by the number of trials, N, and probability of success for each trial, P. N and P can be vectors, matrices, or multidimensional arrays that have the same size, which is also the size of R. 10CHAPTER 12. INTRODUCTION TO SIMULATION USING MATLABA. RAKHSHAN AND H. PISHRO-NIK A scalar input for N or P is expanded to a constant array with the same dimensions as the other input. - Poisson Distribution Y = poisspdf(X,lambda) computes the Poisson pdf at each of the values in X using mean parameters in lambda. P = poisscdf(X,lambda) computes the Poisson cdf at each of the values in X using the corresponding mean param- eters in lambda. R = poissrnd(lambda) generates random numbers from the Poisson distribution with mean parameter lambda. - Geometric Distribution Y = geopdf(X,P) computes the geometric pdf at each of the values in X using the corresponding probabili- ties in P. Y = geocdf(X,P) computes the geometric cdf at each of the values in X using the corresponding probabilities in P. R = geornd(P) generates geometric random numbers with probability parameter P. P can be a vector, a matrix, or a multidimensional array. 12.4.2 Continuous Distributions - Normal Distribution: Y = normpdf(X,mu,sigma) computes the pdf at each of the values in X using the normal distribution with mean mu and standard deviation sigma. P = normcdf(X,mu,sigma) computes the normal cdf at each of the values in X using the corresponding mean mu and standard deviation sigma. 12.5. EXERCISES 11 nlogL = normlike(params,data) returns the negative of the normal log-likelihood function. R = normrnd(mu,sigma) R = randn generates random numbers from the normal distribution with mean parameter mu and standard deviation parameter sigma. - Exponential Distribution: Y = exppdf(X,mu) returns the pdf of the exponential distribution with mean parameter mu, evaluated at the values in X. P = expcdf(X,mu) computes the exponential cdf at each of the values in X using the corresponding mean parameter mu. R = exprnd(mu) generates random numbers from the exponential distribution with mean parameter mu. 12.5 Exercises 1. Write MATLAB programs to generate Geometric(p) and Negative Binomial(i,p) random variables. Solution: To generate a Geometric random variable, we run a loop of Bernoulli trials until the ﬁrst success occurs. K counts the number of failures plus one success, which is equal to the total number of trials. K = 1; p = 0.2; while(rand > p) K = K + 1; end K Now, we can generate Geometric random variable i times to obtain a Negative Binomial(i, p) variable as a sum of i independent Geometric (p) random variables. 12CHAPTER 12. INTRODUCTION TO SIMULATION USING MATLABA. RAKHSHAN AND H. PISHRO-NIK K = 1; p = 0.2; r = 2; success = 0; while(success < r) if rand > p K = K + 1; print = 0 %F ailure else success = success + 1; print = 1 %Success end end K + r − 1 %N umber of trials needed to obtain r successes 2. (Poisson) Use the algorithm for generating discrete random variables to obtain a Poisson random variable with parameter λ = 2. Solution: We know a Poisson random variable takes all nonnegative integer values with probabilities pi = P (X = xi) = e −λ λi i! for i = 0, 1, 2, · · · To generate a P oisson(λ), ﬁrst we generate a random number U . Next, we divide the interval [0, 1] into subintervals such that the jth subinterval has length pj (Figure 12.2). Assume X =    x0 if (U < p0) x1 if (p0 ≤ U < p0 + p1) ... xj if (∑j−1 k=0 pk ≤ U < ∑j k=0 pk) ... Here xi = i − 1, so X = i if p0 + · · · + pi−1 ≤ U < p0 + · · · + pi−1 + pi F (i − 1) ≤ U < F (i) F is CDF 12.5. EXERCISES 13 lambda = 2; i = 0; U = rand; cdf = exp(−lambda); while(U >= cdf ) i = i + 1; cdf = cdf + exp(−lambda) ∗ lambda ∧i/gamma(i + 1); end; X = i; 3. Explain how to generate a random variable with the density f (x) = 2.5x√x for 0 < x < 1 if your random number generator produces a Standard Uniform random variable U . Hint: use the inverse transformation method. Solution: FX (X) = X 5 2 = U (0 < x < 1) X = U 2 5 U = rand; X = U 2 5 ; We have the desired distribution. 4. Use the inverse transformation method to generate a random variable having distribution function F (x) = x2 + x 2 , 0 ≤ x ≤ 1 Solution: X 2 + X 2 = U (X + 1 2 )2 − 1 4 = 2U X + 1 2 = √ 2U + 1 4 X = √ 2U + 1 4 − 1 2 (X, U ∈ [0, 1]) 14CHAPTER 12. INTRODUCTION TO SIMULATION USING MATLABA. RAKHSHAN AND H. PISHRO-NIK By generating a random number, U , we have the desired distribution. U = rand; X = sqrt ( 2U + 1 4 ) − 1 2 ; 5. Let X have a standard Cauchy distribution. FX (x) = 1 π arctan(x) + 1 2 Assuming you have U ∼ U nif orm(0, 1), explain how to generate X. Then, use this result to produce 1000 samples of X and compute the sample mean. Repeat the experiment 100 times. What do you observe and why? Solution: Using Inverse Transformation Method: U − 1 2 = 1 π arctan(X) π ( U − 1 2 ) = arctan(X) X = tan ( π(U − 1 2 ) ) Next, here is the MATLAB code: U = zeros(1000, 1); n = 100; average = zeros(n, 1); f or i = 1 : n U = rand(1000, 1); X = tan(pi ∗ (U − 0.5)); average(i) = mean(X); end plot(average) Cauchy distribution has no mean (Figure 12.6), or higher moments deﬁned. 6. (The Rejection Method) When we use the Inverse Transformation Method, we need a simple form of the cdf F (x) that allows direct computation of X = F −1(U ). When F (x) doesn’t have a simple form but the pdf f (x) is available, random variables with 12.5. EXERCISES 15 Figure 12.6: Cauchy Simulation density f (x) can be generated by the rejection method. Suppose you have a method for generating a random variable having density function g(x). Now, assume you want to generate a random variable having density function f (x). Let c be a constant such that f (y) g(y) ≤ c (for all y) Show that the following method generates a random variable with density function f (x). - Generate Y having density g. - Generate a random number U from Uniform (0, 1). - If U ≤ f (Y ) cg(Y ) , set X = Y . Otherwise, return to step 1. Solution: The number of times N that the ﬁrst two steps of the algorithm need to be called is itself a random variable and has a geometric distribution with “success” probability p = P (U ≤ f (Y ) cg(Y ) ) Thus, E(N ) = 1 p . Also, we can compute p: P ( U ≤ f (Y ) cg(Y ) |Y = y) = f (y) cg(y) p = ∫ ∞ −∞ f (y) cg(y) g(y)dy = 1 c ∫ ∞ −∞ f (y)dy = 1 c Therefore, E(N ) = c 16CHAPTER 12. INTRODUCTION TO SIMULATION USING MATLABA. RAKHSHAN AND H. PISHRO-NIK Let F be the desired CDF (CDF of X). Now, we must show that the conditional distri- bution of Y given that U ≤ f (Y ) cg(Y ) is indeed F , i.e. P (Y ≤ y|U ≤ f (Y ) cg(Y ) ) = F (y). Assume M = {U ≤ f (Y ) cg(Y ) }, K = {Y ≤ y}. We know P (M ) = p = 1 c . Also, we can compute P (U ≤ f (Y ) cg(Y ) |Y ≤ y) = P (U ≤ f (Y ) cg(Y ) , Y ≤ y) G(y) = ∫ y −∞ P (U ≤ f (y) cg(y) |Y = v ≤ y) G(y) g(v)dv = 1 G(y) ∫ y −∞ f (v) cg(v) g(v)dv = 1 cG(y) ∫ y −∞ f (v)dv = F (y) cG(y) Thus, P (K|M ) = P (M |K)P (K)/P (M ) = P (U ≤ f (Y ) cg(Y ) |Y ≤ y) × G(y) 1 c = F (y) cG(y) × G(y) 1 c = F (y) 7. Use the rejection method to generate a random variable having density function Beta(2, 4). Hint: Assume g(x) = 1 for 0 < x < 1. Solution: f (x) = 20x(1 − x)3 0 < x < 1 g(x) = 1 0 < x < 1 f (x) g(x) = 20x(1 − x)3 We need to ﬁnd the smallest constant c such that f (x)/g(x) ≤ c. Diﬀerentiation of this quantity yields d ( f (x) g(x) ) dx = 0 Thus, x = 1 4 Therefore, f (x) g(x) ≤ 135 64 Hence, f (x) cg(x) = 256 27 x(1 − x) 3 12.5. EXERCISES 17 n = 1; while(n == 1) U 1 = rand; U 2 = rand; if U 2 <= 256/27 ∗ U 1 ∗ (1 − U 1) ∧3 X = U 1; n = 0; end end 8. Use the rejection method to generate a random variable having the Gamma( 5 2 , 1) density function. Hint: Assume g(x) is the pdf of the Gamma (α = 5 2 , λ = 1 ). Solution: f (x) = 4 3 √π x 3 2 e −x, x > 0 g(x) = 2 5 e− 2x 5 x > 0 f (x) g(x) = 10 3 √π x 3 2 e − 3x 5 d ( f (x) g(x) ) dx = 0 Hence, x = 5 2 c = 10 3 √π ( 5 2 ) 3 2 e −3 2 f (x) cg(x) = x 3 2 e −3x 5 ( 5 2 ) 3 2 e −3 2 We know how to generate an Exponential random variable. - Generate a random number U1 and set Y = − 5 2 log U1. - Generate a random number U2. - If U2 < Y 3 2 e −3Y 5 ( 5 2 ) 3 2 e −3 2 , set X = Y . Otherwise, execute the step 1. 9. Use the rejection method to generate a standard normal random variable. Hint: Assume g(x) is the pdf of the exponential distribution with λ = 1. 18CHAPTER 12. INTRODUCTION TO SIMULATION USING MATLABA. RAKHSHAN AND H. PISHRO-NIK Solution: f (x) = 2 √2π e− x2 2 0 < x < ∞ g(x) = e −x 0 < x < ∞ (Exponential density function with mean 1) Thus, f (x) g(x) = √ 2 π e x− x2 2 Thus, x = 1 maximizes f (x) g(x) Thus, c = √ 2e π f (x) cg(x) = e − (x−1)2 2 - Generate Y , an exponential random variable with mean 1. - Generate a random number U . - If U ≤ e −(Y −1)2 2 set X = Y . Otherwise, return to step 1. 10. Use the rejection method to generate a Gamma(2, 1) random variable conditional on its value being greater than 5, that is f (x) = xe−x ∫ ∞ 5 xe−xdx = xe−x 6e−5 (x ≥ 5) Hint: Assume g(x) be the density function of exponential distribution. Solution: Since Gamma(2,1) random variable has expected value 2, we use an exponential distribution with mean 2 that is conditioned to be greater than 5. f (x) = xe(−x) ∫ ∞ 5 xe(−x)dx = xe(−x) 6e(−5) x ≥ 5 g(x) = 1 2 e (− x 2 ) e −5 2 x ≥ 5 f (x) g(x) = x 3 e−( x−5 2 ) We obtain the maximum in x = 5 since f (x) g(x) is decreasing. Therefore, c = f (5) g(5) = 5 3 - Generate a random number V . 12.5. EXERCISES 19 - Y = 5 − 2 log(V ). - Generate a random number U . - If U < Y 5 e−( Y −5 2 ), set X = Y ; otherwise return to step 1. 20CHAPTER 12. INTRODUCTION TO SIMULATION USING MATLABA. RAKHSHAN AND H. PISHRO-NIK Bibliography [1] http://projecteuclid.org/download/pdf_1/euclid.aoms/1177706645 [2] http://www.mathworks.com/help/matlab/ [3] Michael Baron, Probability and Statistics for Computer Scientists. CRC Press, 2006 [4] Sheldon M. Ross, Simulation. Academic Press, 2012 21 Chapter 13 Introduction to Simulation Using R A. Rakhshan and H. Pishro-Nik 13.1 Analysis versus Computer Simulation A computer simulation is a computer program which attempts to represent the real world based on a model. The accuracy of the simulation depends on the precision of the model. Suppose that the probability of heads in a coin toss experiment is unknown. We can perform the experiment of tossing the coin n times repetitively to approximate the probability of heads. P (H) = Number of times heads observed Number of times the experiment executed However, for many practical problems it is not possible to determine the probabilities by exe- cuting experiments a large number of times. With today’s computers processing capabilities, we only need a high-level language, such as R, which can generate random numbers, to deal with these problems. In this chapter, we present basic methods of generating random variables and simulate probabilistic systems. The provided algorithms are general and can be implemented in any computer language. However, to have concrete examples, we provide the actual codes in R. If you are unfamiliar with R, you should still be able to understand the algorithms. 13.2 Introduction: What is R? R is a programming language that helps engineers and scientists ﬁnd solutions for given statisti- cal problems with fewer lines of codes than traditional programming languages, such as C/C++ or Java, by utilizing built-in statistical functions. There are many built-in statistical functions and add-on packages available in R. It also has high quality customizable graphics capabilities. R is available for Unix/Linux, Windows, and Mac. Besides all these features, R is free! 13.3 Discrete and Continuous Random Number Generators Most of the programming languages can deliver samples from the uniform distribution to us (In reality, the given values are pseudo-random instead of being completely random.) The rest 1 2CHAPTER 13. INTRODUCTION TO SIMULATION USING RA. RAKHSHAN AND H. PISHRO-NIK of this section shows how to convert uniform random variables to any other desired random variable. The R code for generating uniform random variables is: U = runif (n, min = 0, max = 1) which returns a pseudorandom value drawn from the standard uniform distribution on the open interval (0,1). 13.3.1 Generating Discrete Probability Distributions from Uniform Distri- bution Let’s see a few examples of generating certain simple distributions: Example 1. (Bernoulli) Simulate tossing a coin with probability of heads p. Solution: Let U be a Uniform(0,1) random variable. We can write Bernoulli random variable X as: X = { 1 U < p 0 U ≥ p Thus, P (H) = P (X = 1) = P (U < p) = p Therefore, X has Bernoulli(p) distribution. The R code for Bernoulli(0.5) is: p = 0.5; U = runif (1, min = 0, max = 1); X = (U < p); Since the “runif(1, min = 0, max = 1)” command returns a number between 0 and 1, we divided the interval [0, 1] into two parts, p and 1 − p in length. Then, the value of X is determined based on where the number generated from uniform distribution fell. Example 2. (Coin Toss Simulation) Write codes to simulate tossing a fair coin to see how the law of large numbers works. Solution: You can write: 13.3. DISCRETE AND CONTINUOUS RANDOM NUMBER GENERATORS 3 n = 1000; U = runif (n, min = 0, max = 1); toss = U < 0.5; a = numeric(n + 1); avg = numeric(n); f or(i in 2 : n + 1) { a[i] = a[i − 1] + toss[i − 1]; avg[i − 1] = a[i]/(i − 1); } plot(1 : n, avg[1 : n], type = ”l”, lwd = 5, col = ”blue”, ylab = ”P roportionof Heads”, xlab = ”CoinT ossN umber”, cex.main = 1.25, cex.lab = 1.5, cex.axis = 1.75) If you run the above codes to compute the proportion of ones in the variable “toss,” the result will look like Figure 13.1. You can also assume the coin is unbiased with probability of heads equal to 0.6 by replacing the third line of the previous code with: toss = U < 0.6; Figure 13.1: R - coin toss simualtion 4CHAPTER 13. INTRODUCTION TO SIMULATION USING RA. RAKHSHAN AND H. PISHRO-NIK Example 3. (Binomial) Generate a Binomial(50, 0.2) random variable. Solution: To solve this problem, we can use the following lemma: Lemma 1. If X1, X2, ..., Xn are independent Bernoulli(p) random variables, then the random variable X deﬁned by X = X1 + X2 + ... + Xn has a Binomial(n, p) distribution. To generate a random variable X ∼ Binomial(n, p), we can toss a coin n times and count the number of heads. Counting the number of heads is exactly the same as ﬁnding X1 +X2 +...+Xn, where each Xi is equal to one if the corresponding coin toss results in heads and zero otherwise. Since we know how to generate Bernoulli random variables, we can generate a Binomial(n, p) by adding n independent Bernoulli(p) random variables. p = 0.2; n = 50; U = runif (n, min = 0, max = 1); X = sum(U < p); Generating Arbitrary Discrete Distributions In general, we can generate any discrete random variables similar to the above examples using the following algorithm. Suppose we would like to simulate the discrete random variable X with range RX = {x1, x2, ..., xn} and P (X = xj) = pj, so ∑ j pj = 1. To achieve this, ﬁrst we generate a random number U (i.e., U ∼ U nif orm(0, 1)). Next, we divide the interval [0, 1] into subintervals such that the jth subinterval has length pj (Figure 13.2). Assume X =    x0 if (U < p0) x1 if (p0 ≤ U < p0 + p1) ... xj if (∑j−1 k=0 pk ≤ U < ∑j k=0 pk) ... In other words X = xj if F (xj−1) ≤ U < F (xj), where F (x) is the desired CDF. We have P (X = xj) = P (j−1∑ k=0 pk ≤ U < j∑ k=0 pk ) = pj 13.3. DISCRETE AND CONTINUOUS RANDOM NUMBER GENERATORS 5 0 1 p0 p1 p2 p3 · · · pj Figure 13.2: Generating discrete random variables Example 4. Give an algorithm to simulate the value of a random variable X such that P (X = 1) = 0.35 P (X = 2) = 0.15 P (X = 3) = 0.4 P (X = 4) = 0.1 Solution: We divide the interval [0, 1] into subintervals as follows: A0 = [0, 0.35) A1 = [0.35, 0.5) A2 = [0.5, 0.9) A3 = [0.9, 1) Subinterval Ai has length pi. We obtain a uniform number U . If U belongs to Ai, then X = xi. P (X = xi) = P (U ∈ Ai) = pi P = c(0.35, 0.5, 0.9, 1); X = c(1, 2, 3, 4); counter = 1; r = runif (1, min = 0, max = 1); while(r > P [counter]) counter = counter + 1; end X[counter] 13.3.2 Generating Continuous Probability Distributions from the Uniform Distribution- Inverse Transformation Method At least in principle, there is a way to convert a uniform distribution to any other distribution. Let’s see how we can do this. Let U ∼ U nif orm(0, 1) and F be a CDF. Also, assume F is continuous and strictly increasing as a function. 6CHAPTER 13. INTRODUCTION TO SIMULATION USING RA. RAKHSHAN AND H. PISHRO-NIK Theorem 1. Let U ∼ U nif orm(0, 1) and F be a CDF which is strictly increasing. Also, consider a random variable X deﬁned as X = F −1(U ). Then, X ∼ F (The CDF of X is F ) Proof: P (X ≤ x) = P (F −1(U ) ≤ x) = P (U ≤ F (x)) (increasing function) = F (x) Now, let’s see some examples. Note that to generate any continuous random variable X with the continuous cdf F , F −1(U ) has to be computed. Example 5. (Exponential) Generate an Exponential(1) random variable. Solution: To generate an Exponential random variable with parameter λ = 1, we proceed as follows F (x) = 1 − e −x x > 0 U ∼ U nif orm(0, 1) X = F −1(U ) = − ln(1 − U ) X ∼ F This formula can be simpliﬁed since 1 − U ∼ U nif orm(0, 1) 0 1 U 1 − U Figure 13.3: Symmetry of Uniform Hence we can simulate X using X = − ln(U ) U = runif (1, min = 0, max = 1); X = −log(U ) 13.3. DISCRETE AND CONTINUOUS RANDOM NUMBER GENERATORS 7 Example 6. (Gamma) Generate a Gamma(20,1) random variable. Solution: For this example, F −1 is even more complicated than the complicated gamma cdf F itself. Instead of inverting the CDF, we generate a gamma random variable as a sum of n independent exponential variables. Theorem 2. Let X1, X2, · · · , Xn be independent random variables with Xi ∼ Exponential(λ). Deﬁne Y = X1 + X2 + · · · + Xn By the moment generating function method, you can show that Y has a gamma distribution with parameters n and λ, i.e., Y ∼ Gamma(n, λ). Having this theorem in mind, we can write: n = 20; lambda = 1; X = (−1/lambda) ∗ sum(log(runif (n, min = 0, max = 1))); Example 7. (Poisson) Generate a Poisson random variable. Hint: In this example, use the fact that the number of events in the interval [0, t] has Poisson distribution when the elapsed times between the events are Exponential. Solution: We want to employ the deﬁnition of Poisson processes. Assume N represents the number of events (arrivals) in [0,t]. If the interarrival times are distributed exponentially (with parameter λ) and independently, then the number of arrivals occurred in [0,t], N , has Poisson distribution with parameter λt (Figure 13.4). Therefore, to solve this problem, we can repeat generating Exponential(λ) random variables while their sum is not larger than 1 (choosing t = 1). More speciﬁcally, we generate Exponential(λ) random variables Ti = −1 λ ln(Ui) by ﬁrst generating uniform random variables Ui’s. Then we deﬁne X = max {j : T1 + · · · + Tj ≤ 1} The algorithm can be simpliﬁed: X = max {j : −1 λ ln(U1 · · · Uj) ≤ 1} 8CHAPTER 13. INTRODUCTION TO SIMULATION USING RA. RAKHSHAN AND H. PISHRO-NIK Lambda = 2; i = 0; U = runif (1, min = 0, max = 1); Y = −(1/Lambda) ∗ log(U ); sum = Y ; while(sum < 1) {U = runif (1, min = 0, max = 1); Y = −(1/Lambda) ∗ log(U ); sum = sum + Y ; i = i + 1; } X = i 0 1 Exp(λ) Exp(λ) Exp(λ) Exp(λ) X= Maximum number of exponential random variables Figure 13.4: Poisson Random Variable To ﬁnish this section, let’s see how to convert uniform numbers to normal random variables. Normal distribution is extremely important in science because it is very commonly occuring. Theorem 3. (Box-Muller transformation) We can generate a pair of independent normal vari- ables (Z1, Z2) by transforming a pair of independent U nif orm(0, 1) random variables (U1, U2) [1]. { Z1 = √−2 ln U1 cos(2πU2) Z2 = √−2 ln U1 sin(2πU2) Example 8. (Box-Muller) Generate 5000 pairs of normal random variables and plot both histograms. Solution: We display the pairs in Matrix form. 13.4. R COMMANDS FOR SPECIAL DISTRIBUTIONS 9 n = 5000; U 1 = runif (n, min = 0, max = 1) U 2 = runif (n, min = 0, max = 1) Z1 = sqrt(−2 ∗ log(U 1)) ∗ cos(2 ∗ pi ∗ U 2) Z2 = sqrt(−2 ∗ log(U 1)) ∗ sin(2 ∗ pi ∗ U 2) hist(Z1, col = ”wheat”, label = T ) Figure 13.5: Histogram of Z1, a normal random variable generated by Box-Muller transforma- tion 13.4 R Commands for Special Distributions In this section, we will see some useful commands for commonly employed distributions. Func- tions which start with “p”,“q”,“d”, and “r” give the cumulative distribution function (CDF), the inverse CDF, the density function (PDF), and a random variable having the speciﬁed dis- tribution respectively. Distributions Commands 10CHAPTER 13. INTRODUCTION TO SIMULATION USING RA. RAKHSHAN AND H. PISHRO-NIK Binomial pbinom qbinom dbinom rbinom Geometric pgeom qgeom dgeom rgeom N egativeBinomial pnbinom qnbinom dnbinom rnbinom P oisson ppois qpois dpois rpois Beta pbeta qbeta dbeta rbeta Beta pbeta qbeta dbeta rbeta Exponential pexp qexp dexp rexp Gamma pgamma qgamma dgamma rgamma Studentt pt qt dt rt U nif orm punif qunif dunif runif 13.5 Exercises 1. Write R programs to generate Geometric(p) and Negative Binomial(i,p) random variables. Solution: To generate a Geometric random variable, we run a loop of Bernoulli trials until the ﬁrst success occurs. K counts the number of failures plus one success, which is equal to the total number of trials. K = 1; p = 0.2; while(runif (1) > p) K = K + 1; K Now, we can generate Geometric random variable i times to obtain a Negative Binomial(i, p) variable as a sum of i independent Geometric (p) random variables. 13.5. EXERCISES 11 K = 1; p = 0.2; r = 2; success = 0; while(success < r) {if (runif (1) > p) {K = K + 1; print = 0 #F ailure }else {success = success + 1; print = 1}} #Success K + r − 1 #N umber of trials needed to obtain r successes 2. (Poisson) Use the algorithm for generating discrete random variables to obtain a Poisson random variable with parameter λ = 2. Solution: We know a Poisson random variable takes all nonnegative integer values with probabilities pi = P (X = xi) = e −λ λi i! for i = 0, 1, 2, · · · To generate a P oisson(λ), ﬁrst we generate a random number U . Next, we divide the interval [0, 1] into subintervals such that the jth subinterval has length pj (Figure 13.2). Assume X =    x0 if (U < p0) x1 if (p0 ≤ U < p0 + p1) ... xj if (∑j−1 k=0 pk ≤ U < ∑j k=0 pk) ... Here xi = i − 1, so X = i if p0 + · · · + pi−1 ≤ U < p0 + · · · + pi−1 + pi F (i − 1) ≤ U < F (i) F is CDF 12CHAPTER 13. INTRODUCTION TO SIMULATION USING RA. RAKHSHAN AND H. PISHRO-NIK lambda = 2; i = 0; U = runif (1); cdf = exp(−lambda); while(U >= cdf ) {i = i + 1; cdf = cdf + exp(−lambda) ∗ lambda ∧i/gamma(i + 1); } X = i; 3. Explain how to generate a random variable with the density f (x) = 2.5x√x for 0 < x < 1 if your random number generator produces a Standard Uniform random variable U . Hint: use the inverse transformation method. Solution: FX (X) = X 5 2 = U (0 < x < 1) X = U 2 5 U = runif (1); X = U 2 5 ; We have the desired distribution. 4. Use the inverse transformation method to generate a random variable having distribution function F (x) = x2 + x 2 , 0 ≤ x ≤ 1 Solution: X 2 + X 2 = U (X + 1 2 )2 − 1 4 = 2U X + 1 2 = √ 2U + 1 4 X = √ 2U + 1 4 − 1 2 (X, U ∈ [0, 1]) 13.5. EXERCISES 13 By generating a random number, U , we have the desired distribution. U = runif (1); X = sqrt ( 2U + 1 4 ) − 1 2 ; 5. Let X have a standard Cauchy distribution. FX (x) = 1 π arctan(x) + 1 2 Assuming you have U ∼ U nif orm(0, 1), explain how to generate X. Then, use this result to produce 1000 samples of X and compute the sample mean. Repeat the experiment 100 times. What do you observe and why? Solution: Using Inverse Transformation Method: U − 1 2 = 1 π arctan(X) π ( U − 1 2 ) = arctan(X) X = tan ( π(U − 1 2 ) ) Next, here is the R code: U = numeric(1000); n = 100; average = numeric(n); f or (i in 1 : n) {U = runif (1000); X = tan(pi ∗ (U − 0.5)); average[i] = mean(X); } plot(1 : n, average[1 : n], type = ”l”, lwd = 2, col = ”blue”) Cauchy distribution has no mean (Figure 13.6), or higher moments deﬁned. 6. (The Rejection Method) When we use the Inverse Transformation Method, we need a simple form of the cdf F (x) that allows direct computation of X = F −1(U ). When F (x) doesn’t have a simple form but the pdf f (x) is available, random variables with density f (x) can be generated by the rejection method. Suppose you have a method 14CHAPTER 13. INTRODUCTION TO SIMULATION USING RA. RAKHSHAN AND H. PISHRO-NIK Figure 13.6: Cauchy Mean for generating a random variable having density function g(x). Now, assume you want to generate a random variable having density function f (x). Let c be a constant such that f (y) g(y) ≤ c (for all y) Show that the following method generates a random variable with density function f (x). - Generate Y having density g. - Generate a random number U from Uniform (0, 1). - If U ≤ f (Y ) cg(Y ) , set X = Y . Otherwise, return to step 1. Solution: The number of times N that the ﬁrst two steps of the algorithm need to be called is itself a random variable and has a geometric distribution with “success” probability p = P (U ≤ f (Y ) cg(Y ) ) 13.5. EXERCISES 15 Thus, E(N ) = 1 p . Also, we can compute p: P ( U ≤ f (Y ) cg(Y ) |Y = y) = f (y) cg(y) p = ∫ ∞ −∞ f (y) cg(y) g(y)dy = 1 c ∫ ∞ −∞ f (y)dy = 1 c Therefore, E(N ) = c Let F be the desired CDF (CDF of X). Now, we must show that the conditional distri- bution of Y given that U ≤ f (Y ) cg(Y ) is indeed F , i.e. P (Y ≤ y|U ≤ f (Y ) cg(Y ) ) = F (y). Assume M = {U ≤ f (Y ) cg(Y ) }, K = {Y ≤ y}. We know P (M ) = p = 1 c . Also, we can compute P (U ≤ f (Y ) cg(Y ) |Y ≤ y) = P (U ≤ f (Y ) cg(Y ) , Y ≤ y) G(y) = ∫ y −∞ P (U ≤ f (y) cg(y) |Y = v ≤ y) G(y) g(v)dv = 1 G(y) ∫ y −∞ f (v) cg(v) g(v)dv = 1 cG(y) ∫ y −∞ f (v)dv = F (y) cG(y) Thus, P (K|M ) = P (M |K)P (K)/P (M ) = P (U ≤ f (Y ) cg(Y ) |Y ≤ y) × G(y) 1 c = F (y) cG(y) × G(y) 1 c = F (y) 7. Use the rejection method to generate a random variable having density function Beta(2, 4). Hint: Assume g(x) = 1 for 0 < x < 1. Solution: f (x) = 20x(1 − x)3 0 < x < 1 g(x) = 1 0 < x < 1 f (x) g(x) = 20x(1 − x)3 16CHAPTER 13. INTRODUCTION TO SIMULATION USING RA. RAKHSHAN AND H. PISHRO-NIK We need to ﬁnd the smallest constant c such that f (x)/g(x) ≤ c. Diﬀerentiation of this quantity yields d ( f (x) g(x) ) dx = 0 Thus, x = 1 4 Therefore, f (x) g(x) ≤ 135 64 Hence, f (x) cg(x) = 256 27 x(1 − x) 3 n = 1; while (n == 1) {U 1 = runif (1); U 2 = runif (1); if (U 2 <= 256/27 ∗ U 1 ∗ (1 − U 1) ∧3) {X = U 1; n = 0; }} 8. Use the rejection method to generate a random variable having the Gamma( 5 2 , 1) density function. Hint: Assume g(x) is the pdf of the Gamma (α = 5 2 , λ = 1 ). Solution: f (x) = 4 3 √π x 3 2 e −x, x > 0 g(x) = 2 5 e− 2x 5 x > 0 f (x) g(x) = 10 3 √π x 3 2 e − 3x 5 d ( f (x) g(x) ) dx = 0 Hence, x = 5 2 c = 10 3 √π ( 5 2 ) 3 2 e −3 2 f (x) cg(x) = x 3 2 e −3x 5 ( 5 2 ) 3 2 e −3 2 We know how to generate an Exponential random variable. 13.5. EXERCISES 17 - Generate a random number U1 and set Y = − 5 2 log U1. - Generate a random number U2. - If U2 < Y 3 2 e −3Y 5 ( 5 2 ) 3 2 e −3 2 , set X = Y . Otherwise, execute the step 1. 9. Use the rejection method to generate a standard normal random variable. Hint: Assume g(x) is the pdf of the exponential distribution with λ = 1. Solution: f (x) = 2 √2π e− x2 2 0 < x < ∞ g(x) = e −x 0 < x < ∞ (Exponential density function with mean 1) Thus, f (x) g(x) = √ 2 π e x− x2 2 Thus, x = 1 maximizes f (x) g(x) Thus, c = √ 2e π f (x) cg(x) = e − (x−1)2 2 - Generate Y , an exponential random variable with mean 1. - Generate a random number U . - If U ≤ e −(Y −1)2 2 set X = Y . Otherwise, return to step 1. 10. Use the rejection method to generate a Gamma(2, 1) random variable conditional on its value being greater than 5, that is f (x) = xe−x ∫ ∞ 5 xe−xdx = xe−x 6e−5 (x ≥ 5) Hint: Assume g(x) be the density function of exponential distribution. Solution: Since Gamma(2,1) random variable has expected value 2, we use an exponential distribution with mean 2 that is conditioned to be greater than 5. f (x) = xe(−x) ∫ ∞ 5 xe(−x)dx = xe(−x) 6e(−5) x ≥ 5 g(x) = 1 2 e (− x 2 ) e −5 2 x ≥ 5 f (x) g(x) = x 3 e−( x−5 2 ) 18CHAPTER 13. INTRODUCTION TO SIMULATION USING RA. RAKHSHAN AND H. PISHRO-NIK We obtain the maximum in x = 5 since f (x) g(x) is decreasing. Therefore, c = f (5) g(5) = 5 3 - Generate a random number V . - Y = 5 − 2 log(V ). - Generate a random number U . - If U < Y 5 e−( Y −5 2 ), set X = Y ; otherwise return to step 1. Bibliography [1] http://projecteuclid.org/download/pdf_1/euclid.aoms/1177706645 [2] Michael Baron, Probability and Statistics for Computer Scientists. CRC Press, 2006 [3] Sheldon M. Ross, Simulation. Academic Press, 2012 19 Chapter 14 Recursive Methods 14.1 Using Recursion Some problems in combinatorics and probability can be solved using recursive methods. Here is the basic idea: Suppose we are interested in computing a sequence an, for n = 0, 1, 2, ... . The value an could be the number of elements in a set or the probability of a certain event. We may be able to ﬁnd a recursive relation that relates an to other ai’s where i < n. For example, suppose that for a certain problem, we can show that an = an−1 + 2an−3, for n = 3, 4, 5, ... Now, if we know a0, a1, and a2, we can use the above recursive equation to ﬁnd an for all n. Let’s look at an example. Example 1. Find the total number of sequences of length n (using H and T ) such that no two Hs are next to each other. For example, for n = 2, we have 3 possible sequences: HT, T H, T T . Let an be the number of such sequences. Let’s call these sequences “NO-HH sequences.” We have a1 = 2, a2 = 3. But how do we ﬁnd, say a1000? To do this, we will show that an = an−1 + an−2 for n = 3, 4, 5, ... (14.1) To show this, consider a NO-HH sequence of length n. This sequence either starts with a T or an H. - If it starts with a T , then the rest of the sequence is simply a NO-HH sequence of length n − 1. Conversely, by adding a T in front of any NO-HH sequence of length n − 1, we can obtain a NO-HH sequence of length n. - If it starts with an H, then the second element in the sequence must be a T . In this case, the rest of the sequence is simply a NO-HH sequence of length n − 2. Conversely, by adding HT in front of any NO-HH sequence of length n − 2, we can obtain a NO-HH sequence of length n. 1 2 CHAPTER 14. RECURSIVE METHODS Thus, we conclude that an = an−1 + an−2. Since we already know that a1 = 2 and a2 = 3, we can use this recursive equation to ﬁnd a3 = 5, a4 = 8, a5 = 13, ... Using a computer program we can compute an for the larger values of n. However, there is also a straight-forward method to solving Equation 14.1 in order to obtain a simple formula for an that does not involve previous values in the sequence. We will discuss how to solve these equations in general shortly. Here, we solve Equation 14.1 to ﬁnd a formula for the number of No-HH sequences of length n. The trick, as we will see, is to let ak = xk and ﬁnd non-zero values of x that satisfy the recursive equation. In particular, letting ak = xk in Equation 14.1, we obtain x2 = x + 1, which gives x1 = 1 + √5 2 , x2 = 1 − √5 2 . Then the general solution can be written in the form of an = α1xn 1 + α2xn 2 , where α1 and α2 are constants to be determined from the known values of an. For example, here we know a1 = 2, a2 = 3. Using these two values we can ﬁnd α1 and α2. It is a little bit easier to use a0 and a1. That is, since a2 = a1 + a0, we obtain a0 = 1. Thus we have a0 = 1 = α1 ( 1 + √5 2 )0 + α2 ( 1 − √5 2 )0 a1 = 2 = α1 ( 1 + √5 2 )1 + α2 ( 1 − √5 2 )1 . Thus, we obtain { α1 + α2 = 1 α1( 1+ √ 5 2 ) + α2( 1− √5 2 ) = 2 By solving these equations, we obtain α1 = 5+3 √5 10 , and α2 = 5−3 √5 10 . Finally, an = 5 + 3√5 10 ( 1 + √5 2 )n + 5 − 3√5 10 ( 1 − √5 2 )n (14.2) This might seem somewhat strange as it does not look like an integer. However, you can evaluate the above expression for small values of n to see that, in fact, square roots always cancel out and the resulting values of an are always integers. If the above calculation seems confusing, don’t worry. We will now discuss in general how to solve recursive equations such as the one given in Equation 14.1. 14.1. USING RECURSION 3 14.1.1 Solving Linear Homogeneous Recurrence Equations with Constant Coeﬃcients Suppose that we have the following recursive equation: an + c1an−1 + c2an−2 + c3an−3 + ... + cdan−d = 0 (14.3) where the ci’s are known constants. Also suppose that we already know the values of ai for d diﬀerent values of i. For example, we might know the values of a1, a2, ..., ad. To solve this recursive equation, we ﬁrst solve the following characteristic equation xd + c1xd−1 + c2x d−2 + c3xn−3 + ... + cd = 0 (14.4) This equation is obtained by replacing ai by xi in the recursive Equation 14.3. Let x1, x2, ..., xd be d distinct roots of the characteristic polynomial (we will discuss the case of repeated roots shortly). Then the general format for solutions to the recursive Equation 14.3 is given by an = α1xn 1 + α2xn 2 + α3xn 3 + ... + αdxn d (14.5) The values of α1, α2, ..., αd can be obtained from the known values of ai. If a root is repeated r times, we need to include r terms for that root, each scaled by a power of n. For example, if x1 is a repeated root of multiplicity r, then we write an = α11xn 1 + α12nxn 1 + α13n2xn 1 + ... + α1rnr−1xn 1 + α2x n 2 + α3xn 3 + ... + αdx n d (14.6) To better understand all this, let’s look at some examples. Example 2. Solve the following recurrence equations: (a) an = 3an−1 − 2an−2, where a0 = 2, a1 = 3; (b) an = 4an−1 − 5an−2 + 2an−3, where a0 = 0, a1 = 2, and a2 = 5. Solution (a) The characteristic polynomial for an = 3an−1 − 2an−2 is x2 − 3x + 2. It has roots x1 = 2 and x2 = 1. Thus, the general solution is of the form an = α2n + β. Since a0 = 2, a1 = 3, we obtain α = 1, β = 1. Therefore, an is given by an = 2 n + 1, for n = 0, 1, 2, ... (b) The characteristic polynomial for an = 4an−1 − 5an−2 + 2an−3 is x3 − 4x2 + 5x − 2. We can factor this polynomial as x3 − 4x2 + 5x − 2 = (x − 1) 2(x − 2). Thus we have two roots, x1 = 1 with multiplicity 2, and x2 = 2. The general formula for xn can be written as an = α1 + α2n + α32 n. Using a0 = 0, a1 = 2, and a2 = 5, we obtain an = 2 n + n − 1. 4 CHAPTER 14. RECURSIVE METHODS Note that recurrences could be much more complicated than the form given in Equation 14.3, and sometimes we may not be able to ﬁnd simple closed form solutions for them. Nevertheless, we can usually use computer programs to compute them for at least moderate values of n. In general, if the recursion is not in the form of Equation 14.3, a good start would be to compute an for small n and see if we can identify a pattern and guess a general formula for an. If we are lucky, and we can guess a general formula, then we usually can prove it mathematically using induction. 14.1.2 Using Recursion with Conditioning As we have seen so far, conditioning is a powerful method for solving probability problems. In some problems, after conditioning we get a recursive relation that can help us solve the problem. As an easy example, let’s start with a problem closely related to Example 1. Example 3. I toss a fair coin n times and record the sequence of heads and tails. What is the probability that I do not observe two consecutive heads in the sequence? Solution: Let pn be the probability of not observing two consecutive heads in n coin tosses. One way to solve this problem is to use our answer to Example 1. In that example, we found the total number of sequences of length n with no HH to be an = 5 + 3√5 10 ( 1 + √5 2 )n + 5 − 3 √5 10 ( 1 − √5 2 )n. Now, since the total number of sequences of length n is 2n, and all sequences are equally likely, we obtain pn = an 2n = = 5 + 3√5 10 ( 1 + √5 4 )n + 5 − 3√ 5 10 ( 1 − √5 4 )n. (14.7) Here we will solve this problem directly using conditioning. Let An be the event that we observe no consecutive heads in n coin tosses, i.e., pn = P (An). The idea is to condition on the result of the ﬁrst coin toss. There are two possibilities. Using the law of total probability and by conditioning on the result of the ﬁrst coin toss, we can write pn = P (An) = P (An|H)P (H) + P (An|T )P (T ) = 1 2 P (An|H) + 1 2 P (An|T ) (14.8) Now, to ﬁnd P (An|T ) note that if the ﬁrst coin toss is a T , then in order to not observe an HH in the entire sequence, we must not observe an HH in the remaining n − 1 coin tosses. Thus, we have P (An|T ) = P (An−1) = pn−1. Similarly, if the ﬁrst coin toss results in an H, the second one must be a T and we must not observe an HH in the remaining n − 2 coin tosses, thus we have P (An|H) = 1 2 · P (An−2) = 1 2 pn−2. 14.1. USING RECURSION 5 Plugging back into Equation 14.8 we obtain pn = 1 2 pn−1 + 1 4 pn−2. Note that we also know that p1 = 1 and p2 = 3 4 . Using the recursion, we also obtain p0 = 1. Thus we can solve this recursion. We obtain the following characteristic equation x2 − 1 2 x − 1 4 = 0. The characteristic equation has roots x1 = 1+ √5 4 and x2 = 1−√5 4 , so the general solution is given by pn = α( 1 + √5 4 )n + β( 1 − √5 4 )n. Using p0 = 1 and p1 = 1, we obtain pn = 5 + 3√5 10 ( 1 + √5 4 )n + 5 − 3√ 5 10 ( 1 − √5 4 )n. Which, as we expect, is the same as Equation 14.7. Gambler’s Ruin Problem: Here we discuss a famous problem called the Gambler’s Ruin. It refers to a simple gambling game in which two gamblers play repeatedly until one of them runs out of money. This is also an example of a random walk. Example 4. Two gamblers, call them Gambler A and Gambler B, play repeatedly. In each round, A wins 1 dollar with probability p or loses 1 dollar with probability q = 1 − p (thus, equivalently, in each round B wins 1 dollar with probability q = 1 − p and loses 1 dollar with probability p). We assume diﬀerent rounds are independent. Suppose that initially A has i dollars and B has N − i dollars. The game ends when one of the gamblers runs out of money (in which case the other gambler will have N dollars). Find pi, the probability that A wins the game given that he has initially i dollars. Solution: At ﬁrst it might not be clear that this problem can be solved using recursive methods. The main idea is very simple. Condition on the result of the ﬁrst round. After the ﬁrst round, A will have either i − 1 dollars (if he loses) or will have i + 1 dollars (if he wins). This way we can relate pi to pi−1 and pi+1. In particular, applying the law of total probability, we obtain pi = P (A wins the game|A wins the ﬁrst round)P (A wins the ﬁrst round)+ P (A wins the game|A loses the ﬁrst round)P (A loses the ﬁrst round) = pi+1p + pi−1(1 − p). 6 CHAPTER 14. RECURSIVE METHODS Thus we obtain the recursive equation pi = pi+1p + pi−1(1 − p). We can rewrite the equation as ppi+1 = pi − (1 − p)pi−1. To solve this recursion we need to know the value of pi for two diﬀerent values of i. We already know that p0 = 0. If A starts with 0 dollars he is automatically the loser. Similarly, if B starts with 0 dollars (i.e., A starts with N dollars), then A is automatically the winner. Thus, we have pN = 1. The characteristic equation is px2 − x + (1 − p) = 0. Solving this equation, we obtain two roots, x1 = 1 and x2 = 1−p p = q p . The roots are diﬀerent if p ̸= q. Thus, we need to consider two cases: if p ̸= q (i.e., when p ̸= 1 2 ) we can write the general solution as pi = α + β ( q p )i . Using p0 = 0 and pN = 1, we obtain pi = 1 − ( q p )i 1 − ( q p )N . If p = q = 1 2 , the characteristic equation has a repeated root of x1 = 1, so the general solution can be written as pi = α′ + β′i. Using p0 = 0 and pN = 1, we obtain pi = i N . To summarize, for i = 0, 1, 2, ..., N , we have pi =    1−( q p )i 1−( q p )N if p ̸= 1 2 i N if p = 1 2 Discussion: Using the above answer, we can draw some conclusions. The setup of this prob- lem in some sense can be a model for someone who goes to the casino and gambles repeatedly. We can look at this problem from two points of view. First, let us be somewhat optimistic and assume that the casino games are fair. In that case, you can win or lose with probability p = q = 1 2 each time. But the casino has usually much more money than an individual gambler, that is i << N . This means that your chance of winning, i N is very small. Thus, if you gamble 14.1. USING RECURSION 7 repeatedly you are most likely to lose all your money. What if you are very rich? Assume that you have the same amount of money as the casino. Even in that case, you are in no luck. The reason is that the games are usually unfair (casino has some advantage), so p < 1 2 . Now, if you and the casino both have a large sum of money N 2 then your chance of winning is pi = 1 − ( q p )i 1 − ( q p )N = 1 − ( q p ) N 2 1 − ( q p )N since i = N 2 ≈ −( q p ) N 2 −( q p )N since N is large and q > p = 1 ( q p ) N 2 → 0 as N becomes large. Thus, even if you have the same amount of money as the casino, you will most likely lose all your money if you gamble repeatedly. 14.1.3 Solved Problems 1. Solve the following recursive equations. That is, ﬁnd a closed form formula for an. (a) an = an−1 + n, with a0 = 0. (b) an = nan−1, with a0 = 1. (c) an = 5an−1 − 6an−2, with a0 = 3, a1 = 8. (d) an = 3an−1 − 4an−3, with a0 = 3, a1 = 5, a2 = 17. (e) an = 2an−1 − 2an−2, with a0 = a1 = 2. Solution: (a) Note that this equation is NOT of the form of Equation 14.3, so we cannot use our general methodology to solve this problem. In these situations, it is always a good idea to compute the ﬁrst few terms in the sequence and try to guess a general formula, and then prove it (possibly using mathematical induction). For this problem we quickly observe that an = 1 + 2 + 3 + · · · + n. This is the sum of the numbers from 1 to n, and it is given by an = n(n+1) 2 . To obtain 8 CHAPTER 14. RECURSIVE METHODS this formula, you can write an + an = ( 1 + 2 + 3 + · · · + n) + (n + (n − 1) + · · · + 2 + 1 ) = (1 + n) + (2 + (n − 1)) + · · · + (n + 1) = (n + 1)n. (b) Again, this is not in the form of Equation 14.3. By writing the ﬁrst few ai’s we observe that an = 1 · 2 · 3 · · · n. Thus, we conclude that an = n! (c) This recurrence equation is in the form of Equation 14.3, so we can use our general method. In particular, the characteristic polynomial is x 2 − 5x + 6. This polynomial has two roots, x1 = 2 and x2 = 3, so the solution will be in the form an = α2n + β3 n. Using a0 = 3 and a1 = 8, we obtain an = 2 n + 2 · 3 n. (d) This recurrence equation is in the form of Equation 14.3, so we can use our general method. In particular, the characteristic polynomial is x3 − 3x2 + 4 = (x − 2) 2(x + 1). Thus, the solution will be in the form an = α12 n + α2n2 n + α3(−1) n. Using a0 = 3, a1 = 5, a2 = 17, we obtain an = (2 + n)2 n + (−1) n. (e) This recurrence equation is in the form of Equation 14.3, so we can use our general method. In particular, the characteristic polynomial is x 2 − 2x + 2, 14.1. USING RECURSION 9 which has two complex roots, x1 = 1 + i, x2 = 1 − i (where i = √−1). Thus, the solution will be in the form an = α(1 + i)n + β(1 − i)n. Using a0 = a1 = 2, we obtain an = (1 + i) n + (1 − i) n. 2. Let an be the total number of sequences of length n (using H and T ) that do not include three consecutive Hs. For example, we know that a1 = 2, a2 = 4 and a3 = 7. Find a recurrence equation for an. Solution: We can solve this problem with a similar approach to the method we used in Example 1. In particular, we will show that an = an−1 + an−2 + an−3. Let’s call these sequences NO-HHH sequences. Consider a NO-HHH sequence of length n. This sequence either starts with an T or an H. - If it starts with a T , then the rest of the sequence is simply a NO-HHH sequence of length n − 1. Conversely, by adding a T in front of any NO-HHH sequence of length n − 1, we can obtain a NO-HHH sequence of length n. - If it starts with an H, then the second element in the sequence is either an H or a T : – If the second element is a T , then the rest of the sequence is simply a NO-HHH sequence of length n − 2. Conversely, by adding HT in front of any NO-HHH sequence of length n − 2, we can obtain a NO-HHH sequence of length n. – If the second element is also an H, then the third element must be a T , and thus the rest of the sequence is a NO-HHH sequence of length n − 3. Conversely, by adding HHT in front of any NO-HHH sequence of length n − 3, we will obtain a NO-HHH sequence of length n. Thus, we conclude that an = an−1 + an−2 + an−3. 3. Let k be a ﬁxed given integer larger than 1. Let f (n, k) be the total number of sequences of length n (using H and T ) that do not include k consecutive Hs. Find a recurrence equation for f (n, k). 10 CHAPTER 14. RECURSIVE METHODS Solution: Similar to Example 1 and Problem 2, we can argue that f (n, k) = f (n − 1, k) + f (n − 2, k) + ... + f (n − k, k), for n > k. And f (n, k) = 2n for n = 1, 2, 3, ..., k − 1, and f (k, k) = 2k − 1. Using the above recursion we can deﬁne f (0, k) = 1 so that the above recursion also holds for n = k. 4. Let f (n, k, l) be the number of binary sequences of length n with exactly k ones and at least l consecutive zeros. Find a recursive equation along with initial values to compute f (n, k, l). Assume that n ≥ k + l. Solution: Let also g(n, k, l) be the number of binary sequences of length n with exactly k ones and NO sequences of l consecutive zeros. We have f (n, k, l) = ( n k ) − g(n, k, l). We provide a recursive formula for f (n, k, l). First note that f (n, k, l) = n − l + 1, for k + l = n f (n, 0, l) = 1, for l ≥ 1. Now we prove for n ≥ 1, l ≥ 1, k ≥ 0, and k + l ≤ n we have f (n, k, l) = f (n − 1, k − 1, l) + f (n − 1, k, l) + g(n − l − 1, k − 1, l). Thus we have the following recursion for f (n, k, l): f (n, k, l) = f (n − 1, k − 1, l) + f (n − 1, k, l)+ ( n − l − 1 k − 1 ) − f (n − l − 1, k − 1, l). The above equation can be used to compute f (n, k, l) for moderate values of n, k, and l. Proof: Consider a sequence of length n with exactly k ones and at least l consecutive zeros. We consider two cases: (a) The ﬁrst n − 1 bits (positions) include at least l consecutive zeros. In this case the last bit is either 0 or a 1 which results in f (n − 1, k − 1, l) + f (n − 1, k, l) distinct sequences. (b) The ﬁrst n − 1 bits do NOT include l consecutive zeros. In this case the last l bits must be zeros and the (n − l)th bit must be a one. Thus the ﬁrst n − l − 1 bits must have exactly k − 1 ones and no consecutive l zeros. This results in g(n − l − 1, k − 1, l) distinct sequences. 14.1. USING RECURSION 11 5. I toss a biased coin n times and record the sequence of heads and tails. If P (H) = p (where 0 < p < 1), what is the probability that I do not observe two consecutive heads in the sequence? Solution: Let An be the event that we observe no consecutive heads in n coin tosses, i.e., pn = P (An). The idea is to condition on the result of the ﬁrst coin toss. There are two possibilities. Using the law of total probability and by conditioning on the result of the ﬁrst coin toss, we can write pn = P (An) = P (An|H)P (H) + P (An|T )P (T ) = p · P (An|H) + (1 − p) · P (An|T ) (14.9) Now, to ﬁnd P (An|T ) note that if the ﬁrst coin toss is a T , then in order to not observe an HH in the entire sequence, we must not observe an HH in the remaining n − 1 coin tosses. Thus, we have P (An|T ) = P (An−1) = pn−1. Similarly, if the ﬁrst coin toss results in an H, the second one must be a T and we must not observe an HH in the remaining n − 2 coin tosses, thus we have P (An|H) = (1 − p) · P (An−2) = (1 − p)pn−2. Plugging back into Equation 14.9 we obtain pn = (1 − p)pn−1 + p(1 − p)pn−2. Note that we also know that p1 = 1 and p2 = 1 − p2. Using the recursion, we also obtain p0 = 1. Thus we can solve this recursion. We obtain the following characteristic equation: x2 − (1 − p)x − p(1 − p) = 0. The characteristic equation has roots x1 = 1−p+ √(1−p)(1+3p) 2 and x2 = 1−p− √(1−p)(1+3p) 2 , so the general solution is given by pn = α( 1 − p + √ (1 − p)(1 + 3p) 2 )n + β( 1 − p − √ (1 − p)(1 + 3p) 2 )n. Using p0 = 1 and p1 = 1, we obtain α = 1 + p + √ (1 − p)(1 + 3p) 2 √(1 − p)(1 + 3p) β = 1 − α = √(1 − p)(1 + 3p) − 1 − p 2√ (1 − p)(1 + 3p) . 12 CHAPTER 14. RECURSIVE METHODS 6. Solve the following recurrence equation, that is ﬁnd a closed form formula for an. an = αan−1 + β, with a0 = 1, where α ̸= 0 and β are known constants. Solution: This equation is NOT exactly of the form of Equation 14.3 (because of the constant β), so we cannot directly use our general methodology to solve this problem. However, by computing an for a few values of n we can identify the solution: a1 = α + β a2 = α2 + β(1 + α) a3 = α3 + β(1 + α + α2) a3 = α4 + β(1 + α + α2 + α3). In general, we obtain an = αn + β(1 + α + α2 + · + αn−1) = αn + β ( 1 − αn 1 − α ) . 7. This problem has applications in coding theory: I toss a biased coin n times and record the sequence of heads and tails. If P (H) = p (where 0 < p < 1), what is the probability that I observe an even number of Hs? Solution: Let An be the event that I observe an even number of heads for n = 1, 2, .... Let an = P (An). Conditioning on the last coin toss, we can write for n ≥ 2: an = P (An|H)P (H) + P (An|T )P (T ) = p · P (An|H) + (1 − p) · P (An|T ) = p · P (Ac n−1) + (1 − p) · P (An−1) = p(1 − an−1) + (1 − p)an−1. Thus, we obtain the following recursive equation: an = (1 − 2p)an−1 + p, with a1 = 1 − p. 14.2. END OF CHAPTER PROBLEMS 13 From the recursion, we obtain a0 = 1, so we have the following equation: an = (1 − 2p)an−1 + p, with a0 = 1. This recursion is in the form given in problem 6 (α = 1 − 2p, β = p), so we obtain an = (1 − 2p) n + p ( 1 − (1 − 2p)n 2p ) = 1 + (1 − 2p)n 2 . 14.2 End of Chapter Problems 1. Solve the following recurrence equations, that is, ﬁnd a closed form formula for an. (a) an = 2an−1 − 3 4 an−2, with a0 = 0, a1 = −1. (b) an = 4an−1 − 4an−2, with a0 = 2, a1 = 6. 2. I toss a biased coin n times. Let P (H) = p and let an,k be the probability that I observe k heads. (a) By conditioning, on the last coin toss, show that an+1,k+1 = p · an,k + (1 − p) · an,k+1. (b) Using part (a), prove that for 0 ≤ k < n, we have (n+1 k+1) = ( n k+1 ) + (n k). 3. * You toss a biased coin repeatedly. If P (H) = p, what is the probability that two consecutive Hs are observed before we observe two consecutive T s? For example, this event happens if the observed sequence is T HT HHT HT T · · · . 4. I toss a biased coin n times and record the sequence of heads and tails. Assume P (H) = p (where 0 < p < 1). Let an be the probability that the number of heads is divisible by 3. Write a set of recursive equations to compute an. Review of Fourier T ransform Here, we briefly review some properties of the Fourier transform. For a deterministic function x(t) the Fourier transform (if exists) is defined as F {x(t)} = ∫ ∞ −∞ x(t)e −2πift dt, where i = √−1. The Fourier transform of x(t) is a function of f, so we can show it by X(f) = F {x(t)}. We can obtain x(t) from its fourier transform X(f) using x(t) = F −1{X(f)} = ∫ ∞ −∞ X(f)e 2πift df. In general X(f) is a complex-valued function, i.e., we can write X(f) : R ↦ C. Fourier T ransform Fourier transform X(f) = F {x(t)} = ∫ ∞ −∞ x(t)e −i2πftdt Inversion formula x(t) = F −1{X(f)} = ∫ ∞ −∞ X(f)e i2πftdf When working with Fourier transform, it is often useful to use tables. There are two tables given on this page. One gives the Fourier transform for some important functions and the other provides general properties of the Fourier transform. Using these tables, we can find the Fourier transform for many other functions. Figure 1. Some common functions. Tabl e o f Fo uri er Tran sfo rm Pai rs x(t) Fourier T ransform X(f) δ(t) 1 1 δ(f) δ(t − a) e−i2πfa e i2πat δ(f − a) cos(2πat) δ(f − a) + δ(f + a) sin(2πat) − δ(f + a) + δ(f − a) Π(t) sinc(f) sinc(t) Π(f) Λ(t) sinc 2(f) sinc 2(t) Λ(f) e −atu(t), a > 0 te −atu(t), a > 0 1 2 1 2 1 2i 1 2i 1 a+i2πf 1 (a+i2πf)2 e −a|t| 2πe−2πa|f| e −πt2 e −πf 2 u(t) δ(f) + sgn(t) Tabl e o f Fo uri er Tran sfo rm Pro p erti es Function Fourier T ransform ax1(t) + bx2(t) aX1(f) + bX2(f) x(at) X( ) x(t − a) e −i2πfaX(f) e i2πatx(t) X(f − a) x(t) ∗ y(t) X(f)Y (f) x(t)y(t) X(f) ∗ Y (f) x(t) i2πfX(f) tx(t) ( ) X(f) ∫ t −∞ x(u)du + X(0)δ(f) X(t) = F {x(t)}∣ ∣∣f =t x(−f) = F −1{X(f)}∣ ∣∣t=−f 2a a2+(2πf)2 2a a2+t2 1 2 1 i2πf 1 iπf 1 |a| f a d dt i 2π d df X(f) i2πf 1 2 Some Important Distributions D isc re te D istributio n s X ∼ Bernoulli(p) PMF: PX(k) = { p for k = 1 1 − p for k = 0 CDF: FX(x) = ⎧⎪ ⎨ ⎪⎩ 0 for x < 0 1 − p for 0 ≤ x < 1 1 for1 ≤ x Moment Generating Function (MGF): MX(s) = 1 − p + pe s Characteristic Function: ϕX(ω) = 1 − p + pe iω Expected V alue: EX = p Variance: Var(X) = p(1 − p) X ∼ Binomial(n, p) PMF: PX(k) = ( )p k(1 − p) n−k for k = 0, 1, 2, ⋯ , n Moment Generating Function (MGF): MX(s) = (1 − p + pe s) n Characteristic Function: ϕX(ω) = (1 − p + pe iω) n Expected V alue: EX = np Variance: Var(X) = np(1 − p) MA TLAB: R = binornd( n,p) n k X ∼ Geometric(p) PMF: PX(k) = p(1 − p) k−1 for k = 1, 2, 3, . . . CDF: FX(x) = 1 − (1 − p) ⌊x⌋ for x ≥ 0 Moment Generating Function (MGF): MX(s) = for s < − ln(1 − p) Characteristic Function: ϕX(ω) = Expected V alue: EX = Variance: Var(X) = MA TLAB: R = geornd( p)+1 pe s 1 − (1 − p)es pe iω 1 − (1 − p)eiω 1 p 1 − p p2 X ∼ Pascal(m, p) (Negative Binomial) PMF: PX(k) = ( )p m(1 − p) k−m for k = m, m + 1, m + 2, m + 3, . . . Moment Generating Function (MGF): MX(s) = ( ) m for s < − log(1 − p) Characteristic Function: ϕX(ω) = ( ) m Expected V alue: EX = Variance: Var(X) = MA TLAB: R = nbinrnd( m,p)+1 k − 1 m − 1 pe s 1 − (1 − p)es pe iω 1 − (1 − p)eiω m p m(1 − p) p2 X ∼ Hypergeometric(b, r, k) PMF: PX(x) = for x = max(0, k − r), max(0, k − r) + 1, . . . , min(k, b) Expected V alue: EX = Variance: Var(X) = MA TLAB: R = hygernd( b + r,b,k) ( )( ) b x r k−x ( ) b+r k kb b + r kbr (b + r)2 b + r − k b + r − 1 X ∼ Poisson(λ) PMF: PX(k) = for k = 0, 1, 2, ⋯ Moment Generating Function (MGF): MX(s) = e λ(es−1) Characteristic Function: ϕX(ω) = e λ(eiω−1) Expected V alue: EX = λ Variance: Var(X) = λ MA TLAB: R = poissrnd( λ) C o n tin uo us D istributio n s e −λλ k k! X ∼ Exponential(λ) PDF: fX(x) = λe −λx, x > 0 CDF: FX(x) = 1 − e −λx, x > 0 Moment Generating Function (MGF): MX(s) = (1 − ) −1 for s < λ Characteristic Function: ϕX(ω) = (1 − ) −1 Expected V alue: EX = Variance: Var(X) = MA TLAB: R = exprnd( μ), where μ = . s λ iω λ 1 λ 1 λ2 1 λ X ∼ Laplace(μ, b) PDF: fX(x) = exp(− ) = ⎧⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪⎩ exp( ) if x < μ exp(− ) if x ≥ μ CDF: FX(x) = ⎧⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪⎩ exp( ) if x < μ 1 − exp(− ) if x ≥ μ Moment Generating Function (MGF): MX(s) = for |s| < Characteristic Function: ϕX(ω) = Expected V alue: EX = μ Variance: Var(X) = 2b2 1 2b |x − μ| b 1 2b x−μ b 1 2b x−μ b 1 2 x−μ b 1 2 x−μ b eμs 1 − b2s2 1 b e μiω 1 + b2ω2 X ∼ N(μ, σ2) (Gaussian Distribution) PDF: fX(x) = e − CDF: FX(x) = Φ ( ) Moment Generating Function (MGF): MX(s) = e μs+ σ2s2 Characteristic Function: ϕX(ω) = e iμω− σ2ω2 Expected V alue: EX = μ Variance: Var(X) = σ2 MA TLAB: Z = randn, R = normrnd( μ,σ) 1 σ√2π (x−μ)2 2σ2 x − μ σ 1 2 1 2 X ∼ Beta(a, b) PDF: fX(x) = x (a−1)(1 − x) (b−1),  for 0 ≤ x ≤ 1 Moment Generating Function (MGF): MX(s) = 1 + ∞ ∑ k=1 ( k−1 ∏ r=0 ) Expected V alue: EX = Variance: Var(X) = MA TLAB: R = betarnd( a,b) Γ(a + b) Γ(a)Γ(b) a + r a + b + r sk k! a a + b ab (a + b)2(a + b + 1) X ∼ χ 2(n) (Chi-squared) Note: χ 2(n) = Gamma ( , ) PDF: fX(x) = x −1e − , for x > 0. Moment Generating Function (MGF): MX(s) = (1 − 2s) − for s < Characteristic Function: ϕX(ω) = (1 − 2iω) − Expected V alue: EX = n Variance: Var(X) = 2n MA TLAB: R = chi2rnd( n) n 2 1 2 1 2 Γ ( ) n 2 n 2 n 2 x 2 n 2 1 2 n 2 X ∼ T(n) (The t-Distribution) PDF: fX(x) = (1 + ) − Moment Generating Function (MGF): undefined Expected V alue: EX = 0 Variance: Var(X) = for n > 2, ∞ for 1 < n ≤ 2, undefined otherwise MA TLAB: R = trnd( n) Γ( ) n+1 2 √nπΓ ( ) n 2 x 2 n n+1 2 n n − 2 X ∼ Gamma(α, λ) PDF: fX(x) = , x > 0 Moment Generating Function (MGF): MX(s) = (1 − ) −α for s < λ Expected V alue: EX = Variance: Var(X) = MA TLAB: R = gamrnd( α,λ) λ αx α−1e −λx Γ(α) s λ α λ α λ2 X ∼ Erlang(k, λ) [= Gamma(k, λ)], k > 0 is an integer PDF: fX(x) = , x > 0 Moment Generating Function (MGF): MX(s) = (1 − ) −k for s < λ Expected V alue: EX = Variance: Var(X) = λ kx k−1e −λx (k − 1)! s λ k λ k λ2 X ∼ Uniform(a, b) PDF: fX(x) = , x ∈ [a, b] CDF: FX(x) = ⎧⎪ ⎪ ⎨ ⎪ ⎪⎩ 0 x < a x ∈ [a, b) 1 for x ≥ b Moment Generating Function (MGF): MX(s) = ⎧ ⎨⎩ s ≠ 0 1 s = 0 Characteristic Function: ϕX(ω) = Expected V alue: EX = (a + b) Variance: Var(X) = (b − a) 2 MA TLAB: U = rand or R = unifrnd( a,b) 1 b − a x−a b−a esb−esa s(b−a) e iωb − e iωa iω(b − a) 1 2 1 12 Bibliography [1] //en.wikipedia.org/wiki/Boy_or_Girl_paradox [2] //en.wikipedia.org/wiki/Law_of_total_expectation#cite_note-1 [3] //en.wikipedia.org/wiki/Law_of_total_variance [4] //en.wikipedia.org/wiki/Y oung's_inequality [5] //en.wikipedia.org/wiki/False_positive_paradox [6] Y . Suhov and M. Kelbert, Probability and Statistics by Example . Cambridge University Press, 2005. [7] L. Mlodinow , The Drunkard's W alk . Pantheon, 2008. [8] //en.wikipedia.org/wiki/Coupon_collector's_problem [9] //en.wikipedia.org/wiki/St._Petersburg_paradox [10] //en.wikipedia.org/wiki/Gamma_function [1 1] P . Erdos and A. Renyi, On the Evolution of Random Graphs . Publications of the Mathematical Institute of the Hungarian Academy of Sciences, 5, 17-61, 1960. [12] //en.wikipedia.org/wiki/Erd%C5%91s%E2%80%93R%C3%A9nyi_model [13] //en.wikipedia.org/wiki/Boole's_inequality [14] //en.wikipedia.org/wiki/Laplace_distribution [15] //en.wikipedia.org/wiki/Pareto_distribution [16] //en.wikipedia.org/wiki/Chebyshev’ s_inequality [17] //en.wikipedia.org/wiki/Minkowski_inequality [18] //en.wikipedia.org/wiki/Sampling_(statistics) [19] N. Etemadi, An Elementary Proof of the Strong Law of large numbers . Z. Wahrsch. V erw. Gebiete, 55(1981):1 19--122, 1981. [20] Sheldon Ross, A First Course in Probability . Printice Hall, Upper Saddle River , New Jersey 07458, Eighth Edition, 2010. [21] //en.wikipedia.org/wiki/Overfitting [22] //en.wikipedia.org/wiki/Heteroscedasticity [23] //en.wikipedia.org/wiki/Multicollinearity [24] //en.wikipedia.org/wiki/Poisson_process [25] Ubbo F Wiersema, Brownian Motion Calculus . John Wiley & Sons, Ltd, 2008.","libVersion":"0.3.2","langs":""}