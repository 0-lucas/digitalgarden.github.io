{"path":"Data Engineering/My Books/Linear Algebra Done Right.pdf","text":"Undergraduate Texts in Mathematics Linear Algebra Done Right Sheldon Axler Fourth Edition Undergraduate Texts in Mathematics Undergraduate Texts in Mathematics Series Editors Pamela Gorkin, Mathematics Department, Bucknell University, Lewisburg, PA, USA Jessica Sidman, Mathematics and Statistics, Amherst College, Amherst, MA, USA Colin Adams, Williams College, Williamstown, MA, USA Jayadev S. Athreya, University of Washington, Seattle, WA, USA Nathan Kaplan, University of California, Irvine, CA, USA Jill Pipher, Brown University, Providence, RI, USA Jeremy Tyson, University of Illinois at Urbana-Champaign, Urbana, IL, USA Undergraduate Texts in Mathematics are generally aimed at third- and fourth- year undergraduate mathematics students at North American universities. These texts strive to provide students and teachers with new perspectives and novel approaches. The books include motivation that guides the reader to an appreciation of interrelations among different aspects of the subject. They feature examples that illustrate key concepts as well as exercises that strengthen understanding. Advisory Board Sheldon Axler Linear Algebra Done Right Fourth Edition Sheldon Axler San Francisco, CA, USA ISSN 0172-6056 ISSN 2197-5604 (electronic) ISBN 978-3-031-41025-3 ISBN 978-3-031-41026-0 (eBook) https://doi.org/10.1007/978-3-031-41026-0 Mathematics Subject Classification (2020): 15-01, 15A03, 15A04, 15A15, 15A18, 15A21 Undergraduate Texts in Mathematics Â© Sheldon Axler 1996, 1997, 2015, 2024. This book is an open access publication. Open Access This book is licensed under the terms of the Creative Commons Attribution-NonCommercial 4.0 International License (http://creativecommons.org/licenses/by-nc/4.0/), which permits any noncom- mercial use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license and indicate if changes were made. The images or other third party material in this book are included in the bookâ€™s Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the bookâ€™s Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. This work is subject to copyright. All commercial rights are reserved by the author(s), whether the whole or part of the material is concerned, specifically the rights of translation, reprinting, reuse of illustrations, recitation, broadcasting, reproduction on microfilms or in any other physical way, and transmission or information storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar methodology now known or hereafter developed. Regarding these commercial rights a non-exclusive license has been granted to the publisher. The use of general descriptive names, registered names, trademarks, service marks, etc. in this publication does not imply, even in the absence of a specific statement, that such names are exempt from the relevant protective laws and regulations and therefore free for general use. The publisher, the authors, and the editors are safe to assume that the advice and information in this book are believed to be true and accurate at the date of publication. Neither the publisher nor the authors or the editors give a warranty, expressed or implied, with respect to the material contained herein or for any errors or omissions that may have been made. The publisher remains neutral with regard to jurisdictional claims in published maps and institutional affiliations. This Springer imprint is published by the registered company Springer Nature Switzerland AG. The registered company address is: Gewerbestrasse 11, 6330 Cham, Switzerland. Paper in this product is recyclable. About the Author Sheldon Axler received his undergraduate degree from Princeton University, followed by a PhD in mathematics from the University of California at Berkeley. As a postdoctoral Moore Instructor at MIT, Axler received a university-wide teaching award. He was then an assistant professor, associate professor, and professor at Michigan State University, where he received the first J. Sutherland Frame Teaching Award and the Distinguished Faculty Award. Axler received the Lester R. Ford Award for expository writing from the Math- ematical Association of America in 1996, for a paper that eventually expanded into this book. In addition to publishing numerous research papers, he is the author of six mathematics textbooks, ranging from freshman to graduate level. Previous editions of this book have been adopted as a textbook at over 375 universities and colleges and have been translated into three languages. Axler has served as Editor-in-Chief of the Mathematical Intelligencer and Associate Editor of the American Mathematical Monthly. He has been a member of the Council of the American Mathematical Society and of the Board of Trustees of the Mathematical Sciences Research Institute. He has also served on the editorial board of Springerâ€™s series Undergraduate Texts in Mathematics, Graduate Texts in Mathematics, Universitext, and Springer Monographs in Mathematics. Axler is a Fellow of the American Mathematical Society and has been a recipient of numerous grants from the National Science Foundation. Axler joined San Francisco State University as chair of the Mathematics Department in 1997. He served as dean of the College of Science & Engineering from 2002 to 2015, when he returned to a regular faculty appointment as a professor in the Mathematics Department.CarrieHeeter,BishnuSarangi The author and his cat Moon. Cover equation: Formula for the ğ‘› th Fibonacci number. Exercise 21 in Section 5D uses linear algebra to derive this formula. v Contents About the Author v Preface for Students xii Preface for Instructors xiii Acknowledgments xvii Chapter 1 Vector Spaces 1 1A ğ‘ğ‘› and ğ‚ ğ‘› 2 Complex Numbers 2 Lists 5 ğ…ğ‘› 6 Digression on Fields 10 Exercises 1A 10 1B Definition of Vector Space 12 Exercises 1B 16 1C Subspaces 18 Sums of Subspaces 19 Direct Sums 21 Exercises 1C 24 Chapter 2 Finite-Dimensional Vector Spaces 27 2A Span and Linear Independence 28 Linear Combinations and Span 28 Linear Independence 31 Exercises 2A 37 vi Contents vii 2B Bases 39 Exercises 2B 42 2C Dimension 44 Exercises 2C 48 Chapter 3 Linear Maps 51 3A Vector Space of Linear Maps 52 Definition and Examples of Linear Maps 52 Algebraic Operations on â„’(ğ‘‰, ğ‘Š) 55 Exercises 3A 57 3B Null Spaces and Ranges 59 Null Space and Injectivity 59 Range and Surjectivity 61 Fundamental Theorem of Linear Maps 62 Exercises 3B 66 3C Matrices 69 Representing a Linear Map by a Matrix 69 Addition and Scalar Multiplication of Matrices 71 Matrix Multiplication 72 Columnâ€“Row Factorization and Rank of a Matrix 77 Exercises 3C 79 3D Invertibility and Isomorphisms 82 Invertible Linear Maps 82 Isomorphic Vector Spaces 86 Linear Maps Thought of as Matrix Multiplication 88 Change of Basis 90 Exercises 3D 93 3E Products and Quotients of Vector Spaces 96 Products of Vector Spaces 96 Quotient Spaces 98 Exercises 3E 103 3F Duality 105 Dual Space and Dual Map 105 Null Space and Range of Dual of Linear Map 109 viii Contents Matrix of Dual of Linear Map 113 Exercises 3F 115 Chapter 4 Polynomials 119 Zeros of Polynomials 122 Division Algorithm for Polynomials 123 Factorization of Polynomials over ğ‚ 124 Factorization of Polynomials over ğ‘ 127 Exercises 4 129 Chapter 5 Eigenvalues and Eigenvectors 132 5A Invariant Subspaces 133 Eigenvalues 133 Polynomials Applied to Operators 137 Exercises 5A 139 5B The Minimal Polynomial 143 Existence of Eigenvalues on Complex Vector Spaces 143 Eigenvalues and the Minimal Polynomial 144 Eigenvalues on Odd-Dimensional Real Vector Spaces 149 Exercises 5B 150 5C Upper-Triangular Matrices 154 Exercises 5C 160 5D Diagonalizable Operators 163 Diagonal Matrices 163 Conditions for Diagonalizability 165 Gershgorin Disk Theorem 170 Exercises 5D 172 5E Commuting Operators 175 Exercises 5E 179 Chapter 6 Inner Product Spaces 181 6A Inner Products and Norms 182 Inner Products 182 Contents ix Norms 186 Exercises 6A 191 6B Orthonormal Bases 197 Orthonormal Lists and the Gramâ€“Schmidt Procedure 197 Linear Functionals on Inner Product Spaces 204 Exercises 6B 207 6C Orthogonal Complements and Minimization Problems 211 Orthogonal Complements 211 Minimization Problems 217 Pseudoinverse 220 Exercises 6C 224 Chapter 7 Operators on Inner Product Spaces 227 7A Self-Adjoint and Normal Operators 228 Adjoints 228 Self-Adjoint Operators 233 Normal Operators 235 Exercises 7A 239 7B Spectral Theorem 243 Real Spectral Theorem 243 Complex Spectral Theorem 246 Exercises 7B 247 7C Positive Operators 251 Exercises 7C 255 7D Isometries, Unitary Operators, and Matrix Factorization 258 Isometries 258 Unitary Operators 260 QR Factorization 263 Cholesky Factorization 266 Exercises 7D 268 7E Singular Value Decomposition 270 Singular Values 270 SVD for Linear Maps and for Matrices 273 Exercises 7E 278 x Contents 7F Consequences of Singular Value Decomposition 280 Norms of Linear Maps 280 Approximation by Linear Maps with Lower-Dimensional Range 283 Polar Decomposition 285 Operators Applied to Ellipsoids and Parallelepipeds 287 Volume via Singular Values 291 Properties of an Operator as Determined by Its Eigenvalues 293 Exercises 7F 294 Chapter 8 Operators on Complex Vector Spaces 297 8A Generalized Eigenvectors and Nilpotent Operators 298 Null Spaces of Powers of an Operator 298 Generalized Eigenvectors 300 Nilpotent Operators 303 Exercises 8A 306 8B Generalized Eigenspace Decomposition 308 Generalized Eigenspaces 308 Multiplicity of an Eigenvalue 310 Block Diagonal Matrices 314 Exercises 8B 316 8C Consequences of Generalized Eigenspace Decomposition 319 Square Roots of Operators 319 Jordan Form 321 Exercises 8C 324 8D Trace: A Connection Between Matrices and Operators 326 Exercises 8D 330 Chapter 9 Multilinear Algebra and Determinants 332 9A Bilinear Forms and Quadratic Forms 333 Bilinear Forms 333 Symmetric Bilinear Forms 337 Quadratic Forms 341 Exercises 9A 344 Contents xi 9B Alternating Multilinear Forms 346 Multilinear Forms 346 Alternating Multilinear Forms and Permutations 348 Exercises 9B 352 9C Determinants 354 Defining the Determinant 354 Properties of Determinants 357 Exercises 9C 367 9D Tensor Products 370 Tensor Product of Two Vector Spaces 370 Tensor Product of Inner Product Spaces 376 Tensor Product of Multiple Vector Spaces 378 Exercises 9D 380 Photo Credits 383 Symbol Index 384 Index 385 Colophon: Notes on Typesetting 390 Preface for Students You are probably about to begin your second exposure to linear algebra. Unlike your first brush with the subject, which probably emphasized Euclidean spaces and matrices, this encounter will focus on abstract vector spaces and linear maps. These terms will be defined later, so donâ€™t worry if you do not know what they mean. This book starts from the beginning of the subject, assuming no knowledge of linear algebra. The key point is that you are about to immerse yourself in serious mathematics, with an emphasis on attaining a deep understanding of the definitions, theorems, and proofs. You cannot read mathematics the way you read a novel. If you zip through a page in less than an hour, you are probably going too fast. When you encounter the phrase â€œas you should verifyâ€, you should indeed do the verification, which will usually require some writing on your part. When steps are left out, you need to supply the missing pieces. You should ponder and internalize each definition. For each theorem, you should seek examples to show why each hypothesis is necessary. Discussions with other students should help. As a visual aid, definitions are in yellow boxes and theorems are in blue boxes (in color versions of the book). Each theorem has an infomal descriptive name. Please check the website below for additional information about the book, including a link to videos that are freely available to accompany the book. Your suggestions, comments, and corrections are most welcome. Best wishes for success and enjoyment in learning linear algebra! Sheldon Axler San Francisco State University website: https://linear.axler.net e-mail: linear@axler.net xii Preface for Instructors You are about to teach a course that will probably give students their second exposure to linear algebra. During their first brush with the subject, your students probably worked with Euclidean spaces and matrices. In contrast, this course will emphasize abstract vector spaces and linear maps. The title of this book deserves an explanation. Most linear algebra textbooks use determinants to prove that every linear operator on a finite-dimensional com- plex vector space has an eigenvalue. Determinants are difficult, nonintuitive, and often defined without motivation. To prove the theorem about existence of eigenvalues on complex vector spaces, most books must define determinants, prove that a linear operator is not invertible if and only if its determinant equals 0, and then define the characteristic polynomial. This tortuous (torturous?) path gives students little feeling for why eigenvalues exist. In contrast, the simple determinant-free proofs presented here (for example, see 5.19) offer more insight. Once determinants have been moved to the end of the book, a new route opens to the main goal of linear algebraâ€”understanding the structure of linear operators. This book starts at the beginning of the subject, with no prerequisites other than the usual demand for suitable mathematical maturity. A few examples and exercises involve calculus concepts such as continuity, differentiation, and integration. You can easily skip those examples and exercises if your students have not had calculus. If your students have had calculus, then those examples and exercises can enrich their experience by showing connections between different parts of mathematics. Even if your students have already seen some of the material in the first few chapters, they may be unaccustomed to working exercises of the type presented here, most of which require an understanding of proofs. Here is a chapter-by-chapter summary of the highlights of the book: â€¢ Chapter 1: Vector spaces are defined in this chapter, and their basic properties are developed. â€¢ Chapter 2: Linear independence, span, basis, and dimension are defined in this chapter, which presents the basic theory of finite-dimensional vector spaces. â€¢ Chapter 3: This chapter introduces linear maps. The key result here is the fundamental theorem of linear maps: if ğ‘‡ is a linear map on ğ‘‰, then dim ğ‘‰ = dim null ğ‘‡ + dim range ğ‘‡. Quotient spaces and duality are topics in this chapter at a higher level of abstraction than most of the book; these topics can be skipped (except that duality is needed for tensor products in Section 9D). xiii xiv Preface for Instructors â€¢ Chapter 4: The part of the theory of polynomials that will be needed to un- derstand linear operators is presented in this chapter. This chapter contains no linear algebra. It can be covered quickly, especially if your students are already familiar with these results. â€¢ Chapter 5: The idea of studying a linear operator by restricting it to small sub- spaces leads to eigenvectors in the early part of this chapter. The highlight of this chapter is a simple proof that on complex vector spaces, eigenvalues always ex- ist. This result is then used to show that each linear operator on a complex vector space has an upper-triangular matrix with respect to some basis. The minimal polynomial plays an important role here and later in the book. For example, this chapter gives a characterization of the diagonalizable operators in terms of the minimal polynomial. Section 5E can be skipped if you want to save some time. â€¢ Chapter 6: Inner product spaces are defined in this chapter, and their basic properties are developed along with tools such as orthonormal bases and the Gramâ€“Schmidt procedure. This chapter also shows how orthogonal projections can be used to solve certain minimization problems. The pseudoinverse is then introduced as a useful tool when the inverse does not exist. The material on the pseudoinverse can be skipped if you want to save some time. â€¢ Chapter 7: The spectral theorem, which characterizes the linear operators for which there exists an orthonormal basis consisting of eigenvectors, is one of the highlights of this book. The work in earlier chapters pays off here with espe- cially simple proofs. This chapter also deals with positive operators, isometries, unitary operators, matrix factorizations, and especially the singular value de- composition, which leads to the polar decomposition and norms of linear maps. â€¢ Chapter 8: This chapter shows that for each operator on a complex vector space, there is a basis of the vector space consisting of generalized eigenvectors of the operator. Then the generalized eigenspace decomposition describes a linear operator on a complex vector space. The multiplicity of an eigenvalue is defined as the dimension of the corresponding generalized eigenspace. These tools are used to prove that every invertible linear operator on a complex vector space has a square root. Then the chapter gives a proof that every linear operator on a complex vector space can be put into Jordan form. The chapter concludes with an investigation of the trace of operators. â€¢ Chapter 9: This chapter begins by looking at bilinear forms and showing that the vector space of bilinear forms is the direct sum of the subspaces of symmetric bilinear forms and alternating bilinear forms. Then quadratic forms are diag- onalized. Moving to multilinear forms, the chapter shows that the subspace of alternating ğ‘›-linear forms on an ğ‘›-dimensional vector space has dimension one. This result leads to a clean basis-free definition of the determinant of an opera- tor. For complex vector spaces, the determinant turns out to equal the product of the eigenvalues, with each eigenvalue included in the product as many times as its multiplicity. The chapter concludes with an introduction to tensor products. Preface for Instructors xv This book usually develops linear algebra simultaneously for real and complex vector spaces by letting ğ… denote either the real or the complex numbers. If you and your students prefer to think of ğ… as an arbitrary field, then see the comments at the end of Section 1A. I prefer avoiding arbitrary fields at this level because they intro- duce extra abstraction without leading to any new linear algebra. Also, students are more comfortable thinking of polynomials as functions instead of the more formal objects needed for polynomials with coefficients in finite fields. Finally, even if the beginning part of the theory were developed with arbitrary fields, inner product spaces would push consideration back to just real and complex vector spaces. You probably cannot cover everything in this book in one semester. Going through all the material in the first seven or eight chapters during a one-semester course may require a rapid pace. If you must reach Chapter 9, then consider skipping the material on quotient spaces in Section 3E, skipping Section 3F on duality (unless you intend to cover tensor products in Section 9D), covering Chapter 4 on polynomials in a half hour, skipping Section 5E on commuting operators, and skipping the subsection in Section 6C on the pseudoinverse. A goal more important than teaching any particular theorem is to develop in students the ability to understand and manipulate the objects of linear algebra. Mathematics can be learned only by doing. Fortunately, linear algebra has many good homework exercises. When teaching this course, during each class I usually assign as homework several of the exercises, due the next class. Going over the homework might take up significant time in a typical class. Some of the exercises are intended to lead curious students into important topics beyond what might usually be included in a basic second course in linear algebra. The authorâ€™s top ten Listed below are the authorâ€™s ten favorite results in the book, in order of their appearance in the book. Students who leave your course with a good understanding of these crucial results will have an excellent foundation in linear algebra. â€¢ any two bases of a vector space have the same length (2.34) â€¢ fundamental theorem of linear maps (3.21) â€¢ existence of eigenvalues if ğ… = ğ‚ (5.19) â€¢ upper-triangular form always exists if ğ… = ğ‚ (5.47) â€¢ Cauchyâ€“Schwarz inequality (6.14) â€¢ Gramâ€“Schmidt procedure (6.32) â€¢ spectral theorem (7.29 and 7.31) â€¢ singular value decomposition (7.70) â€¢ generalized eigenspace decomposition theorem when ğ… = ğ‚ (8.22) â€¢ dimension of alternating ğ‘›-linear forms on ğ‘‰ is 1if dim ğ‘‰ = ğ‘› (9.37) xvi Preface for Instructors Major improvements and additions for the fourth edition â€¢ Over 250 new exercises and over 70 new examples. â€¢ Increasing use of the minimal polynomial to provide cleaner proofs of multiple results, including necessary and sufficient conditions for an operator to have an upper-triangular matrix with respect to some basis (see Section 5C), necessary and sufficient conditions for diagonalizability (see Section 5D), and the real spectral theorem (see Section 7B). â€¢ New section on commuting operators (see Section 5E). â€¢ New subsection on pseudoinverse (see Section 6C). â€¢ New subsections on QR factorization/Cholesky factorization (see Section 7D). â€¢ Singular value decomposition now done for linear maps from an inner product space to another (possibly different) inner product space, rather than only deal- ing with linear operators from an inner product space to itself (see Section 7E). â€¢ Polar decomposition now proved from singular value decomposition, rather than in the opposite order; this has led to cleaner proofs of both the singular value decomposition (see Section 7E) and the polar decomposition (see Section 7F). â€¢ New subsection on norms of linear maps on finite-dimensional inner prod- uct spaces, using the singular value decomposition to avoid even mentioning supremum in the definition of the norm of a linear map (see Section7F). â€¢ New subsection on approximation by linear maps with lower-dimensional range (see Section 7F). â€¢ New elementary proof of the important result that if ğ‘‡ is an operator on a finite- dimensional complex vector space ğ‘‰, then there exists a basis of ğ‘‰ consisting of generalized eigenvectors of ğ‘‡ (see 8.9). â€¢ New Chapter 9 on multilinear algebra, including bilinear forms, quadratic forms, multilinear forms, and tensor products. Determinants now are defined using a basis-free approach via alternating multilinear forms. â€¢ New formatting to improve the student-friendly appearance of the book. For example, the definition and result boxes now have rounded corners instead of right-angle corners, for a gentler look. The main font size has been reduced from 11 point to 10.5 point. Please check the website below for additional links and information about the book. Your suggestions, comments, and corrections are most welcome. Best wishes for teaching a successful linear algebra class! Contact the author, or Springer if the author is not available, for permission for translations or other commercial reuse of the contents of this book. Sheldon Axler San Francisco State University website: https://linear.axler.net e-mail: linear@axler.net Acknowledgments I owe a huge intellectual debt to all the mathematicians who created linear algebra over the past two centuries. The results in this book belong to the common heritage of mathematics. A special case of a theorem may first have been proved long ago, then sharpened and improved by many mathematicians in different time periods. Bestowing proper credit on all contributors would be a difficult task that I have not undertaken. In no case should the reader assume that any result presented here represents my original contribution. Many people helped make this a better book. The three previous editions of this book were used as a textbook at over 375 universities and colleges around the world. I received thousands of suggestions and comments from faculty and students who used the book. Many of those suggestions led to improvements in this edition. The manuscript for this fourth edition was class tested at 30 universities. I am extremely grateful for the useful feedback that I received from faculty and students during this class testing. The long list of people who should be thanked for their suggestions would fill up many pages. Lists are boring to read. Thus to represent all contributors to this edition, I will mention only Noel Hinton, a graduate student at Australian National University, who sent me more suggestions and corrections for this fourth edition than anyone else. To everyone who contributed suggestions, let me say how truly grateful I am to all of you. Many many thanks! I thank Springer for providing me with help when I needed it and for allowing me the freedom to make the final decisions about the content and appearance of this book. Special thanks to the two terrific mathematics editors at Springer who worked with me on this projectâ€”Loretta Bartolini during the first half of my work on the fourth edition, and Elizabeth Loew during the second half of my work on the fourth edition. I am deeply indebted to David Kramer, who did a magnificent job of copyediting and prevented me from making many mistakes. Extra special thanks to my fantastic partner Carrie Heeter. Her understanding and encouragement enabled me to work intensely on this new edition. Our won- derful cat Moon, whose picture appears on the About the Author page, provided sweet breaks throughout the writing process. Moon died suddenly due to a blood clot as this book was being finished. We are grateful for five precious years with him. Sheldon Axler xvii Chapter 1 Vector Spaces Linear algebra is the study of linear maps on finite-dimensional vector spaces. Eventually we will learn what all these terms mean. In this chapter we will define vector spaces and discuss their elementary properties. In linear algebra, better theorems and more insight emerge if complex numbers are investigated along with real numbers. Thus we will begin by introducing the complex numbers and their basic properties. We will generalize the examples of a plane and of ordinary space to ğ‘ğ‘› and ğ‚ ğ‘›, which we then will generalize to the notion of a vector space. As we will see, a vector space is a set with operations of addition and scalar multiplication that satisfy natural algebraic properties. Then our next topic will be subspaces, which play a role for vector spaces analogous to the role played by subsets for sets. Finally, we will look at sums of subspaces (analogous to unions of subsets) and direct sums of subspaces (analogous to unions of disjoint sets).PierreLouisDumesnil,NilsForsberg RenÃ© Descartes explaining his work to Queen Christina of Sweden. Vector spaces are a generalization of the description of a plane using two coordinates, as published by Descartes in 1637. 1 Â© Sheldon Axler 2024 S. Axler, Linear Algebra Done Right, Undergraduate Texts in Mathematics, https://doi.org/10.1007/978-3-031-41026-0_1 2 Chapter 1 Vector Spaces 1A ğ‘ğ‘› and ğ‚ ğ‘› Complex Numbers You should already be familiar with basic properties of the set ğ‘ of real numbers. Complex numbers were invented so that we can take square roots of negative numbers. The idea is to assume we have a square root of âˆ’1, denoted by ğ‘–, that obeys the usual rules of arithmetic. Here are the formal definitions. 1.1 definition: complex numbers, ğ‚ â€¢ A complex number is an ordered pair (ğ‘, ğ‘), where ğ‘, ğ‘ âˆˆ ğ‘, but we will write this as ğ‘ + ğ‘ğ‘–. â€¢ The set of all complex numbers is denoted by ğ‚: ğ‚ = {ğ‘ + ğ‘ğ‘– âˆ¶ ğ‘, ğ‘ âˆˆ ğ‘}. â€¢ Addition and multiplication on ğ‚ are defined by (ğ‘ + ğ‘ğ‘–) + (ğ‘ + ğ‘‘ğ‘–) = (ğ‘ + ğ‘) + (ğ‘ + ğ‘‘)ğ‘–, (ğ‘ + ğ‘ğ‘–)(ğ‘ + ğ‘‘ğ‘–) = (ğ‘ğ‘ âˆ’ ğ‘ğ‘‘) + (ğ‘ğ‘‘ + ğ‘ğ‘)ğ‘–; here ğ‘, ğ‘, ğ‘, ğ‘‘ âˆˆ ğ‘. If ğ‘ âˆˆ ğ‘, we identify ğ‘ + 0ğ‘–with the real number ğ‘. Thus we think of ğ‘ as a subset of ğ‚. We usually write 0+ ğ‘ğ‘– as just ğ‘ğ‘–, and we usually write 0+ 1ğ‘–as just ğ‘–. The symbol ğ‘– was first used to denote âˆšâˆ’1by Leonhard Euler in 1777. To motivate the definition of complex multiplication given above, pretend that we knew that ğ‘–2 = âˆ’1and then use the usual rules of arithmetic to derive the formula above for the product of two complex numbers. Then use that formula to verify that we indeed have ğ‘–2 = âˆ’1. Do not memorize the formula for the product of two complex numbersâ€”you can always rederive it by recalling that ğ‘–2 = âˆ’1and then using the usual rules of arithmetic (as given by 1.3). The next example illustrates this procedure. 1.2 example: complex arithmetic The product (2+ 3ğ‘–)(4+ 5ğ‘–)can be evaluated by applying the distributive and commutative properties from 1.3: (2+ 3ğ‘–)(4+ 5ğ‘–) = 2 â‹… (4+ 5ğ‘–)+ (3ğ‘–)(4+ 5ğ‘–) = 2 â‹… 4+ 2 â‹… 5ğ‘–+ 3ğ‘– â‹… 4+ (3ğ‘–)(5ğ‘–) = 8+ 10ğ‘–+ 12ğ‘– âˆ’ 15 = âˆ’7+ 22ğ‘–. Section 1A ğ‘ğ‘› and ğ‚ ğ‘› 3 Our first result states that complex addition and complex multiplication have the familiar properties that we expect. 1.3 properties of complex arithmetic commutativity ğ›¼ + ğ›½ = ğ›½ + ğ›¼ and ğ›¼ğ›½ = ğ›½ğ›¼ for all ğ›¼, ğ›½ âˆˆ ğ‚. associativity (ğ›¼ + ğ›½) + ğœ† = ğ›¼ + (ğ›½ + ğœ†) and (ğ›¼ğ›½)ğœ† = ğ›¼(ğ›½ğœ†) for all ğ›¼, ğ›½, ğœ† âˆˆ ğ‚. identities ğœ† + 0 = ğœ†and ğœ†1 = ğœ†for all ğœ† âˆˆ ğ‚. additive inverse For every ğ›¼ âˆˆ ğ‚, there exists a unique ğ›½ âˆˆ ğ‚ such that ğ›¼ + ğ›½ = 0. multiplicative inverse For every ğ›¼ âˆˆ ğ‚ with ğ›¼ â‰  0, there exists a unique ğ›½ âˆˆ ğ‚ such that ğ›¼ğ›½ = 1. distributive property ğœ†(ğ›¼ + ğ›½) = ğœ†ğ›¼ + ğœ†ğ›½ for all ğœ†, ğ›¼, ğ›½ âˆˆ ğ‚. The properties above are proved using the familiar properties of real numbers and the definitions of complex addition and multiplication. The next example shows how commutativity of complex multiplication is proved. Proofs of the other properties above are left as exercises. 1.4 example: commutativity of complex multiplication To show that ğ›¼ğ›½ = ğ›½ğ›¼ for all ğ›¼, ğ›½ âˆˆ ğ‚, suppose ğ›¼ = ğ‘ + ğ‘ğ‘– and ğ›½ = ğ‘ + ğ‘‘ğ‘–, where ğ‘, ğ‘, ğ‘, ğ‘‘ âˆˆ ğ‘. Then the definition of multiplication of complex numbers shows that ğ›¼ğ›½ = (ğ‘ + ğ‘ğ‘–)(ğ‘ + ğ‘‘ğ‘–) = (ğ‘ğ‘ âˆ’ ğ‘ğ‘‘) + (ğ‘ğ‘‘ + ğ‘ğ‘)ğ‘– and ğ›½ğ›¼ = (ğ‘ + ğ‘‘ğ‘–)(ğ‘ + ğ‘ğ‘–) = (ğ‘ğ‘ âˆ’ ğ‘‘ğ‘) + (ğ‘ğ‘ + ğ‘‘ğ‘)ğ‘–. The equations above and the commutativity of multiplication and addition of real numbers show that ğ›¼ğ›½ = ğ›½ğ›¼. 4 Chapter 1 Vector Spaces Next, we define the additive and multiplicative inverses of complex numbers, and then use those inverses to define subtraction and division operations with complex numbers. 1.5 definition: âˆ’ğ›¼, subtraction, 1/ğ›¼, division Suppose ğ›¼, ğ›½ âˆˆ ğ‚. â€¢ Let âˆ’ğ›¼ denote the additive inverse of ğ›¼. Thus âˆ’ğ›¼ is the unique complex number such that ğ›¼ + (âˆ’ğ›¼) = 0. â€¢ Subtraction on ğ‚ is defined by ğ›½ âˆ’ ğ›¼ = ğ›½ + (âˆ’ğ›¼). â€¢ For ğ›¼ â‰  0, let 1/ğ›¼and 1 ğ›¼ denote the multiplicative inverse of ğ›¼. Thus 1/ğ›¼is the unique complex number such that ğ›¼(1/ğ›¼) = 1. â€¢ For ğ›¼ â‰  0, division by ğ›¼ is defined by ğ›½/ğ›¼ = ğ›½(1/ğ›¼). So that we can conveniently make definitions and prove theorems that apply to both real and complex numbers, we adopt the following notation. 1.6 notation:ğ… Throughout this book, ğ… stands for either ğ‘ or ğ‚. The letter ğ… is used because ğ‘ and ğ‚ are examples of what are called fields. Thus if we prove a theorem involving ğ…, we will know that it holds when ğ… is replaced with ğ‘ and when ğ… is replaced with ğ‚. Elements of ğ… are called scalars. The word â€œscalarâ€ (which is just a fancy word for â€œnumberâ€) is often used when we want to emphasize that an object is a number, as opposed to a vector (vectors will be defined soon). For ğ›¼ âˆˆ ğ… and ğ‘š a positive integer, we defineğ›¼ğ‘š to denote the product of ğ›¼ with itself ğ‘š times: ğ›¼ ğ‘š = ğ›¼â‹¯ğ›¼âŸ ğ‘š times . This definition implies that (ğ›¼ ğ‘š) ğ‘› = ğ›¼ğ‘šğ‘› and (ğ›¼ğ›½) ğ‘š = ğ›¼ğ‘šğ›½ ğ‘š for all ğ›¼, ğ›½ âˆˆ ğ… and all positive integers ğ‘š, ğ‘›. Section 1A ğ‘ğ‘› and ğ‚ ğ‘› 5 Lists Before definingğ‘ğ‘› and ğ‚ ğ‘›, we look at two important examples. 1.7 example:ğ‘2 and ğ‘3 â€¢ The set ğ‘2, which you can think of as a plane, is the set of all ordered pairs of real numbers: ğ‘2 = {(ğ‘¥, ğ‘¦) âˆ¶ ğ‘¥, ğ‘¦ âˆˆ ğ‘}. â€¢ The set ğ‘3, which you can think of as ordinary space, is the set of all ordered triples of real numbers: ğ‘3 = {(ğ‘¥, ğ‘¦, ğ‘§) âˆ¶ ğ‘¥, ğ‘¦, ğ‘§ âˆˆ ğ‘}. To generalize ğ‘2 and ğ‘3 to higher dimensions, we first need to discuss the concept of lists. 1.8 definition: list, length â€¢ Suppose ğ‘› is a nonnegative integer. A list of length ğ‘› is an ordered collec- tion of ğ‘› elements (which might be numbers, other lists, or more abstract objects). â€¢ Two lists are equal if and only if they have the same length and the same elements in the same order. Many mathematicians call a list of length ğ‘› an ğ‘›-tuple. Lists are often written as elements separated by commas and surrounded by parentheses. Thus a list of length two is an ordered pair that might be written as (ğ‘, ğ‘). A list of length three is an ordered triple that might be written as (ğ‘¥, ğ‘¦, ğ‘§). A list of length ğ‘› might look like this: (ğ‘§1, â€¦, ğ‘§ğ‘›). Sometimes we will use the word list without specifying its length. Remember, however, that by definition each list has a finite length that is a nonnegative integer. Thus an object that looks like (ğ‘¥1, ğ‘¥2, â€¦ ), which might be said to have infinite length, is not a list. A list of length 0looks like this: ( ). We consider such an object to be a list so that some of our theorems will not have trivial exceptions. Lists differ from sets in two ways: in lists, order matters and repetitions have meaning; in sets, order and repetitions are irrelevant. 1.9 example: lists versus sets â€¢ The lists (3, 5)and (5, 3)are not equal, but the sets {3, 5}and {5, 3}are equal. â€¢ The lists (4, 4)and (4, 4, 4)are not equal (they do not have the same length), although the sets {4, 4}and {4, 4, 4}both equal the set {4}. 6 Chapter 1 Vector Spaces ğ…ğ‘› To define the higher-dimensional analogues ofğ‘2 and ğ‘3, we will simply replace ğ‘ with ğ… (which equals ğ‘ or ğ‚) and replace the 2or 3with an arbitrary positive integer. 1.10 notation: ğ‘› Fix a positive integer ğ‘› for the rest of this chapter. 1.11 definition: ğ…ğ‘›, coordinate ğ…ğ‘› is the set of all lists of length ğ‘› of elements of ğ…: ğ…ğ‘› = {(ğ‘¥1, â€¦, ğ‘¥ğ‘›) âˆ¶ ğ‘¥ğ‘˜ âˆˆ ğ… for ğ‘˜ = 1, â€¦, ğ‘›}. For (ğ‘¥1, â€¦, ğ‘¥ğ‘›) âˆˆ ğ…ğ‘› and ğ‘˜ âˆˆ {1, â€¦, ğ‘›}, we say that ğ‘¥ğ‘˜ is the ğ‘˜th coordinate of (ğ‘¥1, â€¦, ğ‘¥ğ‘›). If ğ… = ğ‘ and ğ‘› equals 2or 3, then the definition above ofğ…ğ‘› agrees with our previous notions of ğ‘2 and ğ‘3. 1.12 example: ğ‚ 4 ğ‚ 4 is the set of all lists of four complex numbers: ğ‚ 4 = {(ğ‘§1, ğ‘§2, ğ‘§3, ğ‘§4) âˆ¶ ğ‘§1, ğ‘§2, ğ‘§3, ğ‘§4 âˆˆ ğ‚}. Read Flatland: A Romance of Many Dimensions, by Edwin A. Abbott, for an amusing account of how ğ‘3 would be perceived by creatures living in ğ‘2. This novel, published in 1884, may help you imagine a physical space of four or more dimensions. If ğ‘› â‰¥ 4, we cannot visualize ğ‘ğ‘› as a physical object. Similarly, ğ‚ 1 can be thought of as a plane, but for ğ‘› â‰¥ 2, the human brain cannot provide a full image of ğ‚ ğ‘›. However, even if ğ‘› is large, we can perform algebraic manipulations in ğ…ğ‘› as easily as in ğ‘2 or ğ‘3. For example, addition in ğ…ğ‘› is defined as follows. 1.13 definition: addition in ğ…ğ‘› Addition in ğ…ğ‘› is defined by adding corresponding coordinates: (ğ‘¥1, â€¦, ğ‘¥ğ‘›) + (ğ‘¦1, â€¦, ğ‘¦ğ‘›) = (ğ‘¥1 + ğ‘¦1, â€¦, ğ‘¥ğ‘› + ğ‘¦ğ‘›). Often the mathematics of ğ…ğ‘› becomes cleaner if we use a single letter to denote a list of ğ‘› numbers, without explicitly writing the coordinates. For example, the next result is stated with ğ‘¥ and ğ‘¦ in ğ…ğ‘› even though the proof requires the more cumbersome notation of (ğ‘¥1, â€¦, ğ‘¥ğ‘›) and (ğ‘¦1, â€¦, ğ‘¦ğ‘›). Section 1A ğ‘ğ‘› and ğ‚ ğ‘› 7 1.14 commutativity of addition in ğ…ğ‘› If ğ‘¥, ğ‘¦ âˆˆ ğ…ğ‘›, then ğ‘¥ + ğ‘¦ = ğ‘¦ + ğ‘¥. Proof Suppose ğ‘¥ = (ğ‘¥1, â€¦, ğ‘¥ğ‘›) âˆˆ ğ…ğ‘› and ğ‘¦ = (ğ‘¦1, â€¦, ğ‘¦ğ‘›) âˆˆ ğ…ğ‘›. Then ğ‘¥ + ğ‘¦ = (ğ‘¥1, â€¦, ğ‘¥ğ‘›) + (ğ‘¦1, â€¦, ğ‘¦ğ‘›) = (ğ‘¥1 + ğ‘¦1, â€¦, ğ‘¥ğ‘› + ğ‘¦ğ‘›) = (ğ‘¦1 + ğ‘¥1, â€¦, ğ‘¦ğ‘› + ğ‘¥ğ‘›) = (ğ‘¦1, â€¦, ğ‘¦ğ‘›) + (ğ‘¥1, â€¦, ğ‘¥ğ‘›) = ğ‘¦ + ğ‘¥, where the second and fourth equalities above hold because of the definition of addition in ğ…ğ‘› and the third equality holds because of the usual commutativity of addition in ğ…. The symbol means â€œend of proof â€.If a single letter is used to denote an element of ğ…ğ‘›, then the same letter with appropriate subscripts is often used when coordinates must be displayed. For example, if ğ‘¥ âˆˆ ğ…ğ‘›, then letting ğ‘¥ equal (ğ‘¥1, â€¦, ğ‘¥ğ‘›) is good notation, as shown in the proof above. Even better, work with just ğ‘¥ and avoid explicit coordinates when possible. 1.15 notation: 0 Let 0denote the list of length ğ‘› whose coordinates are all 0: 0 = (0, â€¦, 0). Here we are using the symbol 0in two different waysâ€”on the left side of the equation above, the symbol 0denotes a list of length ğ‘›, which is an element of ğ…ğ‘›, whereas on the right side, each 0denotes a number. This potentially confusing practice actually causes no problems because the context should always make clear which 0is intended. 1.16 example:context determines which 0is intended Consider the statement that 0is an additive identity for ğ…ğ‘›: ğ‘¥ + 0 = ğ‘¥ for all ğ‘¥ âˆˆ ğ…ğ‘›. Here the 0above is the list defined in1.15, not the number 0, because we have not defined the sum of an element ofğ…ğ‘› (namely, ğ‘¥) and the number 0. 8 Chapter 1 Vector Spaces Elements of ğ‘2 can be thought of as points or as vectors. A picture can aid our intuition. We will draw pictures in ğ‘2 because we can sketch this space on two-dimensional surfaces such as paper and computer screens. A typical element of ğ‘2 is a point ğ‘£ = (ğ‘, ğ‘). Sometimes we think of ğ‘£ not as a point but as an arrow starting at the origin and ending at (ğ‘, ğ‘), as shown here. When we think of an element of ğ‘2 as an arrow, we refer to it as a vector. A vector. When we think of vectors in ğ‘2 as arrows, we can move an arrow parallel to itself (not changing its length or direction) and still think of it as the same vector. With that viewpoint, you will often gain better understanding by dispensing with the coordinate axes and the explicit coordinates and just thinking of the vector, as shown in the figure here. The two arrows shown here have the same length and same direction, so we think of them as the same vector. Mathematical models of the economy can have thousands of variables, say ğ‘¥1, â€¦, ğ‘¥5000, which means that we must work in ğ‘5000. Such a space cannot be dealt with geometrically. However, the algebraic approach works well. Thus our subject is called linear algebra. Whenever we use pictures in ğ‘2 or use the somewhat vague language of points and vectors, remember that these are just aids to our understanding, not sub- stitutes for the actual mathematics that we will develop. Although we cannot draw good pictures in high-dimensional spaces, the elements of these spaces are as rigorously defined as elements ofğ‘2. For example, (2, âˆ’3, 17, ğœ‹, âˆš2)is an element of ğ‘5, and we may casually refer to it as a point in ğ‘5 or a vector in ğ‘5 without worrying about whether the geometry of ğ‘5 has any physical meaning. Recall that we defined the sum of two elements ofğ…ğ‘› to be the element of ğ…ğ‘› obtained by adding corresponding coordinates; see 1.13. As we will now see, addition has a simple geometric interpretation in the special case of ğ‘2. The sum of two vectors. Suppose we have two vectors ğ‘¢ and ğ‘£ in ğ‘2 that we want to add. Move the vector ğ‘£ parallel to itself so that its initial point coincides with the end point of the vector ğ‘¢, as shown here. The sum ğ‘¢ + ğ‘£ then equals the vector whose initial point equals the initial point of ğ‘¢ and whose end point equals the end point of the vector ğ‘£, as shown here. In the next definition, the0on the right side of the displayed equation is the list 0 âˆˆ ğ… ğ‘›. Section 1A ğ‘ğ‘› and ğ‚ ğ‘› 9 1.17 definition:additive inverse in ğ…ğ‘›, âˆ’ğ‘¥ For ğ‘¥ âˆˆ ğ…ğ‘›, the additive inverse of ğ‘¥, denoted by âˆ’ğ‘¥, is the vector âˆ’ğ‘¥ âˆˆ ğ…ğ‘› such that ğ‘¥ + (âˆ’ğ‘¥) = 0. Thus if ğ‘¥ = (ğ‘¥1, â€¦, ğ‘¥ğ‘›), then âˆ’ğ‘¥ = (âˆ’ğ‘¥1, â€¦, âˆ’ğ‘¥ğ‘›). A vector and its additive inverse. The additive inverse of a vector in ğ‘2 is the vector with the same length but pointing in the opposite direction. The figure here illustrates this way of thinking about the additive inverse in ğ‘2. As you can see, the vector labeled âˆ’ğ‘¥ has the same length as the vector labeled ğ‘¥ but points in the opposite direction. Having dealt with addition in ğ…ğ‘›, we now turn to multiplication. We could definea multiplication in ğ…ğ‘› in a similar fashion, starting with two elements of ğ…ğ‘› and getting another element of ğ…ğ‘› by multiplying corresponding coordinates. Experience shows that this definition is not useful for our purposes. Another type of multiplication, called scalar multiplication, will be central to our subject. Specifically, we need to define what it means to multiply an element ofğ…ğ‘› by an element of ğ…. 1.18 definition: scalar multiplication in ğ…ğ‘› The product of a number ğœ† and a vector in ğ…ğ‘› is computed by multiplying each coordinate of the vector by ğœ†: ğœ†(ğ‘¥1, â€¦, ğ‘¥ğ‘›) = (ğœ†ğ‘¥1, â€¦, ğœ†ğ‘¥ğ‘›); here ğœ† âˆˆ ğ… and (ğ‘¥1, â€¦, ğ‘¥ğ‘›) âˆˆ ğ…ğ‘›. Scalar multiplication in ğ…ğ‘› multiplies together a scalar and a vector, getting a vector. In contrast, the dot product in ğ‘2 or ğ‘3 multiplies together two vec- tors and gets a scalar. Generalizations of the dot product will become impor- tant in Chapter 6. Scalar multiplication has a nice geo- metric interpretation in ğ‘2. If ğœ† > 0and ğ‘¥ âˆˆ ğ‘2, then ğœ†ğ‘¥ is the vector that points in the same direction as ğ‘¥ and whose length is ğœ† times the length of ğ‘¥. In other words, to get ğœ†ğ‘¥, we shrink or stretch ğ‘¥ by a factor of ğœ†, depending on whether ğœ† < 1or ğœ† > 1. Scalar multiplication. If ğœ† < 0and ğ‘¥ âˆˆ ğ‘2, then ğœ†ğ‘¥ is the vector that points in the direction opposite to that of ğ‘¥ and whose length is |ğœ†| times the length of ğ‘¥, as shown here. 10 Chapter 1 Vector Spaces Digression on Fields A field is a set containing at least two distinct elements called 0and 1, along with operations of addition and multiplication satisfying all properties listed in 1.3. Thus ğ‘ and ğ‚ are fields, as is the set of rational numbers along with the usual operations of addition and multiplication. Another example of a field is the set {0, 1}with the usual operations of addition and multiplication except that 1+ 1is defined to equal0. In this book we will not deal with fields other thanğ‘ and ğ‚. However, many of the definitions, theorems, and proofs in linear algebra that work for the fields ğ‘ and ğ‚ also work without change for arbitrary fields. If you prefer to do so, throughout much of this book (except for Chapters 6 and 7, which deal with inner product spaces) you can think of ğ… as denoting an arbitrary field instead ofğ‘ or ğ‚. For results (except in the inner product chapters) that have as a hypothesis that ğ… is ğ‚, you can probably replace that hypothesis with the hypothesis that ğ… is an algebraically closed field, which means that every nonconstant polynomial with coefficients in ğ… has a zero. A few results, such as Exercise 13 in Section 1C, require the hypothesis on ğ… that 1+ 1 â‰  0. Exercises 1A 1 Show that ğ›¼ + ğ›½ = ğ›½ + ğ›¼ for all ğ›¼, ğ›½ âˆˆ ğ‚. 2 Show that (ğ›¼ + ğ›½) + ğœ† = ğ›¼ + (ğ›½ + ğœ†) for all ğ›¼, ğ›½, ğœ† âˆˆ ğ‚. 3 Show that (ğ›¼ğ›½)ğœ† = ğ›¼(ğ›½ğœ†) for all ğ›¼, ğ›½, ğœ† âˆˆ ğ‚. 4 Show that ğœ†(ğ›¼ + ğ›½) = ğœ†ğ›¼ + ğœ†ğ›½ for all ğœ†, ğ›¼, ğ›½ âˆˆ ğ‚. 5 Show that for every ğ›¼ âˆˆ ğ‚, there exists a unique ğ›½ âˆˆ ğ‚ such that ğ›¼ + ğ›½ = 0. 6 Show that for every ğ›¼ âˆˆ ğ‚ with ğ›¼ â‰  0, there exists a unique ğ›½ âˆˆ ğ‚ such that ğ›¼ğ›½ = 1. 7 Show that âˆ’1+ âˆš3ğ‘– 2 is a cube root of 1(meaning that its cube equals 1). 8 Find two distinct square roots of ğ‘–. 9 Find ğ‘¥ âˆˆ ğ‘4 such that (4, âˆ’3, 1, 7)+ 2ğ‘¥ = (5, 9, âˆ’6, 8). 10 Explain why there does not exist ğœ† âˆˆ ğ‚ such that ğœ†(2 âˆ’ 3ğ‘–, 5+ 4ğ‘–, âˆ’6+ 7ğ‘–) = (12 âˆ’ 5ğ‘–, 7+ 22ğ‘–, âˆ’32 âˆ’ 9ğ‘–). Section 1A ğ‘ğ‘› and ğ‚ ğ‘› 11 11 Show that (ğ‘¥ + ğ‘¦) + ğ‘§ = ğ‘¥ + (ğ‘¦ + ğ‘§) for all ğ‘¥, ğ‘¦, ğ‘§ âˆˆ ğ…ğ‘›. 12 Show that (ğ‘ğ‘)ğ‘¥ = ğ‘(ğ‘ğ‘¥) for all ğ‘¥ âˆˆ ğ…ğ‘› and all ğ‘, ğ‘ âˆˆ ğ…. 13 Show that 1ğ‘¥ = ğ‘¥for all ğ‘¥ âˆˆ ğ…ğ‘›. 14 Show that ğœ†(ğ‘¥ + ğ‘¦) = ğœ†ğ‘¥ + ğœ†ğ‘¦ for all ğœ† âˆˆ ğ… and all ğ‘¥, ğ‘¦ âˆˆ ğ…ğ‘›. 15 Show that (ğ‘ + ğ‘)ğ‘¥ = ğ‘ğ‘¥ + ğ‘ğ‘¥ for all ğ‘, ğ‘ âˆˆ ğ… and all ğ‘¥ âˆˆ ğ…ğ‘›. â€œCan you do addition?â€ the White Queen asked. â€œWhatâ€™s one and one and one and one and one and one and one and one and one and one?â€ â€œI donâ€™t know,â€ said Alice. â€œI lost count.â€ â€”Through the Looking Glass, Lewis Carroll 12 Chapter 1 Vector Spaces 1B Definition of Vector Space The motivation for the definition of a vector space comes from properties of addition and scalar multiplication in ğ…ğ‘›: Addition is commutative, associative, and has an identity. Every element has an additive inverse. Scalar multiplication is associative. Scalar multiplication by 1acts as expected. Addition and scalar multiplication are connected by distributive properties. We will define a vector space to be a setğ‘‰ with an addition and a scalar multiplication on ğ‘‰ that satisfy the properties in the paragraph above. 1.19 definition: addition, scalar multiplication â€¢ An addition on a set ğ‘‰ is a function that assigns an element ğ‘¢ + ğ‘£ âˆˆ ğ‘‰ to each pair of elements ğ‘¢, ğ‘£ âˆˆ ğ‘‰. â€¢ A scalar multiplication on a set ğ‘‰ is a function that assigns an element ğœ†ğ‘£ âˆˆ ğ‘‰ to each ğœ† âˆˆ ğ… and each ğ‘£ âˆˆ ğ‘‰. Now we are ready to give the formal definition of a vector space. 1.20 definition: vector space A vector space is a set ğ‘‰ along with an addition on ğ‘‰ and a scalar multiplication on ğ‘‰ such that the following properties hold. commutativity ğ‘¢ + ğ‘£ = ğ‘£ + ğ‘¢ for all ğ‘¢, ğ‘£ âˆˆ ğ‘‰. associativity (ğ‘¢ + ğ‘£) + ğ‘¤ = ğ‘¢ + (ğ‘£ + ğ‘¤) and (ğ‘ğ‘)ğ‘£ = ğ‘(ğ‘ğ‘£) for all ğ‘¢, ğ‘£, ğ‘¤ âˆˆ ğ‘‰ and for all ğ‘, ğ‘ âˆˆ ğ…. additive identity There exists an element 0 âˆˆ ğ‘‰such that ğ‘£ + 0 = ğ‘£for all ğ‘£ âˆˆ ğ‘‰. additive inverse For every ğ‘£ âˆˆ ğ‘‰, there exists ğ‘¤ âˆˆ ğ‘‰ such that ğ‘£ + ğ‘¤ = 0. multiplicative identity 1ğ‘£ = ğ‘£for all ğ‘£ âˆˆ ğ‘‰. distributive properties ğ‘(ğ‘¢ + ğ‘£) = ğ‘ğ‘¢ + ğ‘ğ‘£ and (ğ‘ + ğ‘)ğ‘£ = ğ‘ğ‘£ + ğ‘ğ‘£ for all ğ‘, ğ‘ âˆˆ ğ… and all ğ‘¢, ğ‘£ âˆˆ ğ‘‰. The following geometric language sometimes aids our intuition. 1.21 definition: vector, point Elements of a vector space are called vectors or points. Section 1B Definition of Vector Space 13 The scalar multiplication in a vector space depends on ğ…. Thus when we need to be precise, we will say that ğ‘‰ is a vector space over ğ… instead of saying simply that ğ‘‰ is a vector space. For example, ğ‘ğ‘› is a vector space over ğ‘, and ğ‚ ğ‘› is a vector space over ğ‚. 1.22 definition: real vector space, complex vector space â€¢ A vector space over ğ‘ is called a real vector space. â€¢ A vector space over ğ‚ is called a complex vector space. Usually the choice of ğ… is either clear from the context or irrelevant. Thus we often assume that ğ… is lurking in the background without specifically mentioning it. The simplest vector space is {0}, which contains only one point. With the usual operations of addition and scalar multiplication, ğ…ğ‘› is a vector space over ğ…, as you should verify. The example of ğ…ğ‘› motivated our definition of vector space. 1.23 example: ğ…âˆ ğ…âˆ is defined to be the set of all sequences of elements ofğ…: ğ…âˆ = {(ğ‘¥1, ğ‘¥2, â€¦ ) âˆ¶ ğ‘¥ğ‘˜ âˆˆ ğ… for ğ‘˜ = 1, 2, â€¦}. Addition and scalar multiplication on ğ…âˆ are defined as expected: (ğ‘¥1, ğ‘¥2, â€¦ ) + (ğ‘¦1, ğ‘¦2, â€¦ ) = (ğ‘¥1 + ğ‘¦1, ğ‘¥2 + ğ‘¦2, â€¦ ), ğœ†(ğ‘¥1, ğ‘¥2, â€¦ ) = (ğœ†ğ‘¥1, ğœ†ğ‘¥2, â€¦ ). With these definitions,ğ…âˆ becomes a vector space over ğ…, as you should verify. The additive identity in this vector space is the sequence of all 0â€™s. Our next example of a vector space involves a set of functions. 1.24 notation: ğ…ğ‘† â€¢ If ğ‘† is a set, then ğ…ğ‘† denotes the set of functions from ğ‘† to ğ…. â€¢ For ğ‘“, ğ‘” âˆˆ ğ…ğ‘†, the sum ğ‘“ + ğ‘” âˆˆ ğ…ğ‘† is the function defined by ( ğ‘“ + ğ‘”)(ğ‘¥) = ğ‘“ (ğ‘¥) + ğ‘”(ğ‘¥) for all ğ‘¥ âˆˆ ğ‘†. â€¢ For ğœ† âˆˆ ğ… and ğ‘“ âˆˆ ğ…ğ‘†, the product ğœ† ğ‘“ âˆˆ ğ…ğ‘† is the function defined by (ğœ† ğ‘“ )(ğ‘¥) = ğœ† ğ‘“ (ğ‘¥) for all ğ‘¥ âˆˆ ğ‘†. 14 Chapter 1 Vector Spaces As an example of the notation above, if ğ‘† is the interval [0, 1]and ğ… = ğ‘, then ğ‘[0, 1] is the set of real-valued functions on the interval [0, 1]. You should verify all three bullet points in the next example. 1.25 example: ğ…ğ‘† is a vector space â€¢ If ğ‘† is a nonempty set, then ğ…ğ‘† (with the operations of addition and scalar multiplication as defined above) is a vector space overğ…. â€¢ The additive identity of ğ…ğ‘† is the function 0âˆ¶ ğ‘† â†’ ğ… defined by 0(ğ‘¥) = 0 for all ğ‘¥ âˆˆ ğ‘†. â€¢ For ğ‘“ âˆˆ ğ…ğ‘†, the additive inverse of ğ‘“ is the function âˆ’ ğ‘“ âˆ¶ ğ‘† â†’ ğ… defined by (âˆ’ ğ‘“ )(ğ‘¥) = âˆ’ ğ‘“ (ğ‘¥) for all ğ‘¥ âˆˆ ğ‘†. The elements of the vector space ğ‘[0, 1] are real-valued functions on [0, 1], not lists. In general, a vector space is an abstract entity whose elements might be lists, functions, or weird objects. The vector space ğ…ğ‘› is a special case of the vector space ğ…ğ‘† because each (ğ‘¥1, â€¦, ğ‘¥ğ‘›) âˆˆ ğ…ğ‘› can be thought of as a function ğ‘¥ from the set {1, 2, â€¦, ğ‘›} to ğ… by writing ğ‘¥(ğ‘˜) instead of ğ‘¥ğ‘˜ for the ğ‘˜th coordinate of (ğ‘¥1, â€¦, ğ‘¥ğ‘›). In other words, we can think of ğ…ğ‘› as ğ…{1, 2, â€¦, ğ‘›}. Similarly, we can think of ğ…âˆ as ğ…{1, 2, â€¦ }. Soon we will see further examples of vector spaces, but first we need to develop some of the elementary properties of vector spaces. The definition of a vector space requires it to have an additive identity. The next result states that this identity is unique. 1.26 unique additive identity A vector space has a unique additive identity. Proof Suppose 0and 0 â€² are both additive identities for some vector space ğ‘‰. Then 0 â€² = 0 â€² + 0 = 0+ 0 â€² = 0, where the first equality holds because0is an additive identity, the second equality comes from commutativity, and the third equality holds because 0 â€² is an additive identity. Thus 0 â€² = 0, proving that ğ‘‰ has only one additive identity. Each element ğ‘£ in a vector space has an additive inverse, an element ğ‘¤ in the vector space such that ğ‘£ + ğ‘¤ = 0. The next result shows that each element in a vector space has only one additive inverse. Section 1B Definition of Vector Space 15 1.27 unique additive inverse Every element in a vector space has a unique additive inverse. Proof Suppose ğ‘‰ is a vector space. Let ğ‘£ âˆˆ ğ‘‰. Suppose ğ‘¤ and ğ‘¤â€² are additive inverses of ğ‘£. Then ğ‘¤ = ğ‘¤ + 0 = ğ‘¤+ (ğ‘£ + ğ‘¤â€²) = (ğ‘¤ + ğ‘£) + ğ‘¤â€² = 0+ ğ‘¤â€² = ğ‘¤â€². Thus ğ‘¤ = ğ‘¤â€², as desired. Because additive inverses are unique, the following notation now makes sense. 1.28 notation: âˆ’ğ‘£, ğ‘¤ âˆ’ ğ‘£ Let ğ‘£, ğ‘¤ âˆˆ ğ‘‰. Then â€¢ âˆ’ğ‘£ denotes the additive inverse of ğ‘£; â€¢ ğ‘¤ âˆ’ ğ‘£ is defined to beğ‘¤ + (âˆ’ğ‘£). Almost all results in this book involve some vector space. To avoid having to restate frequently that ğ‘‰ is a vector space, we now make the necessary declaration once and for all. 1.29 notation: ğ‘‰ For the rest of this book, ğ‘‰ denotes a vector space over ğ…. In the next result, 0denotes a scalar (the number 0 âˆˆ ğ…) on the left side of the equation and a vector (the additive identity of ğ‘‰) on the right side of the equation. 1.30 the number 0times a vector 0ğ‘£ = 0for every ğ‘£ âˆˆ ğ‘‰. The result in 1.30 involves the additive identity of ğ‘‰ and scalar multiplication. The only part of the definition of a vec- tor space that connects vector addition and scalar multiplication is the dis- tributive property. Thus the distribu- tive property must be used in the proof of 1.30. Proof For ğ‘£ âˆˆ ğ‘‰, we have 0ğ‘£ = (0+ 0)ğ‘£ = 0ğ‘£+ 0ğ‘£. Adding the additive inverse of 0ğ‘£to both sides of the equation above gives 0 = 0ğ‘£, as desired. In the next result, 0denotes the addi- tive identity of ğ‘‰. Although their proofs are similar, 1.30 and 1.31 are not identical. More precisely, 1.30 states that the product of the scalar 0and any vector equals the vector 0, whereas 1.31 states that the product of any scalar and the vector 0equals the vector 0. 16 Chapter 1 Vector Spaces 1.31 a number times the vector 0 ğ‘0 = 0for every ğ‘ âˆˆ ğ…. Proof For ğ‘ âˆˆ ğ…, we have ğ‘0 = ğ‘(0+ 0) = ğ‘0+ ğ‘0. Adding the additive inverse of ğ‘0to both sides of the equation above gives 0 = ğ‘0, as desired. Now we show that if an element of ğ‘‰ is multiplied by the scalar âˆ’1, then the result is the additive inverse of the element of ğ‘‰. 1.32 the number âˆ’1times a vector (âˆ’1)ğ‘£ = âˆ’ğ‘£for every ğ‘£ âˆˆ ğ‘‰. Proof For ğ‘£ âˆˆ ğ‘‰, we have ğ‘£ + (âˆ’1)ğ‘£ = 1ğ‘£+ (âˆ’1)ğ‘£ =(1+ (âˆ’1))ğ‘£ = 0ğ‘£ = 0. This equation says that (âˆ’1)ğ‘£, when added to ğ‘£, gives 0. Thus (âˆ’1)ğ‘£is the additive inverse of ğ‘£, as desired. Exercises 1B 1 Prove that âˆ’(âˆ’ğ‘£) = ğ‘£ for every ğ‘£ âˆˆ ğ‘‰. 2 Suppose ğ‘ âˆˆ ğ…, ğ‘£ âˆˆ ğ‘‰, and ğ‘ğ‘£ = 0. Prove that ğ‘ = 0or ğ‘£ = 0. 3 Suppose ğ‘£, ğ‘¤ âˆˆ ğ‘‰. Explain why there exists a unique ğ‘¥ âˆˆ ğ‘‰ such that ğ‘£ + 3ğ‘¥ = ğ‘¤. 4 The empty set is not a vector space. The empty set fails to satisfy only one of the requirements listed in the definition of a vector space (1.20). Which one? 5 Show that in the definition of a vector space (1.20), the additive inverse condition can be replaced with the condition that 0ğ‘£ = 0for all ğ‘£ âˆˆ ğ‘‰. Here the 0on the left side is the number 0, and the 0on the right side is the additive identity of ğ‘‰. The phrase a â€œcondition can be replacedâ€ in a definition means that the collection of objects satisfying the definition is unchanged if the original condition is replaced with the new condition. Section 1B Definition of Vector Space 17 6 Let âˆ and âˆ’âˆ denote two distinct objects, neither of which is in ğ‘. Define an addition and scalar multiplication on ğ‘ âˆª {âˆ, âˆ’âˆ} as you could guess from the notation. Specifically, the sum and product of two real numbers is as usual, and for ğ‘¡ âˆˆ ğ‘ define ğ‘¡âˆ = â§ {{ â¨ {{ â© âˆ’âˆ if ğ‘¡ < 0, 0 if ğ‘¡ = 0, âˆ if ğ‘¡ > 0, ğ‘¡(âˆ’âˆ) = â§ {{ â¨ {{ â© âˆ if ğ‘¡ < 0, 0 if ğ‘¡ = 0, âˆ’âˆ if ğ‘¡ > 0, and ğ‘¡ + âˆ = âˆ + ğ‘¡ = âˆ + âˆ = âˆ, ğ‘¡ + (âˆ’âˆ) = (âˆ’âˆ) + ğ‘¡ = (âˆ’âˆ) + (âˆ’âˆ) = âˆ’âˆ, âˆ + (âˆ’âˆ) = (âˆ’âˆ) + âˆ = 0. With these operations of addition and scalar multiplication, is ğ‘ âˆª {âˆ, âˆ’âˆ} a vector space over ğ‘? Explain. 7 Suppose ğ‘† is a nonempty set. Let ğ‘‰ğ‘† denote the set of functions from ğ‘† to ğ‘‰. Define a natural addition and scalar multiplication onğ‘‰ğ‘†, and show that ğ‘‰ğ‘† is a vector space with these definitions. 8 Suppose ğ‘‰ is a real vector space. â€¢ The complexification of ğ‘‰, denoted by ğ‘‰ğ‚, equals ğ‘‰Ã— ğ‘‰. An element of ğ‘‰ğ‚ is an ordered pair (ğ‘¢, ğ‘£), where ğ‘¢, ğ‘£ âˆˆ ğ‘‰, but we write this as ğ‘¢ + ğ‘–ğ‘£. â€¢ Addition on ğ‘‰ğ‚ is defined by (ğ‘¢1 + ğ‘–ğ‘£1) + (ğ‘¢2 + ğ‘–ğ‘£2) = (ğ‘¢1 + ğ‘¢2) + ğ‘–(ğ‘£1 + ğ‘£2) for all ğ‘¢1, ğ‘£1, ğ‘¢2, ğ‘£2 âˆˆ ğ‘‰. â€¢ Complex scalar multiplication on ğ‘‰ğ‚ is defined by (ğ‘ + ğ‘ğ‘–)(ğ‘¢ + ğ‘–ğ‘£) = (ğ‘ğ‘¢ âˆ’ ğ‘ğ‘£) + ğ‘–(ğ‘ğ‘£ + ğ‘ğ‘¢) for all ğ‘, ğ‘ âˆˆ ğ‘ and all ğ‘¢, ğ‘£ âˆˆ ğ‘‰. Prove that with the definitions of addition and scalar multiplication as above, ğ‘‰ğ‚ is a complex vector space. Think of ğ‘‰ as a subset of ğ‘‰ğ‚ by identifying ğ‘¢ âˆˆ ğ‘‰ with ğ‘¢ + ğ‘–0. The construc- tion of ğ‘‰ğ‚ from ğ‘‰ can then be thought of as generalizing the construction of ğ‚ ğ‘› from ğ‘ğ‘›. 18 Chapter 1 Vector Spaces 1C Subspaces By considering subspaces, we can greatly expand our examples of vector spaces. 1.33 definition: subspace A subset ğ‘ˆ of ğ‘‰ is called a subspace of ğ‘‰ if ğ‘ˆ is also a vector space with the same additive identity, addition, and scalar multiplication as on ğ‘‰. Some people use the terminology linear subspace, which means the same as subspace. The next result gives the easiest way to check whether a subset of a vector space is a subspace. 1.34 conditions for a subspace A subset ğ‘ˆ of ğ‘‰ is a subspace of ğ‘‰ if and only if ğ‘ˆ satisfies the following three conditions. additive identity 0 âˆˆ ğ‘ˆ. closed under addition ğ‘¢, ğ‘¤ âˆˆ ğ‘ˆ implies ğ‘¢ + ğ‘¤ âˆˆ ğ‘ˆ. closed under scalar multiplication ğ‘ âˆˆ ğ… and ğ‘¢ âˆˆ ğ‘ˆ implies ğ‘ğ‘¢ âˆˆ ğ‘ˆ. The additive identity condition above could be replaced with the condition that ğ‘ˆ is nonempty (because then tak- ing ğ‘¢ âˆˆ ğ‘ˆ and multiplying it by 0 would imply that 0 âˆˆ ğ‘ˆ). However, if a subset ğ‘ˆ of ğ‘‰ is indeed a sub- space, then usually the quickest way to show that ğ‘ˆ is nonempty is to show that 0 âˆˆ ğ‘ˆ. Proof If ğ‘ˆ is a subspace of ğ‘‰, then ğ‘ˆ satisfies the three conditions above by the definition of vector space. Conversely, suppose ğ‘ˆ satisfies the three conditions above. The first condi- tion ensures that the additive identity of ğ‘‰ is in ğ‘ˆ. The second condition ensures that addition makes sense on ğ‘ˆ. The third condition ensures that scalar multiplica- tion makes sense on ğ‘ˆ. If ğ‘¢ âˆˆ ğ‘ˆ, then âˆ’ğ‘¢ [which equals (âˆ’1)ğ‘¢by 1.32]is also in ğ‘ˆ by the third condition above. Hence every element of ğ‘ˆ has an additive inverse in ğ‘ˆ. The other parts of the definition of a vector space, such as associativity and commutativity, are automatically satisfied forğ‘ˆ because they hold on the larger space ğ‘‰. Thus ğ‘ˆ is a vector space and hence is a subspace of ğ‘‰. The three conditions in the result above usually enable us to determine quickly whether a given subset of ğ‘‰ is a subspace of ğ‘‰. You should verify all assertions in the next example. Section 1C Subspaces 19 1.35 example: subspaces (a) If ğ‘ âˆˆ ğ…, then {(ğ‘¥1, ğ‘¥2, ğ‘¥3, ğ‘¥4) âˆˆ ğ…4 âˆ¶ ğ‘¥3 = 5ğ‘¥4 + ğ‘} is a subspace of ğ…4 if and only if ğ‘ = 0. (b) The set of continuous real-valued functions on the interval [0, 1]is a subspace of ğ‘[0, 1]. (c) The set of differentiable real-valued functions on ğ‘ is a subspace of ğ‘ğ‘. (d) The set of differentiable real-valued functions ğ‘“ on the interval (0, 3)such that ğ‘“ â€²(2) = ğ‘is a subspace of ğ‘(0, 3)if and only if ğ‘ = 0. (e) The set of all sequences of complex numbers with limit 0is a subspace of ğ‚ âˆ. The set {0}is the smallest subspace of ğ‘‰, and ğ‘‰ itself is the largest subspace of ğ‘‰. The empty set is not a subspace of ğ‘‰ because a subspace must be a vector space and hence must contain at least one element, namely, an additive identity. Verifying some of the items above shows the linear structure underlying parts of calculus. For example, (b) above requires the result that the sum of two continuous functions is continuous. As another example, (d) above requires the result that for a constant ğ‘, the derivative of ğ‘ ğ‘“ equals ğ‘ times the derivative of ğ‘“. The subspaces of ğ‘2 are precisely {0}, all lines in ğ‘2 containing the origin, and ğ‘2. The subspaces of ğ‘3 are precisely {0}, all lines in ğ‘3 containing the origin, all planes in ğ‘3 containing the origin, and ğ‘3. To prove that all these objects are indeed subspaces is straightforwardâ€”the hard part is to show that they are the only subspaces of ğ‘2 and ğ‘3. That task will be easier after we introduce some additional tools in the next chapter. Sums of Subspaces The union of subspaces is rarely a sub- space (see Exercise 12), which is why we usually work with sums rather than unions. When dealing with vector spaces, we are usually interested only in subspaces, as opposed to arbitrary subsets. The notion of the sum of subspaces will be useful. 1.36 definition:sum of subspaces Suppose ğ‘‰1, â€¦, ğ‘‰ğ‘š are subspaces of ğ‘‰. The sum of ğ‘‰1, â€¦, ğ‘‰ğ‘š, denoted by ğ‘‰1 + â‹¯ + ğ‘‰ğ‘š, is the set of all possible sums of elements of ğ‘‰1, â€¦, ğ‘‰ğ‘š. More precisely, ğ‘‰1 + â‹¯ + ğ‘‰ğ‘š = {ğ‘£1 + â‹¯ + ğ‘£ğ‘š âˆ¶ ğ‘£1 âˆˆ ğ‘‰1, â€¦, ğ‘£ğ‘š âˆˆ ğ‘‰ğ‘š}. 20 Chapter 1 Vector Spaces Letâ€™s look at some examples of sums of subspaces. 1.37 example:a sum of subspaces of ğ…3 Suppose ğ‘ˆ is the set of all elements of ğ…3 whose second and third coordinates equal 0, and ğ‘Š is the set of all elements of ğ…3 whose first and third coordinates equal 0: ğ‘ˆ = {(ğ‘¥, 0, 0) âˆˆ ğ… 3 âˆ¶ ğ‘¥ âˆˆ ğ…} and ğ‘Š = {(0, ğ‘¦, 0) âˆˆ ğ… 3 âˆ¶ ğ‘¦ âˆˆ ğ…}. Then ğ‘ˆ + ğ‘Š = {(ğ‘¥, ğ‘¦, 0) âˆˆ ğ… 3 âˆ¶ ğ‘¥, ğ‘¦ âˆˆ ğ…}, as you should verify. 1.38 example: a sum of subspaces of ğ…4 Suppose ğ‘ˆ = {(ğ‘¥, ğ‘¥, ğ‘¦, ğ‘¦) âˆˆ ğ…4 âˆ¶ ğ‘¥, ğ‘¦ âˆˆ ğ…} and ğ‘Š = {(ğ‘¥, ğ‘¥, ğ‘¥, ğ‘¦) âˆˆ ğ…4 âˆ¶ ğ‘¥, ğ‘¦ âˆˆ ğ…}. Using words rather than symbols, we could say that ğ‘ˆ is the set of elements of ğ…4 whose first two coordinates equal each other and whose third and fourth coordinates equal each other. Similarly, ğ‘Š is the set of elements of ğ…4 whose first three coordinates equal each other. To find a description ofğ‘ˆ + ğ‘Š, consider a typical element (ğ‘, ğ‘, ğ‘, ğ‘) of ğ‘ˆ and a typical element (ğ‘, ğ‘, ğ‘, ğ‘‘) of ğ‘Š, where ğ‘, ğ‘, ğ‘, ğ‘‘ âˆˆ ğ…. We have (ğ‘, ğ‘, ğ‘, ğ‘) + (ğ‘, ğ‘, ğ‘, ğ‘‘) = (ğ‘ + ğ‘, ğ‘ + ğ‘, ğ‘ + ğ‘, ğ‘ + ğ‘‘), which shows that every element of ğ‘ˆ + ğ‘Š has its first two coordinates equal to each other. Thus 1.39 ğ‘ˆ + ğ‘Š âŠ†{(ğ‘¥, ğ‘¥, ğ‘¦, ğ‘§) âˆˆ ğ…4 âˆ¶ ğ‘¥, ğ‘¦, ğ‘§ âˆˆ ğ…}. To prove the inclusion in the other direction, suppose ğ‘¥, ğ‘¦, ğ‘§ âˆˆ ğ…. Then (ğ‘¥, ğ‘¥, ğ‘¦, ğ‘§) = (ğ‘¥, ğ‘¥, ğ‘¦, ğ‘¦) + (0, 0, 0, ğ‘§ âˆ’ ğ‘¦), where the first vector on the right is inğ‘ˆ and the second vector on the right is in ğ‘Š. Thus (ğ‘¥, ğ‘¥, ğ‘¦, ğ‘§) âˆˆ ğ‘ˆ + ğ‘Š, showing that the inclusion 1.39 also holds in the opposite direction. Hence ğ‘ˆ + ğ‘Š = {(ğ‘¥, ğ‘¥, ğ‘¦, ğ‘§) âˆˆ ğ…4 âˆ¶ ğ‘¥, ğ‘¦, ğ‘§ âˆˆ ğ…}, which shows that ğ‘ˆ + ğ‘Š is the set of elements of ğ…4 whose first two coordinates equal each other. The next result states that the sum of subspaces is a subspace, and is in fact the smallest subspace containing all the summands (which means that every subspace containing all the summands also contains the sum). Section 1C Subspaces 21 1.40 sum of subspaces is the smallest containing subspace Suppose ğ‘‰1, â€¦, ğ‘‰ğ‘š are subspaces of ğ‘‰. Then ğ‘‰1 + â‹¯ + ğ‘‰ğ‘š is the smallest subspace of ğ‘‰ containing ğ‘‰1, â€¦, ğ‘‰ğ‘š. Proof The reader can verify that ğ‘‰1 + â‹¯ + ğ‘‰ğ‘š contains the additive identity 0 and is closed under addition and scalar multiplication. Thus 1.34 implies that ğ‘‰1 + â‹¯ + ğ‘‰ğ‘š is a subspace of ğ‘‰. Sums of subspaces in the theory of vec- tor spaces are analogous to unions of subsets in set theory. Given two sub- spaces of a vector space, the smallest subspace containing them is their sum. Analogously, given two subsets of a set, the smallest subset containing them is their union. The subspaces ğ‘‰1, â€¦, ğ‘‰ğ‘š are all con- tained in ğ‘‰1+â‹¯+ğ‘‰ğ‘š (to see this, consider sums ğ‘£1 + â‹¯ + ğ‘£ğ‘š where all except one of the ğ‘£ğ‘˜â€™s are 0). Conversely, every sub- space of ğ‘‰ containing ğ‘‰1, â€¦, ğ‘‰ğ‘š contains ğ‘‰1 + â‹¯ + ğ‘‰ğ‘š (because subspaces must contain all finite sums of their elements). Thus ğ‘‰1+â‹¯+ğ‘‰ğ‘š is the smallest subspace of ğ‘‰ containing ğ‘‰1, â€¦, ğ‘‰ğ‘š. Direct Sums Suppose ğ‘‰1, â€¦, ğ‘‰ğ‘š are subspaces of ğ‘‰. Every element of ğ‘‰1 + â‹¯ + ğ‘‰ğ‘š can be written in the form ğ‘£1 + â‹¯ + ğ‘£ğ‘š, where each ğ‘£ğ‘˜ âˆˆ ğ‘‰ğ‘˜. Of special interest are cases in which each vector in ğ‘‰1 + â‹¯ + ğ‘‰ğ‘š can be represented in the form above in only one way. This situation is so important that it gets a special name (direct sum) and a special symbol (âŠ•). 1.41 definition: direct sum, âŠ• Suppose ğ‘‰1, â€¦, ğ‘‰ğ‘š are subspaces of ğ‘‰. â€¢ The sum ğ‘‰1 + â‹¯ + ğ‘‰ğ‘š is called a direct sum if each element of ğ‘‰1 + â‹¯ + ğ‘‰ğ‘š can be written in only one way as a sum ğ‘£1 + â‹¯ + ğ‘£ğ‘š, where each ğ‘£ğ‘˜ âˆˆ ğ‘‰ğ‘˜. â€¢ If ğ‘‰1 + â‹¯ + ğ‘‰ğ‘š is a direct sum, then ğ‘‰1 âŠ• â‹¯ âŠ• ğ‘‰ğ‘š denotes ğ‘‰1 + â‹¯ + ğ‘‰ğ‘š, with the âŠ• notation serving as an indication that this is a direct sum. 1.42 example: a direct sum of two subspaces Suppose ğ‘ˆ is the subspace of ğ…3 of those vectors whose last coordinate equals 0, and ğ‘Š is the subspace of ğ…3 of those vectors whose first two coordinates equal0: ğ‘ˆ = {(ğ‘¥, ğ‘¦, 0) âˆˆ ğ… 3 âˆ¶ ğ‘¥, ğ‘¦ âˆˆ ğ…} and ğ‘Š = {(0, 0, ğ‘§) âˆˆ ğ…3 âˆ¶ ğ‘§ âˆˆ ğ…}. Then ğ…3 = ğ‘ˆ âŠ• ğ‘Š, as you should verify. â‰ˆ 7ğœ‹ Chapter 1 Vector Spaces 1.43 example: a direct sum of multiple subspaces To produce âŠ• in TEX, type \\oplus.Suppose ğ‘‰ğ‘˜ is the subspace of ğ…ğ‘› of those vectors whose coordinates are all 0, except possibly in the ğ‘˜th slot; for example, ğ‘‰2 = {(0, ğ‘¥, 0, â€¦, 0) âˆˆ ğ… ğ‘› âˆ¶ ğ‘¥ âˆˆ ğ…}. Then ğ…ğ‘› = ğ‘‰1 âŠ• â‹¯ âŠ• ğ‘‰ğ‘›, as you should verify. Sometimes nonexamples add to our understanding as much as examples. 1.44 example: a sum that is not a direct sum Suppose ğ‘‰1 = {(ğ‘¥, ğ‘¦, 0) âˆˆ ğ… 3 âˆ¶ ğ‘¥, ğ‘¦ âˆˆ ğ…}, ğ‘‰2 = {(0, 0, ğ‘§) âˆˆ ğ…3 âˆ¶ ğ‘§ âˆˆ ğ…}, ğ‘‰3 = {(0, ğ‘¦, ğ‘¦) âˆˆ ğ…3 âˆ¶ ğ‘¦ âˆˆ ğ…}. Then ğ…3 = ğ‘‰1 + ğ‘‰2 + ğ‘‰3 because every vector (ğ‘¥, ğ‘¦, ğ‘§) âˆˆ ğ…3 can be written as (ğ‘¥, ğ‘¦, ğ‘§) = (ğ‘¥, ğ‘¦, 0)+ (0, 0, ğ‘§) + (0, 0, 0), where the first vector on the right side is inğ‘‰1, the second vector is in ğ‘‰2, and the third vector is in ğ‘‰3. However, ğ…3 does not equal the direct sum of ğ‘‰1, ğ‘‰2, ğ‘‰3, because the vector (0, 0, 0)can be written in more than one way as a sum ğ‘£1 + ğ‘£2 + ğ‘£3, with each ğ‘£ğ‘˜ âˆˆ ğ‘‰ğ‘˜. Specifically, we have (0, 0, 0) = (0, 1, 0)+ (0, 0, 1)+ (0, âˆ’1, âˆ’1) and, of course, (0, 0, 0) = (0, 0, 0)+ (0, 0, 0)+ (0, 0, 0), where the first vector on the right side of each equation above is inğ‘‰1, the second vector is in ğ‘‰2, and the third vector is in ğ‘‰3. Thus the sum ğ‘‰1 + ğ‘‰2 + ğ‘‰3 is not a direct sum. The symbol âŠ•, which is a plus sign inside a circle, reminds us that we are dealing with a special type of sum of subspacesâ€”each element in the direct sum can be represented in only one way as a sum of elements from the specified subspaces. The definition of direct sum requires every vector in the sum to have a unique representation as an appropriate sum. The next result shows that when deciding whether a sum of subspaces is a direct sum, we only need to consider whether 0 can be uniquely written as an appropriate sum. Section 1C Subspaces 23 1.45 condition for a direct sum Suppose ğ‘‰1, â€¦, ğ‘‰ğ‘š are subspaces of ğ‘‰. Then ğ‘‰1 + â‹¯ + ğ‘‰ğ‘š is a direct sum if and only if the only way to write 0as a sum ğ‘£1 + â‹¯ + ğ‘£ğ‘š, where each ğ‘£ğ‘˜ âˆˆ ğ‘‰ğ‘˜, is by taking each ğ‘£ğ‘˜ equal to 0. Proof First suppose ğ‘‰1 + â‹¯ + ğ‘‰ğ‘š is a direct sum. Then the definition of direct sum implies that the only way to write 0as a sum ğ‘£1 + â‹¯+ ğ‘£ğ‘š, where each ğ‘£ğ‘˜ âˆˆ ğ‘‰ğ‘˜, is by taking each ğ‘£ğ‘˜ equal to 0. Now suppose that the only way to write 0as a sum ğ‘£1 + â‹¯ + ğ‘£ğ‘š, where each ğ‘£ğ‘˜ âˆˆ ğ‘‰ğ‘˜, is by taking each ğ‘£ğ‘˜ equal to 0. To show that ğ‘‰1 + â‹¯ + ğ‘‰ğ‘š is a direct sum, let ğ‘£ âˆˆ ğ‘‰1 + â‹¯ + ğ‘‰ğ‘š. We can write ğ‘£ = ğ‘£1 + â‹¯ + ğ‘£ğ‘š for some ğ‘£1 âˆˆ ğ‘‰1, â€¦, ğ‘£ğ‘š âˆˆ ğ‘‰ğ‘š. To show that this representation is unique, suppose we also have ğ‘£ = ğ‘¢1 + â‹¯ + ğ‘¢ğ‘š, where ğ‘¢1 âˆˆ ğ‘‰1, â€¦, ğ‘¢ğ‘š âˆˆ ğ‘‰ğ‘š. Subtracting these two equations, we have 0 = (ğ‘£1 âˆ’ ğ‘¢1) + â‹¯ + (ğ‘£ğ‘š âˆ’ ğ‘¢ğ‘š). Because ğ‘£1 âˆ’ ğ‘¢1 âˆˆ ğ‘‰1, â€¦, ğ‘£ğ‘š âˆ’ ğ‘¢ğ‘š âˆˆ ğ‘‰ğ‘š, the equation above implies that each ğ‘£ğ‘˜ âˆ’ ğ‘¢ğ‘˜ equals 0. Thus ğ‘£1 = ğ‘¢1, â€¦, ğ‘£ğ‘š = ğ‘¢ğ‘š, as desired. The symbol âŸº used below means â€œif and only if â€; this symbol could also be read to mean â€œis equivalent toâ€. The next result gives a simple con- dition for testing whether a sum of two subspaces is a direct sum. 1.46 direct sum of two subspaces Suppose ğ‘ˆ and ğ‘Š are subspaces of ğ‘‰. Then ğ‘ˆ + ğ‘Š is a direct sum âŸº ğ‘ˆ âˆ© ğ‘Š = {0}. Proof First suppose that ğ‘ˆ + ğ‘Š is a direct sum. If ğ‘£ âˆˆ ğ‘ˆ âˆ© ğ‘Š, then 0 = ğ‘£+ (âˆ’ğ‘£), where ğ‘£ âˆˆ ğ‘ˆ and âˆ’ğ‘£ âˆˆ ğ‘Š. By the unique representation of 0as the sum of a vector in ğ‘ˆ and a vector in ğ‘Š, we have ğ‘£ = 0. Thus ğ‘ˆ âˆ© ğ‘Š = {0}, completing the proof in one direction. To prove the other direction, now suppose ğ‘ˆ âˆ© ğ‘Š = {0}. To prove that ğ‘ˆ + ğ‘Š is a direct sum, suppose ğ‘¢ âˆˆ ğ‘ˆ, ğ‘¤ âˆˆ ğ‘Š, and 0 = ğ‘¢+ ğ‘¤. To complete the proof, we only need to show that ğ‘¢ = ğ‘¤ = 0(by 1.45). The equation above implies that ğ‘¢ = âˆ’ğ‘¤ âˆˆ ğ‘Š. Thus ğ‘¢ âˆˆ ğ‘ˆ âˆ© ğ‘Š. Hence ğ‘¢ = 0, which by the equation above implies that ğ‘¤ = 0, completing the proof. 24 Chapter 1 Vector Spaces Sums of subspaces are analogous to unions of subsets. Similarly, direct sums of subspaces are analogous to disjoint unions of subsets. No two sub- spaces of a vector space can be disjoint, because both contain 0. So disjoint- ness is replaced, at least in the case of two subspaces, with the requirement that the intersection equal {0}. The result above deals only with the case of two subspaces. When ask- ing about a possible direct sum with more than two subspaces, it is not enough to test that each pair of the subspaces intersect only at 0. To see this, consider Example 1.44. In that nonexample of a direct sum, we have ğ‘‰1 âˆ© ğ‘‰2 = ğ‘‰1 âˆ© ğ‘‰3 = ğ‘‰2 âˆ© ğ‘‰3 = {0}. Exercises 1C 1 For each of the following subsets of ğ…3, determine whether it is a subspace of ğ…3. (a) {(ğ‘¥1, ğ‘¥2, ğ‘¥3) âˆˆ ğ…3 âˆ¶ ğ‘¥1 + 2ğ‘¥2 + 3ğ‘¥3 = 0} (b) {(ğ‘¥1, ğ‘¥2, ğ‘¥3) âˆˆ ğ…3 âˆ¶ ğ‘¥1 + 2ğ‘¥2 + 3ğ‘¥3 = 4} (c) {(ğ‘¥1, ğ‘¥2, ğ‘¥3) âˆˆ ğ…3 âˆ¶ ğ‘¥1ğ‘¥2ğ‘¥3 = 0} (d) {(ğ‘¥1, ğ‘¥2, ğ‘¥3) âˆˆ ğ…3 âˆ¶ ğ‘¥1 = 5ğ‘¥3} 2 Verify all assertions about subspaces in Example 1.35. 3 Show that the set of differentiable real-valued functions ğ‘“ on the interval (âˆ’4, 4)such that ğ‘“ â€²(âˆ’1) = 3 ğ‘“ (2)is a subspace of ğ‘(âˆ’4, 4). 4 Suppose ğ‘ âˆˆ ğ‘. Show that the set of continuous real-valued functions ğ‘“ on the interval [0, 1]such that âˆ«1 0 ğ‘“ = ğ‘ is a subspace of ğ‘[0, 1] if and only if ğ‘ = 0. 5 Is ğ‘2 a subspace of the complex vector space ğ‚ 2? 6 (a) Is {(ğ‘, ğ‘, ğ‘) âˆˆ ğ‘3 âˆ¶ ğ‘3 = ğ‘3}a subspace of ğ‘3? (b) Is {(ğ‘, ğ‘, ğ‘) âˆˆ ğ‚3 âˆ¶ ğ‘3 = ğ‘3}a subspace of ğ‚ 3? 7 Prove or give a counterexample: If ğ‘ˆ is a nonempty subset of ğ‘2 such that ğ‘ˆ is closed under addition and under taking additive inverses (meaning âˆ’ğ‘¢ âˆˆ ğ‘ˆ whenever ğ‘¢ âˆˆ ğ‘ˆ), then ğ‘ˆ is a subspace of ğ‘2. 8 Give an example of a nonempty subset ğ‘ˆ of ğ‘2 such that ğ‘ˆ is closed under scalar multiplication, but ğ‘ˆ is not a subspace of ğ‘2. 9 A function ğ‘“âˆ¶ ğ‘ â†’ ğ‘ is called periodic if there exists a positive number ğ‘ such that ğ‘“ (ğ‘¥) = ğ‘“ (ğ‘¥ + ğ‘) for all ğ‘¥ âˆˆ ğ‘. Is the set of periodic functions from ğ‘ to ğ‘ a subspace of ğ‘ğ‘? Explain. 10 Suppose ğ‘‰1 and ğ‘‰2 are subspaces of ğ‘‰. Prove that the intersection ğ‘‰1 âˆ© ğ‘‰2 is a subspace of ğ‘‰. Section 1C Subspaces 25 11 Prove that the intersection of every collection of subspaces of ğ‘‰ is a subspace of ğ‘‰. 12 Prove that the union of two subspaces of ğ‘‰ is a subspace of ğ‘‰ if and only if one of the subspaces is contained in the other. 13 Prove that the union of three subspaces of ğ‘‰ is a subspace of ğ‘‰ if and only if one of the subspaces contains the other two. This exercise is surprisingly harder than Exercise 12, possibly because this exercise is not true if we replace ğ… with a field containing only two elements. 14 Suppose ğ‘ˆ = {(ğ‘¥, âˆ’ğ‘¥, 2ğ‘¥) âˆˆ ğ… 3 âˆ¶ ğ‘¥ âˆˆ ğ…} and ğ‘Š = {(ğ‘¥, ğ‘¥, 2ğ‘¥) âˆˆ ğ… 3 âˆ¶ ğ‘¥ âˆˆ ğ…}. Describe ğ‘ˆ + ğ‘Š using symbols, and also give a description of ğ‘ˆ + ğ‘Š that uses no symbols. 15 Suppose ğ‘ˆ is a subspace of ğ‘‰. What is ğ‘ˆ + ğ‘ˆ? 16 Is the operation of addition on the subspaces of ğ‘‰ commutative? In other words, if ğ‘ˆ and ğ‘Š are subspaces of ğ‘‰, is ğ‘ˆ + ğ‘Š = ğ‘Š + ğ‘ˆ? 17 Is the operation of addition on the subspaces of ğ‘‰ associative? In other words, if ğ‘‰1, ğ‘‰2, ğ‘‰3 are subspaces of ğ‘‰, is (ğ‘‰1 + ğ‘‰2) + ğ‘‰3 = ğ‘‰1 + (ğ‘‰2 + ğ‘‰3)? 18 Does the operation of addition on the subspaces of ğ‘‰ have an additive identity? Which subspaces have additive inverses? 19 Prove or give a counterexample: If ğ‘‰1, ğ‘‰2, ğ‘ˆ are subspaces of ğ‘‰ such that ğ‘‰1 + ğ‘ˆ = ğ‘‰2 + ğ‘ˆ, then ğ‘‰1 = ğ‘‰2. 20 Suppose ğ‘ˆ = {(ğ‘¥, ğ‘¥, ğ‘¦, ğ‘¦) âˆˆ ğ…4 âˆ¶ ğ‘¥, ğ‘¦ âˆˆ ğ…}. Find a subspace ğ‘Š of ğ…4 such that ğ…4 = ğ‘ˆ âŠ• ğ‘Š. 21 Suppose ğ‘ˆ = {(ğ‘¥, ğ‘¦, ğ‘¥ + ğ‘¦, ğ‘¥ âˆ’ ğ‘¦, 2ğ‘¥) âˆˆ ğ… 5 âˆ¶ ğ‘¥, ğ‘¦ âˆˆ ğ…}. Find a subspace ğ‘Š of ğ…5 such that ğ…5 = ğ‘ˆ âŠ• ğ‘Š. 22 Suppose ğ‘ˆ = {(ğ‘¥, ğ‘¦, ğ‘¥ + ğ‘¦, ğ‘¥ âˆ’ ğ‘¦, 2ğ‘¥) âˆˆ ğ… 5 âˆ¶ ğ‘¥, ğ‘¦ âˆˆ ğ…}. Find three subspaces ğ‘Š1, ğ‘Š2, ğ‘Š3 of ğ…5, none of which equals {0}, such that ğ…5 = ğ‘ˆ âŠ• ğ‘Š1 âŠ• ğ‘Š2 âŠ• ğ‘Š3. 26 Chapter 1 Vector Spaces 23 Prove or give a counterexample: If ğ‘‰1, ğ‘‰2, ğ‘ˆ are subspaces of ğ‘‰ such that ğ‘‰ = ğ‘‰1 âŠ• ğ‘ˆ and ğ‘‰ = ğ‘‰2 âŠ• ğ‘ˆ, then ğ‘‰1 = ğ‘‰2. Hint: When trying to discover whether a conjecture in linear algebra is true or false, it is often useful to start by experimenting in ğ…2. 24 A function ğ‘“âˆ¶ ğ‘ â†’ ğ‘ is called even if ğ‘“ (âˆ’ğ‘¥) = ğ‘“ (ğ‘¥) for all ğ‘¥ âˆˆ ğ‘. A function ğ‘“âˆ¶ ğ‘ â†’ ğ‘ is called odd if ğ‘“ (âˆ’ğ‘¥) = âˆ’ ğ‘“ (ğ‘¥) for all ğ‘¥ âˆˆ ğ‘. Let ğ‘‰e denote the set of real-valued even functions on ğ‘ and let ğ‘‰o denote the set of real-valued odd functions on ğ‘. Show that ğ‘ğ‘ = ğ‘‰e âŠ• ğ‘‰o. Open Access This chapter is licensed under the terms of the Creative Commons Attribution-NonCommercial 4.0 International License (https://creativecommons.org/licenses/by-nc/4.0), which permits any noncommercial use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to original author and source, provide a link to the Creative Commons license, and indicate if changes were made. The images or other third party material in this chapter are included in the chapterâ€™s Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the chapterâ€™s Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. Chapter 2 Finite-Dimensional Vector Spaces In the last chapter we learned about vector spaces. Linear algebra focuses not on arbitrary vector spaces, but on finite-dimensional vector spaces, which we introduce in this chapter. We begin this chapter by considering linear combinations of lists of vectors. This leads us to the crucial concept of linear independence. The linear dependence lemma will become one of our most useful tools. A list of vectors in a vector space that is small enough to be linearly independent and big enough so the linear combinations of the list fill up the vector space is called a basis of the vector space. We will see that every basis of a vector space has the same length, which will allow us to define the dimension of a vector space. This chapter ends with a formula for the dimension of the sum of two subspaces. standing assumptions for this chapter â€¢ ğ… denotes ğ‘ or ğ‚. â€¢ ğ‘‰ denotes a vector space over ğ…. The main building of the Institute for Advanced Study, in Princeton, New Jersey. Paul Halmos (1916â€“2006) wrote the first modern linear algebra book in this building. Halmosâ€™s linear algebra book was published in 1942 (second edition published in 1958). The title of Halmosâ€™s book was the same as the title of this chapter. 27 S. Axler, Linear Algebra Done Right, Undergraduate Texts in Mathematics, https://doi.org/10.1007/978-3-031-41026-0_2 Â© Sheldon Axler 2024 28 Chapter 2 Finite-Dimensional Vector Spaces 2A Span and Linear Independence We have been writing lists of numbers surrounded by parentheses, and we will continue to do so for elements of ğ…ğ‘›; for example, (2, âˆ’7, 8) âˆˆ ğ… 3. However, now we need to consider lists of vectors (which may be elements of ğ…ğ‘› or of other vector spaces). To avoid confusion, we will usually write lists of vectors without surrounding parentheses. For example, (4, 1, 6), (9, 5, 7)is a list of length two of vectors in ğ‘3. 2.1 notation: list of vectors We will usually write lists of vectors without surrounding parentheses. Linear Combinations and Span A sum of scalar multiples of the vectors in a list is called a linear combination of the list. Here is the formal definition. 2.2 definition: linear combination A linear combination of a list ğ‘£1, â€¦, ğ‘£ğ‘š of vectors in ğ‘‰ is a vector of the form ğ‘1ğ‘£1 + â‹¯ + ğ‘ğ‘šğ‘£ğ‘š, where ğ‘1, â€¦, ğ‘ğ‘š âˆˆ ğ…. 2.3 example: linear combinations in ğ‘3 â€¢ (17, âˆ’4, 2)is a linear combination of (2, 1, âˆ’3), (1, âˆ’2, 4), which is a list of length two of vectors in ğ‘3, because (17, âˆ’4, 2) = 6(2, 1, âˆ’3)+ 5(1, âˆ’2, 4). â€¢ (17, âˆ’4, 5)is not a linear combination of (2, 1, âˆ’3), (1, âˆ’2, 4), which is a list of length two of vectors in ğ‘3, because there do not exist numbers ğ‘1, ğ‘2 âˆˆ ğ… such that (17, âˆ’4, 5) = ğ‘1(2, 1, âˆ’3)+ ğ‘2(1, âˆ’2, 4). In other words, the system of equations 17 = 2ğ‘1 + ğ‘2 âˆ’4 = ğ‘1 âˆ’ 2ğ‘2 5 = âˆ’3ğ‘1 + 4ğ‘2 has no solutions (as you should verify). Section 2A Span and Linear Independence 29 2.4 definition: span The set of all linear combinations of a list of vectors ğ‘£1, â€¦, ğ‘£ğ‘š in ğ‘‰ is called the span of ğ‘£1, â€¦, ğ‘£ğ‘š, denoted by span(ğ‘£1, â€¦, ğ‘£ğ‘š). In other words, span(ğ‘£1, â€¦, ğ‘£ğ‘š) = {ğ‘1ğ‘£1 + â‹¯ + ğ‘ğ‘šğ‘£ğ‘š âˆ¶ ğ‘1, â€¦, ğ‘ğ‘š âˆˆ ğ…}. The span of the empty list ( ) is defined to be{0}. 2.5 example: span The previous example shows that in ğ…3, â€¢ (17, âˆ’4, 2) âˆˆspan((2, 1, âˆ’3), (1, âˆ’2, 4)); â€¢ (17, âˆ’4, 5) âˆ‰span((2, 1, âˆ’3), (1, âˆ’2, 4)). Some mathematicians use the term linear span, which means the same as span. 2.6 span is the smallest containing subspace The span of a list of vectors in ğ‘‰ is the smallest subspace of ğ‘‰ containing all vectors in the list. Proof Suppose ğ‘£1, â€¦, ğ‘£ğ‘š is a list of vectors in ğ‘‰. First we show that span(ğ‘£1, â€¦, ğ‘£ğ‘š) is a subspace of ğ‘‰. The additive identity is in span(ğ‘£1, â€¦, ğ‘£ğ‘š) because 0 = 0ğ‘£1 + â‹¯ + 0ğ‘£ğ‘š. Also, span(ğ‘£1, â€¦, ğ‘£ğ‘š) is closed under addition because (ğ‘1ğ‘£1 + â‹¯ + ğ‘ğ‘šğ‘£ğ‘š) + (ğ‘1ğ‘£1 + â‹¯ + ğ‘ğ‘šğ‘£ğ‘š) = (ğ‘1 + ğ‘1)ğ‘£1 + â‹¯ + (ğ‘ğ‘š + ğ‘ğ‘š)ğ‘£ğ‘š. Furthermore, span(ğ‘£1, â€¦, ğ‘£ğ‘š) is closed under scalar multiplication because ğœ†(ğ‘1ğ‘£1 + â‹¯ + ğ‘ğ‘šğ‘£ğ‘š) = ğœ†ğ‘1ğ‘£1 + â‹¯ + ğœ†ğ‘ğ‘šğ‘£ğ‘š. Thus span(ğ‘£1, â€¦, ğ‘£ğ‘š) is a subspace of ğ‘‰ (by 1.34). Each ğ‘£ğ‘˜ is a linear combination of ğ‘£1, â€¦, ğ‘£ğ‘š (to show this, set ğ‘ğ‘˜ = 1and let the other ğ‘â€™s in 2.2 equal 0). Thus span(ğ‘£1, â€¦, ğ‘£ğ‘š) contains each ğ‘£ğ‘˜. Conversely, because subspaces are closed under scalar multiplication and addition, every sub- space of ğ‘‰ that contains each ğ‘£ğ‘˜ contains span(ğ‘£1, â€¦, ğ‘£ğ‘š). Thus span(ğ‘£1, â€¦, ğ‘£ğ‘š) is the smallest subspace of ğ‘‰ containing all the vectors ğ‘£1, â€¦, ğ‘£ğ‘š. 2.7 definition:spans If span(ğ‘£1, â€¦, ğ‘£ğ‘š) equals ğ‘‰, we say that the list ğ‘£1, â€¦, ğ‘£ğ‘š spans ğ‘‰. 30 Chapter 2 Finite-Dimensional Vector Spaces 2.8 example: a list that spans ğ…ğ‘› Suppose ğ‘› is a positive integer. We want to show that (1, 0, â€¦, 0), (0, 1, 0, â€¦, 0), â€¦, (0, â€¦, 0, 1) spans ğ…ğ‘›. Here the ğ‘˜th vector in the list above has 1in the ğ‘˜th slot and 0in all other slots. Suppose (ğ‘¥1, â€¦, ğ‘¥ğ‘›) âˆˆ ğ…ğ‘›. Then (ğ‘¥1, â€¦, ğ‘¥ğ‘›) = ğ‘¥1(1, 0, â€¦, 0)+ ğ‘¥2(0, 1, 0, â€¦, 0)+ â‹¯ + ğ‘¥ğ‘›(0, â€¦, 0, 1). Thus (ğ‘¥1, â€¦, ğ‘¥ğ‘›) âˆˆ span((1, 0, â€¦, 0), (0, 1, 0, â€¦, 0), â€¦, (0, â€¦, 0, 1)), as desired. Now we can make one of the key definitions in linear algebra. 2.9 definition: finite-dimensional vector space A vector space is called finite-dimensional if some list of vectors in it spans the space. Recall that by definition every list has finite length. Example 2.8 above shows that ğ…ğ‘› is a finite-dimensional vector space for every positive integer ğ‘›. The definition of a polynomial is no doubt already familiar to you. 2.10 definition: polynomial, ğ’«(ğ…) â€¢ A function ğ‘âˆ¶ ğ… â†’ ğ… is called a polynomial with coefficients in ğ… if there exist ğ‘0, â€¦, ğ‘ğ‘š âˆˆ ğ… such that ğ‘(ğ‘§) = ğ‘0 + ğ‘1ğ‘§ + ğ‘2ğ‘§2 + â‹¯ + ğ‘ğ‘šğ‘§ğ‘š for all ğ‘§ âˆˆ ğ…. â€¢ ğ’«(ğ…) is the set of all polynomials with coefficients in ğ…. With the usual operations of addition and scalar multiplication, ğ’«(ğ…) is a vector space over ğ…, as you should verify. Hence ğ’«(ğ…) is a subspace of ğ…ğ…, the vector space of functions from ğ… to ğ…. If a polynomial (thought of as a function from ğ… to ğ…) is represented by two sets of coefficients, then subtracting one representation of the polynomial from the other produces a polynomial that is identically zero as a function on ğ… and hence has all zero coefficients (if you are unfamiliar with this fact, just believe it for now; we will prove it laterâ€”see 4.8). Conclusion: the coefficients of a polynomial are uniquely determined by the polynomial. Thus the next definition uniquely defines the degree of a polynomial. Section 2A Span and Linear Independence 31 2.11 definition: degree of a polynomial, deg ğ‘ â€¢ A polynomial ğ‘ âˆˆ ğ’«(ğ…) is said to have degree ğ‘š if there exist scalars ğ‘0, ğ‘1, â€¦, ğ‘ğ‘š âˆˆ ğ… with ğ‘ğ‘š â‰  0such that for every ğ‘§ âˆˆ ğ…, we have ğ‘(ğ‘§) = ğ‘0 + ğ‘1ğ‘§ + â‹¯ + ğ‘ğ‘šğ‘§ ğ‘š. â€¢ The polynomial that is identically 0is said to have degree âˆ’âˆ. â€¢ The degree of a polynomial ğ‘ is denoted by deg ğ‘. In the next definition, we use the convention thatâˆ’âˆ < ğ‘š, which means that the polynomial 0is in ğ’«ğ‘š(ğ…). 2.12 notation: ğ’«ğ‘š(ğ…) For ğ‘š a nonnegative integer, ğ’«ğ‘š(ğ…) denotes the set of all polynomials with coefficients in ğ… and degree at most ğ‘š. If ğ‘š is a nonnegative integer, then ğ’«ğ‘š(ğ…) = span(1, ğ‘§, â€¦, ğ‘§ ğ‘š) [here we slightly abuse notation by letting ğ‘§ ğ‘˜ denote a function]. Thus ğ’«ğ‘š(ğ…) is a finite-dimensional vector space for each nonnegative integer ğ‘š. 2.13 definition: infinite-dimensional vector space A vector space is called infinite-dimensional if it is not finite-dimensional. 2.14 example: ğ’«(ğ…) is infinite-dimensional. Consider any list of elements of ğ’«(ğ…). Let ğ‘š denote the highest degree of the polynomials in this list. Then every polynomial in the span of this list has degree at most ğ‘š. Thus ğ‘§ ğ‘š + 1 is not in the span of our list. Hence no list spans ğ’«(ğ…). Thus ğ’«(ğ…) is infinite-dimensional. Linear Independence Suppose ğ‘£1, â€¦, ğ‘£ğ‘š âˆˆ ğ‘‰ and ğ‘£ âˆˆ span(ğ‘£1, â€¦, ğ‘£ğ‘š). By the definition of span, there exist ğ‘1, â€¦, ğ‘ğ‘š âˆˆ ğ… such that ğ‘£ = ğ‘1ğ‘£1 + â‹¯ + ğ‘ğ‘šğ‘£ğ‘š. Consider the question of whether the choice of scalars in the equation above is unique. Suppose ğ‘1, â€¦, ğ‘ğ‘š is another set of scalars such that ğ‘£ = ğ‘1ğ‘£1 + â‹¯ + ğ‘ğ‘šğ‘£ğ‘š. Subtracting the last two equations, we have 0 = (ğ‘1 âˆ’ ğ‘1)ğ‘£1 + â‹¯ + (ğ‘ğ‘š âˆ’ ğ‘ğ‘š)ğ‘£ğ‘š. 32 Chapter 2 Finite-Dimensional Vector Spaces Thus we have written 0as a linear combination of (ğ‘£1, â€¦, ğ‘£ğ‘š). If the only way to do this is by using 0for all the scalars in the linear combination, then each ğ‘ğ‘˜ âˆ’ ğ‘ğ‘˜ equals 0, which means that each ğ‘ğ‘˜ equals ğ‘ğ‘˜ (and thus the choice of scalars was indeed unique). This situation is so important that we give it a special nameâ€”linear independenceâ€”which we now define. 2.15 definition: linearly independent â€¢ A list ğ‘£1, â€¦, ğ‘£ğ‘š of vectors in ğ‘‰ is called linearly independent if the only choice of ğ‘1, â€¦, ğ‘ğ‘š âˆˆ ğ… that makes ğ‘1ğ‘£1 + â‹¯ + ğ‘ğ‘šğ‘£ğ‘š = 0 is ğ‘1 = â‹¯ = ğ‘ğ‘š = 0. â€¢ The empty list ( ) is also declared to be linearly independent. The reasoning above shows that ğ‘£1, â€¦, ğ‘£ğ‘š is linearly independent if and only if each vector in span(ğ‘£1, â€¦, ğ‘£ğ‘š) has only one representation as a linear combination of ğ‘£1, â€¦, ğ‘£ğ‘š. 2.16 example:linearly independent lists (a) To see that the list (1, 0, 0, 0), (0, 1, 0, 0), (0, 0, 1, 0)is linearly independent in ğ…4, suppose ğ‘1, ğ‘2, ğ‘3 âˆˆ ğ… and ğ‘1(1, 0, 0, 0)+ ğ‘2(0, 1, 0, 0)+ ğ‘3(0, 0, 1, 0) = (0, 0, 0, 0). Thus (ğ‘1, ğ‘2, ğ‘3, 0) = (0, 0, 0, 0). Hence ğ‘1 = ğ‘2 = ğ‘3 = 0. Thus the list (1, 0, 0, 0), (0, 1, 0, 0), (0, 0, 1, 0)is linearly independent in ğ…4. (b) Suppose ğ‘š is a nonnegative integer. To see that the list 1, ğ‘§, â€¦, ğ‘§ ğ‘š is linearly independent in ğ’«(ğ…), suppose ğ‘0, ğ‘1, â€¦, ğ‘ğ‘š âˆˆ ğ… and ğ‘0 + ğ‘1ğ‘§ + â‹¯ + ğ‘ğ‘šğ‘§ğ‘š = 0, where we think of both sides as elements of ğ’«(ğ…). Then ğ‘0 + ğ‘1ğ‘§ + â‹¯ + ğ‘ğ‘šğ‘§ ğ‘š = 0 for all ğ‘§ âˆˆ ğ…. As discussed earlier (and as follows from 4.8), this implies that ğ‘0 = ğ‘1 = â‹¯ = ğ‘ğ‘š = 0. Thus 1, ğ‘§, â€¦, ğ‘§ ğ‘š is a linearly independent list in ğ’«(ğ…). (c) A list of length one in a vector space is linearly independent if and only if the vector in the list is not 0. (d) A list of length two in a vector space is linearly independent if and only if neither of the two vectors in the list is a scalar multiple of the other. Section 2A Span and Linear Independence 33 If some vectors are removed from a linearly independent list, the remaining list is also linearly independent, as you should verify. 2.17 definition:linearly dependent â€¢ A list of vectors in ğ‘‰ is called linearly dependent if it is not linearly inde- pendent. â€¢ In other words, a list ğ‘£1, â€¦, ğ‘£ğ‘š of vectors in ğ‘‰ is linearly dependent if there exist ğ‘1, â€¦, ğ‘ğ‘š âˆˆ ğ…, not all 0, such that ğ‘1ğ‘£1 + â‹¯ + ğ‘ğ‘šğ‘£ğ‘š = 0. 2.18 example: linearly dependent lists â€¢ (2, 3, 1), (1, âˆ’1, 2), (7, 3, 8)is linearly dependent in ğ…3 because 2(2, 3, 1)+ 3(1, âˆ’1, 2)+ (âˆ’1)(7, 3, 8) = (0, 0, 0). â€¢ The list (2, 3, 1), (1, âˆ’1, 2), (7, 3, ğ‘) is linearly dependent in ğ…3 if and only if ğ‘ = 8, as you should verify. â€¢ If some vector in a list of vectors in ğ‘‰ is a linear combination of the other vectors, then the list is linearly dependent. (Proof: After writing one vector in the list as equal to a linear combination of the other vectors, move that vector to the other side of the equation, where it will be multiplied by âˆ’1.) â€¢ Every list of vectors in ğ‘‰ containing the 0vector is linearly dependent. (This is a special case of the previous bullet point.) The next lemma is a terrific tool. It states that given a linearly dependent list of vectors, one of the vectors is in the span of the previous ones. Furthermore, we can throw out that vector without changing the span of the original list. 2.19 linear dependence lemma Suppose ğ‘£1, â€¦, ğ‘£ğ‘š is a linearly dependent list in ğ‘‰. Then there exists ğ‘˜ âˆˆ {1, 2, â€¦, ğ‘š} such that ğ‘£ğ‘˜ âˆˆ span(ğ‘£1, â€¦, ğ‘£ğ‘˜ âˆ’ 1). Furthermore, if ğ‘˜ satisfies the condition above and theğ‘˜th term is removed from ğ‘£1, â€¦, ğ‘£ğ‘š, then the span of the remaining list equals span(ğ‘£1, â€¦, ğ‘£ğ‘š). Proof Because the list ğ‘£1, â€¦, ğ‘£ğ‘š is linearly dependent, there exist numbers ğ‘1, â€¦, ğ‘ğ‘š âˆˆ ğ…, not all 0, such that ğ‘1ğ‘£1 + â‹¯ + ğ‘ğ‘šğ‘£ğ‘š = 0. Let ğ‘˜ be the largest element of {1, â€¦, ğ‘š} such that ğ‘ğ‘˜ â‰  0. Then ğ‘£ğ‘˜ = âˆ’ ğ‘1 ğ‘ğ‘˜ ğ‘£1 âˆ’ â‹¯ âˆ’ ğ‘ğ‘˜ âˆ’ 1 ğ‘ğ‘˜ ğ‘£ğ‘˜ âˆ’ 1, which proves that ğ‘£ğ‘˜ âˆˆ span(ğ‘£1, â€¦, ğ‘£ğ‘˜ âˆ’ 1), as desired. 34 Chapter 2 Finite-Dimensional Vector Spaces Now suppose ğ‘˜ is any element of {1, â€¦, ğ‘š} such that ğ‘£ğ‘˜ âˆˆ span(ğ‘£1, â€¦, ğ‘£ğ‘˜ âˆ’ 1). Let ğ‘1, â€¦, ğ‘ğ‘˜ âˆ’ 1 âˆˆ ğ… be such that 2.20 ğ‘£ğ‘˜ = ğ‘1ğ‘£1 + â‹¯ + ğ‘ğ‘˜ âˆ’ 1ğ‘£ğ‘˜ âˆ’ 1. Suppose ğ‘¢ âˆˆ span(ğ‘£1, â€¦, ğ‘£ğ‘š). Then there exist ğ‘1, â€¦, ğ‘ğ‘š âˆˆ ğ… such that ğ‘¢ = ğ‘1ğ‘£1 + â‹¯ + ğ‘ğ‘šğ‘£ğ‘š. In the equation above, we can replace ğ‘£ğ‘˜ with the right side of 2.20, which shows that ğ‘¢ is in the span of the list obtained by removing the ğ‘˜th term from ğ‘£1, â€¦, ğ‘£ğ‘š. Thus removing the ğ‘˜th term of the list ğ‘£1, â€¦, ğ‘£ğ‘š does not change the span of the list. If ğ‘˜ = 1in the linear dependence lemma, then ğ‘£ğ‘˜ âˆˆ span(ğ‘£1, â€¦, ğ‘£ğ‘˜ âˆ’ 1) means that ğ‘£1 = 0, because span( ) = {0}. Note also that parts of the proof of the linear dependence lemma need to be modified ifğ‘˜ = 1. In general, the proofs in the rest of the book will not call attention to special cases that must be considered involving lists of length 0, the subspace {0}, or other trivial cases for which the result is true but needs a slightly different proof. Be sure to check these special cases yourself. 2.21 example: smallest ğ‘˜ in linear dependence lemma Consider the list (1, 2, 3), (6, 5, 4), (15, 16, 17), (8, 9, 7) in ğ‘3. This list of length four is linearly dependent, as we will soon see. Thus the linear dependence lemma implies that there exists ğ‘˜ âˆˆ {1, 2, 3, 4}such that the ğ‘˜th vector in this list is a linear combination of the previous vectors in the list. Letâ€™s see how to find the smallest value ofğ‘˜ that works. Taking ğ‘˜ = 1in the linear dependence lemma works if and only if the first vector in the list equals 0. Because (1, 2, 3)is not the 0vector, we cannot take ğ‘˜ = 1for this list. Taking ğ‘˜ = 2in the linear dependence lemma works if and only if the second vector in the list is a scalar multiple of the first vector. However, there does not exist ğ‘ âˆˆ ğ‘ such that (6, 5, 4) = ğ‘(1, 2, 3). Thus we cannot take ğ‘˜ = 2for this list. Taking ğ‘˜ = 3in the linear dependence lemma works if and only if the third vector in the list is a linear combination of the first two vectors. Thus for the list in this example, we want to know whether there exist ğ‘, ğ‘ âˆˆ ğ‘ such that (15, 16, 17) = ğ‘(1, 2, 3)+ ğ‘(6, 5, 4). The equation above is equivalent to a system of three linear equations in the two unknowns ğ‘, ğ‘. Using Gaussian elimination or appropriate software, we find that ğ‘ = 3, ğ‘ = 2is a solution of the equation above, as you can verify. Thus for the list in this example, taking ğ‘˜ = 3is the smallest value of ğ‘˜ that works in the linear dependence lemma. Section 2A Span and Linear Independence 35 Now we come to a key result. It says that no linearly independent list in ğ‘‰ is longer than a spanning list in ğ‘‰. 2.22 length of linearly independent list â‰¤ length of spanning list In a finite-dimensional vector space, the length of every linearly independent list of vectors is less than or equal to the length of every spanning list of vectors. Proof Suppose that ğ‘¢1, â€¦, ğ‘¢ğ‘š is linearly independent in ğ‘‰. Suppose also that ğ‘¤1, â€¦, ğ‘¤ğ‘› spans ğ‘‰. We need to prove that ğ‘š â‰¤ ğ‘›. We do so through the process described below with ğ‘š steps; note that in each step we add one of the ğ‘¢â€™s and remove one of the ğ‘¤â€™s. Step 1 Let ğµ be the list ğ‘¤1, â€¦, ğ‘¤ğ‘›, which spans ğ‘‰. Adjoining ğ‘¢1 at the beginning of this list produces a linearly dependent list (because ğ‘¢1 can be written as a linear combination of ğ‘¤1, â€¦, ğ‘¤ğ‘š). In other words, the list ğ‘¢1, ğ‘¤1, â€¦, ğ‘¤ğ‘› is linearly dependent. Thus by the linear dependence lemma (2.19), one of the vectors in the list above is a linear combination of the previous vectors in the list. We know that ğ‘¢1 â‰  0 because the list ğ‘¢1, â€¦, ğ‘¢ğ‘š is linearly independent. Thus ğ‘¢1 is not in the span of the previous vectors in the list above (because ğ‘¢1 is not in {0}, which is the span of the empty list). Hence the linear dependence lemma implies that we can remove one of the ğ‘¤â€™s so that the new list ğµ (of length ğ‘›) consisting of ğ‘¢1 and the remaining ğ‘¤â€™s spans ğ‘‰. Step k, for k = 2, â€¦, m The list ğµ (of length ğ‘›) from step ğ‘˜ âˆ’ 1spans ğ‘‰. In particular, ğ‘¢ğ‘˜ is in the span of the list ğµ. Thus the list of length (ğ‘› + 1)obtained by adjoining ğ‘¢ğ‘˜ to ğµ, placing it just after ğ‘¢1, â€¦, ğ‘¢ğ‘˜ âˆ’ 1, is linearly dependent. By the linear dependence lemma (2.19), one of the vectors in this list is in the span of the previous ones, and because ğ‘¢1, â€¦, ğ‘¢ğ‘˜ is linearly independent, this vector cannot be one of the ğ‘¢â€™s. Hence there still must be at least one remaining ğ‘¤ at this step. We can remove from our new list (after adjoining ğ‘¢ğ‘˜ in the proper place) a ğ‘¤ that is a linear combination of the previous vectors in the list, so that the new list ğµ (of length ğ‘›) consisting of ğ‘¢1, â€¦, ğ‘¢ğ‘˜ and the remaining ğ‘¤â€™s spans ğ‘‰. After step ğ‘š, we have added all the ğ‘¢â€™s and the process stops. At each step as we add a ğ‘¢ to ğµ, the linear dependence lemma implies that there is some ğ‘¤ to remove. Thus there are at least as many ğ‘¤â€™s as ğ‘¢â€™s. 36 Chapter 2 Finite-Dimensional Vector Spaces The next two examples show how the result above can be used to show, without any computations, that certain lists are not linearly independent and that certain lists do not span a given vector space. 2.23 example: no list of length 4is linearly independent in ğ‘3 The list (1, 0, 0), (0, 1, 0), (0, 0, 1), which has length three, spans ğ‘3. Thus no list of length larger than three is linearly independent in ğ‘3. For example, we now know that (1, 2, 3), (4, 5, 8), (9, 6, 7), (âˆ’3, 2, 8), which is a list of length four, is not linearly independent in ğ‘3. 2.24 example: no list of length 3spans ğ‘4 The list (1, 0, 0, 0), (0, 1, 0, 0), (0, 0, 1, 0), (0, 0, 0, 1), which has length four, is linearly independent in ğ‘4. Thus no list of length less than four spans ğ‘4. For example, we now know that (1, 2, 3, âˆ’5), (4, 5, 8, 3), (9, 6, 7, âˆ’1), which is a list of length three, does not span ğ‘4. Our intuition suggests that every subspace of a finite-dimensional vector space should also be finite-dimensional. We now prove that this intuition is correct. 2.25 finite-dimensional subspaces Every subspace of a finite-dimensional vector space is finite-dimensional. Proof Suppose ğ‘‰ is finite-dimensional andğ‘ˆ is a subspace of ğ‘‰. We need to prove that ğ‘ˆ is finite-dimensional. We do this through the following multistep construction. Step 1 If ğ‘ˆ = {0}, then ğ‘ˆ is finite-dimensional and we are done. Ifğ‘ˆ â‰  {0}, then choose a nonzero vector ğ‘¢1 âˆˆ ğ‘ˆ. Step k If ğ‘ˆ = span(ğ‘¢1, â€¦, ğ‘¢ğ‘˜ âˆ’ 1), then ğ‘ˆ is finite-dimensional and we are done. If ğ‘ˆ â‰  span(ğ‘¢1, â€¦, ğ‘¢ğ‘˜ âˆ’ 1), then choose a vector ğ‘¢ğ‘˜ âˆˆ ğ‘ˆ such that ğ‘¢ğ‘˜ âˆ‰ span(ğ‘¢1, â€¦, ğ‘¢ğ‘˜ âˆ’ 1). After each step, as long as the process continues, we have constructed a list of vectors such that no vector in this list is in the span of the previous vectors. Thus after each step we have constructed a linearly independent list, by the linear dependence lemma (2.19). This linearly independent list cannot be longer than any spanning list of ğ‘‰ (by 2.22). Thus the process eventually terminates, which means that ğ‘ˆ is finite-dimensional. Section 2A Span and Linear Independence 37 Exercises 2A 1 Find a list of four distinct vectors in ğ…3 whose span equals {(ğ‘¥, ğ‘¦, ğ‘§) âˆˆ ğ…3 âˆ¶ ğ‘¥ + ğ‘¦ + ğ‘§ = 0}. 2 Prove or give a counterexample: If ğ‘£1, ğ‘£2, ğ‘£3, ğ‘£4 spans ğ‘‰, then the list ğ‘£1 âˆ’ ğ‘£2, ğ‘£2 âˆ’ ğ‘£3, ğ‘£3 âˆ’ ğ‘£4, ğ‘£4 also spans ğ‘‰. 3 Suppose ğ‘£1, â€¦, ğ‘£ğ‘š is a list of vectors in ğ‘‰. For ğ‘˜ âˆˆ {1, â€¦, ğ‘š}, let ğ‘¤ğ‘˜ = ğ‘£1 + â‹¯ + ğ‘£ğ‘˜. Show that span(ğ‘£1, â€¦, ğ‘£ğ‘š) = span(ğ‘¤1, â€¦, ğ‘¤ğ‘š). 4 (a) Show that a list of length one in a vector space is linearly independent if and only if the vector in the list is not 0. (b) Show that a list of length two in a vector space is linearly independent if and only if neither of the two vectors in the list is a scalar multiple of the other. 5 Find a number ğ‘¡ such that (3, 1, 4), (2, âˆ’3, 5), (5, 9, ğ‘¡) is not linearly independent in ğ‘3. 6 Show that the list (2, 3, 1), (1, âˆ’1, 2), (7, 3, ğ‘) is linearly dependent in ğ…3 if and only if ğ‘ = 8. 7 (a) Show that if we think of ğ‚ as a vector space over ğ‘, then the list 1+ ğ‘–, 1 âˆ’ ğ‘–is linearly independent. (b) Show that if we think of ğ‚ as a vector space over ğ‚, then the list 1+ ğ‘–, 1 âˆ’ ğ‘–is linearly dependent. 8 Suppose ğ‘£1, ğ‘£2, ğ‘£3, ğ‘£4 is linearly independent in ğ‘‰. Prove that the list ğ‘£1 âˆ’ ğ‘£2, ğ‘£2 âˆ’ ğ‘£3, ğ‘£3 âˆ’ ğ‘£4, ğ‘£4 is also linearly independent. 9 Prove or give a counterexample: If ğ‘£1, ğ‘£2, â€¦, ğ‘£ğ‘š is a linearly independent list of vectors in ğ‘‰, then 5ğ‘£1 âˆ’ 4ğ‘£2, ğ‘£2, ğ‘£3, â€¦, ğ‘£ğ‘š is linearly independent. 38 Chapter 2 Finite-Dimensional Vector Spaces 10 Prove or give a counterexample: If ğ‘£1, ğ‘£2, â€¦, ğ‘£ğ‘š is a linearly independent list of vectors in ğ‘‰ and ğœ† âˆˆ ğ… with ğœ† â‰  0, then ğœ†ğ‘£1, ğœ†ğ‘£2, â€¦, ğœ†ğ‘£ğ‘š is linearly independent. 11 Prove or give a counterexample: If ğ‘£1, â€¦, ğ‘£ğ‘š and ğ‘¤1, â€¦, ğ‘¤ğ‘š are linearly independent lists of vectors in ğ‘‰, then the list ğ‘£1 + ğ‘¤1, â€¦, ğ‘£ğ‘š + ğ‘¤ğ‘š is linearly independent. 12 Suppose ğ‘£1, â€¦, ğ‘£ğ‘š is linearly independent in ğ‘‰ and ğ‘¤ âˆˆ ğ‘‰. Prove that if ğ‘£1 + ğ‘¤, â€¦, ğ‘£ğ‘š + ğ‘¤ is linearly dependent, then ğ‘¤ âˆˆ span(ğ‘£1, â€¦, ğ‘£ğ‘š). 13 Suppose ğ‘£1, â€¦, ğ‘£ğ‘š is linearly independent in ğ‘‰ and ğ‘¤ âˆˆ ğ‘‰. Show that ğ‘£1, â€¦, ğ‘£ğ‘š, ğ‘¤ is linearly independent âŸº ğ‘¤ âˆ‰ span(ğ‘£1, â€¦, ğ‘£ğ‘š). 14 Suppose ğ‘£1, â€¦, ğ‘£ğ‘š is a list of vectors in ğ‘‰. For ğ‘˜ âˆˆ {1, â€¦, ğ‘š}, let ğ‘¤ğ‘˜ = ğ‘£1 + â‹¯ + ğ‘£ğ‘˜. Show that the list ğ‘£1, â€¦, ğ‘£ğ‘š is linearly independent if and only if the list ğ‘¤1, â€¦, ğ‘¤ğ‘š is linearly independent. 15 Explain why there does not exist a list of six polynomials that is linearly independent in ğ’«4(ğ…). 16 Explain why no list of four polynomials spans ğ’«4(ğ…). 17 Prove that ğ‘‰ is infinite-dimensional if and only if there is a sequenceğ‘£1, ğ‘£2, â€¦ of vectors in ğ‘‰ such that ğ‘£1, â€¦, ğ‘£ğ‘š is linearly independent for every positive integer ğ‘š. 18 Prove that ğ…âˆ is infinite-dimensional. 19 Prove that the real vector space of all continuous real-valued functions on the interval [0, 1]is infinite-dimensional. 20 Suppose ğ‘0, ğ‘1, â€¦, ğ‘ğ‘š are polynomials in ğ’«ğ‘š(ğ…) such that ğ‘ğ‘˜(2) = 0for each ğ‘˜ âˆˆ {0, â€¦, ğ‘š}. Prove that ğ‘0, ğ‘1, â€¦, ğ‘ğ‘š is not linearly independent in ğ’«ğ‘š(ğ…). Open Access This chapter is licensed under the terms of the Creative Commons Attribution-NonCommercial 4.0 International License (https://creativecommons.org/licenses/by-nc/4.0), which permits any noncommercial use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to original author and source, provide a link to the Creative Commons license, and indicate if changes were made. The images or other third party material in this chapter are included in the chapterâ€™s Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the chapterâ€™s Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. Section 2B Bases 39 2B Bases In the previous section, we discussed linearly independent lists and we also discussed spanning lists. Now we bring these concepts together by considering lists that have both properties. 2.26 definition:basis A basis of ğ‘‰ is a list of vectors in ğ‘‰ that is linearly independent and spans ğ‘‰. 2.27 example:bases (a) The list (1, 0, â€¦, 0), (0, 1, 0, â€¦, 0), â€¦, (0, â€¦, 0, 1)is a basis of ğ…ğ‘›, called the standard basis of ğ…ğ‘›. (b) The list (1, 2), (3, 5)is a basis of ğ…2. Note that this list has length two, which is the same as the length of the standard basis of ğ…2. In the next section, we will see that this is not a coincidence. (c) The list (1, 2, âˆ’4), (7, âˆ’5, 6)is linearly independent in ğ…3 but is not a basis of ğ…3 because it does not span ğ…3. (d) The list (1, 2), (3, 5), (4, 13)spans ğ…2 but is not a basis of ğ…2 because it is not linearly independent. (e) The list (1, 1, 0), (0, 0, 1)is a basis of {(ğ‘¥, ğ‘¥, ğ‘¦) âˆˆ ğ…3 âˆ¶ ğ‘¥, ğ‘¦ âˆˆ ğ…}. (f) The list (1, âˆ’1, 0), (1, 0, âˆ’1)is a basis of {(ğ‘¥, ğ‘¦, ğ‘§) âˆˆ ğ…3 âˆ¶ ğ‘¥ + ğ‘¦ + ğ‘§ = 0}. (g) The list 1, ğ‘§, â€¦, ğ‘§ ğ‘š is a basis of ğ’«ğ‘š(ğ…), called the standard basis of ğ’«ğ‘š(ğ…). In addition to the standard basis, ğ…ğ‘› has many other bases. For example, (7, 5), (âˆ’4, 9) and (1, 2), (3, 5) are both bases of ğ…2. The next result helps explain why bases are useful. Recall that â€œuniquelyâ€ means â€œin only one wayâ€. 2.28 criterion for basis A list ğ‘£1, â€¦, ğ‘£ğ‘› of vectors in ğ‘‰ is a basis of ğ‘‰ if and only if every ğ‘£ âˆˆ ğ‘‰ can be written uniquely in the form 2.29 ğ‘£ = ğ‘1ğ‘£1 + â‹¯ + ğ‘ğ‘›ğ‘£ğ‘›, where ğ‘1, â€¦, ğ‘ğ‘› âˆˆ ğ…. 40 Chapter 2 Finite-Dimensional Vector Spaces This proof is essentially a repetition of the ideas that led us to the definition of linear independence. Proof First suppose that ğ‘£1, â€¦, ğ‘£ğ‘› is a basis of ğ‘‰. Let ğ‘£ âˆˆ ğ‘‰. Because ğ‘£1, â€¦, ğ‘£ğ‘› spans ğ‘‰, there exist ğ‘1, â€¦, ğ‘ğ‘› âˆˆ ğ… such that 2.29 holds. To show that the repre- sentation in 2.29 is unique, suppose ğ‘1, â€¦, ğ‘ğ‘› are scalars such that we also have ğ‘£ = ğ‘1ğ‘£1 + â‹¯ + ğ‘ğ‘›ğ‘£ğ‘›. Subtracting the last equation from 2.29, we get 0 = (ğ‘1 âˆ’ ğ‘1)ğ‘£1 + â‹¯ + (ğ‘ğ‘› âˆ’ ğ‘ğ‘›)ğ‘£ğ‘›. This implies that each ğ‘ğ‘˜ âˆ’ ğ‘ğ‘˜ equals 0(because ğ‘£1, â€¦, ğ‘£ğ‘› is linearly independent). Hence ğ‘1 = ğ‘1, â€¦, ğ‘ğ‘› = ğ‘ğ‘›. We have the desired uniqueness, completing the proof in one direction. For the other direction, suppose every ğ‘£ âˆˆ ğ‘‰ can be written uniquely in the form given by 2.29. This implies that the list ğ‘£1, â€¦, ğ‘£ğ‘› spans ğ‘‰. To show that ğ‘£1, â€¦, ğ‘£ğ‘› is linearly independent, suppose ğ‘1, â€¦, ğ‘ğ‘› âˆˆ ğ… are such that 0 = ğ‘1ğ‘£1 + â‹¯ + ğ‘ğ‘›ğ‘£ğ‘›. The uniqueness of the representation 2.29 (taking ğ‘£ = 0) now implies that ğ‘1 = â‹¯ = ğ‘ğ‘› = 0. Thus ğ‘£1, â€¦, ğ‘£ğ‘› is linearly independent and hence is a basis of ğ‘‰. A spanning list in a vector space may not be a basis because it is not linearly independent. Our next result says that given any spanning list, some (possibly none) of the vectors in it can be discarded so that the remaining list is linearly independent and still spans the vector space. As an example in the vector space ğ…2, if the procedure in the proof below is applied to the list (1, 2), (3, 6), (4, 7), (5, 9), then the second and fourth vectors will be removed. This leaves (1, 2), (4, 7), which is a basis of ğ…2. 2.30 every spanning list contains a basis Every spanning list in a vector space can be reduced to a basis of the vector space. Proof Suppose ğ‘£1, â€¦, ğ‘£ğ‘› spans ğ‘‰. We want to remove some of the vectors from ğ‘£1, â€¦, ğ‘£ğ‘› so that the remaining vectors form a basis of ğ‘‰. We do this through the multistep process described below. Start with ğµ equal to the list ğ‘£1, â€¦, ğ‘£ğ‘›. Step 1 If ğ‘£1 = 0, then delete ğ‘£1 from ğµ. If ğ‘£1 â‰  0, then leave ğµ unchanged. Step k If ğ‘£ğ‘˜ is in span(ğ‘£1, â€¦, ğ‘£ğ‘˜ âˆ’ 1), then delete ğ‘£ğ‘˜ from the list ğµ. If ğ‘£ğ‘˜ is not in span(ğ‘£1, â€¦, ğ‘£ğ‘˜ âˆ’ 1), then leave ğµ unchanged. Section 2B Bases 41 Stop the process after step ğ‘›, getting a list ğµ. This list ğµ spans ğ‘‰ because our original list spanned ğ‘‰ and we have discarded only vectors that were already in the span of the previous vectors. The process ensures that no vector in ğµ is in the span of the previous ones. Thus ğµ is linearly independent, by the linear dependence lemma (2.19). Hence ğµ is a basis of ğ‘‰. We now come to an important corollary of the previous result. 2.31 basis of finite-dimensional vector space Every finite-dimensional vector space has a basis. Proof By definition, a finite-dimensional vector space has a spanning list. The previous result tells us that each spanning list can be reduced to a basis. Our next result is in some sense a dual of 2.30, which said that every spanning list can be reduced to a basis. Now we show that given any linearly independent list, we can adjoin some additional vectors (this includes the possibility of adjoining no additional vectors) so that the extended list is still linearly independent but also spans the space. 2.32 every linearly independent list extends to a basis Every linearly independent list of vectors in a finite-dimensional vector space can be extended to a basis of the vector space. Proof Suppose ğ‘¢1, â€¦, ğ‘¢ğ‘š is linearly independent in a finite-dimensional vector space ğ‘‰. Let ğ‘¤1, â€¦, ğ‘¤ğ‘› be a list of vectors in ğ‘‰ that spans ğ‘‰. Thus the list ğ‘¢1, â€¦, ğ‘¢ğ‘š, ğ‘¤1, â€¦, ğ‘¤ğ‘› spans ğ‘‰. Applying the procedure of the proof of 2.30 to reduce this list to a basis of ğ‘‰ produces a basis consisting of the vectors ğ‘¢1, â€¦, ğ‘¢ğ‘š and some of the ğ‘¤â€™s (none of the ğ‘¢â€™s get deleted in this procedure because ğ‘¢1, â€¦, ğ‘¢ğ‘š is linearly independent). As an example in ğ…3, suppose we start with the linearly independent list (2, 3, 4), (9, 6, 8). If we take ğ‘¤1, ğ‘¤2, ğ‘¤3 to be the standard basis of ğ…3, then applying the procedure in the proof above produces the list (2, 3, 4), (9, 6, 8), (0, 1, 0), which is a basis of ğ…3. Using the same ideas but more ad- vanced tools, the next result can be proved without the hypothesis that ğ‘‰ is finite-dimensional. As an application of the result above, we now show that every subspace of a finite-dimensional vector space can be paired with another subspace to form a direct sum of the whole space. 42 Chapter 2 Finite-Dimensional Vector Spaces 2.33 every subspace of ğ‘‰ is part of a direct sum equal to ğ‘‰ Suppose ğ‘‰ is finite-dimensional andğ‘ˆ is a subspace of ğ‘‰. Then there is a subspace ğ‘Š of ğ‘‰ such that ğ‘‰ = ğ‘ˆ âŠ• ğ‘Š. Proof Because ğ‘‰ is finite-dimensional, so isğ‘ˆ (see 2.25). Thus there is a basis ğ‘¢1, â€¦, ğ‘¢ğ‘š of ğ‘ˆ (by 2.31). Of course ğ‘¢1, â€¦, ğ‘¢ğ‘š is a linearly independent list of vectors in ğ‘‰. Hence this list can be extended to a basis ğ‘¢1, â€¦, ğ‘¢ğ‘š, ğ‘¤1, â€¦, ğ‘¤ğ‘› of ğ‘‰ (by 2.32). Let ğ‘Š = span(ğ‘¤1, â€¦, ğ‘¤ğ‘›). To prove that ğ‘‰ = ğ‘ˆ âŠ• ğ‘Š, by 1.46 we only need to show that ğ‘‰ = ğ‘ˆ + ğ‘Š and ğ‘ˆ âˆ© ğ‘Š = {0}. To prove the first equation above, supposeğ‘£ âˆˆ ğ‘‰. Then, because the list ğ‘¢1, â€¦, ğ‘¢ğ‘š, ğ‘¤1, â€¦, ğ‘¤ğ‘› spans ğ‘‰, there exist ğ‘1, â€¦, ğ‘ğ‘š, ğ‘1, â€¦, ğ‘ğ‘› âˆˆ ğ… such that ğ‘£ = ğ‘1ğ‘¢1 + â‹¯ + ğ‘ğ‘šğ‘¢ğ‘šâŸâŸâŸâŸâŸâŸâŸ ğ‘¢ + ğ‘1ğ‘¤1 + â‹¯ + ğ‘ğ‘›ğ‘¤ğ‘›âŸâŸâŸâŸâŸâŸâŸ ğ‘¤ . We have ğ‘£ = ğ‘¢ + ğ‘¤, where ğ‘¢ âˆˆ ğ‘ˆ and ğ‘¤ âˆˆ ğ‘Š are defined as above. Thus ğ‘£ âˆˆ ğ‘ˆ + ğ‘Š, completing the proof that ğ‘‰ = ğ‘ˆ + ğ‘Š. To show that ğ‘ˆ âˆ© ğ‘Š = {0}, suppose ğ‘£ âˆˆ ğ‘ˆ âˆ© ğ‘Š. Then there exist scalars ğ‘1, â€¦, ğ‘ğ‘š, ğ‘1, â€¦, ğ‘ğ‘› âˆˆ ğ… such that ğ‘£ = ğ‘1ğ‘¢1 + â‹¯ + ğ‘ğ‘šğ‘¢ğ‘š = ğ‘1ğ‘¤1 + â‹¯ + ğ‘ğ‘›ğ‘¤ğ‘›. Thus ğ‘1ğ‘¢1 + â‹¯ + ğ‘ğ‘šğ‘¢ğ‘š âˆ’ ğ‘1ğ‘¤1 âˆ’ â‹¯ âˆ’ ğ‘ğ‘›ğ‘¤ğ‘› = 0. Because ğ‘¢1, â€¦, ğ‘¢ğ‘š, ğ‘¤1, â€¦, ğ‘¤ğ‘› is linearly independent, this implies that ğ‘1 = â‹¯ = ğ‘ğ‘š = ğ‘1 = â‹¯ = ğ‘ğ‘› = 0. Thus ğ‘£ = 0, completing the proof that ğ‘ˆ âˆ© ğ‘Š = {0}. Exercises 2B 1 Find all vector spaces that have exactly one basis. 2 Verify all assertions in Example 2.27. 3 (a) Let ğ‘ˆ be the subspace of ğ‘5 defined by ğ‘ˆ = {(ğ‘¥1, ğ‘¥2, ğ‘¥3, ğ‘¥4, ğ‘¥5) âˆˆ ğ‘5 âˆ¶ ğ‘¥1 = 3ğ‘¥2 and ğ‘¥3 = 7ğ‘¥4}. Find a basis of ğ‘ˆ. (b) Extend the basis in (a) to a basis of ğ‘5. (c) Find a subspace ğ‘Š of ğ‘5 such that ğ‘5 = ğ‘ˆ âŠ• ğ‘Š. Section 2B Bases 43 4 (a) Let ğ‘ˆ be the subspace of ğ‚ 5 defined by ğ‘ˆ = {(ğ‘§1, ğ‘§2, ğ‘§3, ğ‘§4, ğ‘§5) âˆˆ ğ‚5 âˆ¶ 6ğ‘§1 = ğ‘§2 and ğ‘§3 + 2ğ‘§4 + 3ğ‘§5 = 0}. Find a basis of ğ‘ˆ. (b) Extend the basis in (a) to a basis of ğ‚ 5. (c) Find a subspace ğ‘Š of ğ‚ 5 such that ğ‚ 5 = ğ‘ˆ âŠ• ğ‘Š. 5 Suppose ğ‘‰ is finite-dimensional andğ‘ˆ, ğ‘Š are subspaces of ğ‘‰ such that ğ‘‰ = ğ‘ˆ + ğ‘Š. Prove that there exists a basis of ğ‘‰ consisting of vectors in ğ‘ˆ âˆª ğ‘Š. 6 Prove or give a counterexample: If ğ‘0, ğ‘1, ğ‘2, ğ‘3 is a list in ğ’«3(ğ…) such that none of the polynomials ğ‘0, ğ‘1, ğ‘2, ğ‘3 has degree 2, then ğ‘0, ğ‘1, ğ‘2, ğ‘3 is not a basis of ğ’«3(ğ…). 7 Suppose ğ‘£1, ğ‘£2, ğ‘£3, ğ‘£4 is a basis of ğ‘‰. Prove that ğ‘£1 + ğ‘£2, ğ‘£2 + ğ‘£3, ğ‘£3 + ğ‘£4, ğ‘£4 is also a basis of ğ‘‰. 8 Prove or give a counterexample: If ğ‘£1, ğ‘£2, ğ‘£3, ğ‘£4 is a basis of ğ‘‰ and ğ‘ˆ is a subspace of ğ‘‰ such that ğ‘£1, ğ‘£2 âˆˆ ğ‘ˆ and ğ‘£3 âˆ‰ ğ‘ˆ and ğ‘£4 âˆ‰ ğ‘ˆ, then ğ‘£1, ğ‘£2 is a basis of ğ‘ˆ. 9 Suppose ğ‘£1, â€¦, ğ‘£ğ‘š is a list of vectors in ğ‘‰. For ğ‘˜ âˆˆ {1, â€¦, ğ‘š}, let ğ‘¤ğ‘˜ = ğ‘£1 + â‹¯ + ğ‘£ğ‘˜. Show that ğ‘£1, â€¦, ğ‘£ğ‘š is a basis of ğ‘‰ if and only if ğ‘¤1, â€¦, ğ‘¤ğ‘š is a basis of ğ‘‰. 10 Suppose ğ‘ˆ and ğ‘Š are subspaces of ğ‘‰ such that ğ‘‰ = ğ‘ˆ âŠ• ğ‘Š. Suppose also that ğ‘¢1, â€¦, ğ‘¢ğ‘š is a basis of ğ‘ˆ and ğ‘¤1, â€¦, ğ‘¤ğ‘› is a basis of ğ‘Š. Prove that ğ‘¢1, â€¦, ğ‘¢ğ‘š, ğ‘¤1, â€¦, ğ‘¤ğ‘› is a basis of ğ‘‰. 11 Suppose ğ‘‰ is a real vector space. Show that if ğ‘£1, â€¦, ğ‘£ğ‘› is a basis of ğ‘‰ (as a real vector space), then ğ‘£1, â€¦, ğ‘£ğ‘› is also a basis of the complexificationğ‘‰ğ‚ (as a complex vector space). See Exercise 8 in Section 1B for the definition of the complexification ğ‘‰ğ‚. 44 Chapter 2 Finite-Dimensional Vector Spaces 2C Dimension Although we have been discussing finite-dimensional vector spaces, we have not yet defined the dimension of such an object. How should dimension be defined? A reasonable definition should force the dimension ofğ…ğ‘› to equal ğ‘›. Notice that the standard basis (1, 0, â€¦, 0), (0, 1, 0, â€¦, 0), â€¦, (0, â€¦, 0, 1) of ğ…ğ‘› has length ğ‘›. Thus we are tempted to define the dimension as the length of a basis. However, a finite-dimensional vector space in general has many different bases, and our attempted definition makes sense only if all bases in a given vector space have the same length. Fortunately that turns out to be the case, as we now show. 2.34 basis length does not depend on basis Any two bases of a finite-dimensional vector space have the same length. Proof Suppose ğ‘‰ is finite-dimensional. Letğµ1 and ğµ2 be two bases of ğ‘‰. Then ğµ1 is linearly independent in ğ‘‰ and ğµ2 spans ğ‘‰, so the length of ğµ1 is at most the length of ğµ2 (by 2.22). Interchanging the roles of ğµ1 and ğµ2, we also see that the length of ğµ2 is at most the length of ğµ1. Thus the length of ğµ1 equals the length of ğµ2, as desired. Now that we know that any two bases of a finite-dimensional vector space have the same length, we can formally define the dimension of such spaces. 2.35 definition: dimension, dim ğ‘‰ â€¢ The dimension of a finite-dimensional vector space is the length of any basis of the vector space. â€¢ The dimension of a finite-dimensional vector spaceğ‘‰ is denoted by dim ğ‘‰. 2.36 example:dimensions â€¢ dim ğ…ğ‘› = ğ‘› because the standard basis of ğ…ğ‘› has length ğ‘›. â€¢ dim ğ’«ğ‘š(ğ…) = ğ‘š + 1because the standard basis 1, ğ‘§, â€¦, ğ‘§ ğ‘š of ğ’«ğ‘š(ğ…) has length ğ‘š + 1. â€¢ If ğ‘ˆ = {(ğ‘¥, ğ‘¥, ğ‘¦) âˆˆ ğ…3 âˆ¶ ğ‘¥, ğ‘¦ âˆˆ ğ…}, then dim ğ‘ˆ = 2because (1, 1, 0), (0, 0, 1)is a basis of ğ‘ˆ. â€¢ If ğ‘ˆ = {(ğ‘¥, ğ‘¦, ğ‘§) âˆˆ ğ…3 âˆ¶ ğ‘¥ + ğ‘¦ + ğ‘§ = 0}, then dim ğ‘ˆ = 2because the list (1, âˆ’1, 0), (1, 0, âˆ’1)is a basis of ğ‘ˆ. Section 2C Dimension 45 Every subspace of a finite-dimensional vector space is finite-dimensional (by 2.25) and so has a dimension. The next result gives the expected inequality about the dimension of a subspace. 2.37 dimension of a subspace If ğ‘‰ is finite-dimensional andğ‘ˆ is a subspace of ğ‘‰, then dim ğ‘ˆ â‰¤dim ğ‘‰. Proof Suppose ğ‘‰ is finite-dimensional andğ‘ˆ is a subspace of ğ‘‰. Think of a basis of ğ‘ˆ as a linearly independent list in ğ‘‰, and think of a basis of ğ‘‰ as a spanning list in ğ‘‰. Now use 2.22 to conclude that dim ğ‘ˆ â‰¤dim ğ‘‰. The real vector space ğ‘2 has dimen- sion two; the complex vector space ğ‚ has dimension one. As sets, ğ‘2 can be identified with ğ‚ (and addition is the same on both spaces, as is scalar multiplication by real numbers). Thus when we talk about the dimension of a vector space, the role played by the choice of ğ… cannot be neglected. To check that a list of vectors in ğ‘‰ is a basis of ğ‘‰, we must, according to the definition, show that the list in ques- tion satisfies two properties: it must be linearly independent and it must span ğ‘‰. The next two results show that if the list in question has the right length, then we only need to check that it satisfies one of the two required properties. First we prove that every linearly independent list of the right length is a basis. 2.38 linearly independent list of the right length is a basis Suppose ğ‘‰ is finite-dimensional. Then every linearly independent list of vectors in ğ‘‰ of length dim ğ‘‰ is a basis of ğ‘‰. Proof Suppose dim ğ‘‰ = ğ‘› and ğ‘£1, â€¦, ğ‘£ğ‘› is linearly independent in ğ‘‰. The list ğ‘£1, â€¦, ğ‘£ğ‘› can be extended to a basis of ğ‘‰ (by 2.32). However, every basis of ğ‘‰ has length ğ‘›, so in this case the extension is the trivial one, meaning that no elements are adjoined to ğ‘£1, â€¦, ğ‘£ğ‘›. Thus ğ‘£1, â€¦, ğ‘£ğ‘› is a basis of ğ‘‰, as desired. The next result is a useful consequence of the previous result. 2.39 subspace of full dimension equals the whole space Suppose that ğ‘‰ is finite-dimensional andğ‘ˆ is a subspace of ğ‘‰ such that dim ğ‘ˆ = dim ğ‘‰. Then ğ‘ˆ = ğ‘‰. Proof Let ğ‘¢1, â€¦, ğ‘¢ğ‘› be a basis of ğ‘ˆ. Thus ğ‘› = dim ğ‘ˆ, and by hypothesis we also have ğ‘› = dim ğ‘‰. Thus ğ‘¢1, â€¦, ğ‘¢ğ‘› is a linearly independent list of vectors in ğ‘‰ (because it is a basis of ğ‘ˆ) of length dim ğ‘‰. From 2.38, we see that ğ‘¢1, â€¦, ğ‘¢ğ‘› is a basis of ğ‘‰. In particular every vector in ğ‘‰ is a linear combination of ğ‘¢1, â€¦, ğ‘¢ğ‘›. Thus ğ‘ˆ = ğ‘‰. 46 Chapter 2 Finite-Dimensional Vector Spaces 2.40 example: a basis of ğ…2 Consider the list (5, 7), (4, 3)of vectors in ğ…2. This list of length two is linearly independent in ğ…2 (because neither vector is a scalar multiple of the other). Note that ğ…2 has dimension two. Thus 2.38 implies that the linearly independent list (5, 7), (4, 3)of length two is a basis of ğ…2 (we do not need to bother checking that it spans ğ…2). 2.41 example: a basis of a subspace of ğ’«3(ğ‘) Let ğ‘ˆ be the subspace of ğ’«3(ğ‘) defined by ğ‘ˆ = {ğ‘ âˆˆ ğ’«3(ğ‘) âˆ¶ ğ‘ â€²(5) = 0}. To find a basis ofğ‘ˆ, first note that each of the polynomials1, (ğ‘¥ âˆ’ 5) 2, and (ğ‘¥ âˆ’ 5) 3 is in ğ‘ˆ. Suppose ğ‘, ğ‘, ğ‘ âˆˆ ğ‘ and ğ‘ + ğ‘(ğ‘¥ âˆ’ 5) 2 + ğ‘(ğ‘¥ âˆ’ 5) 3 = 0 for every ğ‘¥ âˆˆ ğ‘. Without explicitly expanding the left side of the equation above, we can see that the left side has a ğ‘ğ‘¥3 term. Because the right side has no ğ‘¥3 term, this implies that ğ‘ = 0. Because ğ‘ = 0, we see that the left side has a ğ‘ğ‘¥2 term, which implies that ğ‘ = 0. Because ğ‘ = ğ‘ = 0, we can also conclude that ğ‘ = 0. Thus the equation above implies that ğ‘ = ğ‘ = ğ‘ = 0. Hence the list 1, (ğ‘¥ âˆ’ 5) 2, (ğ‘¥ âˆ’ 5) 3 is linearly independent in ğ‘ˆ. Thus 3 â‰¤dim ğ‘ˆ. Hence 3 â‰¤dim ğ‘ˆ â‰¤dim ğ’«3(ğ‘) = 4, where we have used 2.37. The polynomial ğ‘¥ is not in ğ‘ˆ because its derivative is the constant function 1. Thus ğ‘ˆ â‰  ğ’«3(ğ‘). Hence dim ğ‘ˆ â‰  4(by 2.39). The inequality above now implies that dim ğ‘ˆ = 3. Thus the linearly independent list 1, (ğ‘¥ âˆ’ 5) 2, (ğ‘¥ âˆ’ 5) 3 in ğ‘ˆ has length dim ğ‘ˆ and hence is a basis of ğ‘ˆ (by 2.38). Now we prove that a spanning list of the right length is a basis. 2.42 spanning list of the right length is a basis Suppose ğ‘‰ is finite-dimensional. Then every spanning list of vectors inğ‘‰ of length dim ğ‘‰ is a basis of ğ‘‰. Proof Suppose dim ğ‘‰ = ğ‘› and ğ‘£1, â€¦, ğ‘£ğ‘› spans ğ‘‰. The list ğ‘£1, â€¦, ğ‘£ğ‘› can be reduced to a basis of ğ‘‰ (by 2.30). However, every basis of ğ‘‰ has length ğ‘›, so in this case the reduction is the trivial one, meaning that no elements are deleted from ğ‘£1, â€¦, ğ‘£ğ‘›. Thus ğ‘£1, â€¦, ğ‘£ğ‘› is a basis of ğ‘‰, as desired. Section 2C Dimension 47 The next result gives a formula for the dimension of the sum of two subspaces of a finite-dimensional vector space. This formula is analogous to a familiar counting formula: the number of elements in the union of two finite sets equals the number of elements in the first set, plus the number of elements in the second set, minus the number of elements in the intersection of the two sets. 2.43 dimension of a sum If ğ‘‰1 and ğ‘‰2 are subspaces of a finite-dimensional vector space, then dim(ğ‘‰1 + ğ‘‰2) = dim ğ‘‰1 + dim ğ‘‰2 âˆ’ dim(ğ‘‰1 âˆ© ğ‘‰2). Proof Let ğ‘£1, â€¦, ğ‘£ğ‘š be a basis of ğ‘‰1 âˆ© ğ‘‰2; thus dim(ğ‘‰1 âˆ© ğ‘‰2) = ğ‘š. Because ğ‘£1, â€¦, ğ‘£ğ‘š is a basis of ğ‘‰1 âˆ© ğ‘‰2, it is linearly independent in ğ‘‰1. Hence this list can be extended to a basis ğ‘£1, â€¦, ğ‘£ğ‘š, ğ‘¢1, â€¦, ğ‘¢ğ‘— of ğ‘‰1 (by 2.32). Thus dim ğ‘‰1 = ğ‘š + ğ‘—. Also extend ğ‘£1, â€¦, ğ‘£ğ‘š to a basis ğ‘£1, â€¦, ğ‘£ğ‘š, ğ‘¤1, â€¦, ğ‘¤ğ‘˜ of ğ‘‰2; thus dim ğ‘‰2 = ğ‘š + ğ‘˜. We will show that 2.44 ğ‘£1, â€¦, ğ‘£ğ‘š, ğ‘¢1, â€¦, ğ‘¢ğ‘—, ğ‘¤1, â€¦, ğ‘¤ğ‘˜ is a basis of ğ‘‰1 + ğ‘‰2. This will complete the proof, because then we will have dim(ğ‘‰1 + ğ‘‰2) = ğ‘š + ğ‘— + ğ‘˜ = (ğ‘š + ğ‘—) + (ğ‘š + ğ‘˜) âˆ’ ğ‘š = dim ğ‘‰1 + dim ğ‘‰2 âˆ’ dim(ğ‘‰1 âˆ© ğ‘‰2). The list 2.44 is contained in ğ‘‰1 âˆª ğ‘‰2 and thus is contained in ğ‘‰1 + ğ‘‰2. The span of this list contains ğ‘‰1 and contains ğ‘‰2 and hence is equal to ğ‘‰1 + ğ‘‰2. Thus to show that 2.44 is a basis of ğ‘‰1 + ğ‘‰2 we only need to show that it is linearly independent. To prove that 2.44 is linearly independent, suppose ğ‘1ğ‘£1 + â‹¯ + ğ‘ğ‘šğ‘£ğ‘š + ğ‘1ğ‘¢1 + â‹¯ + ğ‘ğ‘—ğ‘¢ğ‘— + ğ‘1ğ‘¤1 + â‹¯ + ğ‘ğ‘˜ğ‘¤ğ‘˜ = 0, where all the ğ‘â€™s, ğ‘â€™s, and ğ‘â€™s are scalars. We need to prove that all the ğ‘â€™s, ğ‘â€™s, and ğ‘â€™s equal 0. The equation above can be rewritten as 2.45 ğ‘1ğ‘¤1 + â‹¯ + ğ‘ğ‘˜ğ‘¤ğ‘˜ = âˆ’ğ‘1ğ‘£1 âˆ’ â‹¯ âˆ’ ğ‘ğ‘šğ‘£ğ‘š âˆ’ ğ‘1ğ‘¢1 âˆ’ â‹¯ âˆ’ ğ‘ğ‘—ğ‘¢ğ‘—, which shows that ğ‘1ğ‘¤1 + â‹¯ + ğ‘ğ‘˜ğ‘¤ğ‘˜ âˆˆ ğ‘‰1. All the ğ‘¤â€™s are in ğ‘‰2, so this implies that ğ‘1ğ‘¤1 + â‹¯ + ğ‘ğ‘˜ğ‘¤ğ‘˜ âˆˆ ğ‘‰1 âˆ© ğ‘‰2. Because ğ‘£1, â€¦, ğ‘£ğ‘š is a basis of ğ‘‰1 âˆ© ğ‘‰2, we have ğ‘1ğ‘¤1 + â‹¯ + ğ‘ğ‘˜ğ‘¤ğ‘˜ = ğ‘‘1ğ‘£1 + â‹¯ + ğ‘‘ğ‘šğ‘£ğ‘š for some scalars ğ‘‘1, â€¦, ğ‘‘ğ‘š. But ğ‘£1, â€¦, ğ‘£ğ‘š, ğ‘¤1, â€¦, ğ‘¤ğ‘˜ is linearly independent, so the last equation implies that all the ğ‘â€™s (and ğ‘‘â€™s) equal 0. Thus 2.45 becomes the equation ğ‘1ğ‘£1 + â‹¯ + ğ‘ğ‘šğ‘£ğ‘š + ğ‘1ğ‘¢1 + â‹¯ + ğ‘ğ‘—ğ‘¢ğ‘— = 0. Because the list ğ‘£1, â€¦, ğ‘£ğ‘š, ğ‘¢1, â€¦, ğ‘¢ğ‘— is linearly independent, this equation implies that all the ğ‘â€™s and ğ‘â€™s are 0, completing the proof. 48 Chapter 2 Finite-Dimensional Vector Spaces For ğ‘† a finite set, let#ğ‘† denote the number of elements of ğ‘†. The table below compares finite sets with finite-dimensional vector spaces, showing the analogy between #ğ‘† (for sets) and dim ğ‘‰ (for vector spaces), as well as the analogy between unions of subsets (in the context of sets) and sums of subspaces (in the context of vector spaces). sets vector spaces ğ‘† is a finite set ğ‘‰ is a finite-dimensional vector space #ğ‘† dim ğ‘‰ for subsets ğ‘†1, ğ‘†2 of ğ‘†, the union ğ‘†1 âˆª ğ‘†2 is the smallest subset of ğ‘† containing ğ‘†1 and ğ‘†2 for subspaces ğ‘‰1, ğ‘‰2 of ğ‘‰, the sum ğ‘‰1 +ğ‘‰2 is the smallest subspace of ğ‘‰ containing ğ‘‰1 and ğ‘‰2 #(ğ‘†1 âˆª ğ‘†2) dim(ğ‘‰1 + ğ‘‰2) = #ğ‘†1 + #ğ‘†2 âˆ’ #(ğ‘†1 âˆ© ğ‘†2) = dim ğ‘‰1 + dim ğ‘‰2 âˆ’ dim(ğ‘‰1 âˆ© ğ‘‰2) #(ğ‘†1 âˆª ğ‘†2) = #ğ‘†1 + #ğ‘†2 dim(ğ‘‰1 + ğ‘‰2) = dim ğ‘‰1 + dim ğ‘‰2 âŸº ğ‘†1 âˆ© ğ‘†2 = âˆ… âŸº ğ‘‰1 âˆ© ğ‘‰2 = {0} ğ‘†1 âˆª â‹¯ âˆª ğ‘†ğ‘š is a disjoint union âŸº #(ğ‘†1 âˆª â‹¯ âˆª ğ‘†ğ‘š) = #ğ‘†1 + â‹¯ + #ğ‘†ğ‘š ğ‘‰1 + â‹¯ + ğ‘‰ğ‘š is a direct sum âŸº dim(ğ‘‰1 + â‹¯ + ğ‘‰ğ‘š) = dim ğ‘‰1 + â‹¯ + dim ğ‘‰ğ‘š The last row above focuses on the analogy between disjoint unions (for sets) and direct sums (for vector spaces). The proof of the result in the last box above will be given in 3.94. You should be able to find results about sets that correspond, via analogy, to the results about vector spaces in Exercises 12 through 18. Exercises 2C 1 Show that the subspaces of ğ‘2 are precisely {0}, all lines in ğ‘2 containing the origin, and ğ‘2. 2 Show that the subspaces of ğ‘3 are precisely {0}, all lines in ğ‘3 containing the origin, all planes in ğ‘3 containing the origin, and ğ‘3. 3 (a) Let ğ‘ˆ = {ğ‘ âˆˆ ğ’«4(ğ…) âˆ¶ ğ‘(6) = 0}. Find a basis of ğ‘ˆ. (b) Extend the basis in (a) to a basis of ğ’«4(ğ…). (c) Find a subspace ğ‘Š of ğ’«4(ğ…) such that ğ’«4(ğ…) = ğ‘ˆ âŠ• ğ‘Š. 4 (a) Let ğ‘ˆ = {ğ‘ âˆˆ ğ’«4(ğ‘) âˆ¶ ğ‘ â€³(6) = 0}. Find a basis of ğ‘ˆ. (b) Extend the basis in (a) to a basis of ğ’«4(ğ‘). (c) Find a subspace ğ‘Š of ğ’«4(ğ‘) such that ğ’«4(ğ‘) = ğ‘ˆ âŠ• ğ‘Š. 5 (a) Let ğ‘ˆ = {ğ‘ âˆˆ ğ’«4(ğ…) âˆ¶ ğ‘(2) = ğ‘(5)}. Find a basis of ğ‘ˆ. (b) Extend the basis in (a) to a basis of ğ’«4(ğ…). (c) Find a subspace ğ‘Š of ğ’«4(ğ…) such that ğ’«4(ğ…) = ğ‘ˆ âŠ• ğ‘Š. Section 2C Dimension 49 6 (a) Let ğ‘ˆ = {ğ‘ âˆˆ ğ’«4(ğ…) âˆ¶ ğ‘(2) = ğ‘(5) = ğ‘(6)}. Find a basis of ğ‘ˆ. (b) Extend the basis in (a) to a basis of ğ’«4(ğ…). (c) Find a subspace ğ‘Š of ğ’«4(ğ…) such that ğ’«4(ğ…) = ğ‘ˆ âŠ• ğ‘Š. 7 (a) Let ğ‘ˆ = {ğ‘ âˆˆ ğ’«4(ğ‘) âˆ¶ âˆ« 1 âˆ’1 ğ‘ = 0}. Find a basis of ğ‘ˆ. (b) Extend the basis in (a) to a basis of ğ’«4(ğ‘). (c) Find a subspace ğ‘Š of ğ’«4(ğ‘) such that ğ’«4(ğ‘) = ğ‘ˆ âŠ• ğ‘Š. 8 Suppose ğ‘£1, â€¦, ğ‘£ğ‘š is linearly independent in ğ‘‰ and ğ‘¤ âˆˆ ğ‘‰. Prove that dim span(ğ‘£1 + ğ‘¤, â€¦, ğ‘£ğ‘š + ğ‘¤) â‰¥ ğ‘š âˆ’ 1. 9 Suppose ğ‘š is a positive integer and ğ‘0, ğ‘1, â€¦, ğ‘ğ‘š âˆˆ ğ’«(ğ…) are such that each ğ‘ğ‘˜ has degree ğ‘˜. Prove that ğ‘0, ğ‘1, â€¦, ğ‘ğ‘š is a basis of ğ’«ğ‘š(ğ…). 10 Suppose ğ‘š is a positive integer. For 0 â‰¤ ğ‘˜ â‰¤ ğ‘š, let ğ‘ğ‘˜(ğ‘¥) = ğ‘¥ğ‘˜(1 âˆ’ ğ‘¥) ğ‘š âˆ’ ğ‘˜. Show that ğ‘0, â€¦, ğ‘ğ‘š is a basis of ğ’«ğ‘š(ğ…). The basis in this exercise leads to what are called Bernstein polynomials. You can do a web search to learn how Bernstein polynomials are used to approximate continuous functions on [0, 1]. 11 Suppose ğ‘ˆ and ğ‘Š are both four-dimensional subspaces of ğ‚ 6. Prove that there exist two vectors in ğ‘ˆ âˆ© ğ‘Šsuch that neither of these vectors is a scalar multiple of the other. 12 Suppose that ğ‘ˆ and ğ‘Š are subspaces of ğ‘8 such that dim ğ‘ˆ = 3, dim ğ‘Š = 5, and ğ‘ˆ + ğ‘Š = ğ‘8. Prove that ğ‘8 = ğ‘ˆ âŠ• ğ‘Š. 13 Suppose ğ‘ˆ and ğ‘Š are both five-dimensional subspaces ofğ‘9. Prove that ğ‘ˆ âˆ© ğ‘Š â‰  {0}. 14 Suppose ğ‘‰ is a ten-dimensional vector space and ğ‘‰1, ğ‘‰2, ğ‘‰3 are subspaces of ğ‘‰ with dim ğ‘‰1 = dim ğ‘‰2 = dim ğ‘‰3 = 7. Prove that ğ‘‰1 âˆ© ğ‘‰2 âˆ© ğ‘‰3 â‰  {0}. 15 Suppose ğ‘‰ is finite-dimensional andğ‘‰1, ğ‘‰2, ğ‘‰3 are subspaces of ğ‘‰ with dim ğ‘‰1 + dim ğ‘‰2 + dim ğ‘‰3 > 2dim ğ‘‰. Prove that ğ‘‰1 âˆ© ğ‘‰2 âˆ© ğ‘‰3 â‰  {0}. 16 Suppose ğ‘‰ is finite-dimensional andğ‘ˆ is a subspace of ğ‘‰ with ğ‘ˆ â‰  ğ‘‰. Let ğ‘› = dim ğ‘‰ and ğ‘š = dim ğ‘ˆ. Prove that there exist ğ‘› âˆ’ ğ‘š subspaces of ğ‘‰, each of dimension ğ‘› âˆ’ 1, whose intersection equals ğ‘ˆ. 17 Suppose that ğ‘‰1, â€¦, ğ‘‰ğ‘š are finite-dimensional subspaces ofğ‘‰. Prove that ğ‘‰1 + â‹¯ + ğ‘‰ğ‘š is finite-dimensional and dim(ğ‘‰1 + â‹¯ + ğ‘‰ğ‘š) â‰¤dim ğ‘‰1 + â‹¯ + dim ğ‘‰ğ‘š. The inequality above is an equality if and only if ğ‘‰1 + â‹¯ + ğ‘‰ğ‘š is a direct sum, as will be shown in 3.94. 50 Chapter 2 Finite-Dimensional Vector Spaces 18 Suppose ğ‘‰ is finite-dimensional, withdim ğ‘‰ = ğ‘› â‰¥ 1. Prove that there exist one-dimensional subspaces ğ‘‰1, â€¦, ğ‘‰ğ‘› of ğ‘‰ such that ğ‘‰ = ğ‘‰1 âŠ• â‹¯ âŠ• ğ‘‰ğ‘›. 19 Explain why you might guess, motivated by analogy with the formula for the number of elements in the union of three finite sets, that ifğ‘‰1, ğ‘‰2, ğ‘‰3 are subspaces of a finite-dimensional vector space, then dim(ğ‘‰1 + ğ‘‰2 + ğ‘‰3) = dim ğ‘‰1 + dim ğ‘‰2 + dim ğ‘‰3 âˆ’ dim(ğ‘‰1 âˆ© ğ‘‰2) âˆ’ dim(ğ‘‰1 âˆ© ğ‘‰3) âˆ’ dim(ğ‘‰2 âˆ© ğ‘‰3) + dim(ğ‘‰1 âˆ© ğ‘‰2 âˆ© ğ‘‰3). Then either prove the formula above or give a counterexample. 20 Prove that if ğ‘‰1, ğ‘‰2, and ğ‘‰3 are subspaces of a finite-dimensional vector space, then dim(ğ‘‰1 + ğ‘‰2 + ğ‘‰3) = dim ğ‘‰1 + dim ğ‘‰2 + dim ğ‘‰3 âˆ’ dim(ğ‘‰1 âˆ© ğ‘‰2) + dim(ğ‘‰1 âˆ© ğ‘‰3) + dim(ğ‘‰2 âˆ© ğ‘‰3) 3 âˆ’ dim((ğ‘‰1+ğ‘‰2)âˆ©ğ‘‰3)+ dim((ğ‘‰1+ğ‘‰3)âˆ©ğ‘‰2)+ dim((ğ‘‰2 +ğ‘‰3)âˆ©ğ‘‰1) 3 . The formula above may seem strange because the right side does not look like an integer. I at once gave up my former occupations, set down natural history and all its progeny as a deformed and abortive creation, and entertained the greatest disdain for a would-be science which could never even step within the threshold of real knowledge. In this mood I betook myself to the mathematics and the branches of study appertaining to that science as being built upon secure foundations, and so worthy of my consideration. â€”Frankenstein, Mary Wollstonecraft Shelley Chapter 3 Linear Maps So far our attention has focused on vector spaces. No one gets excited about vector spaces. The interesting part of linear algebra is the subject to which we now turnâ€”linear maps. We will frequently use the powerful fundamental theorem of linear maps, which states that the dimension of the domain of a linear map equals the dimension of the subspace that gets sent to 0plus the dimension of the range. This will imply the striking result that a linear map from a finite-dimensional vector space to itself is one-to-one if and only if its range is the whole space. A major concept that we will introduce in this chapter is the matrix associated with a linear map and with a basis of the domain space and a basis of the target space. This correspondence between linear maps and matrices provides much insight into key aspects of linear algebra. This chapter concludes by introducing product, quotient, and dual spaces. In this chapter we will need additional vector spaces, which we call ğ‘ˆ and ğ‘Š, in addition to ğ‘‰. Thus our standing assumptions are now as follows. standing assumptions for this chapter â€¢ ğ… denotes ğ‘ or ğ‚. â€¢ ğ‘ˆ, ğ‘‰, and ğ‘Š denote vector spaces over ğ….StefanSchÃ¤ferCCBY-SA The twelfth-century Dankwarderode Castle in Brunswick (Braunschweig), where Carl Friedrich Gauss (1777â€“1855) was born and grew up. In 1809 Gauss published a method for solving systems of linear equations. This method, now called Gaussian elimination, was used in a Chinese book written over 1600 years earlier. 51 S. Axler, Linear Algebra Done Right, Undergraduate Texts in Mathematics, https://doi.org/10.1007/978-3-031-41026-0_3 Â© Sheldon Axler 2024 52 Chapter 3 Linear Maps 3A Vector Space of Linear Maps Definition and Examples of Linear Maps Now we are ready for one of the key definitions in linear algebra. 3.1 definition: linear map A linear map from ğ‘‰ to ğ‘Š is a function ğ‘‡âˆ¶ ğ‘‰ â†’ ğ‘Š with the following properties. additivity ğ‘‡(ğ‘¢ + ğ‘£) = ğ‘‡ğ‘¢ + ğ‘‡ğ‘£ for all ğ‘¢, ğ‘£ âˆˆ ğ‘‰. homogeneity ğ‘‡(ğœ†ğ‘£) = ğœ†(ğ‘‡ğ‘£) for all ğœ† âˆˆ ğ… and all ğ‘£ âˆˆ ğ‘‰. Some mathematicians use the phrase linear transformation, which means the same as linear map. Note that for linear maps we often use the notation ğ‘‡ğ‘£ as well as the usual function notation ğ‘‡(ğ‘£). 3.2 notation: â„’(ğ‘‰, ğ‘Š), â„’(ğ‘‰) â€¢ The set of linear maps from ğ‘‰ to ğ‘Š is denoted by â„’(ğ‘‰, ğ‘Š). â€¢ The set of linear maps from ğ‘‰ to ğ‘‰ is denoted by â„’(ğ‘‰). In other words, â„’(ğ‘‰) = â„’(ğ‘‰, ğ‘‰). Letâ€™s look at some examples of linear maps. Make sure you verify that each of the functions defined in the next example is indeed a linear map: 3.3 example: linear maps zero In addition to its other uses, we let the symbol 0denote the linear map that takes every element of some vector space to the additive identity of another (or possibly the same) vector space. To be specific,0 âˆˆâ„’(ğ‘‰, ğ‘Š) is defined by 0ğ‘£ = 0. The 0on the left side of the equation above is a function from ğ‘‰ to ğ‘Š, whereas the 0on the right side is the additive identity in ğ‘Š. As usual, the context should allow you to distinguish between the many uses of the symbol 0. identity operator The identity operator, denoted by ğ¼, is the linear map on some vector space that takes each element to itself. To be specific,ğ¼ âˆˆ â„’(ğ‘‰) is defined by ğ¼ğ‘£ = ğ‘£. Section 3A Vector Space of Linear Maps 53 differentiation Defineğ· âˆˆ â„’(ğ’«(ğ‘))by ğ·ğ‘ = ğ‘â€². The assertion that this function is a linear map is another way of stating a basic result about differentiation: ( ğ‘“ + ğ‘”) â€² = ğ‘“ â€² + ğ‘”â€² and (ğœ† ğ‘“ )â€² = ğœ† ğ‘“ â€² whenever ğ‘“, ğ‘” are differentiable and ğœ† is a constant. integration Defineğ‘‡ âˆˆ â„’(ğ’«(ğ‘), ğ‘)by ğ‘‡ğ‘ = âˆ«1 0 ğ‘. The assertion that this function is linear is another way of stating a basic result about integration: the integral of the sum of two functions equals the sum of the integrals, and the integral of a constant times a function equals the constant times the integral of the function. multiplication by ğ‘¥2 Define a linear mapğ‘‡ âˆˆ â„’(ğ’«(ğ‘))by (ğ‘‡ğ‘)(ğ‘¥) = ğ‘¥2ğ‘(ğ‘¥) for each ğ‘¥ âˆˆ ğ‘. backward shift Recall that ğ…âˆ denotes the vector space of all sequences of elements of ğ…. Define a linear map ğ‘‡ âˆˆ â„’(ğ…âˆ)by ğ‘‡(ğ‘¥1, ğ‘¥2, ğ‘¥3, â€¦ ) = (ğ‘¥2, ğ‘¥3, â€¦ ). from ğ‘3 to ğ‘2 Define a linear mapğ‘‡ âˆˆ â„’(ğ‘3, ğ‘2)by ğ‘‡(ğ‘¥, ğ‘¦, ğ‘§) = (2ğ‘¥ âˆ’ ğ‘¦+ 3ğ‘§, 7ğ‘¥+ 5ğ‘¦ âˆ’ 6ğ‘§). from ğ…ğ‘› to ğ…ğ‘š To generalize the previous example, let ğ‘š and ğ‘› be positive integers, let ğ´ğ‘—, ğ‘˜ âˆˆ ğ… for each ğ‘— = 1, â€¦, ğ‘š and each ğ‘˜ = 1, â€¦, ğ‘›, and define a linear mapğ‘‡ âˆˆ â„’(ğ…ğ‘›, ğ…ğ‘š) by ğ‘‡(ğ‘¥1, â€¦, ğ‘¥ğ‘›) = (ğ´1, 1ğ‘¥1 + â‹¯ + ğ´1, ğ‘› ğ‘¥ğ‘›, â€¦, ğ´ğ‘š, 1ğ‘¥1 + â‹¯ + ğ´ğ‘š, ğ‘› ğ‘¥ğ‘›). Actually every linear map from ğ…ğ‘› to ğ…ğ‘š is of this form. composition Fix a polynomial ğ‘ âˆˆ ğ’«(ğ‘). Define a linear mapğ‘‡ âˆˆ â„’(ğ’«(ğ‘))by (ğ‘‡ğ‘)(ğ‘¥) = ğ‘(ğ‘(ğ‘¥)). The existence part of the next result means that we can find a linear map that takes on whatever values we wish on the vectors in a basis. The uniqueness part of the next result means that a linear map is completely determined by its values on a basis. 54 Chapter 3 Linear Maps 3.4 linear map lemma Suppose ğ‘£1, â€¦, ğ‘£ğ‘› is a basis of ğ‘‰ and ğ‘¤1, â€¦, ğ‘¤ğ‘› âˆˆ ğ‘Š. Then there exists a unique linear map ğ‘‡âˆ¶ ğ‘‰ â†’ ğ‘Š such that ğ‘‡ğ‘£ğ‘˜ = ğ‘¤ğ‘˜ for each ğ‘˜ = 1, â€¦, ğ‘›. Proof First we show the existence of a linear map ğ‘‡ with the desired property. Defineğ‘‡âˆ¶ ğ‘‰ â†’ ğ‘Š by ğ‘‡(ğ‘1ğ‘£1 + â‹¯ + ğ‘ğ‘›ğ‘£ğ‘›) = ğ‘1ğ‘¤1 + â‹¯ + ğ‘ğ‘›ğ‘¤ğ‘›, where ğ‘1, â€¦, ğ‘ğ‘› are arbitrary elements of ğ…. The list ğ‘£1, â€¦, ğ‘£ğ‘› is a basis of ğ‘‰. Thus the equation above does indeed define a functionğ‘‡ from ğ‘‰ to ğ‘Š (because each element of ğ‘‰ can be uniquely written in the form ğ‘1ğ‘£1 + â‹¯ + ğ‘ğ‘›ğ‘£ğ‘›). For each ğ‘˜, taking ğ‘ğ‘˜ = 1and the other ğ‘â€™s equal to 0in the equation above shows that ğ‘‡ğ‘£ğ‘˜ = ğ‘¤ğ‘˜. If ğ‘¢, ğ‘£ âˆˆ ğ‘‰ with ğ‘¢ = ğ‘1ğ‘£1 + â‹¯ + ğ‘ğ‘›ğ‘£ğ‘› and ğ‘£ = ğ‘1ğ‘£1 + â‹¯ + ğ‘ğ‘›ğ‘£ğ‘›, then ğ‘‡(ğ‘¢ + ğ‘£) = ğ‘‡((ğ‘1 + ğ‘1)ğ‘£1 + â‹¯ + (ğ‘ğ‘› + ğ‘ğ‘›)ğ‘£ğ‘›) = (ğ‘1 + ğ‘1)ğ‘¤1 + â‹¯ + (ğ‘ğ‘› + ğ‘ğ‘›)ğ‘¤ğ‘› = (ğ‘1ğ‘¤1 + â‹¯ + ğ‘ğ‘›ğ‘¤ğ‘›) + (ğ‘1ğ‘¤1 + â‹¯ + ğ‘ğ‘›ğ‘¤ğ‘›) = ğ‘‡ğ‘¢ + ğ‘‡ğ‘£. Similarly, if ğœ† âˆˆ ğ… and ğ‘£ = ğ‘1ğ‘£1 + â‹¯ + ğ‘ğ‘›ğ‘£ğ‘›, then ğ‘‡(ğœ†ğ‘£) = ğ‘‡(ğœ†ğ‘1ğ‘£1 + â‹¯ + ğœ†ğ‘ğ‘›ğ‘£ğ‘›) = ğœ†ğ‘1ğ‘¤1 + â‹¯ + ğœ†ğ‘ğ‘›ğ‘¤ğ‘› = ğœ†(ğ‘1ğ‘¤1 + â‹¯ + ğ‘ğ‘›ğ‘¤ğ‘›) = ğœ†ğ‘‡ğ‘£. Thus ğ‘‡ is a linear map from ğ‘‰ to ğ‘Š. To prove uniqueness, now suppose that ğ‘‡ âˆˆ â„’(ğ‘‰, ğ‘Š) and that ğ‘‡ğ‘£ğ‘˜ = ğ‘¤ğ‘˜ for each ğ‘˜ = 1, â€¦, ğ‘›. Let ğ‘1, â€¦, ğ‘ğ‘› âˆˆ ğ…. Then the homogeneity of ğ‘‡ implies that ğ‘‡(ğ‘ğ‘˜ğ‘£ğ‘˜) = ğ‘ğ‘˜ğ‘¤ğ‘˜ for each ğ‘˜ = 1, â€¦, ğ‘›. The additivity of ğ‘‡ now implies that ğ‘‡(ğ‘1ğ‘£1 + â‹¯ + ğ‘ğ‘›ğ‘£ğ‘›) = ğ‘1ğ‘¤1 + â‹¯ + ğ‘ğ‘›ğ‘¤ğ‘›. Thus ğ‘‡ is uniquely determined on span(ğ‘£1, â€¦, ğ‘£ğ‘›) by the equation above. Because ğ‘£1, â€¦, ğ‘£ğ‘› is a basis of ğ‘‰, this implies that ğ‘‡ is uniquely determined on ğ‘‰, as desired. Section 3A Vector Space of Linear Maps 55 Algebraic Operations on â„’(ğ‘‰, ğ‘Š) We begin by defining addition and scalar multiplication onâ„’(ğ‘‰, ğ‘Š). 3.5 definition: addition and scalar multiplication on â„’(ğ‘‰, ğ‘Š) Suppose ğ‘†, ğ‘‡ âˆˆ â„’(ğ‘‰, ğ‘Š) and ğœ† âˆˆ ğ…. The sum ğ‘† + ğ‘‡ and the product ğœ†ğ‘‡ are the linear maps from ğ‘‰ to ğ‘Š defined by (ğ‘† + ğ‘‡)(ğ‘£) = ğ‘†ğ‘£ + ğ‘‡ğ‘£ and (ğœ†ğ‘‡)(ğ‘£) = ğœ†(ğ‘‡ğ‘£) for all ğ‘£ âˆˆ ğ‘‰. Linear maps are pervasive throughout mathematics. However, they are not as ubiquitous as imagined by people who seem to think cos is a linear map from ğ‘ to ğ‘ when they incorrectly write that cos(ğ‘¥+ğ‘¦) equals cos ğ‘¥+cos ğ‘¦ and that cos 2ğ‘¥equals 2cos ğ‘¥. You should verify that ğ‘† + ğ‘‡ and ğœ†ğ‘‡ as defined above are indeed linear maps. In other words, if ğ‘†, ğ‘‡ âˆˆ â„’(ğ‘‰, ğ‘Š) and ğœ† âˆˆ ğ…, then ğ‘† + ğ‘‡ âˆˆ â„’(ğ‘‰, ğ‘Š) and ğœ†ğ‘‡ âˆˆ â„’(ğ‘‰, ğ‘Š). Because we took the trouble to de- fine addition and scalar multiplication on â„’(ğ‘‰, ğ‘Š), the next result should not be a surprise. 3.6 â„’(ğ‘‰, ğ‘Š) is a vector space With the operations of addition and scalar multiplication as defined above, â„’(ğ‘‰, ğ‘Š) is a vector space. The routine proof of the result above is left to the reader. Note that the additive identity of â„’(ğ‘‰, ğ‘Š) is the zero linear map defined in Example3.3. Usually it makes no sense to multiply together two elements of a vector space, but for some pairs of linear maps a useful product exists, as in the next definition. 3.7 definition:product of linear maps If ğ‘‡ âˆˆ â„’(ğ‘ˆ, ğ‘‰) and ğ‘† âˆˆ â„’(ğ‘‰, ğ‘Š), then the product ğ‘†ğ‘‡ âˆˆ â„’(ğ‘ˆ, ğ‘Š) is defined by (ğ‘†ğ‘‡)(ğ‘¢) = ğ‘†(ğ‘‡ğ‘¢) for all ğ‘¢ âˆˆ ğ‘ˆ. Thus ğ‘†ğ‘‡ is just the usual composition ğ‘† âˆ˜ ğ‘‡ of two functions, but when both functions are linear, we usually write ğ‘†ğ‘‡ instead of ğ‘† âˆ˜ ğ‘‡. The product notation ğ‘†ğ‘‡ helps make the distributive properties (see next result) seem natural. Note that ğ‘†ğ‘‡ is defined only whenğ‘‡ maps into the domain of ğ‘†. You should verify that ğ‘†ğ‘‡ is indeed a linear map from ğ‘ˆ to ğ‘Š whenever ğ‘‡ âˆˆ â„’(ğ‘ˆ, ğ‘‰) and ğ‘† âˆˆ â„’(ğ‘‰, ğ‘Š). 56 Chapter 3 Linear Maps 3.8 algebraic properties of products of linear maps associativity (ğ‘‡1ğ‘‡2)ğ‘‡3 = ğ‘‡1(ğ‘‡2ğ‘‡3) whenever ğ‘‡1, ğ‘‡2, and ğ‘‡3 are linear maps such that the products make sense (meaning ğ‘‡3 maps into the domain of ğ‘‡2, and ğ‘‡2 maps into the domain of ğ‘‡1). identity ğ‘‡ğ¼ = ğ¼ğ‘‡ = ğ‘‡ whenever ğ‘‡ âˆˆ â„’(ğ‘‰, ğ‘Š); here the firstğ¼ is the identity operator on ğ‘‰, and the second ğ¼ is the identity operator on ğ‘Š. distributive properties (ğ‘†1 + ğ‘†2)ğ‘‡ = ğ‘†1ğ‘‡ + ğ‘†2ğ‘‡ and ğ‘†(ğ‘‡1 + ğ‘‡2) = ğ‘†ğ‘‡1 + ğ‘†ğ‘‡2 whenever ğ‘‡, ğ‘‡1, ğ‘‡2 âˆˆ â„’(ğ‘ˆ, ğ‘‰) and ğ‘†, ğ‘†1, ğ‘†2 âˆˆ â„’(ğ‘‰, ğ‘Š). The routine proof of the result above is left to the reader. Multiplication of linear maps is not commutative. In other words, it is not necessarily true that ğ‘†ğ‘‡ = ğ‘‡ğ‘†, even if both sides of the equation make sense. 3.9 example: two noncommuting linear maps from ğ’«(ğ‘) to ğ’«(ğ‘) Suppose ğ· âˆˆ â„’(ğ’«(ğ‘))is the differentiation map defined in Example3.3 and ğ‘‡ âˆˆ â„’(ğ’«(ğ‘))is the multiplication by ğ‘¥2 map defined earlier in this section. Then ((ğ‘‡ğ·)ğ‘)(ğ‘¥) = ğ‘¥2ğ‘ â€²(ğ‘¥) but ((ğ·ğ‘‡)ğ‘)(ğ‘¥) = ğ‘¥2ğ‘â€²(ğ‘¥) + 2ğ‘¥ğ‘(ğ‘¥). Thus ğ‘‡ğ· â‰  ğ·ğ‘‡â€”differentiating and then multiplying by ğ‘¥2 is not the same as multiplying by ğ‘¥2 and then differentiating. 3.10 linear maps take 0to 0 Suppose ğ‘‡ is a linear map from ğ‘‰ to ğ‘Š. Then ğ‘‡(0) = 0. Proof By additivity, we have ğ‘‡(0) = ğ‘‡(0+ 0) = ğ‘‡(0)+ ğ‘‡(0). Add the additive inverse of ğ‘‡(0)to each side of the equation above to conclude that ğ‘‡(0) = 0. Suppose ğ‘š, ğ‘ âˆˆ ğ‘. The function ğ‘“âˆ¶ ğ‘ â†’ ğ‘ defined by ğ‘“ (ğ‘¥) = ğ‘šğ‘¥ + ğ‘ is a linear map if and only if ğ‘ = 0(use 3.10). Thus the linear functions of high school algebra are not the same as linear maps in the context of linear algebra. Section 3A Vector Space of Linear Maps 57 Exercises 3A 1 Suppose ğ‘, ğ‘ âˆˆ ğ‘. Defineğ‘‡âˆ¶ ğ‘3 â†’ ğ‘2 by ğ‘‡(ğ‘¥, ğ‘¦, ğ‘§) = (2ğ‘¥ âˆ’ 4ğ‘¦+ 3ğ‘§+ ğ‘, 6ğ‘¥+ ğ‘ğ‘¥ğ‘¦ğ‘§). Show that ğ‘‡ is linear if and only if ğ‘ = ğ‘ = 0. 2 Suppose ğ‘, ğ‘ âˆˆ ğ‘. Defineğ‘‡âˆ¶ ğ’«(ğ‘) â†’ ğ‘2 by ğ‘‡ğ‘ = (3ğ‘(4)+ 5ğ‘ â€²(6)+ ğ‘ğ‘(1)ğ‘(2), âˆ«2 âˆ’1 ğ‘¥3ğ‘(ğ‘¥) ğ‘‘ğ‘¥ + ğ‘ sin ğ‘(0)). Show that ğ‘‡ is linear if and only if ğ‘ = ğ‘ = 0. 3 Suppose that ğ‘‡ âˆˆ â„’(ğ…ğ‘›, ğ…ğ‘š). Show that there exist scalars ğ´ğ‘—, ğ‘˜ âˆˆ ğ… for ğ‘— = 1, â€¦, ğ‘š and ğ‘˜ = 1, â€¦, ğ‘› such that ğ‘‡(ğ‘¥1, â€¦, ğ‘¥ğ‘›) = (ğ´1, 1ğ‘¥1 + â‹¯ + ğ´1, ğ‘› ğ‘¥ğ‘›, â€¦, ğ´ğ‘š, 1ğ‘¥1 + â‹¯ + ğ´ğ‘š, ğ‘› ğ‘¥ğ‘›) for every (ğ‘¥1, â€¦, ğ‘¥ğ‘›) âˆˆ ğ…ğ‘›. This exercise shows that the linear map ğ‘‡ has the form promised in the second to last item of Example 3.3. 4 Suppose ğ‘‡ âˆˆ â„’(ğ‘‰, ğ‘Š) and ğ‘£1, â€¦, ğ‘£ğ‘š is a list of vectors in ğ‘‰ such that ğ‘‡ğ‘£1, â€¦, ğ‘‡ğ‘£ğ‘š is a linearly independent list in ğ‘Š. Prove that ğ‘£1, â€¦, ğ‘£ğ‘š is linearly independent. 5 Prove that â„’(ğ‘‰, ğ‘Š) is a vector space, as was asserted in 3.6. 6 Prove that multiplication of linear maps has the associative, identity, and distributive properties asserted in 3.8. 7 Show that every linear map from a one-dimensional vector space to itself is multiplication by some scalar. More precisely, prove that if dim ğ‘‰ = 1and ğ‘‡ âˆˆ â„’(ğ‘‰), then there exists ğœ† âˆˆ ğ… such that ğ‘‡ğ‘£ = ğœ†ğ‘£ for all ğ‘£ âˆˆ ğ‘‰. 8 Give an example of a function ğœ‘âˆ¶ ğ‘2 â†’ ğ‘ such that ğœ‘(ğ‘ğ‘£) = ğ‘ğœ‘(ğ‘£) for all ğ‘ âˆˆ ğ‘ and all ğ‘£ âˆˆ ğ‘2 but ğœ‘ is not linear. This exercise and the next exercise show that neither homogeneity nor additivity alone is enough to imply that a function is a linear map. 9 Give an example of a function ğœ‘âˆ¶ ğ‚ â†’ ğ‚ such that ğœ‘(ğ‘¤ + ğ‘§) = ğœ‘(ğ‘¤) + ğœ‘(ğ‘§) for all ğ‘¤, ğ‘§ âˆˆ ğ‚ but ğœ‘ is not linear. (Here ğ‚ is thought of as a complex vector space.) There also exists a function ğœ‘âˆ¶ ğ‘ â†’ ğ‘ such that ğœ‘ satisfies the additivity condition above but ğœ‘ is not linear. However, showing the existence of such a function involves considerably more advanced tools. 58 Chapter 3 Linear Maps 10 Prove or give a counterexample: If ğ‘ âˆˆ ğ’«(ğ‘) and ğ‘‡âˆ¶ ğ’«(ğ‘) â†’ ğ’«(ğ‘) is defined byğ‘‡ğ‘ = ğ‘ âˆ˜ ğ‘, then ğ‘‡ is a linear map. The function ğ‘‡ defined here differs from the function ğ‘‡ defined in the last bullet point of 3.3 by the order of the functions in the compositions. 11 Suppose ğ‘‰ is finite-dimensional andğ‘‡ âˆˆ â„’(ğ‘‰). Prove that ğ‘‡ is a scalar multiple of the identity if and only if ğ‘†ğ‘‡ = ğ‘‡ğ‘† for every ğ‘† âˆˆ â„’(ğ‘‰). 12 Suppose ğ‘ˆ is a subspace of ğ‘‰ with ğ‘ˆ â‰  ğ‘‰. Suppose ğ‘† âˆˆ â„’(ğ‘ˆ, ğ‘Š) and ğ‘† â‰  0(which means that ğ‘†ğ‘¢ â‰  0for some ğ‘¢ âˆˆ ğ‘ˆ). Defineğ‘‡âˆ¶ ğ‘‰ â†’ ğ‘Š by ğ‘‡ğ‘£ = â§{ â¨{â© ğ‘†ğ‘£ if ğ‘£ âˆˆ ğ‘ˆ, 0 if ğ‘£ âˆˆ ğ‘‰ and ğ‘£ âˆ‰ ğ‘ˆ. Prove that ğ‘‡ is not a linear map on ğ‘‰. 13 Suppose ğ‘‰ is finite-dimensional. Prove that every linear map on a subspace of ğ‘‰ can be extended to a linear map on ğ‘‰. In other words, show that if ğ‘ˆ is a subspace of ğ‘‰ and ğ‘† âˆˆ â„’(ğ‘ˆ, ğ‘Š), then there exists ğ‘‡ âˆˆ â„’(ğ‘‰, ğ‘Š) such that ğ‘‡ğ‘¢ = ğ‘†ğ‘¢ for all ğ‘¢ âˆˆ ğ‘ˆ. The result in this exercise is used in the proof of 3.125. 14 Suppose ğ‘‰ is finite-dimensional withdim ğ‘‰ > 0, and suppose ğ‘Š is infinite- dimensional. Prove that â„’(ğ‘‰, ğ‘Š) is infinite-dimensional. 15 Suppose ğ‘£1, â€¦, ğ‘£ğ‘š is a linearly dependent list of vectors in ğ‘‰. Suppose also that ğ‘Š â‰  {0}. Prove that there exist ğ‘¤1, â€¦, ğ‘¤ğ‘š âˆˆ ğ‘Š such that no ğ‘‡ âˆˆ â„’(ğ‘‰, ğ‘Š) satisfiesğ‘‡ğ‘£ğ‘˜ = ğ‘¤ğ‘˜ for each ğ‘˜ = 1, â€¦, ğ‘š. 16 Suppose ğ‘‰ is finite-dimensional withdim ğ‘‰ > 1. Prove that there exist ğ‘†, ğ‘‡ âˆˆ â„’(ğ‘‰) such that ğ‘†ğ‘‡ â‰  ğ‘‡ğ‘†. 17 Suppose ğ‘‰ is finite-dimensional. Show that the only two-sided ideals of â„’(ğ‘‰) are {0}and â„’(ğ‘‰). A subspace â„° of â„’(ğ‘‰) is called a two-sided ideal of â„’(ğ‘‰) if ğ‘‡ğ¸ âˆˆ â„° and ğ¸ğ‘‡ âˆˆ â„° for all ğ¸ âˆˆ â„° and all ğ‘‡ âˆˆ â„’(ğ‘‰). Open Access This chapter is licensed under the terms of the Creative Commons Attribution-NonCommercial 4.0 International License (https://creativecommons.org/licenses/by-nc/4.0), which permits any noncommercial use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to original author and source, provide a link to the Creative Commons license, and indicate if changes were made. The images or other third party material in this chapter are included in the chapterâ€™s Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the chapterâ€™s Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. Section 3B Null Spaces and Ranges 59 3B Null Spaces and Ranges Null Space and Injectivity In this section we will learn about two subspaces that are intimately connected with each linear map. We begin with the set of vectors that get mapped to 0. 3.11 definition: null space, null ğ‘‡ For ğ‘‡ âˆˆ â„’(ğ‘‰, ğ‘Š), the null space of ğ‘‡, denoted by null ğ‘‡, is the subset of ğ‘‰ consisting of those vectors that ğ‘‡ maps to 0: null ğ‘‡ = {ğ‘£ âˆˆ ğ‘‰ âˆ¶ ğ‘‡ğ‘£ = 0}. 3.12 example: null space â€¢ If ğ‘‡ is the zero map from ğ‘‰ to ğ‘Š, meaning that ğ‘‡ğ‘£ = 0for every ğ‘£ âˆˆ ğ‘‰, then null ğ‘‡ = ğ‘‰. â€¢ Suppose ğœ‘ âˆˆ â„’(ğ‚ 3, ğ‚)is defined byğœ‘(ğ‘§1, ğ‘§2, ğ‘§3) = ğ‘§1 + 2ğ‘§2 + 3ğ‘§3. Then null ğœ‘ equals {(ğ‘§1, ğ‘§2, ğ‘§3) âˆˆ ğ‚3 âˆ¶ ğ‘§1 + 2ğ‘§2 + 3ğ‘§3 = 0}, which is a subspace of the domain of ğœ‘. We will soon see that the null space of each linear map is a subspace of its domain. â€¢ The word â€œnullâ€ means zero. Thus the term â€œnull spaceâ€should remind you of the connection to 0. Some mathe- maticians use the term kernel instead of null space. Suppose ğ· âˆˆ â„’(ğ’«(ğ‘))is the dif- ferentiation map defined byğ·ğ‘ = ğ‘â€². The only functions whose derivative equals the zero function are the con- stant functions. Thus the null space of ğ· equals the set of constant functions. â€¢ Suppose that ğ‘‡ âˆˆ â„’(ğ’«(ğ‘))is the multiplication by ğ‘¥2 map defined by (ğ‘‡ğ‘)(ğ‘¥) = ğ‘¥2ğ‘(ğ‘¥). The only polynomial ğ‘ such that ğ‘¥2ğ‘(ğ‘¥) = 0for all ğ‘¥ âˆˆ ğ‘ is the 0polynomial. Thus null ğ‘‡ = {0}. â€¢ Suppose ğ‘‡ âˆˆ â„’(ğ…âˆ)is the backward shift defined by ğ‘‡(ğ‘¥1, ğ‘¥2, ğ‘¥3, â€¦ ) = (ğ‘¥2, ğ‘¥3, â€¦ ). Then ğ‘‡(ğ‘¥1, ğ‘¥2, ğ‘¥3, â€¦ ) equals 0if and only if the numbers ğ‘¥2, ğ‘¥3, â€¦ are all 0. Thus null ğ‘‡ = {(ğ‘, 0, 0, â€¦ ) âˆ¶ ğ‘ âˆˆ ğ…}. The next result shows that the null space of each linear map is a subspace of the domain. In particular, 0is in the null space of every linear map. 3.13 the null space is a subspace Suppose ğ‘‡ âˆˆ â„’(ğ‘‰, ğ‘Š). Then null ğ‘‡ is a subspace of ğ‘‰. 60 Chapter 3 Linear Maps Proof Because ğ‘‡ is a linear map, ğ‘‡(0) = 0(by 3.10). Thus 0 âˆˆnull ğ‘‡. Suppose ğ‘¢, ğ‘£ âˆˆ null ğ‘‡. Then ğ‘‡(ğ‘¢ + ğ‘£) = ğ‘‡ğ‘¢ + ğ‘‡ğ‘£ = 0+ 0 = 0. Hence ğ‘¢ + ğ‘£ âˆˆ null ğ‘‡. Thus null ğ‘‡ is closed under addition. Suppose ğ‘¢ âˆˆ null ğ‘‡ and ğœ† âˆˆ ğ…. Then ğ‘‡(ğœ†ğ‘¢) = ğœ†ğ‘‡ğ‘¢ = ğœ†0 = 0. Hence ğœ†ğ‘¢ âˆˆ null ğ‘‡. Thus null ğ‘‡ is closed under scalar multiplication. We have shown that null ğ‘‡ contains 0and is closed under addition and scalar multiplication. Thus null ğ‘‡ is a subspace of ğ‘‰ (by 1.34). As we will soon see, for a linear map the next definition is closely connected to the null space. 3.14 definition: injective A function ğ‘‡âˆ¶ ğ‘‰ â†’ ğ‘Š is called injective if ğ‘‡ğ‘¢ = ğ‘‡ğ‘£ implies ğ‘¢ = ğ‘£. The term one-to-one means the same as injective. We could rephrase the definition above to say that ğ‘‡ is injective if ğ‘¢ â‰  ğ‘£ implies that ğ‘‡ğ‘¢ â‰  ğ‘‡ğ‘£. Thus ğ‘‡ is injective if and only if it maps distinct inputs to distinct outputs. The next result says that we can check whether a linear map is injective by checking whether 0is the only vector that gets mapped to 0. As a simple application of this result, we see that of the linear maps whose null spaces we computed in 3.12, only multiplication by ğ‘¥2 is injective (except that the zero map is injective in the special case ğ‘‰ = {0}). 3.15 injectivity âŸº null space equals {0} Let ğ‘‡ âˆˆ â„’(ğ‘‰, ğ‘Š). Then ğ‘‡ is injective if and only if null ğ‘‡ = {0}. Proof First suppose ğ‘‡ is injective. We want to prove that null ğ‘‡ = {0}. We already know that {0} âŠ†null ğ‘‡ (by 3.10). To prove the inclusion in the other direction, suppose ğ‘£ âˆˆ null ğ‘‡. Then ğ‘‡(ğ‘£) = 0 = ğ‘‡(0). Because ğ‘‡ is injective, the equation above implies that ğ‘£ = 0. Thus we can conclude that null ğ‘‡ = {0}, as desired. To prove the implication in the other direction, now suppose null ğ‘‡ = {0}. We want to prove that ğ‘‡ is injective. To do this, suppose ğ‘¢, ğ‘£ âˆˆ ğ‘‰ and ğ‘‡ğ‘¢ = ğ‘‡ğ‘£. Then 0 = ğ‘‡ğ‘¢ âˆ’ ğ‘‡ğ‘£ = ğ‘‡(ğ‘¢ âˆ’ ğ‘£). Thus ğ‘¢ âˆ’ ğ‘£ is in null ğ‘‡, which equals {0}. Hence ğ‘¢ âˆ’ ğ‘£ = 0, which implies that ğ‘¢ = ğ‘£. Hence ğ‘‡ is injective, as desired. Section 3B Null Spaces and Ranges 61 Range and Surjectivity Now we give a name to the set of outputs of a linear map. 3.16 definition:range For ğ‘‡ âˆˆ â„’(ğ‘‰, ğ‘Š), the range of ğ‘‡ is the subset of ğ‘Š consisting of those vectors that are equal to ğ‘‡ğ‘£ for some ğ‘£ âˆˆ ğ‘‰: range ğ‘‡ = {ğ‘‡ğ‘£ âˆ¶ ğ‘£ âˆˆ ğ‘‰}. 3.17 example:range â€¢ If ğ‘‡ is the zero map from ğ‘‰ to ğ‘Š, meaning that ğ‘‡ğ‘£ = 0for every ğ‘£ âˆˆ ğ‘‰, then range ğ‘‡ = {0}. â€¢ Suppose ğ‘‡ âˆˆ â„’(ğ‘2, ğ‘3)is defined byğ‘‡(ğ‘¥, ğ‘¦) = (2ğ‘¥, 5ğ‘¦, ğ‘¥ + ğ‘¦). Then range ğ‘‡ = {(2ğ‘¥, 5ğ‘¦, ğ‘¥ + ğ‘¦) âˆ¶ ğ‘¥, ğ‘¦ âˆˆ ğ‘}. Note that range ğ‘‡ is a subspace of ğ‘3. We will soon see that the range of each element of â„’(ğ‘‰, ğ‘Š) is a subspace of ğ‘Š. â€¢ Suppose ğ· âˆˆ â„’(ğ’«(ğ‘))is the differentiation map defined byğ·ğ‘ = ğ‘â€². Because for every polynomial ğ‘ âˆˆ ğ’«(ğ‘) there exists a polynomial ğ‘ âˆˆ ğ’«(ğ‘) such that ğ‘ â€² = ğ‘, the range of ğ· is ğ’«(ğ‘). The next result shows that the range of each linear map is a subspace of the vector space into which it is being mapped. 3.18 the range is a subspace If ğ‘‡ âˆˆ â„’(ğ‘‰, ğ‘Š), then range ğ‘‡ is a subspace of ğ‘Š. Proof Suppose ğ‘‡ âˆˆ â„’(ğ‘‰, ğ‘Š). Then ğ‘‡(0) = 0(by 3.10), which implies that 0 âˆˆrange ğ‘‡. If ğ‘¤1, ğ‘¤2 âˆˆ range ğ‘‡, then there exist ğ‘£1, ğ‘£2 âˆˆ ğ‘‰ such that ğ‘‡ğ‘£1 = ğ‘¤1 and ğ‘‡ğ‘£2 = ğ‘¤2. Thus ğ‘‡(ğ‘£1 + ğ‘£2) = ğ‘‡ğ‘£1 + ğ‘‡ğ‘£2 = ğ‘¤1 + ğ‘¤2. Hence ğ‘¤1 + ğ‘¤2 âˆˆ range ğ‘‡. Thus range ğ‘‡ is closed under addition. If ğ‘¤ âˆˆ range ğ‘‡ and ğœ† âˆˆ ğ…, then there exists ğ‘£ âˆˆ ğ‘‰ such that ğ‘‡ğ‘£ = ğ‘¤. Thus ğ‘‡(ğœ†ğ‘£) = ğœ†ğ‘‡ğ‘£ = ğœ†ğ‘¤. Hence ğœ†ğ‘¤ âˆˆ range ğ‘‡. Thus range ğ‘‡ is closed under scalar multiplication. We have shown that range ğ‘‡ contains 0and is closed under addition and scalar multiplication. Thus range ğ‘‡ is a subspace of ğ‘Š (by 1.34). 62 Chapter 3 Linear Maps 3.19 definition: surjective A function ğ‘‡âˆ¶ ğ‘‰ â†’ ğ‘Š is called surjective if its range equals ğ‘Š. To illustrate the definition above, note that of the ranges we computed in3.17, only the differentiation map is surjective (except that the zero map is surjective in the special case ğ‘Š = {0}). Some people use the term onto, which means the same as surjective. Whether a linear map is surjective de- pends on what we are thinking of as the vector space into which it maps. 3.20 example: surjectivity depends on the target space The differentiation map ğ· âˆˆ â„’(ğ’«5(ğ‘))defined byğ·ğ‘ = ğ‘â€² is not surjective, because the polynomial ğ‘¥5 is not in the range of ğ·. However, the differentiation map ğ‘† âˆˆ â„’(ğ’«5(ğ‘), ğ’«4(ğ‘))defined byğ‘†ğ‘ = ğ‘â€² is surjective, because its range equals ğ’«4(ğ‘), which is the vector space into which ğ‘† maps. Fundamental Theorem of Linear Maps The next result is so important that it gets a dramatic name. 3.21 fundamental theorem of linear maps Suppose ğ‘‰ is finite-dimensional andğ‘‡ âˆˆ â„’(ğ‘‰, ğ‘Š). Then range ğ‘‡ is finite- dimensional and dim ğ‘‰ = dim null ğ‘‡ + dim range ğ‘‡. Proof Let ğ‘¢1, â€¦, ğ‘¢ğ‘š be a basis of null ğ‘‡; thus dim null ğ‘‡ = ğ‘š. The linearly independent list ğ‘¢1, â€¦, ğ‘¢ğ‘š can be extended to a basis ğ‘¢1, â€¦, ğ‘¢ğ‘š, ğ‘£1, â€¦, ğ‘£ğ‘› of ğ‘‰ (by 2.32). Thus dim ğ‘‰ = ğ‘š + ğ‘›. To complete the proof, we need to show that range ğ‘‡ is finite-dimensional anddim range ğ‘‡ = ğ‘›. We will do this by proving that ğ‘‡ğ‘£1, â€¦, ğ‘‡ğ‘£ğ‘› is a basis of range ğ‘‡. Let ğ‘£ âˆˆ ğ‘‰. Because ğ‘¢1, â€¦, ğ‘¢ğ‘š, ğ‘£1, â€¦, ğ‘£ğ‘› spans ğ‘‰, we can write ğ‘£ = ğ‘1ğ‘¢1 + â‹¯ + ğ‘ğ‘šğ‘¢ğ‘š + ğ‘1ğ‘£1 + â‹¯ + ğ‘ğ‘›ğ‘£ğ‘›, where the ğ‘â€™s and ğ‘â€™s are in ğ…. Applying ğ‘‡ to both sides of this equation, we get ğ‘‡ğ‘£ = ğ‘1ğ‘‡ğ‘£1 + â‹¯ + ğ‘ğ‘›ğ‘‡ğ‘£ğ‘›, where the terms of the form ğ‘‡ğ‘¢ğ‘˜ disappeared because each ğ‘¢ğ‘˜ is in null ğ‘‡. The last equation implies that the list ğ‘‡ğ‘£1, â€¦, ğ‘‡ğ‘£ğ‘› spans range ğ‘‡. In particular, range ğ‘‡ is finite-dimensional. Section 3B Null Spaces and Ranges 63 To show ğ‘‡ğ‘£1, â€¦, ğ‘‡ğ‘£ğ‘› is linearly independent, suppose ğ‘1, â€¦, ğ‘ğ‘› âˆˆ ğ… and ğ‘1ğ‘‡ğ‘£1 + â‹¯ + ğ‘ğ‘›ğ‘‡ğ‘£ğ‘› = 0. Then ğ‘‡(ğ‘1ğ‘£1 + â‹¯ + ğ‘ğ‘›ğ‘£ğ‘›) = 0. Hence ğ‘1ğ‘£1 + â‹¯ + ğ‘ğ‘›ğ‘£ğ‘› âˆˆ null ğ‘‡. Because ğ‘¢1, â€¦, ğ‘¢ğ‘š spans null ğ‘‡, we can write ğ‘1ğ‘£1 + â‹¯ + ğ‘ğ‘›ğ‘£ğ‘› = ğ‘‘1ğ‘¢1 + â‹¯ + ğ‘‘ğ‘šğ‘¢ğ‘š, where the ğ‘‘â€™s are in ğ…. This equation implies that all the ğ‘â€™s (and ğ‘‘â€™s) are 0 (because ğ‘¢1, â€¦, ğ‘¢ğ‘š, ğ‘£1, â€¦, ğ‘£ğ‘› is linearly independent). Thus ğ‘‡ğ‘£1, â€¦, ğ‘‡ğ‘£ğ‘› is linearly independent and hence is a basis of range ğ‘‡, as desired. Now we can show that no linear map from a finite-dimensional vector space to a â€œsmallerâ€ vector space can be injective, where â€œsmallerâ€ is measured by dimension. 3.22 linear map to a lower-dimensional space is not injective Suppose ğ‘‰ and ğ‘Š are finite-dimensional vector spaces such that dim ğ‘‰ > dim ğ‘Š. Then no linear map from ğ‘‰ to ğ‘Š is injective. Proof Let ğ‘‡ âˆˆ â„’(ğ‘‰, ğ‘Š). Then dim null ğ‘‡ = dim ğ‘‰ âˆ’ dim range ğ‘‡ â‰¥dim ğ‘‰ âˆ’ dim ğ‘Š > 0, where the first line above comes from the fundamental theorem of linear maps (3.21) and the second line follows from 2.37. The inequality above states that dim null ğ‘‡ > 0. This means that null ğ‘‡ contains vectors other than 0. Thus ğ‘‡ is not injective (by 3.15). 3.23 example: linear map from ğ…4 to ğ…3 is not injective Define a linear mapğ‘‡âˆ¶ ğ…4 â†’ ğ…3 by ğ‘‡(ğ‘§1, ğ‘§2, ğ‘§3, ğ‘§4) = (âˆš7ğ‘§1 + ğœ‹ğ‘§2 + ğ‘§4, 97ğ‘§1 + 3ğ‘§2 + 2ğ‘§3, ğ‘§2 + 6ğ‘§3 + 7ğ‘§4). Because dim ğ…4 > dim ğ…3, we can use 3.22 to assert that ğ‘‡ is not injective, without doing any calculations. 64 Chapter 3 Linear Maps The next result shows that no linear map from a finite-dimensional vector space to a â€œbiggerâ€ vector space can be surjective, where â€œbiggerâ€ is measured by dimension. 3.24 linear map to a higher-dimensional space is not surjective Suppose ğ‘‰ and ğ‘Š are finite-dimensional vector spaces such that dim ğ‘‰ < dim ğ‘Š. Then no linear map from ğ‘‰ to ğ‘Š is surjective. Proof Let ğ‘‡ âˆˆ â„’(ğ‘‰, ğ‘Š). Then dim range ğ‘‡ = dim ğ‘‰ âˆ’ dim null ğ‘‡ â‰¤dim ğ‘‰ < dim ğ‘Š, where the equality above comes from the fundamental theorem of linear maps (3.21). The inequality above states that dim range ğ‘‡ < dim ğ‘Š. This means that range ğ‘‡ cannot equal ğ‘Š. Thus ğ‘‡ is not surjective. As we will soon see, 3.22 and 3.24 have important consequences in the theory of linear equations. The idea is to express questions about systems of linear equations in terms of linear maps. Letâ€™s begin by rephrasing in terms of linear maps the question of whether a homogeneous system of linear equations has a nonzero solution. Homogeneous, in this context, means that the constant term on the right side of each equation below is 0. Fix positive integers ğ‘š and ğ‘›, and let ğ´ğ‘—, ğ‘˜ âˆˆ ğ… for ğ‘— = 1, â€¦, ğ‘š and ğ‘˜ = 1, â€¦, ğ‘›. Consider the homogeneous system of lin- ear equations ğ‘› âˆ‘ ğ‘˜ = 1 ğ´1, ğ‘˜ ğ‘¥ğ‘˜ = 0 â‹® ğ‘› âˆ‘ ğ‘˜ = 1 ğ´ğ‘š, ğ‘˜ ğ‘¥ğ‘˜ = 0. Clearly ğ‘¥1 = â‹¯ = ğ‘¥ğ‘› = 0is a solution of the system of equations above; the question here is whether any other solutions exist. Defineğ‘‡âˆ¶ ğ…ğ‘› â†’ ğ…ğ‘š by 3.25 ğ‘‡(ğ‘¥1, â€¦, ğ‘¥ğ‘›) = ( ğ‘› âˆ‘ ğ‘˜ = 1 ğ´1, ğ‘˜ ğ‘¥ğ‘˜, â€¦, ğ‘› âˆ‘ ğ‘˜ = 1 ğ´ğ‘š, ğ‘˜ ğ‘¥ğ‘˜). The equation ğ‘‡(ğ‘¥1, â€¦, ğ‘¥ğ‘›) = 0(the 0here is the additive identity in ğ…ğ‘š, namely, the list of length ğ‘š of all 0â€™s) is the same as the homogeneous system of linear equations above. Thus we want to know if null ğ‘‡ is strictly bigger than {0}, which is equivalent to ğ‘‡ not being injective (by 3.15). The next result gives an important condition for ensuring that ğ‘‡ is not injective. Section 3B Null Spaces and Ranges 65 3.26 homogeneous system of linear equations A homogeneous system of linear equations with more variables than equations has nonzero solutions. Proof Use the notation and result from the discussion above. Thus ğ‘‡ is a linear map from ğ…ğ‘› to ğ…ğ‘š, and we have a homogeneous system of ğ‘š linear equations with ğ‘› variables ğ‘¥1, â€¦, ğ‘¥ğ‘›. From 3.22 we see that ğ‘‡ is not injective if ğ‘› > ğ‘š. Example of the result above: a homogeneous system of four linear equations with five variables has nonzero solutions. Inhomogeneous, as used in this con- text, means that the constant term on the right side of at least one equation below does not equal 0. Now we consider the question of whether an inhomogeneous system of lin- ear equations has no solutions for some choice of the constant terms. To rephrase this question in terms of a linear map, fix positive integers ğ‘š and ğ‘›, and let ğ´ğ‘—, ğ‘˜ âˆˆ ğ… for all ğ‘— = 1, â€¦, ğ‘š and all ğ‘˜ = 1, â€¦, ğ‘›. For ğ‘1, â€¦, ğ‘ğ‘š âˆˆ ğ…, consider the system of linear equations ğ‘› âˆ‘ ğ‘˜ = 1 ğ´1, ğ‘˜ ğ‘¥ğ‘˜ = ğ‘1 â‹®3.27 ğ‘› âˆ‘ ğ‘˜ = 1 ğ´ğ‘š, ğ‘˜ ğ‘¥ğ‘˜ = ğ‘ğ‘š. The question here is whether there is some choice of ğ‘1, â€¦, ğ‘ğ‘š âˆˆ ğ… such that no solution exists to the system above. The results 3.26 and 3.28, which com- pare the number of variables and the number of equations, can also be proved using Gaussian elimina- tion. The abstract approach taken here seems to provide cleaner proofs. Defineğ‘‡âˆ¶ ğ…ğ‘› â†’ ğ…ğ‘š as in 3.25. The equation ğ‘‡(ğ‘¥1, â€¦, ğ‘¥ğ‘›) = (ğ‘1, â€¦, ğ‘ğ‘š) is the same as the system of equations 3.27. Thus we want to know if range ğ‘‡ â‰  ğ…ğ‘š. Hence we can rephrase our question about not having a solution for some choice of ğ‘1, â€¦, ğ‘ğ‘š âˆˆ ğ… as follows: What condition ensures that ğ‘‡ is not surjective? The next result gives one such condition. 3.28 inhomogeneous system of linear equations An inhomogeneous system of linear equations with more equations than variables has no solution for some choice of the constant terms. Proof Use the notation and result from the example above. Thus ğ‘‡ is a linear map from ğ…ğ‘› to ğ…ğ‘š, and we have a system of ğ‘š equations with ğ‘› variables ğ‘¥1, â€¦, ğ‘¥ğ‘›. From 3.24 we see that ğ‘‡ is not surjective if ğ‘› < ğ‘š. Example of the result above: an inhomogeneous system of five linear equations with four variables has no solution for some choice of the constant terms. 66 Chapter 3 Linear Maps Exercises 3B 1 Give an example of a linear map ğ‘‡ with dim null ğ‘‡ = 3and dim range ğ‘‡ = 2. 2 Suppose ğ‘†, ğ‘‡ âˆˆ â„’(ğ‘‰) are such that range ğ‘† âŠ†null ğ‘‡. Prove that (ğ‘†ğ‘‡)2 = 0. 3 Suppose ğ‘£1, â€¦, ğ‘£ğ‘š is a list of vectors in ğ‘‰. Defineğ‘‡ âˆˆ â„’(ğ…ğ‘š, ğ‘‰)by ğ‘‡(ğ‘§1, â€¦, ğ‘§ğ‘š) = ğ‘§1ğ‘£1 + â‹¯ + ğ‘§ğ‘šğ‘£ğ‘š. (a) What property of ğ‘‡ corresponds to ğ‘£1, â€¦, ğ‘£ğ‘š spanning ğ‘‰? (b) What property of ğ‘‡ corresponds to the list ğ‘£1, â€¦, ğ‘£ğ‘š being linearly independent? 4 Show that {ğ‘‡ âˆˆ â„’(ğ‘5, ğ‘4)âˆ¶ dim null ğ‘‡ > 2}is not a subspace of â„’(ğ‘5, ğ‘4). 5 Give an example of ğ‘‡ âˆˆ â„’(ğ‘4)such that range ğ‘‡ = null ğ‘‡. 6 Prove that there does not exist ğ‘‡ âˆˆ â„’(ğ‘5)such that range ğ‘‡ = null ğ‘‡. 7 Suppose ğ‘‰ and ğ‘Š are finite-dimensional with2 â‰¤dim ğ‘‰ â‰¤dim ğ‘Š. Show that {ğ‘‡ âˆˆ â„’(ğ‘‰, ğ‘Š) âˆ¶ ğ‘‡ is not injective}is not a subspace of â„’(ğ‘‰, ğ‘Š). 8 Suppose ğ‘‰ and ğ‘Š are finite-dimensional withdim ğ‘‰ â‰¥dim ğ‘Š â‰¥ 2. Show that {ğ‘‡ âˆˆ â„’(ğ‘‰, ğ‘Š) âˆ¶ ğ‘‡ is not surjective}is not a subspace of â„’(ğ‘‰, ğ‘Š). 9 Suppose ğ‘‡ âˆˆ â„’(ğ‘‰, ğ‘Š) is injective and ğ‘£1, â€¦, ğ‘£ğ‘› is linearly independent in ğ‘‰. Prove that ğ‘‡ğ‘£1, â€¦, ğ‘‡ğ‘£ğ‘› is linearly independent in ğ‘Š. 10 Suppose ğ‘£1, â€¦, ğ‘£ğ‘› spans ğ‘‰ and ğ‘‡ âˆˆ â„’(ğ‘‰, ğ‘Š). Show that ğ‘‡ğ‘£1, â€¦, ğ‘‡ğ‘£ğ‘› spans range ğ‘‡. 11 Suppose that ğ‘‰ is finite-dimensional and thatğ‘‡ âˆˆ â„’(ğ‘‰, ğ‘Š). Prove that there exists a subspace ğ‘ˆ of ğ‘‰ such that ğ‘ˆ âˆ©null ğ‘‡ = {0} and range ğ‘‡ = {ğ‘‡ğ‘¢ âˆ¶ ğ‘¢ âˆˆ ğ‘ˆ}. 12 Suppose ğ‘‡ is a linear map from ğ…4 to ğ…2 such that null ğ‘‡ = {(ğ‘¥1, ğ‘¥2, ğ‘¥3, ğ‘¥4) âˆˆ ğ…4 âˆ¶ ğ‘¥1 = 5ğ‘¥2 and ğ‘¥3 = 7ğ‘¥4}. Prove that ğ‘‡ is surjective. 13 Suppose ğ‘ˆ is a three-dimensional subspace of ğ‘8 and that ğ‘‡ is a linear map from ğ‘8 to ğ‘5 such that null ğ‘‡ = ğ‘ˆ. Prove that ğ‘‡ is surjective. 14 Prove that there does not exist a linear map from ğ…5 to ğ…2 whose null space equals {(ğ‘¥1, ğ‘¥2, ğ‘¥3, ğ‘¥4, ğ‘¥5) âˆˆ ğ…5 âˆ¶ ğ‘¥1 = 3ğ‘¥2 and ğ‘¥3 = ğ‘¥4 = ğ‘¥5}. 15 Suppose there exists a linear map on ğ‘‰ whose null space and range are both finite-dimensional. Prove thatğ‘‰ is finite-dimensional. Section 3B Null Spaces and Ranges 67 16 Suppose ğ‘‰ and ğ‘Š are both finite-dimensional. Prove that there exists an injective linear map from ğ‘‰ to ğ‘Š if and only if dim ğ‘‰ â‰¤dim ğ‘Š. 17 Suppose ğ‘‰ and ğ‘Š are both finite-dimensional. Prove that there exists a surjective linear map from ğ‘‰ onto ğ‘Š if and only if dim ğ‘‰ â‰¥dim ğ‘Š. 18 Suppose ğ‘‰ and ğ‘Š are finite-dimensional and thatğ‘ˆ is a subspace of ğ‘‰. Prove that there exists ğ‘‡ âˆˆ â„’(ğ‘‰, ğ‘Š) such that null ğ‘‡ = ğ‘ˆ if and only if dim ğ‘ˆ â‰¥dim ğ‘‰ âˆ’ dim ğ‘Š. 19 Suppose ğ‘Š is finite-dimensional andğ‘‡ âˆˆ â„’(ğ‘‰, ğ‘Š). Prove that ğ‘‡ is injective if and only if there exists ğ‘† âˆˆ â„’(ğ‘Š, ğ‘‰) such that ğ‘†ğ‘‡ is the identity operator on ğ‘‰. 20 Suppose ğ‘Š is finite-dimensional andğ‘‡ âˆˆ â„’(ğ‘‰, ğ‘Š). Prove that ğ‘‡ is surjective if and only if there exists ğ‘† âˆˆ â„’(ğ‘Š, ğ‘‰) such that ğ‘‡ğ‘† is the identity operator on ğ‘Š. 21 Suppose ğ‘‰ is finite-dimensional,ğ‘‡ âˆˆ â„’(ğ‘‰, ğ‘Š), and ğ‘ˆ is a subspace of ğ‘Š. Prove that {ğ‘£ âˆˆ ğ‘‰ âˆ¶ ğ‘‡ğ‘£ âˆˆ ğ‘ˆ} is a subspace of ğ‘‰ and dim{ğ‘£ âˆˆ ğ‘‰ âˆ¶ ğ‘‡ğ‘£ âˆˆ ğ‘ˆ} = dim null ğ‘‡ + dim(ğ‘ˆ âˆ©range ğ‘‡). 22 Suppose ğ‘ˆ and ğ‘‰ are finite-dimensional vector spaces andğ‘† âˆˆ â„’(ğ‘‰, ğ‘Š) and ğ‘‡ âˆˆ â„’(ğ‘ˆ, ğ‘‰). Prove that dim null ğ‘†ğ‘‡ â‰¤dim null ğ‘† + dim null ğ‘‡. 23 Suppose ğ‘ˆ and ğ‘‰ are finite-dimensional vector spaces andğ‘† âˆˆ â„’(ğ‘‰, ğ‘Š) and ğ‘‡ âˆˆ â„’(ğ‘ˆ, ğ‘‰). Prove that dim range ğ‘†ğ‘‡ â‰¤min{dim range ğ‘†, dim range ğ‘‡}. 24 (a) Suppose dim ğ‘‰ = 5and ğ‘†, ğ‘‡ âˆˆ â„’(ğ‘‰) are such that ğ‘†ğ‘‡ = 0. Prove that dim range ğ‘‡ğ‘† â‰¤ 2. (b) Give an example of ğ‘†, ğ‘‡ âˆˆ â„’(ğ…5)with ğ‘†ğ‘‡ = 0and dim range ğ‘‡ğ‘† = 2. 25 Suppose that ğ‘Š is finite-dimensional andğ‘†, ğ‘‡ âˆˆ â„’(ğ‘‰, ğ‘Š). Prove that null ğ‘† âŠ†null ğ‘‡ if and only if there exists ğ¸ âˆˆ â„’(ğ‘Š) such that ğ‘‡ = ğ¸ğ‘†. 26 Suppose that ğ‘‰ is finite-dimensional andğ‘†, ğ‘‡ âˆˆ â„’(ğ‘‰, ğ‘Š). Prove that range ğ‘† âŠ†range ğ‘‡ if and only if there exists ğ¸ âˆˆ â„’(ğ‘‰) such that ğ‘† = ğ‘‡ğ¸. 27 Suppose ğ‘ƒ âˆˆ â„’(ğ‘‰) and ğ‘ƒ2 = ğ‘ƒ. Prove that ğ‘‰ = null ğ‘ƒ âŠ• range ğ‘ƒ. 28 Suppose ğ· âˆˆ â„’(ğ’«(ğ‘))is such that deg ğ·ğ‘ = (deg ğ‘) âˆ’ 1for every non- constant polynomial ğ‘ âˆˆ ğ’«(ğ‘). Prove that ğ· is surjective. The notation ğ· is used above to remind you of the differentiation map that sends a polynomial ğ‘ to ğ‘ â€². 68 Chapter 3 Linear Maps 29 Suppose ğ‘ âˆˆ ğ’«(ğ‘). Prove that there exists a polynomial ğ‘ âˆˆ ğ’«(ğ‘) such that 5ğ‘ â€³ + 3ğ‘ â€² = ğ‘. This exercise can be done without linear algebra, but itâ€™s more fun to do it using linear algebra. 30 Suppose ğœ‘ âˆˆ â„’(ğ‘‰, ğ…) and ğœ‘ â‰  0. Suppose ğ‘¢ âˆˆ ğ‘‰ is not in null ğœ‘. Prove that ğ‘‰ = null ğœ‘ âŠ• {ğ‘ğ‘¢ âˆ¶ ğ‘ âˆˆ ğ…}. 31 Suppose ğ‘‰ is finite-dimensional,ğ‘‹ is a subspace of ğ‘‰, and ğ‘Œ is a finite- dimensional subspace of ğ‘Š. Prove that there exists ğ‘‡ âˆˆ â„’(ğ‘‰, ğ‘Š) such that null ğ‘‡ = ğ‘‹ and range ğ‘‡ = ğ‘Œ if and only if dim ğ‘‹ + dim ğ‘Œ = dim ğ‘‰. 32 Suppose ğ‘‰ is finite-dimensional withdim ğ‘‰ > 1. Show that if ğœ‘âˆ¶ â„’(ğ‘‰) â†’ ğ… is a linear map such that ğœ‘(ğ‘†ğ‘‡) = ğœ‘(ğ‘†)ğœ‘(ğ‘‡) for all ğ‘†, ğ‘‡ âˆˆ â„’(ğ‘‰), then ğœ‘ = 0. Hint: The description of the two-sided ideals of â„’(ğ‘‰) given by Exercise 17 in Section 3A might be useful. 33 Suppose that ğ‘‰ and ğ‘Š are real vector spaces and ğ‘‡ âˆˆ â„’(ğ‘‰, ğ‘Š). Define ğ‘‡ğ‚ âˆ¶ ğ‘‰ğ‚ â†’ ğ‘Šğ‚ by ğ‘‡ğ‚(ğ‘¢ + ğ‘–ğ‘£) = ğ‘‡ğ‘¢ + ğ‘–ğ‘‡ğ‘£ for all ğ‘¢, ğ‘£ âˆˆ ğ‘‰. (a) Show that ğ‘‡ğ‚ is a (complex) linear map from ğ‘‰ğ‚ to ğ‘Šğ‚. (b) Show that ğ‘‡ğ‚ is injective if and only if ğ‘‡ is injective. (c) Show that range ğ‘‡ğ‚ = ğ‘Šğ‚ if and only if range ğ‘‡ = ğ‘Š. See Exercise 8 in Section 1B for the definition of the complexification ğ‘‰ğ‚. The linear map ğ‘‡ğ‚ is called the complexification of the linear map ğ‘‡. Section 3C Matrices 69 3C Matrices Representing a Linear Map by a Matrix We know that if ğ‘£1, â€¦, ğ‘£ğ‘› is a basis of ğ‘‰ and ğ‘‡âˆ¶ ğ‘‰ â†’ ğ‘Š is linear, then the values of ğ‘‡ğ‘£1, â€¦, ğ‘‡ğ‘£ğ‘› determine the values of ğ‘‡ on arbitrary vectors in ğ‘‰â€”see the linear map lemma (3.4). As we will soon see, matrices provide an efficient method of recording the values of the ğ‘‡ğ‘£ğ‘˜â€™s in terms of a basis of ğ‘Š. 3.29 definition: matrix, ğ´ğ‘—, ğ‘˜ Suppose ğ‘š and ğ‘› are nonnegative integers. An ğ‘š-by-ğ‘› matrix ğ´ is a rectangular array of elements of ğ… with ğ‘š rows and ğ‘› columns: ğ´ = â›âœâœâœ â ğ´1, 1 â‹¯ ğ´1, ğ‘› â‹® â‹® ğ´ğ‘š, 1 â‹¯ ğ´ğ‘š, ğ‘› ââŸâŸâŸ â  . The notation ğ´ğ‘—, ğ‘˜ denotes the entry in row ğ‘—, column ğ‘˜ of ğ´. 3.30 example: ğ´ğ‘—, ğ‘˜ equals entry in row ğ‘—, column ğ‘˜ of ğ´ When dealing with matrices, the first index refers to the row number; the sec- ond index refers to the column number. Suppose ğ´ = ( 8 4 5 âˆ’ 3ğ‘– 1 9 7). Thus ğ´2, 3 refers to the entry in the sec- ond row, third column of ğ´, which means that ğ´2, 3 = 7. Now we come to the key definition in this section. 3.31 definition: matrix of a linear map, â„³(ğ‘‡) Suppose ğ‘‡ âˆˆ â„’(ğ‘‰, ğ‘Š) and ğ‘£1, â€¦, ğ‘£ğ‘› is a basis of ğ‘‰ and ğ‘¤1, â€¦, ğ‘¤ğ‘š is a basis of ğ‘Š. The matrix of ğ‘‡ with respect to these bases is the ğ‘š-by-ğ‘› matrix â„³(ğ‘‡) whose entries ğ´ğ‘—, ğ‘˜ are defined by ğ‘‡ğ‘£ğ‘˜ = ğ´1, ğ‘˜ğ‘¤1 + â‹¯ + ğ´ğ‘š, ğ‘˜ğ‘¤ğ‘š. If the bases ğ‘£1, â€¦, ğ‘£ğ‘› and ğ‘¤1, â€¦, ğ‘¤ğ‘š are not clear from the context, then the notation â„³(ğ‘‡, (ğ‘£1, â€¦, ğ‘£ğ‘›), (ğ‘¤1, â€¦, ğ‘¤ğ‘š))is used. The matrix â„³(ğ‘‡) of a linear map ğ‘‡ âˆˆ â„’(ğ‘‰, ğ‘Š) depends on the basis ğ‘£1, â€¦, ğ‘£ğ‘› of ğ‘‰ and the basis ğ‘¤1, â€¦, ğ‘¤ğ‘š of ğ‘Š, as well as on ğ‘‡. However, the bases should be clear from the context, and thus they are often not included in the notation. To remember how â„³(ğ‘‡) is constructed from ğ‘‡, you might write across the top of the matrix the basis vectors ğ‘£1, â€¦, ğ‘£ğ‘› for the domain and along the left the basis vectors ğ‘¤1, â€¦, ğ‘¤ğ‘š for the vector space into which ğ‘‡ maps, as follows: 70 Chapter 3 Linear Maps ğ‘£1 â‹¯ ğ‘£ğ‘˜ â‹¯ ğ‘£ğ‘› ğ‘¤1 â„³(ğ‘‡) = â‹® ğ‘¤ğ‘š â›âœâœâœ â ğ´1, ğ‘˜ â‹® ğ´ğ‘š, ğ‘˜ ââŸâŸâŸ â  . The ğ‘˜th column of â„³(ğ‘‡) consists of the scalars needed to write ğ‘‡ğ‘£ğ‘˜ as a linear combination of ğ‘¤1, â€¦, ğ‘¤ğ‘š: ğ‘‡ğ‘£ğ‘˜ = ğ‘š âˆ‘ ğ‘— = 1 ğ´ğ‘—, ğ‘˜ğ‘¤ğ‘—. In the matrix above only the ğ‘˜th col- umn is shown. Thus the second index of each displayed entry of the matrix above is ğ‘˜. The picture above should remind you that ğ‘‡ğ‘£ğ‘˜ can be computed from â„³(ğ‘‡) by multiplying each entry in the ğ‘˜th column by the corresponding ğ‘¤ğ‘— from the left col- umn, and then adding up the resulting vectors. If ğ‘‡ is a linear map from an ğ‘›-dimensional vector space to an ğ‘š-dimensional vector space, then â„³(ğ‘‡) is an ğ‘š-by-ğ‘› matrix. If ğ‘‡ is a linear map from ğ…ğ‘› to ğ…ğ‘š, then unless stated otherwise, assume the bases in question are the standard ones (where the ğ‘˜th basis vector is 1in the ğ‘˜th slot and 0in all other slots). If you think of elements of ğ…ğ‘š as columns of ğ‘š numbers, then you can think of the ğ‘˜th column of â„³(ğ‘‡) as ğ‘‡ applied to the ğ‘˜th standard basis vector. 3.32 example: the matrix of a linear map from ğ…2 to ğ…3 Suppose ğ‘‡ âˆˆ â„’(ğ…2, ğ…3)is defined by ğ‘‡(ğ‘¥, ğ‘¦) = (ğ‘¥ + 3ğ‘¦, 2ğ‘¥+ 5ğ‘¦, 7ğ‘¥+ 9ğ‘¦). Because ğ‘‡(1, 0) = (1, 2, 7)and ğ‘‡(0, 1) = (3, 5, 9), the matrix of ğ‘‡ with respect to the standard bases is the 3-by-2matrix below: â„³(ğ‘‡) = â›âœâœâœ â 1 3 2 5 7 9 ââŸâŸâŸ â  . When working with ğ’«ğ‘š(ğ…), use the standard basis 1, ğ‘¥, ğ‘¥2, â€¦, ğ‘¥ğ‘š unless the context indicates otherwise. 3.33 example: matrix of the differentiation map from ğ’«3(ğ‘) to ğ’«2(ğ‘) Suppose ğ· âˆˆ â„’(ğ’«3(ğ‘), ğ’«2(ğ‘))is the differentiation map defined byğ·ğ‘ = ğ‘â€². Because (ğ‘¥ğ‘›) â€² = ğ‘›ğ‘¥ğ‘› âˆ’ 1, the matrix of ğ· with respect to the standard bases is the 3-by-4matrix below: â„³(ğ·) = â›âœâœâœ â 0 1 0 0 0 0 2 0 0 0 0 3 ââŸâŸâŸ â  . Section 3C Matrices 71 Addition and Scalar Multiplication of Matrices For the rest of this section, assume that ğ‘ˆ, ğ‘‰, and ğ‘Š are finite-dimensional and that a basis has been chosen for each of these vector spaces. Thus for each linear map from ğ‘‰ to ğ‘Š, we can talk about its matrix (with respect to the chosen bases). Is the matrix of the sum of two linear maps equal to the sum of the matrices of the two maps? Right now this question does not yet make sense because although we have defined the sum of two linear maps, we have not defined the sum of two matrices. Fortunately, the natural definition of the sum of two matrices has the right properties. Specifically, we make the following definition. 3.34 definition: matrix addition The sum of two matrices of the same size is the matrix obtained by adding corresponding entries in the matrices: â›âœâœâœ â ğ´1, 1 â‹¯ ğ´1, ğ‘› â‹® â‹® ğ´ğ‘š, 1 â‹¯ ğ´ğ‘š, ğ‘› ââŸâŸâŸ â  + â›âœâœâœ â ğ¶1, 1 â‹¯ ğ¶1, ğ‘› â‹® â‹® ğ¶ğ‘š, 1 â‹¯ ğ¶ğ‘š, ğ‘› ââŸâŸâŸ â  = â›âœâœâœ â ğ´1, 1 + ğ¶1, 1 â‹¯ ğ´1, ğ‘› + ğ¶1, ğ‘› â‹® â‹® ğ´ğ‘š, 1 + ğ¶ğ‘š, 1 â‹¯ ğ´ğ‘š, ğ‘› + ğ¶ğ‘š, ğ‘› ââŸâŸâŸ â  . In the next result, the assumption is that the same bases are used for all three linear maps ğ‘† + ğ‘‡, ğ‘†, and ğ‘‡. 3.35 matrix of the sum of linear maps Suppose ğ‘†, ğ‘‡ âˆˆ â„’(ğ‘‰, ğ‘Š). Then â„³(ğ‘† + ğ‘‡) = â„³(ğ‘†) + â„³(ğ‘‡). The verification of the result above follows from the definitions and is left to the reader. Still assuming that we have some bases in mind, is the matrix of a scalar times a linear map equal to the scalar times the matrix of the linear map? Again, the question does not yet make sense because we have not defined scalar multiplication on matrices. Fortunately, the natural definition again has the right properties. 3.36 definition:scalar multiplication of a matrix The product of a scalar and a matrix is the matrix obtained by multiplying each entry in the matrix by the scalar: ğœ†â›âœâœâœ â ğ´1, 1 â‹¯ ğ´1, ğ‘› â‹® â‹® ğ´ğ‘š, 1 â‹¯ ğ´ğ‘š, ğ‘› ââŸâŸâŸ â  = â›âœâœâœ â ğœ†ğ´1, 1 â‹¯ ğœ†ğ´1, ğ‘› â‹® â‹® ğœ†ğ´ğ‘š, 1 â‹¯ ğœ†ğ´ğ‘š, ğ‘› ââŸâŸâŸ â  . 72 Chapter 3 Linear Maps 3.37 example:addition and scalar multiplication of matrices 2( 3 1 âˆ’1 5 )+ ( 4 2 1 6 )= ( 6 2 âˆ’2 10 )+ ( 4 2 1 6 )= ( 10 4 âˆ’1 16 ) In the next result, the assumption is that the same bases are used for both the linear maps ğœ†ğ‘‡ and ğ‘‡. 3.38 the matrix of a scalar times a linear map Suppose ğœ† âˆˆ ğ… and ğ‘‡ âˆˆ â„’(ğ‘‰, ğ‘Š). Then â„³(ğœ†ğ‘‡) = ğœ†â„³(ğ‘‡). The verification of the result above is also left to the reader. Because addition and scalar multiplication have now been defined for matrices, you should not be surprised that a vector space is about to appear. First we introduce a bit of notation so that this new vector space has a name, and then we find the dimension of this new vector space. 3.39 notation: ğ…ğ‘š, ğ‘› For ğ‘š and ğ‘› positive integers, the set of all ğ‘š-by-ğ‘› matrices with entries in ğ… is denoted by ğ…ğ‘š, ğ‘›. 3.40 dim ğ…ğ‘š, ğ‘› = ğ‘šğ‘› Suppose ğ‘š and ğ‘› are positive integers. With addition and scalar multiplication defined as above,ğ…ğ‘š, ğ‘› is a vector space of dimension ğ‘šğ‘›. Proof The verification thatğ…ğ‘š, ğ‘› is a vector space is left to the reader. Note that the additive identity of ğ…ğ‘š, ğ‘› is the ğ‘š-by-ğ‘› matrix all of whose entries equal 0. The reader should also verify that the list of distinct ğ‘š-by-ğ‘› matrices that have 0in all entries except for a 1in one entry is a basis of ğ…ğ‘š, ğ‘›. There are ğ‘šğ‘› such matrices, so the dimension of ğ…ğ‘š, ğ‘› equals ğ‘šğ‘›. Matrix Multiplication Suppose, as previously, that ğ‘£1, â€¦, ğ‘£ğ‘› is a basis of ğ‘‰ and ğ‘¤1, â€¦, ğ‘¤ğ‘š is a basis of ğ‘Š. Suppose also that ğ‘¢1, â€¦, ğ‘¢ğ‘ is a basis of ğ‘ˆ. Consider linear maps ğ‘‡âˆ¶ ğ‘ˆ â†’ ğ‘‰ and ğ‘†âˆ¶ ğ‘‰ â†’ ğ‘Š. The composition ğ‘†ğ‘‡ is a linear map from ğ‘ˆ to ğ‘Š. Does â„³(ğ‘†ğ‘‡) equal â„³(ğ‘†)â„³(ğ‘‡)? This question does not yet make sense because we have not defined the product of two matrices. We will choose a definition of matrix multiplication that forces this question to have a positive answer. Letâ€™s see how to do this. Section 3C Matrices 73 Suppose â„³(ğ‘†) = ğ´ and â„³(ğ‘‡) = ğµ. For 1 â‰¤ ğ‘˜ â‰¤ ğ‘, we have (ğ‘†ğ‘‡)ğ‘¢ğ‘˜ = ğ‘†( ğ‘› âˆ‘ ğ‘Ÿ = 1 ğµğ‘Ÿ, ğ‘˜ğ‘£ğ‘Ÿ) = ğ‘› âˆ‘ ğ‘Ÿ = 1 ğµğ‘Ÿ, ğ‘˜ğ‘†ğ‘£ğ‘Ÿ = ğ‘› âˆ‘ ğ‘Ÿ = 1 ğµğ‘Ÿ, ğ‘˜ ğ‘š âˆ‘ ğ‘— = 1 ğ´ğ‘—, ğ‘Ÿğ‘¤ğ‘— = ğ‘š âˆ‘ ğ‘— = 1( ğ‘› âˆ‘ ğ‘Ÿ = 1 ğ´ğ‘—, ğ‘Ÿğµğ‘Ÿ, ğ‘˜)ğ‘¤ğ‘—. Thus â„³(ğ‘†ğ‘‡) is the ğ‘š-by-ğ‘ matrix whose entry in row ğ‘—, column ğ‘˜, equals ğ‘› âˆ‘ ğ‘Ÿ = 1 ğ´ğ‘—, ğ‘Ÿğµğ‘Ÿ, ğ‘˜. Now we see how to define matrix multiplication so that the desired equation â„³(ğ‘†ğ‘‡) = â„³(ğ‘†)â„³(ğ‘‡) holds. 3.41 definition: matrix multiplication Suppose ğ´ is an ğ‘š-by-ğ‘› matrix and ğµ is an ğ‘›-by-ğ‘ matrix. Then ğ´ğµ is defined to be the ğ‘š-by-ğ‘ matrix whose entry in row ğ‘—, column ğ‘˜, is given by the equation (ğ´ğµ)ğ‘—, ğ‘˜ = ğ‘› âˆ‘ ğ‘Ÿ = 1 ğ´ğ‘—, ğ‘Ÿğµğ‘Ÿ, ğ‘˜. Thus the entry in row ğ‘—, column ğ‘˜, of ğ´ğµ is computed by taking row ğ‘— of ğ´ and column ğ‘˜ of ğµ, multiplying together corresponding entries, and then summing. You may have learned this definition of matrix multiplication in an earlier course, although you may not have seen this motivation for it. Note that we define the product of two matrices only when the number of columns of the first matrix equals the number of rows of the second matrix. 3.42 example: matrix multiplication Here we multiply together a 3-by-2matrix and a 2-by-4matrix, obtaining a 3-by-4matrix: â›âœâœâœ â 1 2 3 4 5 6 ââŸâŸâŸ â  ( 6 5 4 3 2 1 0 âˆ’1 )= â›âœâœâœ â 10 7 4 1 26 19 12 5 42 31 20 9 ââŸâŸâŸ â  . Matrix multiplication is not commutativeâ€”ğ´ğµ is not necessarily equal to ğµğ´ even if both products are defined (see Exercise10). Matrix multiplication is distributive and associative (see Exercises 11 and 12). 74 Chapter 3 Linear Maps In the next result, we assume that the same basis of ğ‘‰ is used in considering ğ‘‡ âˆˆ â„’(ğ‘ˆ, ğ‘‰) and ğ‘† âˆˆ â„’(ğ‘‰, ğ‘Š), the same basis of ğ‘Š is used in considering ğ‘† âˆˆ â„’(ğ‘‰, ğ‘Š) and ğ‘†ğ‘‡ âˆˆ â„’(ğ‘ˆ, ğ‘Š), and the same basis of ğ‘ˆ is used in considering ğ‘‡ âˆˆ â„’(ğ‘ˆ, ğ‘‰) and ğ‘†ğ‘‡ âˆˆ â„’(ğ‘ˆ, ğ‘Š). 3.43 matrix of product of linear maps If ğ‘‡ âˆˆ â„’(ğ‘ˆ, ğ‘‰) and ğ‘† âˆˆ â„’(ğ‘‰, ğ‘Š), then â„³(ğ‘†ğ‘‡) = â„³(ğ‘†)â„³(ğ‘‡). The proof of the result above is the calculation that was done as motivation before the definition of matrix multiplication. In the next piece of notation, note that as usual the first index refers to a row and the second index refers to a column, with a vertically centered dot used as a placeholder. 3.44 notation: ğ´ğ‘—, â‹… , ğ´â‹…, ğ‘˜ Suppose ğ´ is an ğ‘š-by-ğ‘› matrix. â€¢ If 1 â‰¤ ğ‘— â‰¤ ğ‘š, then ğ´ğ‘—, â‹… denotes the 1-by-ğ‘› matrix consisting of row ğ‘— of ğ´. â€¢ If 1 â‰¤ ğ‘˜ â‰¤ ğ‘›, then ğ´â‹…, ğ‘˜ denotes the ğ‘š-by-1matrix consisting of column ğ‘˜ of ğ´. 3.45 example: ğ´ğ‘—, â‹… equals ğ‘—th row of ğ´ and ğ´â‹…, ğ‘˜ equals ğ‘˜th column of ğ´ The notation ğ´2, â‹… denotes the second row of ğ´ and ğ´â‹…, 2 denotes the second column of ğ´. Thus if ğ´ = ( 8 4 5 1 9 7 ), then ğ´2, â‹… = (1 9 7) and ğ´â‹…, 2 = ( 4 9). The product of a 1-by-ğ‘› matrix and an ğ‘›-by-1matrix is a 1-by-1matrix. How- ever, we will frequently identify a 1-by-1matrix with its entry. For example, (3 4)( 6 2)= (26) because 3 â‹… 6+ 4 â‹… 2 = 26. However, we can identify (26)with 26, writing (3 4)( 6 2)= 26. The next result uses the convention discussed in the paragraph above to give another way to think of matrix multiplication. For example, the next result and the calculation in the paragraph above explain why the entry in row 2, column 1, of the product in Example 3.42 equals 26. Section 3C Matrices 75 3.46 entry of matrix product equals row times column Suppose ğ´ is an ğ‘š-by-ğ‘› matrix and ğµ is an ğ‘›-by-ğ‘ matrix. Then (ğ´ğµ)ğ‘—, ğ‘˜ = ğ´ğ‘—, â‹… ğµâ‹…, ğ‘˜ if 1 â‰¤ ğ‘— â‰¤ ğ‘šand 1 â‰¤ ğ‘˜ â‰¤ ğ‘. In other words, the entry in row ğ‘—, column ğ‘˜, of ğ´ğµ equals (row ğ‘— of ğ´) times (column ğ‘˜ of ğµ). Proof Suppose 1 â‰¤ ğ‘— â‰¤ ğ‘šand 1 â‰¤ ğ‘˜ â‰¤ ğ‘. The definition of matrix multiplication states that 3.47 (ğ´ğµ)ğ‘—, ğ‘˜ = ğ´ğ‘—, 1ğµ1, ğ‘˜ + â‹¯ + ğ´ğ‘—, ğ‘›ğµğ‘›, ğ‘˜. The definition of matrix multiplication also implies that the product of the1-by-ğ‘› matrix ğ´ğ‘—, â‹… and the ğ‘›-by-1matrix ğµâ‹…, ğ‘˜ is the 1-by-1matrix whose entry is the number on the right side of the equation above. Thus the entry in row ğ‘—, column ğ‘˜, of ğ´ğµ equals (row ğ‘— of ğ´) times (column ğ‘˜ of ğµ). The next result gives yet another way to think of matrix multiplication. In the result below, (ğ´ğµ)â‹…, ğ‘˜ is column ğ‘˜ of the ğ‘š-by-ğ‘ matrix ğ´ğµ. Thus (ğ´ğµ)â‹…, ğ‘˜ is an ğ‘š-by-1matrix. Also, ğ´ğµâ‹…, ğ‘˜ is an ğ‘š-by-1matrix because it is the product of an ğ‘š-by-ğ‘› matrix and an ğ‘›-by-1matrix. Thus the two sides of the equation in the result below have the same size, making it reasonable that they might be equal. 3.48 column of matrix product equals matrix times column Suppose ğ´ is an ğ‘š-by-ğ‘› matrix and ğµ is an ğ‘›-by-ğ‘ matrix. Then (ğ´ğµ)â‹…, ğ‘˜ = ğ´ğµâ‹…, ğ‘˜ if 1 â‰¤ ğ‘˜ â‰¤ ğ‘. In other words, column ğ‘˜ of ğ´ğµ equals ğ´ times column ğ‘˜ of ğµ. Proof As discussed above, (ğ´ğµ)â‹…, ğ‘˜ and ğ´ğµâ‹…, ğ‘˜ are both ğ‘š-by-1matrices. If 1 â‰¤ ğ‘— â‰¤ ğ‘š, then the entry in row ğ‘— of (ğ´ğµ)â‹…, ğ‘˜ is the left side of 3.47 and the entry in row ğ‘— of ğ´ğµâ‹…, ğ‘˜ is the right side of 3.47. Thus (ğ´ğµ)â‹…, ğ‘˜ = ğ´ğµâ‹…, ğ‘˜. Our next result will give another way of thinking about the product of an ğ‘š-by-ğ‘› matrix and an ğ‘›-by-1matrix, motivated by the next example. 3.49 example: product of a 3-by-2matrix and a 2-by-1matrix Use our definitions and basic arithmetic to verify that â›âœâœâœ â 1 2 3 4 5 6 ââŸâŸâŸ â  ( 5 1)= â›âœâœâœ â 7 19 31 ââŸâŸâŸ â  = 5 â›âœâœâœ â 1 3 5 ââŸâŸâŸ â  + 1 â›âœâœâœ â 2 4 6 ââŸâŸâŸ â  . Thus in this example, the product of a 3-by-2matrix and a 2-by-1matrix is a linear combination of the columns of the 3-by-2matrix, with the scalars (5and 1) that multiply the columns coming from the 2-by-1matrix. 76 Chapter 3 Linear Maps The next result generalizes the example above. 3.50 linear combination of columns Suppose ğ´ is an ğ‘š-by-ğ‘› matrix and ğ‘ = â›âœâœâœ â ğ‘1 â‹® ğ‘ğ‘› ââŸâŸâŸ â  is an ğ‘›-by-1matrix. Then ğ´ğ‘ = ğ‘1ğ´â‹…, 1 + â‹¯ + ğ‘ğ‘› ğ´â‹…, ğ‘›. In other words, ğ´ğ‘ is a linear combination of the columns of ğ´, with the scalars that multiply the columns coming from ğ‘. Proof If ğ‘˜ âˆˆ {1, â€¦, ğ‘š}, then the definition of matrix multiplication implies that the entry in row ğ‘˜ of the ğ‘š-by-1matrix ğ´ğ‘ is ğ´ğ‘˜, 1ğ‘1 + â‹¯ + ğ´ğ‘˜, ğ‘›ğ‘ğ‘›. The entry in row ğ‘˜ of ğ‘1ğ´â‹…, 1 + â‹¯ + ğ‘ğ‘› ğ´â‹…, ğ‘› also equals the number displayed above. Because ğ´ğ‘ and ğ‘1ğ´â‹…, 1 + â‹¯ + ğ‘ğ‘› ğ´â‹…, ğ‘› have the same entry in row ğ‘˜ for each ğ‘˜ âˆˆ {1, â€¦, ğ‘š}, we conclude that ğ´ğ‘ = ğ‘1ğ´â‹…, 1 + â‹¯ + ğ‘ğ‘› ğ´â‹…, ğ‘›. Our two previous results focus on the columns of a matrix. Analogous results hold for the rows of a matrix. Specifically, see Exercises8 and 9, which can be proved using appropriate modifications of the proofs of3.48 and 3.50. The next result is the main tool used in the next subsection to prove the columnâ€“row factorization (3.56) and to prove that the column rank of a matrix equals the row rank (3.57). To be consistent with the notation often used with the columnâ€“row factorization, including in the next subsection, the matrices in the next result are called ğ¶ and ğ‘… instead of ğ´ and ğµ. 3.51 matrix multiplication as linear combinations of columns Suppose ğ¶ is an ğ‘š-by-ğ‘ matrix and ğ‘… is a ğ‘-by-ğ‘› matrix. (a) If ğ‘˜ âˆˆ {1, â€¦, ğ‘›}, then column ğ‘˜ of ğ¶ğ‘… is a linear combination of the columns of ğ¶, with the coefficients of this linear combination coming from column ğ‘˜ of ğ‘…. (b) If ğ‘— âˆˆ {1, â€¦, ğ‘š}, then row ğ‘— of ğ¶ğ‘… is a linear combination of the rows of ğ‘…, with the coefficients of this linear combination coming from row ğ‘— of ğ¶. Proof Suppose ğ‘˜ âˆˆ {1, â€¦, ğ‘›}. Then column ğ‘˜ of ğ¶ğ‘… equals ğ¶ğ‘…â‹…, ğ‘˜ (by 3.48), which equals the linear combination of the columns of ğ¶ with coefficients coming from ğ‘…â‹…, ğ‘˜ (by 3.50). Thus (a) holds. To prove (b), follow the pattern of the proof of (a) but use rows instead of columns and use Exercises 8 and 9 instead of 3.48 and 3.50. Section 3C Matrices 77 Columnâ€“Row Factorization and Rank of a Matrix We begin by defining two nonnegative integers associated with each matrix. 3.52 definition: column rank, row rank Suppose ğ´ is an ğ‘š-by-ğ‘› matrix with entries in ğ…. â€¢ The column rank of ğ´ is the dimension of the span of the columns of ğ´ in ğ…ğ‘š, 1. â€¢ The row rank of ğ´ is the dimension of the span of the rows of ğ´ in ğ…1, ğ‘›. If ğ´ is an ğ‘š-by-ğ‘› matrix, then the column rank of ğ´ is at most ğ‘› (because ğ´ has ğ‘› columns) and the column rank of ğ´ is also at most ğ‘š (because dim ğ…ğ‘š, 1 = ğ‘š). Similarly, the row rank of ğ´ is also at most min{ğ‘š, ğ‘›}. 3.53 example: column rank and row rank of a 2-by-4matrix Suppose ğ´ = ( 4 7 1 8 3 5 2 9 ). The column rank of ğ´ is the dimension of span â›âœâœ â ( 4 3), ( 7 5), ( 1 2), ( 8 9)ââŸâŸ â  in ğ…2, 1. Neither of the first two vectors listed above inğ…2, 1 is a scalar multiple of the other. Thus the span of this list of length four has dimension at least two. The span of this list of vectors in ğ…2, 1 cannot have dimension larger than two because dim ğ…2, 1 = 2. Thus the span of this list has dimension two, which means that the column rank of ğ´ is two. The row rank of ğ´ is the dimension of span((4 7 1 8), (3 5 2 9)) in ğ…1, 4. Neither of the two vectors listed above in ğ…1, 4 is a scalar multiple of the other. Thus the span of this list of length two has dimension two, which means that the row rank of ğ´ is two. We now define the transpose of a matrix. 3.54 definition: transpose, ğ´ t The transpose of a matrix ğ´, denoted by ğ´ t, is the matrix obtained from ğ´ by interchanging rows and columns. Specifically, ifğ´ is an ğ‘š-by-ğ‘› matrix, then ğ´ t is the ğ‘›-by-ğ‘š matrix whose entries are given by the equation (ğ´ t) ğ‘˜, ğ‘— = ğ´ğ‘—, ğ‘˜. 78 Chapter 3 Linear Maps 3.55 example: transpose of a matrix If ğ´ = â›âœâœâœ â 5 âˆ’7 3 8 âˆ’4 2 ââŸâŸâŸ â  , then ğ´ t = ( 5 3 âˆ’4 âˆ’7 8 2 ). Note that here ğ´ is a 3-by-2matrix and ğ´ t is a 2-by-3matrix. The transpose has nice algebraic properties: (ğ´ + ğµ)t = ğ´t + ğµ t, (ğœ†ğ´)t = ğœ†ğ´t, and (ğ´ğ¶) t = ğ¶ tğ´ t for all ğ‘š-by-ğ‘› matrices ğ´, ğµ, all ğœ† âˆˆ ğ…, and all ğ‘›-by-ğ‘ matrices ğ¶ (see Exercises 14 and 15). The next result will be the main tool used to prove that the column rank equals the row rank (see 3.57). 3.56 columnâ€“row factorization Suppose ğ´ is an ğ‘š-by-ğ‘› matrix with entries in ğ… and column rank ğ‘ â‰¥ 1. Then there exist an ğ‘š-by-ğ‘ matrix ğ¶ and a ğ‘-by-ğ‘› matrix ğ‘…, both with entries in ğ…, such that ğ´ = ğ¶ğ‘…. Proof Each column of ğ´ is an ğ‘š-by-1matrix. The list ğ´â‹…, 1, â€¦, ğ´â‹…, ğ‘› of columns of ğ´ can be reduced to a basis of the span of the columns of ğ´ (by 2.30). This basis has length ğ‘, by the definition of the column rank. Theğ‘ columns in this basis can be put together to form an ğ‘š-by-ğ‘ matrix ğ¶. If ğ‘˜ âˆˆ {1, â€¦, ğ‘›}, then column ğ‘˜ of ğ´ is a linear combination of the columns of ğ¶. Make the coefficients of this linear combination into column ğ‘˜ of a ğ‘-by-ğ‘› matrix that we call ğ‘…. Then ğ´ = ğ¶ğ‘…, as follows from 3.51(a). In Example 3.53, the column rank and row rank turned out to equal each other. The next result states that this happens for all matrices. 3.57 column rank equals row rank Suppose ğ´ âˆˆ ğ…ğ‘š, ğ‘›. Then the column rank of ğ´ equals the row rank of ğ´. Proof Let ğ‘ denote the column rank of ğ´. Let ğ´ = ğ¶ğ‘… be the columnâ€“row factorization of ğ´ given by 3.56, where ğ¶ is an ğ‘š-by-ğ‘ matrix and ğ‘… is a ğ‘-by-ğ‘› matrix. Then 3.51(b) tells us that every row of ğ´ is a linear combination of the rows of ğ‘…. Because ğ‘… has ğ‘ rows, this implies that the row rank of ğ´ is less than or equal to the column rank ğ‘ of ğ´. To prove the inequality in the other direction, apply the result in the previous paragraph to ğ´ t, getting column rank of ğ´ = row rank of ğ´ t â‰¤column rank of ğ´ t = row rank of ğ´. Thus the column rank of ğ´ equals the row rank of ğ´. Section 3C Matrices 79 Because the column rank equals the row rank, the last result allows us to dispense with the terms â€œcolumn rankâ€ and â€œrow rankâ€ and just use the simpler term â€œrankâ€. 3.58 definition: rank The rank of a matrix ğ´ âˆˆ ğ…ğ‘š, ğ‘› is the column rank of ğ´. See 3.133 and Exercise 8 in Section 7A for alternative proofs that the column rank equals the row rank. Exercises 3C 1 Suppose ğ‘‡ âˆˆ â„’(ğ‘‰, ğ‘Š). Show that with respect to each choice of bases of ğ‘‰ and ğ‘Š, the matrix of ğ‘‡ has at least dim range ğ‘‡ nonzero entries. 2 Suppose ğ‘‰ and ğ‘Š are finite-dimensional andğ‘‡ âˆˆ â„’(ğ‘‰, ğ‘Š). Prove that dim range ğ‘‡ = 1if and only if there exist a basis of ğ‘‰ and a basis of ğ‘Š such that with respect to these bases, all entries of â„³(ğ‘‡) equal 1. 3 Suppose ğ‘£1, â€¦, ğ‘£ğ‘› is a basis of ğ‘‰ and ğ‘¤1, â€¦, ğ‘¤ğ‘š is a basis of ğ‘Š. (a) Show that if ğ‘†, ğ‘‡ âˆˆ â„’(ğ‘‰, ğ‘Š), then â„³(ğ‘† + ğ‘‡) = â„³(ğ‘†) + â„³(ğ‘‡). (b) Show that if ğœ† âˆˆ ğ… and ğ‘‡ âˆˆ â„’(ğ‘‰, ğ‘Š), then â„³(ğœ†ğ‘‡) = ğœ†â„³(ğ‘‡). This exercise asks you to verify 3.35 and 3.38. 4 Suppose that ğ· âˆˆ â„’(ğ’«3(ğ‘), ğ’«2(ğ‘))is the differentiation map defined by ğ·ğ‘ = ğ‘â€². Find a basis of ğ’«3(ğ‘) and a basis of ğ’«2(ğ‘) such that the matrix of ğ· with respect to these bases is â›âœâœâœ â 1 0 0 0 0 1 0 0 0 0 1 0 ââŸâŸâŸ â  . Compare with Example 3.33. The next exercise generalizes this exercise. 5 Suppose ğ‘‰ and ğ‘Š are finite-dimensional andğ‘‡ âˆˆ â„’(ğ‘‰, ğ‘Š). Prove that there exist a basis of ğ‘‰ and a basis of ğ‘Š such that with respect to these bases, all entries of â„³(ğ‘‡) are 0except that the entries in row ğ‘˜, column ğ‘˜, equal 1if 1 â‰¤ ğ‘˜ â‰¤dim range ğ‘‡. 6 Suppose ğ‘£1, â€¦, ğ‘£ğ‘š is a basis of ğ‘‰ and ğ‘Š is finite-dimensional. Suppose ğ‘‡ âˆˆ â„’(ğ‘‰, ğ‘Š). Prove that there exists a basis ğ‘¤1, â€¦, ğ‘¤ğ‘› of ğ‘Š such that all entries in the first column ofâ„³(ğ‘‡) [with respect to the basesğ‘£1, â€¦, ğ‘£ğ‘š and ğ‘¤1, â€¦, ğ‘¤ğ‘›] are0except for possibly a 1in the first row, first column. In this exercise, unlike Exercise 5, you are given the basis of ğ‘‰ instead of being able to choose a basis of ğ‘‰. 80 Chapter 3 Linear Maps 7 Suppose ğ‘¤1, â€¦, ğ‘¤ğ‘› is a basis of ğ‘Š and ğ‘‰ is finite-dimensional. Suppose ğ‘‡ âˆˆ â„’(ğ‘‰, ğ‘Š). Prove that there exists a basis ğ‘£1, â€¦, ğ‘£ğ‘š of ğ‘‰ such that all entries in the first row ofâ„³(ğ‘‡) [with respect to the basesğ‘£1, â€¦, ğ‘£ğ‘š and ğ‘¤1, â€¦, ğ‘¤ğ‘›] are0except for possibly a 1in the first row, first column. In this exercise, unlike Exercise 5, you are given the basis of ğ‘Š instead of being able to choose a basis of ğ‘Š. 8 Suppose ğ´ is an ğ‘š-by-ğ‘› matrix and ğµ is an ğ‘›-by-ğ‘ matrix. Prove that (ğ´ğµ)ğ‘—, â‹… = ğ´ğ‘—, â‹… ğµ for each 1 â‰¤ ğ‘— â‰¤ ğ‘š. In other words, show that row ğ‘— of ğ´ğµ equals (row ğ‘— of ğ´) times ğµ. This exercise gives the row version of 3.48. 9 Suppose ğ‘ = (ğ‘1 â‹¯ ğ‘ğ‘› )is a 1-by-ğ‘› matrix and ğµ is an ğ‘›-by-ğ‘ matrix. Prove that ğ‘ğµ = ğ‘1ğµ1, â‹… + â‹¯ + ğ‘ğ‘›ğµğ‘›, â‹… . In other words, show that ğ‘ğµ is a linear combination of the rows of ğµ, with the scalars that multiply the rows coming from ğ‘. This exercise gives the row version of 3.50. 10 Give an example of 2-by-2matrices ğ´ and ğµ such that ğ´ğµ â‰  ğµğ´. 11 Prove that the distributive property holds for matrix addition and matrix multiplication. In other words, suppose ğ´, ğµ, ğ¶, ğ·, ğ¸, and ğ¹ are matrices whose sizes are such that ğ´(ğµ + ğ¶) and (ğ· + ğ¸)ğ¹ make sense. Explain why ğ´ğµ + ğ´ğ¶ and ğ·ğ¹ + ğ¸ğ¹ both make sense and prove that ğ´(ğµ + ğ¶) = ğ´ğµ + ğ´ğ¶ and (ğ· + ğ¸)ğ¹ = ğ·ğ¹ + ğ¸ğ¹. 12 Prove that matrix multiplication is associative. In other words, suppose ğ´, ğµ, and ğ¶ are matrices whose sizes are such that (ğ´ğµ)ğ¶ makes sense. Explain why ğ´(ğµğ¶) makes sense and prove that (ğ´ğµ)ğ¶ = ğ´(ğµğ¶). Try to find a clean proof that illustrates the following quote from Emil Artin: â€œIt is my experience that proofs involving matrices can be shortened by 50% if one throws the matrices out.â€ 13 Suppose ğ´ is an ğ‘›-by-ğ‘› matrix and 1 â‰¤ ğ‘—, ğ‘˜ â‰¤ ğ‘›. Show that the entry in row ğ‘—, column ğ‘˜, of ğ´ 3 (which is defined to meanğ´ğ´ğ´) is ğ‘› âˆ‘ ğ‘ = 1 ğ‘› âˆ‘ ğ‘Ÿ = 1 ğ´ğ‘—, ğ‘ ğ´ğ‘, ğ‘Ÿ ğ´ğ‘Ÿ, ğ‘˜. 14 Suppose ğ‘š and ğ‘› are positive integers. Prove that the function ğ´ â†¦ ğ´t is a linear map from ğ…ğ‘š, ğ‘› to ğ…ğ‘›, ğ‘š. Section 3C Matrices 81 15 Prove that if ğ´ is an ğ‘š-by-ğ‘› matrix and ğ¶ is an ğ‘›-by-ğ‘ matrix, then (ğ´ğ¶) t = ğ¶ tğ´ t. This exercise shows that the transpose of the product of two matrices is the product of the transposes in the opposite order. 16 Suppose ğ´ is an ğ‘š-by-ğ‘› matrix with ğ´ â‰  0. Prove that the rank of ğ´ is 1 if and only if there exist (ğ‘1, â€¦, ğ‘ğ‘š) âˆˆ ğ…ğ‘š and (ğ‘‘1, â€¦, ğ‘‘ğ‘›) âˆˆ ğ…ğ‘› such that ğ´ğ‘—, ğ‘˜ = ğ‘ğ‘—ğ‘‘ğ‘˜ for every ğ‘— = 1, â€¦, ğ‘š and every ğ‘˜ = 1, â€¦, ğ‘›. 17 Suppose ğ‘‡ âˆˆ â„’(ğ‘‰), and ğ‘¢1, â€¦, ğ‘¢ğ‘› and ğ‘£1, â€¦, ğ‘£ğ‘› are bases of ğ‘‰. Prove that the following are equivalent. (a) ğ‘‡ is injective. (b) The columns of â„³(ğ‘‡) are linearly independent in ğ…ğ‘›, 1. (c) The columns of â„³(ğ‘‡) span ğ…ğ‘›, 1. (d) The rows of â„³(ğ‘‡) span ğ…1, ğ‘›. (e) The rows of â„³(ğ‘‡) are linearly independent in ğ…1, ğ‘›. Here â„³(ğ‘‡) means â„³(ğ‘‡, (ğ‘¢1, â€¦, ğ‘¢ğ‘›), (ğ‘£1, â€¦, ğ‘£ğ‘›)). 82 Chapter 3 Linear Maps 3D Invertibility and Isomorphisms Invertible Linear Maps We begin this section by defining the notions of invertible and inverse in the context of linear maps. 3.59 definition: invertible, inverse â€¢ A linear map ğ‘‡ âˆˆ â„’(ğ‘‰, ğ‘Š) is called invertible if there exists a linear map ğ‘† âˆˆ â„’(ğ‘Š, ğ‘‰) such that ğ‘†ğ‘‡ equals the identity operator on ğ‘‰ and ğ‘‡ğ‘† equals the identity operator on ğ‘Š. â€¢ A linear map ğ‘† âˆˆ â„’(ğ‘Š, ğ‘‰) satisfying ğ‘†ğ‘‡ = ğ¼ and ğ‘‡ğ‘† = ğ¼ is called an inverse of ğ‘‡ (note that the firstğ¼ is the identity operator on ğ‘‰ and the second ğ¼ is the identity operator on ğ‘Š). The definition above mentions â€œan inverseâ€. However, the next result shows that we can change this terminology to â€œthe inverseâ€. 3.60 inverse is unique An invertible linear map has a unique inverse. Proof Suppose ğ‘‡ âˆˆ â„’(ğ‘‰, ğ‘Š) is invertible and ğ‘†1 and ğ‘†2 are inverses of ğ‘‡. Then ğ‘†1 = ğ‘†1ğ¼ = ğ‘†1(ğ‘‡ğ‘†2) = (ğ‘†1ğ‘‡)ğ‘†2 = ğ¼ğ‘†2 = ğ‘†2. Thus ğ‘†1 = ğ‘†2. Now that we know that the inverse is unique, we can give it a notation. 3.61 notation:ğ‘‡âˆ’1 If ğ‘‡ is invertible, then its inverse is denoted by ğ‘‡âˆ’1. In other words, if ğ‘‡ âˆˆ â„’(ğ‘‰, ğ‘Š) is invertible, then ğ‘‡âˆ’1 is the unique element of â„’(ğ‘Š, ğ‘‰) such that ğ‘‡âˆ’1ğ‘‡ = ğ¼ and ğ‘‡ğ‘‡âˆ’1 = ğ¼. 3.62 example:inverse of a linear map from ğ‘3 to ğ‘3 Suppose ğ‘‡ âˆˆ â„’(ğ‘3)is defined byğ‘‡(ğ‘¥, ğ‘¦, ğ‘§) = (âˆ’ğ‘¦, ğ‘¥, 4ğ‘§). Thus ğ‘‡ is a counterclockwise rotation by 90 âˆ˜ in the ğ‘¥ğ‘¦-plane and a stretch by a factor of 4in the direction of the ğ‘§-axis. Hence the inverse map ğ‘‡âˆ’1 âˆˆ â„’(ğ‘3)is the clockwise rotation by 90 âˆ˜ in the ğ‘¥ğ‘¦-plane and a stretch by a factor of 1 4 in the direction of the ğ‘§-axis: ğ‘‡âˆ’1(ğ‘¥, ğ‘¦, ğ‘§) = (ğ‘¦, âˆ’ğ‘¥, 1 4 ğ‘§). Section 3D Invertibility and Isomorphisms 83 The next result shows that a linear map is invertible if and only if it is one-to- one and onto. 3.63 invertibility âŸº injectivity and surjectivity A linear map is invertible if and only if it is injective and surjective. Proof Suppose ğ‘‡ âˆˆ â„’(ğ‘‰, ğ‘Š). We need to show that ğ‘‡ is invertible if and only if it is injective and surjective. First suppose ğ‘‡ is invertible. To show that ğ‘‡ is injective, suppose ğ‘¢, ğ‘£ âˆˆ ğ‘‰ and ğ‘‡ğ‘¢ = ğ‘‡ğ‘£. Then ğ‘¢ = ğ‘‡âˆ’1(ğ‘‡ğ‘¢) = ğ‘‡âˆ’1(ğ‘‡ğ‘£) = ğ‘£, so ğ‘¢ = ğ‘£. Hence ğ‘‡ is injective. We are still assuming that ğ‘‡ is invertible. Now we want to prove that ğ‘‡ is surjective. To do this, let ğ‘¤ âˆˆ ğ‘Š. Then ğ‘¤ = ğ‘‡(ğ‘‡âˆ’1ğ‘¤), which shows that ğ‘¤ is in the range of ğ‘‡. Thus range ğ‘‡ = ğ‘Š. Hence ğ‘‡ is surjective, completing this direction of the proof. Now suppose ğ‘‡ is injective and surjective. We want to prove that ğ‘‡ is invertible. For each ğ‘¤ âˆˆ ğ‘Š, defineğ‘†(ğ‘¤) to be the unique element of ğ‘‰ such that ğ‘‡(ğ‘†(ğ‘¤))= ğ‘¤ (the existence and uniqueness of such an element follow from the surjectivity and injectivity of ğ‘‡). The definition ofğ‘† implies that ğ‘‡ âˆ˜ ğ‘† equals the identity operator on ğ‘Š. To prove that ğ‘† âˆ˜ ğ‘‡ equals the identity operator on ğ‘‰, let ğ‘£ âˆˆ ğ‘‰. Then ğ‘‡((ğ‘† âˆ˜ ğ‘‡)ğ‘£)= (ğ‘‡ âˆ˜ ğ‘†)(ğ‘‡ğ‘£) = ğ¼(ğ‘‡ğ‘£) = ğ‘‡ğ‘£. This equation implies that (ğ‘† âˆ˜ ğ‘‡)ğ‘£ = ğ‘£ (because ğ‘‡ is injective). Thus ğ‘† âˆ˜ ğ‘‡ equals the identity operator on ğ‘‰. To complete the proof, we need to show that ğ‘† is linear. To do this, suppose ğ‘¤1, ğ‘¤2 âˆˆ ğ‘Š. Then ğ‘‡(ğ‘†(ğ‘¤1) + ğ‘†(ğ‘¤2))= ğ‘‡(ğ‘†(ğ‘¤1))+ ğ‘‡(ğ‘†(ğ‘¤2))= ğ‘¤1 + ğ‘¤2. Thus ğ‘†(ğ‘¤1) + ğ‘†(ğ‘¤2) is the unique element of ğ‘‰ that ğ‘‡ maps to ğ‘¤1 + ğ‘¤2. By the definition ofğ‘†, this implies that ğ‘†(ğ‘¤1 + ğ‘¤2) = ğ‘†(ğ‘¤1) + ğ‘†(ğ‘¤2). Hence ğ‘† satisfies the additive property required for linearity. The proof of homogeneity is similar. Specifically, ifğ‘¤ âˆˆ ğ‘Š and ğœ† âˆˆ ğ…, then ğ‘‡(ğœ†ğ‘†(ğ‘¤))= ğœ†ğ‘‡(ğ‘†(ğ‘¤))= ğœ†ğ‘¤. Thus ğœ†ğ‘†(ğ‘¤) is the unique element of ğ‘‰ that ğ‘‡ maps to ğœ†ğ‘¤. By the definition ofğ‘†, this implies that ğ‘†(ğœ†ğ‘¤) = ğœ†ğ‘†(ğ‘¤). Hence ğ‘† is linear, as desired. For a linear map from a vector space to itself, you might wonder whether injectivity alone, or surjectivity alone, is enough to imply invertibility. On infinite- dimensional vector spaces, neither condition alone implies invertibility, as illus- trated by the next example, which uses two familiar linear maps from Example 3.3. 84 Chapter 3 Linear Maps 3.64 example:neither injectivity nor surjectivity implies invertibility â€¢ The multiplication by ğ‘¥2 linear map from ğ’«(ğ‘) to ğ’«(ğ‘) (see 3.3) is injective but it is not invertible because it is not surjective (the polynomial 1is not in the range). â€¢ The backward shift linear map from ğ…âˆ to ğ…âˆ (see 3.3) is surjective but it is not invertible because it is not injective [the vector (1, 0, 0, 0, â€¦ ) is in the null space]. In view of the example above, the next result is remarkableâ€”it states that for a linear map from a finite-dimensional vector space to a vector space of the same dimension, either injectivity or surjectivity alone implies the other condition. Note that the hypothesis below that dim ğ‘‰ = dim ğ‘Š is automatically satisfied in the important special case where ğ‘‰ is finite-dimensional andğ‘Š = ğ‘‰. 3.65 injectivity is equivalent to surjectivity (if dim ğ‘‰ = dim ğ‘Š < âˆ) Suppose that ğ‘‰ and ğ‘Š are finite-dimensional vector spaces,dim ğ‘‰ = dim ğ‘Š, and ğ‘‡ âˆˆ â„’(ğ‘‰, ğ‘Š). Then ğ‘‡ is invertible âŸº ğ‘‡ is injective âŸº ğ‘‡ is surjective. Proof The fundamental theorem of linear maps (3.21) states that 3.66 dim ğ‘‰ = dim null ğ‘‡ + dim range ğ‘‡. If ğ‘‡ is injective (which by 3.15 is equivalent to the condition dim null ğ‘‡ = 0), then the equation above implies that dim range ğ‘‡ = dim ğ‘‰ âˆ’ dim null ğ‘‡ = dim ğ‘‰ = dim ğ‘Š, which implies that ğ‘‡ is surjective (by 2.39). Conversely, if ğ‘‡ is surjective, then 3.66 implies that dim null ğ‘‡ = dim ğ‘‰ âˆ’ dim range ğ‘‡ = dim ğ‘‰ âˆ’ dim ğ‘Š = 0, which implies that ğ‘‡ is injective. Thus we have shown that ğ‘‡ is injective if and only if ğ‘‡ is surjective. Thus if ğ‘‡ is either injective or surjective, then ğ‘‡ is both injective and surjective, which implies that ğ‘‡ is invertible. Hence ğ‘‡ is invertible if and only if ğ‘‡ is injective if and only if ğ‘‡ is surjective. The next example illustrates the power of the previous result. Although it is possible to prove the result in the example below without using linear algebra, the proof using linear algebra is cleaner and easier. Section 3D Invertibility and Isomorphisms 85 3.67 example:there exists a polynomial ğ‘ such that ((ğ‘¥2 + 5ğ‘¥+ 7)ğ‘) â€³ = ğ‘ The linear map ğ‘ â†¦ ((ğ‘¥2 + 5ğ‘¥+ 7)ğ‘) â€³ from ğ’«(ğ‘) to itself is injective, as you can show. Thus we are tempted to use 3.65 to show that this map is surjective. However, Example 3.64 shows that the magic of 3.65 does not apply to the infinite-dimensional vector spaceğ’«(ğ‘). We will get around this problem by restricting attention to the finite-dimensional vector space ğ’«ğ‘š(ğ‘). Suppose ğ‘ âˆˆ ğ’«(ğ‘). There exists a nonnegative integer ğ‘š such that ğ‘ âˆˆ ğ’«ğ‘š(ğ‘). Defineğ‘‡âˆ¶ ğ’«ğ‘š(ğ‘) â†’ ğ’«ğ‘š(ğ‘) by ğ‘‡ğ‘ = ((ğ‘¥2 + 5ğ‘¥+ 7)ğ‘) â€³ . Multiplying a nonzero polynomial by (ğ‘¥2 + 5ğ‘¥+ 7)increases the degree by 2, and then differentiating twice reduces the degree by 2. Thus ğ‘‡ is indeed a linear map from ğ’«ğ‘š(ğ‘) to itself. Every polynomial whose second derivative equals 0is of the form ğ‘ğ‘¥ + ğ‘, where ğ‘, ğ‘ âˆˆ ğ‘. Thus null ğ‘‡ = {0}. Hence ğ‘‡ is injective. Thus ğ‘‡ is surjective (by 3.65), which means that there exists a polynomial ğ‘ âˆˆ ğ’«ğ‘š(ğ‘) such that ((ğ‘¥2 + 5ğ‘¥+ 7)ğ‘) â€³ = ğ‘, as claimed in the title of this example. Exercise 35 in Section 6A gives a similar but more spectacular example of using 3.65. The hypothesis in the result below that dim ğ‘‰ = dim ğ‘Š holds in the important special case in which ğ‘‰ is finite-dimensional andğ‘Š = ğ‘‰. Thus in that case, the equation ğ‘†ğ‘‡ = ğ¼ implies that ğ‘†ğ‘‡ = ğ‘‡ğ‘†, even though we do not have multiplicative commutativity of arbitrary linear maps from ğ‘‰ to ğ‘‰. 3.68 ğ‘†ğ‘‡ = ğ¼ âŸº ğ‘‡ğ‘† = ğ¼ (on vector spaces of the same dimension) Suppose ğ‘‰ and ğ‘Š are finite-dimensional vector spaces of the same dimension, ğ‘† âˆˆ â„’(ğ‘‰, ğ‘Š), and ğ‘‡ âˆˆ â„’(ğ‘Š, ğ‘‰). Then ğ‘†ğ‘‡ = ğ¼ if and only if ğ‘‡ğ‘† = ğ¼. Proof First suppose ğ‘†ğ‘‡ = ğ¼. If ğ‘£ âˆˆ ğ‘‰ and ğ‘‡ğ‘£ = 0, then ğ‘£ = ğ¼ğ‘£ = (ğ‘†ğ‘‡)ğ‘£ = ğ‘†(ğ‘‡ğ‘£) = ğ‘†(0) = 0. Thus ğ‘‡ is injective (by 3.15). Because ğ‘‰ and ğ‘Š have the same dimension, this implies that ğ‘‡ is invertible (by 3.65). Now multiply both sides of the equation ğ‘†ğ‘‡ = ğ¼ by ğ‘‡âˆ’1 on the right, getting ğ‘† = ğ‘‡âˆ’1. Thus ğ‘‡ğ‘† = ğ‘‡ğ‘‡âˆ’1 = ğ¼, as desired. To prove the implication in the other direction, simply reverse the roles of ğ‘† and ğ‘‡ (and ğ‘‰ and ğ‘Š) in the direction we have already proved, showing that if ğ‘‡ğ‘† = ğ¼, then ğ‘†ğ‘‡ = ğ¼. 86 Chapter 3 Linear Maps Isomorphic Vector Spaces The next definition captures the idea of two vector spaces that are essentially the same, except for the names of their elements. 3.69 definition:isomorphism, isomorphic â€¢ An isomorphism is an invertible linear map. â€¢ Two vector spaces are called isomorphic if there is an isomorphism from one vector space onto the other one. Think of an isomorphism ğ‘‡âˆ¶ ğ‘‰ â†’ ğ‘Š as relabeling ğ‘£ âˆˆ ğ‘‰ as ğ‘‡ğ‘£ âˆˆ ğ‘Š. This viewpoint explains why two isomorphic vector spaces have the same vector space properties. The terms â€œisomorphismâ€ and â€œinvertible linear mapâ€ mean the same thing. Use â€œisomorphismâ€ when you want to emphasize that the two spaces are essentially the same. It can be difficult to determine whether two mathematical structures (such as groups or topological spaces) are essentially the same, differing only in the names of the elements of underlying sets. However, the next result shows that we need to look at only a single number (the dimension) to determine whether two vector spaces are isomorphic. 3.70 dimension shows whether vector spaces are isomorphic Two finite-dimensional vector spaces overğ… are isomorphic if and only if they have the same dimension. Proof First suppose ğ‘‰ and ğ‘Š are isomorphic finite-dimensional vector spaces. Thus there exists an isomorphism ğ‘‡ from ğ‘‰ onto ğ‘Š. Because ğ‘‡ is invertible, we have null ğ‘‡ = {0}and range ğ‘‡ = ğ‘Š. Thus dim null ğ‘‡ = 0 and dim range ğ‘‡ = dim ğ‘Š. The formula dim ğ‘‰ = dim null ğ‘‡ + dim range ğ‘‡ (the fundamental theorem of linear maps, which is 3.21) thus becomes the equation dim ğ‘‰ = dim ğ‘Š, completing the proof in one direction. To prove the other direction, suppose ğ‘‰ and ğ‘Š are finite-dimensional vector spaces of the same dimension. Let ğ‘£1, â€¦, ğ‘£ğ‘› be a basis of ğ‘‰ and ğ‘¤1, â€¦, ğ‘¤ğ‘› be a basis of ğ‘Š. Let ğ‘‡ âˆˆ â„’(ğ‘‰, ğ‘Š) be defined by ğ‘‡(ğ‘1ğ‘£1 + â‹¯ + ğ‘ğ‘›ğ‘£ğ‘›) = ğ‘1ğ‘¤1 + â‹¯ + ğ‘ğ‘›ğ‘¤ğ‘›. Then ğ‘‡ is a well-defined linear map becauseğ‘£1, â€¦, ğ‘£ğ‘› is a basis of ğ‘‰. Also, ğ‘‡ is surjective because ğ‘¤1, â€¦, ğ‘¤ğ‘› spans ğ‘Š. Furthermore, null ğ‘‡ = {0}because ğ‘¤1, â€¦, ğ‘¤ğ‘› is linearly independent. Thus ğ‘‡ is injective. Because ğ‘‡ is injective and surjective, it is an isomorphism (see 3.63). Hence ğ‘‰ and ğ‘Š are isomorphic. Section 3D Invertibility and Isomorphisms 87 Every finite-dimensional vector space is isomorphic to some ğ…ğ‘›. Thus why not just study ğ…ğ‘› instead of more general vector spaces? To answer this ques- tion, note that an investigation of ğ…ğ‘› would soon lead to other vector spaces. For example, we would encounter the null space and range of linear maps. Although each of these vector spaces is isomorphic to some ğ…ğ‘š, thinking of them that way often adds complexity but no new insight. The previous result implies that each finite-dimensional vector spaceğ‘‰ is iso- morphic to ğ…ğ‘›, where ğ‘› = dim ğ‘‰. For example, if ğ‘š is a nonnegative integer, then ğ’«ğ‘š(ğ…) is isomorphic to ğ…ğ‘š + 1. Recall that the notation ğ…ğ‘š, ğ‘› denotes the vector space of ğ‘š-by-ğ‘› matrices with entries in ğ…. If ğ‘£1, â€¦, ğ‘£ğ‘› is a basis of ğ‘‰ and ğ‘¤1, â€¦, ğ‘¤ğ‘š is a basis of ğ‘Š, then for each ğ‘‡ âˆˆ â„’(ğ‘‰, ğ‘Š), we have a matrix â„³(ğ‘‡) âˆˆ ğ…ğ‘š, ğ‘›. Thus once bases have been fixed forğ‘‰ and ğ‘Š, â„³ becomes a function from â„’(ğ‘‰, ğ‘Š) to ğ…ğ‘š, ğ‘›. Notice that 3.35 and 3.38 show that â„³ is a lin- ear map. This linear map is actually an isomorphism, as we now show. 3.71 â„’(ğ‘‰, ğ‘Š) and ğ…ğ‘š, ğ‘› are isomorphic Suppose ğ‘£1, â€¦, ğ‘£ğ‘› is a basis of ğ‘‰ and ğ‘¤1, â€¦, ğ‘¤ğ‘š is a basis of ğ‘Š. Then â„³ is an isomorphism between â„’(ğ‘‰, ğ‘Š) and ğ…ğ‘š, ğ‘›. Proof We already noted that â„³ is linear. We need to prove that â„³ is injective and surjective. We begin with injectivity. If ğ‘‡ âˆˆ â„’(ğ‘‰, ğ‘Š) and â„³(ğ‘‡) = 0, then ğ‘‡ğ‘£ğ‘˜ = 0for each ğ‘˜ = 1, â€¦, ğ‘›. Because ğ‘£1, â€¦, ğ‘£ğ‘› is a basis of ğ‘‰, this implies ğ‘‡ = 0. Thus â„³ is injective (by 3.15). To prove that â„³ is surjective, suppose ğ´ âˆˆ ğ…ğ‘š, ğ‘›. By the linear map lemma (3.4), there exists ğ‘‡ âˆˆ â„’(ğ‘‰, ğ‘Š) such that ğ‘‡ğ‘£ğ‘˜ = ğ‘š âˆ‘ ğ‘— = 1 ğ´ğ‘—, ğ‘˜ğ‘¤ğ‘— for each ğ‘˜ = 1, â€¦, ğ‘›. Because â„³(ğ‘‡) equals ğ´, the range of â„³ equals ğ…ğ‘š, ğ‘›, as desired. Now we can determine the dimension of the vector space of linear maps from one finite-dimensional vector space to another. 3.72 dim â„’(ğ‘‰, ğ‘Š) = (dim ğ‘‰)(dim ğ‘Š) Suppose ğ‘‰ and ğ‘Š are finite-dimensional. Thenâ„’(ğ‘‰, ğ‘Š) is finite-dimensional and dim â„’(ğ‘‰, ğ‘Š) = (dim ğ‘‰)(dim ğ‘Š). Proof The desired result follows from 3.71, 3.70, and 3.40. 88 Chapter 3 Linear Maps Linear Maps Thought of as Matrix Multiplication Previously we defined the matrix of a linear map. Now we define the matrix of a vector. 3.73 definition:matrix of a vector, â„³(ğ‘£) Suppose ğ‘£ âˆˆ ğ‘‰ and ğ‘£1, â€¦, ğ‘£ğ‘› is a basis of ğ‘‰. The matrix of ğ‘£ with respect to this basis is the ğ‘›-by-1matrix â„³(ğ‘£) = â›âœâœâœ â ğ‘1 â‹® ğ‘ğ‘› ââŸâŸâŸ â  , where ğ‘1, â€¦, ğ‘ğ‘› are the scalars such that ğ‘£ = ğ‘1ğ‘£1 + â‹¯ + ğ‘ğ‘›ğ‘£ğ‘›. The matrix â„³(ğ‘£) of a vector ğ‘£ âˆˆ ğ‘‰ depends on the basis ğ‘£1, â€¦, ğ‘£ğ‘› of ğ‘‰, as well as on ğ‘£. However, the basis should be clear from the context and thus it is not included in the notation. 3.74 example:matrix of a vector â€¢ The matrix of the polynomial 2 âˆ’ 7ğ‘¥+ 5ğ‘¥ 3 + ğ‘¥4 with respect to the standard basis of ğ’«4(ğ‘) is â›âœâœâœâœâœâœâœâœâœâœ â 2 âˆ’7 0 5 1 ââŸâŸâŸâŸâŸâŸâŸâŸâŸâŸ â  . â€¢ The matrix of a vector ğ‘¥ âˆˆ ğ…ğ‘› with respect to the standard basis is obtained by writing the coordinates of ğ‘¥ as the entries in an ğ‘›-by-1matrix. In other words, if ğ‘¥ = (ğ‘¥1, â€¦, ğ‘¥ğ‘›) âˆˆ ğ…ğ‘›, then â„³(ğ‘¥) = â›âœâœâœ â ğ‘¥1 â‹® ğ‘¥ğ‘› ââŸâŸâŸ â  . Occasionally we want to think of elements of ğ‘‰ as relabeled to be ğ‘›-by-1 matrices. Once a basis ğ‘£1, â€¦, ğ‘£ğ‘› is chosen, the function â„³ that takes ğ‘£ âˆˆ ğ‘‰ to â„³(ğ‘£) is an isomorphism of ğ‘‰ onto ğ…ğ‘›, 1 that implements this relabeling. Recall that if ğ´ is an ğ‘š-by-ğ‘› matrix, then ğ´â‹…, ğ‘˜ denotes the ğ‘˜th column of ğ´, thought of as an ğ‘š-by-1matrix. In the next result, â„³(ğ‘‡ğ‘£ğ‘˜) is computed with respect to the basis ğ‘¤1, â€¦, ğ‘¤ğ‘š of ğ‘Š. Section 3D Invertibility and Isomorphisms 89 3.75 â„³(ğ‘‡)â‹…, ğ‘˜ = â„³(ğ‘‡ğ‘£ğ‘˜). Suppose ğ‘‡ âˆˆ â„’(ğ‘‰, ğ‘Š) and ğ‘£1, â€¦, ğ‘£ğ‘› is a basis of ğ‘‰ and ğ‘¤1, â€¦, ğ‘¤ğ‘š is a basis of ğ‘Š. Let 1 â‰¤ ğ‘˜ â‰¤ ğ‘›. Then the ğ‘˜th column of â„³(ğ‘‡), which is denoted by â„³(ğ‘‡)â‹…, ğ‘˜, equals â„³(ğ‘‡ğ‘£ğ‘˜). Proof The desired result follows immediately from the definitions ofâ„³(ğ‘‡) and â„³(ğ‘‡ğ‘£ğ‘˜). The next result shows how the notions of the matrix of a linear map, the matrix of a vector, and matrix multiplication fit together. 3.76 linear maps act like matrix multiplication Suppose ğ‘‡ âˆˆ â„’(ğ‘‰, ğ‘Š) and ğ‘£ âˆˆ ğ‘‰. Suppose ğ‘£1, â€¦, ğ‘£ğ‘› is a basis of ğ‘‰ and ğ‘¤1, â€¦, ğ‘¤ğ‘š is a basis of ğ‘Š. Then â„³(ğ‘‡ğ‘£) = â„³(ğ‘‡)â„³(ğ‘£). Proof Suppose ğ‘£ = ğ‘1ğ‘£1 + â‹¯ + ğ‘ğ‘›ğ‘£ğ‘›, where ğ‘1, â€¦, ğ‘ğ‘› âˆˆ ğ…. Thus 3.77 ğ‘‡ğ‘£ = ğ‘1ğ‘‡ğ‘£1 + â‹¯ + ğ‘ğ‘›ğ‘‡ğ‘£ğ‘›. Hence â„³(ğ‘‡ğ‘£) = ğ‘1â„³(ğ‘‡ğ‘£1) + â‹¯ + ğ‘ğ‘›â„³(ğ‘‡ğ‘£ğ‘›) = ğ‘1â„³(ğ‘‡)â‹…, 1 + â‹¯ + ğ‘ğ‘›â„³(ğ‘‡)â‹…, ğ‘› = â„³(ğ‘‡)â„³(ğ‘£), where the first equality follows from3.77 and the linearity of â„³, the second equality comes from 3.75, and the last equality comes from 3.50. Each ğ‘š-by-ğ‘› matrix ğ´ induces a linear map from ğ…ğ‘›, 1 to ğ…ğ‘š, 1, namely the matrix multiplication function that takes ğ‘¥ âˆˆ ğ…ğ‘›, 1 to ğ´ğ‘¥ âˆˆ ğ…ğ‘š, 1. The result above can be used to think of every linear map (from a finite-dimensional vector space to another finite-dimensional vector space) as a matrix multiplication map after suitable relabeling via the isomorphisms given by â„³. Specifically, ifğ‘‡ âˆˆ â„’(ğ‘‰, ğ‘Š) and we identify ğ‘£ âˆˆ ğ‘‰ with â„³(ğ‘£) âˆˆ ğ…ğ‘›, 1, then the result above says that we can identify ğ‘‡ğ‘£ with â„³(ğ‘‡)â„³(ğ‘£). Because the result above allows us to think (via isomorphisms) of each linear map as multiplication on ğ…ğ‘›, 1 by some matrix ğ´, keep in mind that the specific matrix ğ´ depends not only on the linear map but also on the choice of bases. One of the themes of many of the most important results in later chapters will be the choice of a basis that makes the matrix ğ´ as simple as possible. In this book, we concentrate on linear maps rather than on matrices. However, sometimes thinking of linear maps as matrices (or thinking of matrices as linear maps) gives important insights that we will find useful. 90 Chapter 3 Linear Maps Notice that no bases are in sight in the statement of the next result. Although â„³(ğ‘‡) in the next result depends on a choice of bases of ğ‘‰ and ğ‘Š, the next result shows that the column rank of â„³(ğ‘‡) is the same for all such choices (because range ğ‘‡ does not depend on a choice of basis). 3.78 dimension of range ğ‘‡ equals column rank of â„³(ğ‘‡) Suppose ğ‘‰ and ğ‘Š are finite-dimensional andğ‘‡ âˆˆ â„’(ğ‘‰, ğ‘Š). Then dim range ğ‘‡ equals the column rank of â„³(ğ‘‡). Proof Suppose ğ‘£1, â€¦, ğ‘£ğ‘› is a basis of ğ‘‰ and ğ‘¤1, â€¦, ğ‘¤ğ‘š is a basis of ğ‘Š. The linear map that takes ğ‘¤ âˆˆ ğ‘Š to â„³(ğ‘¤) is an isomorphism from ğ‘Š onto the space ğ…ğ‘š, 1 of ğ‘š-by-1column vectors. The restriction of this isomorphism to range ğ‘‡ [which equals span(ğ‘‡ğ‘£1, â€¦, ğ‘‡ğ‘£ğ‘›) by Exercise 10 in Section 3B]is an isomorphism from range ğ‘‡ onto span(â„³(ğ‘‡ğ‘£1), â€¦, â„³(ğ‘‡ğ‘£ğ‘›)). For each ğ‘˜ âˆˆ {1, â€¦, ğ‘›}, the ğ‘š-by-1 matrix â„³(ğ‘‡ğ‘£ğ‘˜) equals column ğ‘˜ of â„³(ğ‘‡). Thus dim range ğ‘‡ = the column rank of â„³(ğ‘‡), as desired. Change of Basis In Section 3C we defined the matrix â„³(ğ‘‡, (ğ‘£1, â€¦, ğ‘£ğ‘›), (ğ‘¤1, â€¦, ğ‘¤ğ‘š)) of a linear map ğ‘‡ from ğ‘‰ to a possibly different vector space ğ‘Š, where ğ‘£1, â€¦, ğ‘£ğ‘› is a basis of ğ‘‰ and ğ‘¤1, â€¦, ğ‘¤ğ‘š is a basis of ğ‘Š. For linear maps from a vector space to itself, we usually use the same basis for both the domain vector space and the target vector space. When using a single basis in both capacities, we often write the basis only once. In other words, if ğ‘‡ âˆˆ â„’(ğ‘‰) and ğ‘£1, â€¦, ğ‘£ğ‘› is a basis of ğ‘‰, then the notation â„³(ğ‘‡, (ğ‘£1, â€¦, ğ‘£ğ‘›))is defined by the equation â„³(ğ‘‡, (ğ‘£1, â€¦, ğ‘£ğ‘›))= â„³(ğ‘‡, (ğ‘£1, â€¦, ğ‘£ğ‘›), (ğ‘£1, â€¦, ğ‘£ğ‘›)). If the basis ğ‘£1, â€¦, ğ‘£ğ‘› is clear from the context, then we can write just â„³(ğ‘‡). 3.79 definition:identity matrix, I Suppose ğ‘› is a positive integer. The ğ‘›-by-ğ‘› matrix â›âœâœâœ â 1 0 â‹± 0 1 ââŸâŸâŸ â  with 1â€™s on the diagonal (the entries where the row number equals the column number) and 0â€™s elsewhere is called the identity matrix and is denoted by ğ¼. Section 3D Invertibility and Isomorphisms 91 In the definition above, the0in the lower left corner of the matrix indicates that all entries below the diagonal are 0, and the 0in the upper right corner indicates that all entries above the diagonal are 0. With respect to each basis of ğ‘‰, the matrix of the identity operator ğ¼ âˆˆ â„’(ğ‘‰) is the identity matrix ğ¼. Note that the symbol ğ¼ is used to denote both the identity operator and the identity matrix. The context indicates which meaning of ğ¼ is intended. For example, consider the equation â„³(ğ¼) = ğ¼; on the left side ğ¼ denotes the identity operator, and on the right side ğ¼ denotes the identity matrix. If ğ´ is a square matrix (with entries in ğ…, as usual) of the same size as ğ¼, then ğ´ğ¼ = ğ¼ğ´ = ğ´, as you should verify. 3.80 definition: invertible, inverse, ğ´ âˆ’1 A square matrix ğ´ is called invertible if there is a square matrix ğµ of the same size such that ğ´ğµ = ğµğ´ = ğ¼; we call ğµ the inverse of ğ´ and denote it by ğ´ âˆ’1. Some mathematicians use the terms nonsingular and singular, which mean the same as invertible and non- invertible. The same proof as used in 3.60 shows that if ğ´ is an invertible square matrix, then there is a unique matrix ğµ such that ğ´ğµ = ğµğ´ = ğ¼ (and thus the notation ğµ = ğ´âˆ’1 is justified). If ğ´ is an invertible matrix, then (ğ´ âˆ’1) âˆ’1 = ğ´ because ğ´ âˆ’1ğ´ = ğ´ğ´âˆ’1 = ğ¼. Also, if ğ´ and ğ¶ are invertible square matrices of the same size, then ğ´ğ¶ is invertible and (ğ´ğ¶) âˆ’1 = ğ¶âˆ’1ğ´ âˆ’1 because (ğ´ğ¶)(ğ¶âˆ’1ğ´ âˆ’1)= ğ´(ğ¶ğ¶âˆ’1)ğ´ âˆ’1 = ğ´ğ¼ğ´âˆ’1 = ğ´ğ´âˆ’1 = ğ¼, and similarly (ğ¶âˆ’1ğ´ âˆ’1)(ğ´ğ¶) = ğ¼. The next result holds because we defined matrix multiplication to make it trueâ€”see 3.43 and the material preceding it. Now we are just being more explicit about the bases involved. 3.81 matrix of product of linear maps Suppose ğ‘‡ âˆˆ â„’(ğ‘ˆ, ğ‘‰) and ğ‘† âˆˆ â„’(ğ‘‰, ğ‘Š). If ğ‘¢1, â€¦, ğ‘¢ğ‘š is a basis of ğ‘ˆ, ğ‘£1, â€¦, ğ‘£ğ‘› is a basis of ğ‘‰, and ğ‘¤1, â€¦, ğ‘¤ğ‘ is a basis of ğ‘Š, then â„³(ğ‘†ğ‘‡, (ğ‘¢1, â€¦, ğ‘¢ğ‘š), (ğ‘¤1, â€¦, ğ‘¤ğ‘))= â„³(ğ‘†, (ğ‘£1, â€¦, ğ‘£ğ‘›), (ğ‘¤1, â€¦, ğ‘¤ğ‘))â„³(ğ‘‡, (ğ‘¢1, â€¦, ğ‘¢ğ‘š), (ğ‘£1, â€¦, ğ‘£ğ‘›)). 92 Chapter 3 Linear Maps The next result deals with the matrix of the identity operator ğ¼ with respect to two different bases. Note that the ğ‘˜th column of â„³(ğ¼, (ğ‘¢1, â€¦, ğ‘¢ğ‘›), (ğ‘£1, â€¦, ğ‘£ğ‘›)) consists of the scalars needed to write ğ‘¢ğ‘˜ as a linear combination of the basis ğ‘£1, â€¦, ğ‘£ğ‘›. In the statement of the next result, ğ¼ denotes the identity operator from ğ‘‰ to ğ‘‰. In the proof, ğ¼ also denotes the ğ‘›-by-ğ‘› identity matrix. 3.82 matrix of identity operator with respect to two bases Suppose that ğ‘¢1, â€¦, ğ‘¢ğ‘› and ğ‘£1, â€¦, ğ‘£ğ‘› are bases of ğ‘‰. Then the matrices â„³(ğ¼, (ğ‘¢1, â€¦, ğ‘¢ğ‘›), (ğ‘£1, â€¦, ğ‘£ğ‘›)) and â„³(ğ¼, (ğ‘£1, â€¦, ğ‘£ğ‘›), (ğ‘¢1, â€¦, ğ‘¢ğ‘›)) are invertible, and each is the inverse of the other. Proof In 3.81, replace ğ‘¤ğ‘˜ with ğ‘¢ğ‘˜, and replace ğ‘† and ğ‘‡ with ğ¼, getting ğ¼ = â„³(ğ¼, (ğ‘£1, â€¦, ğ‘£ğ‘›), (ğ‘¢1, â€¦, ğ‘¢ğ‘›))â„³(ğ¼, (ğ‘¢1, â€¦, ğ‘¢ğ‘›), (ğ‘£1, â€¦, ğ‘£ğ‘›)). Now interchange the roles of the ğ‘¢â€™s and ğ‘£â€™s, getting ğ¼ = â„³(ğ¼, (ğ‘¢1, â€¦, ğ‘¢ğ‘›), (ğ‘£1, â€¦, ğ‘£ğ‘›))â„³(ğ¼, (ğ‘£1, â€¦, ğ‘£ğ‘›), (ğ‘¢1, â€¦, ğ‘¢ğ‘›)). These two equations above give the desired result. 3.83 example: matrix of identity on ğ…2 with respect to two bases Consider the bases (4, 2), (5, 3)and (1, 0), (0, 1)of ğ…2. Because ğ¼(4, 2) = 4(1, 0)+ 2(0, 1)and ğ¼(5, 3) = 5(1, 0)+ 3(0, 1), we have â„³(ğ¼, ((4, 2), (5, 3)), ((1, 0), (0, 1)))= ( 4 5 2 3 ). The inverse of the matrix above is â›âœ â 3 2 âˆ’ 5 2 âˆ’1 2 ââŸ â  , as you should verify. Thus 3.82 implies that â„³(ğ¼, ((1, 0), (0, 1)), ((4, 2), (5, 3)))= â›âœ â 3 2 âˆ’ 5 2 âˆ’1 2 ââŸ â  . Our next result shows how the matrix of ğ‘‡ changes when we change bases. In the next result, we have two different bases of ğ‘‰, each of which is used as a basis for the domain space and as a basis for the target space. Recall our shorthand notation that allows us to display a basis only once when it is used in both capacities: â„³(ğ‘‡, (ğ‘¢1, â€¦, ğ‘¢ğ‘›))= â„³(ğ‘‡, (ğ‘¢1, â€¦, ğ‘¢ğ‘›), (ğ‘¢1, â€¦, ğ‘¢ğ‘›)). Section 3D Invertibility and Isomorphisms 93 3.84 change-of-basis formula Suppose ğ‘‡ âˆˆ â„’(ğ‘‰). Suppose ğ‘¢1, â€¦, ğ‘¢ğ‘› and ğ‘£1, â€¦, ğ‘£ğ‘› are bases of ğ‘‰. Let ğ´ = â„³(ğ‘‡, (ğ‘¢1, â€¦, ğ‘¢ğ‘›)) and ğµ = â„³(ğ‘‡, (ğ‘£1, â€¦, ğ‘£ğ‘›)) and ğ¶ = â„³(ğ¼, (ğ‘¢1, â€¦, ğ‘¢ğ‘›), (ğ‘£1, â€¦, ğ‘£ğ‘›)). Then ğ´ = ğ¶âˆ’1ğµğ¶. Proof In 3.81, replace ğ‘¤ğ‘˜ with ğ‘¢ğ‘˜ and replace ğ‘† with ğ¼, getting 3.85 ğ´ = ğ¶âˆ’1â„³(ğ‘‡, (ğ‘¢1, â€¦, ğ‘¢ğ‘›), (ğ‘£1, â€¦, ğ‘£ğ‘›)), where we have used 3.82. Again use 3.81, this time replacing ğ‘¤ğ‘˜ with ğ‘£ğ‘˜. Also replace ğ‘‡ with ğ¼ and replace ğ‘† with ğ‘‡, getting â„³(ğ‘‡, (ğ‘¢1, â€¦, ğ‘¢ğ‘›), (ğ‘£1, â€¦, ğ‘£ğ‘›))= ğµğ¶. Substituting the equation above into 3.85 gives the equation ğ´ = ğ¶âˆ’1ğµğ¶. The proof of the next result is left as an exercise. 3.86 matrix of inverse equals inverse of matrix Suppose that ğ‘£1, â€¦, ğ‘£ğ‘› is a basis of ğ‘‰ and ğ‘‡ âˆˆ â„’(ğ‘‰) is invertible. Then â„³(ğ‘‡âˆ’1)= (â„³(ğ‘‡)) âˆ’1, where both matrices are with respect to the basis ğ‘£1, â€¦, ğ‘£ğ‘›. Exercises 3D 1 Suppose ğ‘‡ âˆˆ â„’(ğ‘‰, ğ‘Š) is invertible. Show that ğ‘‡âˆ’1 is invertible and (ğ‘‡âˆ’1) âˆ’1 = ğ‘‡. 2 Suppose ğ‘‡ âˆˆ â„’(ğ‘ˆ, ğ‘‰) and ğ‘† âˆˆ â„’(ğ‘‰, ğ‘Š) are both invertible linear maps. Prove that ğ‘†ğ‘‡ âˆˆ â„’(ğ‘ˆ, ğ‘Š) is invertible and that (ğ‘†ğ‘‡) âˆ’1 = ğ‘‡âˆ’1ğ‘† âˆ’1. 3 Suppose ğ‘‰ is finite-dimensional andğ‘‡ âˆˆ â„’(ğ‘‰). Prove that the following are equivalent. (a) ğ‘‡ is invertible. (b) ğ‘‡ğ‘£1, â€¦, ğ‘‡ğ‘£ğ‘› is a basis of ğ‘‰ for every basis ğ‘£1, â€¦, ğ‘£ğ‘› of ğ‘‰. (c) ğ‘‡ğ‘£1, â€¦, ğ‘‡ğ‘£ğ‘› is a basis of ğ‘‰ for some basis ğ‘£1, â€¦, ğ‘£ğ‘› of ğ‘‰. 4 Suppose ğ‘‰ is finite-dimensional anddim ğ‘‰ > 1. Prove that the set of noninvertible linear maps from ğ‘‰ to itself is not a subspace of â„’(ğ‘‰). 94 Chapter 3 Linear Maps 5 Suppose ğ‘‰ is finite-dimensional,ğ‘ˆ is a subspace of ğ‘‰, and ğ‘† âˆˆ â„’(ğ‘ˆ, ğ‘‰). Prove that there exists an invertible linear map ğ‘‡ from ğ‘‰ to itself such that ğ‘‡ğ‘¢ = ğ‘†ğ‘¢ for every ğ‘¢ âˆˆ ğ‘ˆ if and only if ğ‘† is injective. 6 Suppose that ğ‘Š is finite-dimensional andğ‘†, ğ‘‡ âˆˆ â„’(ğ‘‰, ğ‘Š). Prove that null ğ‘† = null ğ‘‡ if and only if there exists an invertible ğ¸ âˆˆ â„’(ğ‘Š) such that ğ‘† = ğ¸ğ‘‡. 7 Suppose that ğ‘‰ is finite-dimensional andğ‘†, ğ‘‡ âˆˆ â„’(ğ‘‰, ğ‘Š). Prove that range ğ‘† = range ğ‘‡ if and only if there exists an invertible ğ¸ âˆˆ â„’(ğ‘‰) such that ğ‘† = ğ‘‡ğ¸. 8 Suppose ğ‘‰ and ğ‘Š are finite-dimensional andğ‘†, ğ‘‡ âˆˆ â„’(ğ‘‰, ğ‘Š). Prove that there exist invertible ğ¸1 âˆˆ â„’(ğ‘‰) and ğ¸2 âˆˆ â„’(ğ‘Š) such that ğ‘† = ğ¸2ğ‘‡ğ¸1 if and only if dim null ğ‘† = dim null ğ‘‡. 9 Suppose ğ‘‰ is finite-dimensional andğ‘‡âˆ¶ ğ‘‰ â†’ ğ‘Š is a surjective linear map of ğ‘‰ onto ğ‘Š. Prove that there is a subspace ğ‘ˆ of ğ‘‰ such that ğ‘‡|ğ‘ˆ is an isomorphism of ğ‘ˆ onto ğ‘Š. Here ğ‘‡|ğ‘ˆ means the function ğ‘‡ restricted to ğ‘ˆ. Thus ğ‘‡|ğ‘ˆ is the function whose domain is ğ‘ˆ, with ğ‘‡|ğ‘ˆ defined by ğ‘‡|ğ‘ˆ(ğ‘¢) = ğ‘‡ğ‘¢ for every ğ‘¢ âˆˆ ğ‘ˆ. 10 Suppose ğ‘‰ and ğ‘Š are finite-dimensional andğ‘ˆ is a subspace of ğ‘‰. Let â„° = {ğ‘‡ âˆˆ â„’(ğ‘‰, ğ‘Š) âˆ¶ ğ‘ˆ âŠ†null ğ‘‡}. (a) Show that â„° is a subspace of â„’(ğ‘‰, ğ‘Š). (b) Find a formula for dim â„° in terms of dim ğ‘‰, dim ğ‘Š, and dim ğ‘ˆ. Hint: Define Î¦âˆ¶ â„’(ğ‘‰, ğ‘Š) â†’ â„’(ğ‘ˆ, ğ‘Š) by Î¦(ğ‘‡) = ğ‘‡|ğ‘ˆ. What is null Î¦? What is range Î¦? 11 Suppose ğ‘‰ is finite-dimensional andğ‘†, ğ‘‡ âˆˆ â„’(ğ‘‰). Prove that ğ‘†ğ‘‡ is invertible âŸº ğ‘† and ğ‘‡ are invertible. 12 Suppose ğ‘‰ is finite-dimensional andğ‘†, ğ‘‡, ğ‘ˆ âˆˆ â„’(ğ‘‰) and ğ‘†ğ‘‡ğ‘ˆ = ğ¼. Show that ğ‘‡ is invertible and that ğ‘‡âˆ’1 = ğ‘ˆğ‘†. 13 Show that the result in Exercise 12 can fail without the hypothesis that ğ‘‰ is finite-dimensional. 14 Prove or give a counterexample: If ğ‘‰ is a finite-dimensional vector space and ğ‘…, ğ‘†, ğ‘‡ âˆˆ â„’(ğ‘‰) are such that ğ‘…ğ‘†ğ‘‡ is surjective, then ğ‘† is injective. 15 Suppose ğ‘‡ âˆˆ â„’(ğ‘‰) and ğ‘£1, â€¦, ğ‘£ğ‘š is a list in ğ‘‰ such that ğ‘‡ğ‘£1, â€¦, ğ‘‡ğ‘£ğ‘š spans ğ‘‰. Prove that ğ‘£1, â€¦, ğ‘£ğ‘š spans ğ‘‰. 16 Prove that every linear map from ğ…ğ‘›, 1 to ğ…ğ‘š, 1 is given by a matrix multipli- cation. In other words, prove that if ğ‘‡ âˆˆ â„’(ğ…ğ‘›, 1, ğ…ğ‘š, 1), then there exists an ğ‘š-by-ğ‘› matrix ğ´ such that ğ‘‡ğ‘¥ = ğ´ğ‘¥ for every ğ‘¥ âˆˆ ğ…ğ‘›, 1. Section 3D Invertibility and Isomorphisms 95 17 Suppose ğ‘‰ is finite-dimensional andğ‘† âˆˆ â„’(ğ‘‰). Defineğ’œ âˆˆ â„’(â„’(ğ‘‰))by ğ’œ(ğ‘‡) = ğ‘†ğ‘‡ for ğ‘‡ âˆˆ â„’(ğ‘‰). (a) Show that dim null ğ’œ = (dim ğ‘‰)(dim null ğ‘†). (b) Show that dim range ğ’œ = (dim ğ‘‰)(dim range ğ‘†). 18 Show that ğ‘‰ and â„’(ğ…, ğ‘‰) are isomorphic vector spaces. 19 Suppose ğ‘‰ is finite-dimensional andğ‘‡ âˆˆ â„’(ğ‘‰). Prove that ğ‘‡ has the same matrix with respect to every basis of ğ‘‰ if and only if ğ‘‡ is a scalar multiple of the identity operator. 20 Suppose ğ‘ âˆˆ ğ’«(ğ‘). Prove that there exists a polynomial ğ‘ âˆˆ ğ’«(ğ‘) such that ğ‘(ğ‘¥) = (ğ‘¥2 + ğ‘¥)ğ‘ â€³(ğ‘¥) + 2ğ‘¥ğ‘ â€²(ğ‘¥) + ğ‘(3) for all ğ‘¥ âˆˆ ğ‘. 21 Suppose ğ‘› is a positive integer and ğ´ğ‘—, ğ‘˜ âˆˆ ğ… for all ğ‘—, ğ‘˜ = 1, â€¦, ğ‘›. Prove that the following are equivalent (note that in both parts below, the number of equations equals the number of variables). (a) The trivial solution ğ‘¥1 = â‹¯ = ğ‘¥ğ‘› = 0is the only solution to the homogeneous system of equations ğ‘› âˆ‘ ğ‘˜ = 1 ğ´1, ğ‘˜ ğ‘¥ğ‘˜ = 0 â‹® ğ‘› âˆ‘ ğ‘˜ = 1 ğ´ğ‘›, ğ‘˜ ğ‘¥ğ‘˜ = 0. (b) For every ğ‘1, â€¦, ğ‘ğ‘› âˆˆ ğ…, there exists a solution to the system of equations ğ‘› âˆ‘ ğ‘˜ = 1 ğ´1, ğ‘˜ ğ‘¥ğ‘˜ = ğ‘1 â‹® ğ‘› âˆ‘ ğ‘˜ = 1 ğ´ğ‘›, ğ‘˜ ğ‘¥ğ‘˜ = ğ‘ğ‘›. 22 Suppose ğ‘‡ âˆˆ â„’(ğ‘‰) and ğ‘£1, â€¦, ğ‘£ğ‘› is a basis of ğ‘‰. Prove that â„³(ğ‘‡, (ğ‘£1, â€¦, ğ‘£ğ‘›))is invertible âŸº ğ‘‡ is invertible. 23 Suppose that ğ‘¢1, â€¦, ğ‘¢ğ‘› and ğ‘£1, â€¦, ğ‘£ğ‘› are bases of ğ‘‰. Let ğ‘‡ âˆˆ â„’(ğ‘‰) be such that ğ‘‡ğ‘£ğ‘˜ = ğ‘¢ğ‘˜ for each ğ‘˜ = 1, â€¦, ğ‘›. Prove that â„³(ğ‘‡, (ğ‘£1, â€¦, ğ‘£ğ‘›))= â„³(ğ¼, (ğ‘¢1, â€¦, ğ‘¢ğ‘›), (ğ‘£1, â€¦, ğ‘£ğ‘›)). 24 Suppose ğ´ and ğµ are square matrices of the same size and ğ´ğµ = ğ¼. Prove that ğµğ´ = ğ¼. 96 Chapter 3 Linear Maps 3E Products and Quotients of Vector Spaces Products of Vector Spaces As usual when dealing with more than one vector space, all vector spaces in use should be over the same field. 3.87 definition:product of vector spaces Suppose ğ‘‰1, â€¦, ğ‘‰ğ‘š are vector spaces over ğ…. â€¢ The product ğ‘‰1 Ã— â‹¯ Ã— ğ‘‰ğ‘š is defined by ğ‘‰1 Ã— â‹¯ Ã— ğ‘‰ğ‘š = {(ğ‘£1, â€¦, ğ‘£ğ‘š) âˆ¶ ğ‘£1 âˆˆ ğ‘‰1, â€¦, ğ‘£ğ‘š âˆˆ ğ‘‰ğ‘š}. â€¢ Addition on ğ‘‰1 Ã— â‹¯ Ã— ğ‘‰ğ‘š is defined by (ğ‘¢1, â€¦, ğ‘¢ğ‘š) + (ğ‘£1, â€¦, ğ‘£ğ‘š) = (ğ‘¢1 + ğ‘£1, â€¦, ğ‘¢ğ‘š + ğ‘£ğ‘š). â€¢ Scalar multiplication on ğ‘‰1 Ã— â‹¯ Ã— ğ‘‰ğ‘š is defined by ğœ†(ğ‘£1, â€¦, ğ‘£ğ‘š) = (ğœ†ğ‘£1, â€¦, ğœ†ğ‘£ğ‘š). (5 âˆ’ 6ğ‘¥+ 4ğ‘¥ 2, (3, 8, 7))+ (ğ‘¥ + 9ğ‘¥ 5, (2, 2, 2)) = (5 âˆ’ 5ğ‘¥+ 4ğ‘¥ 2 + 9ğ‘¥ 5, (5, 10, 9)). Also, 2(5 âˆ’ 6ğ‘¥+ 4ğ‘¥ 2, (3, 8, 7))= (10 âˆ’ 12ğ‘¥+ 8ğ‘¥ 2, (6, 16, 14)). The next result should be interpreted to mean that the product of vector spaces is a vector space with the operations of addition and scalar multiplication as defined by3.87. 3.89 product of vector spaces is a vector space Suppose ğ‘‰1, â€¦, ğ‘‰ğ‘š are vector spaces over ğ…. Then ğ‘‰1 Ã— â‹¯ Ã— ğ‘‰ğ‘š is a vector space over ğ…. The proof of the result above is left to the reader. Note that the additive identity of ğ‘‰1 Ã— â‹¯ Ã— ğ‘‰ğ‘š is (0, â€¦, 0), where the 0in the ğ‘˜th slot is the additive identity of ğ‘‰ğ‘˜. The additive inverse of (ğ‘£1, â€¦, ğ‘£ğ‘š) âˆˆ ğ‘‰1 Ã— â‹¯ Ã— ğ‘‰ğ‘š is (âˆ’ğ‘£1, â€¦, âˆ’ğ‘£ğ‘š). 3.88 example:product of the vector spaces ğ’«5(ğ‘)and ğ‘3 Elements of ğ’«5(ğ‘) Ã— ğ‘ 3 are lists of length two, with the first item in the list an element of ğ’«5(ğ‘)and the second item in the list an element of ğ‘3. For example, (5 âˆ’ 6ğ‘¥+ 4ğ‘¥ 2, (3, 8, 7))and (ğ‘¥+ 9ğ‘¥ 5, (2, 2, 2))are elements of ğ’«5(ğ‘) Ã— ğ‘ 3. Their sum is defined by Section 3E Products and Quotients of Vector Spaces 97 3.90 example: ğ‘2 Ã— ğ‘3 â‰  ğ‘5 but ğ‘2 Ã— ğ‘3 is isomorphic to ğ‘5 Elements of the vector space ğ‘2 Ã— ğ‘3 are lists ((ğ‘¥1, ğ‘¥2), (ğ‘¥3, ğ‘¥4, ğ‘¥5)), where ğ‘¥1, ğ‘¥2, ğ‘¥3, ğ‘¥4, ğ‘¥5 âˆˆ ğ‘. Elements of ğ‘5 are lists (ğ‘¥1, ğ‘¥2, ğ‘¥3, ğ‘¥4, ğ‘¥5), where ğ‘¥1, ğ‘¥2, ğ‘¥3, ğ‘¥4, ğ‘¥5 âˆˆ ğ‘. Although elements of ğ‘2 Ã— ğ‘3 and ğ‘5 look similar, they are not the same kind of object. Elements of ğ‘2 Ã— ğ‘3 are lists of length two (with the first item itself a list of length two and the second item a list of length three), and elements of ğ‘5 are lists of length five. Thusğ‘2 Ã— ğ‘3 does not equal ğ‘5. This isomorphism is so natural that we should think of it as a relabel- ing. Some people informally say that ğ‘2Ã—ğ‘3 equals ğ‘5, which is not techni- cally correct but which captures the spirit of identification via relabeling. The linear map ((ğ‘¥1, ğ‘¥2), (ğ‘¥3, ğ‘¥4, ğ‘¥5))â†¦ (ğ‘¥1, ğ‘¥2, ğ‘¥3, ğ‘¥4, ğ‘¥5) is an isomorphism of the vector space ğ‘2 Ã— ğ‘3 onto the vector space ğ‘5. Thus these two vector spaces are isomorphic, al- though they are not equal. The next example illustrates the idea that we will use in the proof of 3.92. 3.91 example: a basis of ğ’«2(ğ‘) Ã— ğ‘2 Consider this list of length five of elements ofğ’«2(ğ‘) Ã— ğ‘2: (1, (0, 0)), (ğ‘¥, (0, 0)), (ğ‘¥2, (0, 0)), (0, (1, 0)), (0, (0, 1)). The list above is linearly independent and it spans ğ’«2(ğ‘) Ã— ğ‘2. Thus it is a basis of ğ’«2(ğ‘) Ã— ğ‘2. 3.92 dimension of a product is the sum of dimensions Suppose ğ‘‰1, â€¦, ğ‘‰ğ‘š are finite-dimensional vector spaces. Thenğ‘‰1 Ã— â‹¯ Ã— ğ‘‰ğ‘š is finite-dimensional and dim(ğ‘‰1 Ã— â‹¯ Ã— ğ‘‰ğ‘š) = dim ğ‘‰1 + â‹¯ + dim ğ‘‰ğ‘š. Proof Choose a basis of each ğ‘‰ğ‘˜. For each basis vector of each ğ‘‰ğ‘˜, consider the element of ğ‘‰1 Ã— â‹¯ Ã— ğ‘‰ğ‘š that equals the basis vector in the ğ‘˜th slot and 0in the other slots. The list of all such vectors is linearly independent and spans ğ‘‰1 Ã— â‹¯ Ã— ğ‘‰ğ‘š. Thus it is a basis of ğ‘‰1 Ã— â‹¯ Ã— ğ‘‰ğ‘š. The length of this basis is dim ğ‘‰1 + â‹¯ + dim ğ‘‰ğ‘š, as desired. 98 Chapter 3 Linear Maps In the next result, the map Î“ is surjective by the definition ofğ‘‰1 +â‹¯+ğ‘‰ğ‘š. Thus the last word in the result below could be changed from â€œinjectiveâ€ to â€œinvertibleâ€. 3.93 products and direct sums Suppose that ğ‘‰1, â€¦, ğ‘‰ğ‘š are subspaces of ğ‘‰. Define a linear map Î“ âˆ¶ ğ‘‰1 Ã— â‹¯ Ã— ğ‘‰ğ‘š â†’ ğ‘‰1 + â‹¯ + ğ‘‰ğ‘š by Î“(ğ‘£1, â€¦, ğ‘£ğ‘š) = ğ‘£1 + â‹¯ + ğ‘£ğ‘š. Then ğ‘‰1 + â‹¯ + ğ‘‰ğ‘š is a direct sum if and only if Î“ is injective. Proof By 3.15, Î“ is injective if and only if the only way to write 0as a sum ğ‘£1 + â‹¯ + ğ‘£ğ‘š, where each ğ‘£ğ‘˜ is in ğ‘‰ğ‘˜, is by taking each ğ‘£ğ‘˜ equal to 0. Thus 1.45 shows that Î“ is injective if and only if ğ‘‰1 + â‹¯ + ğ‘‰ğ‘š is a direct sum, as desired. 3.94 a sum is a direct sum if and only if dimensions add up Suppose ğ‘‰ is finite-dimensional andğ‘‰1, â€¦, ğ‘‰ğ‘š are subspaces of ğ‘‰. Then ğ‘‰1 + â‹¯ + ğ‘‰ğ‘š is a direct sum if and only if dim(ğ‘‰1 + â‹¯ + ğ‘‰ğ‘š) = dim ğ‘‰1 + â‹¯ + dim ğ‘‰ğ‘š. Proof The map Î“ in 3.93 is surjective. Thus by the fundamental theorem of linear maps (3.21), Î“ is injective if and only if dim(ğ‘‰1 + â‹¯ + ğ‘‰ğ‘š) = dim(ğ‘‰1 Ã— â‹¯ Ã— ğ‘‰ğ‘š). Combining 3.93 and 3.92 now shows that ğ‘‰1 + â‹¯ + ğ‘‰ğ‘š is a direct sum if and only if dim(ğ‘‰1 + â‹¯ + ğ‘‰ğ‘š) = dim ğ‘‰1 + â‹¯ + dim ğ‘‰ğ‘š, as desired. In the special case ğ‘š = 2, an alternative proof that ğ‘‰1 + ğ‘‰2 is a direct sum if and only if dim(ğ‘‰1 + ğ‘‰2) = dim ğ‘‰1 + dim ğ‘‰2 can be obtained by combining 1.46 and 2.43. Quotient Spaces We begin our approach to quotient spaces by defining the sum of a vector and a subset. 3.95 notation: ğ‘£ + ğ‘ˆ Suppose ğ‘£ âˆˆ ğ‘‰ and ğ‘ˆ âŠ† ğ‘‰. Then ğ‘£ + ğ‘ˆ is the subset of ğ‘‰ defined by ğ‘£ + ğ‘ˆ = {ğ‘£ + ğ‘¢ âˆ¶ ğ‘¢ âˆˆ ğ‘ˆ}. Section 3E Products and Quotients of Vector Spaces 99 3.96 example:sum of a vector and a one-dimensional subspace of ğ‘2 (17, 20)+ ğ‘ˆ is parallel to the subspace ğ‘ˆ. Suppose ğ‘ˆ = {(ğ‘¥, 2ğ‘¥) âˆˆ ğ‘ 2 âˆ¶ ğ‘¥ âˆˆ ğ‘}. Hence ğ‘ˆ is the line in ğ‘2 through the origin with slope 2. Thus (17, 20)+ ğ‘ˆ is the line in ğ‘2 that contains the point (17, 20) and has slope 2. Because (10, 20) âˆˆ ğ‘ˆand (17, 20) âˆˆ (17, 20)+ ğ‘ˆ, we see that (17, 20)+ ğ‘ˆ is obtained by moving ğ‘ˆ to the right by 7units. 3.97 definition:translate For ğ‘£ âˆˆ ğ‘‰ and ğ‘ˆ a subset of ğ‘‰, the set ğ‘£ + ğ‘ˆ is said to be a translate of ğ‘ˆ. 3.98 example: translates â€¢ If ğ‘ˆ is the line in ğ‘2 defined byğ‘ˆ = {(ğ‘¥, 2ğ‘¥) âˆˆ ğ‘ 2 âˆ¶ ğ‘¥ âˆˆ ğ‘}, then all lines in ğ‘2 with slope 2are translates of ğ‘ˆ. See Example 3.96 above for a drawing of ğ‘ˆ and one of its translates. â€¢ More generally, if ğ‘ˆ is a line in ğ‘2, then the set of all translates of ğ‘ˆ is the set of all lines in ğ‘2 that are parallel to ğ‘ˆ. â€¢ If ğ‘ˆ = {(ğ‘¥, ğ‘¦, 0) âˆˆ ğ‘ 3 âˆ¶ ğ‘¥, ğ‘¦ âˆˆ ğ‘}, then the translates of ğ‘ˆ are the planes in ğ‘3 that are parallel to the ğ‘¥ğ‘¦-plane ğ‘ˆ. â€¢ More generally, if ğ‘ˆ is a plane in ğ‘3, then the set of all translates of ğ‘ˆ is the set of all planes in ğ‘3 that are parallel to ğ‘ˆ (see, for example, Exercise 7). 3.99 definition: quotient space, ğ‘‰/ğ‘ˆ Suppose ğ‘ˆ is a subspace of ğ‘‰. Then the quotient space ğ‘‰/ğ‘ˆ is the set of all translates of ğ‘ˆ. Thus ğ‘‰/ğ‘ˆ = {ğ‘£ + ğ‘ˆ âˆ¶ ğ‘£ âˆˆ ğ‘‰}. 100 Chapter 3 Linear Maps 3.100 example: quotient spaces â€¢ If ğ‘ˆ = {(ğ‘¥, 2ğ‘¥) âˆˆ ğ‘ 2 âˆ¶ ğ‘¥ âˆˆ ğ‘}, then ğ‘2/ğ‘ˆ is the set of all lines in ğ‘2 that have slope 2. â€¢ If ğ‘ˆ is a line in ğ‘3 containing the origin, then ğ‘3/ğ‘ˆ is the set of all lines in ğ‘3 parallel to ğ‘ˆ. â€¢ If ğ‘ˆ is a plane in ğ‘3 containing the origin, then ğ‘3/ğ‘ˆ is the set of all planes in ğ‘3 parallel to ğ‘ˆ. Our next goal is to make ğ‘‰/ğ‘ˆ into a vector space. To do this, we will need the next result. 3.101 two translates of a subspace are equal or disjoint Suppose ğ‘ˆ is a subspace of ğ‘‰ and ğ‘£, ğ‘¤ âˆˆ ğ‘‰. Then ğ‘£ âˆ’ ğ‘¤ âˆˆ ğ‘ˆ âŸº ğ‘£ + ğ‘ˆ = ğ‘¤ + ğ‘ˆ âŸº (ğ‘£ + ğ‘ˆ) âˆ© (ğ‘¤+ ğ‘ˆ) â‰  âˆ…. Proof First suppose ğ‘£ âˆ’ ğ‘¤ âˆˆ ğ‘ˆ. If ğ‘¢ âˆˆ ğ‘ˆ, then ğ‘£ + ğ‘¢ = ğ‘¤ + ((ğ‘£ âˆ’ ğ‘¤) + ğ‘¢)âˆˆ ğ‘¤ + ğ‘ˆ. Thus ğ‘£ + ğ‘ˆ âŠ† ğ‘¤+ ğ‘ˆ. Similarly, ğ‘¤ + ğ‘ˆ âŠ† ğ‘£+ ğ‘ˆ. Thus ğ‘£ + ğ‘ˆ = ğ‘¤ + ğ‘ˆ, completing the proof that ğ‘£ âˆ’ ğ‘¤ âˆˆ ğ‘ˆ implies ğ‘£ + ğ‘ˆ = ğ‘¤ + ğ‘ˆ. The equation ğ‘£ + ğ‘ˆ = ğ‘¤ + ğ‘ˆ implies that (ğ‘£ + ğ‘ˆ) âˆ© (ğ‘¤+ ğ‘ˆ) â‰  âˆ…. Now suppose (ğ‘£ + ğ‘ˆ) âˆ© (ğ‘¤+ ğ‘ˆ) â‰  âˆ…. Thus there exist ğ‘¢1, ğ‘¢2 âˆˆ ğ‘ˆ such that ğ‘£ + ğ‘¢1 = ğ‘¤ + ğ‘¢2. Thus ğ‘£ âˆ’ ğ‘¤ = ğ‘¢2 âˆ’ ğ‘¢1. Hence ğ‘£ âˆ’ ğ‘¤ âˆˆ ğ‘ˆ, showing that (ğ‘£ + ğ‘ˆ) âˆ© (ğ‘¤+ ğ‘ˆ) â‰  âˆ… implies ğ‘£ âˆ’ ğ‘¤ âˆˆ ğ‘ˆ, which completes the proof. Now we can define addition and scalar multiplication onğ‘‰/ğ‘ˆ. 3.102 definition: addition and scalar multiplication on ğ‘‰/ğ‘ˆ Suppose ğ‘ˆ is a subspace of ğ‘‰. Then addition and scalar multiplication are defined onğ‘‰/ğ‘ˆ by (ğ‘£ + ğ‘ˆ) + (ğ‘¤ + ğ‘ˆ) = (ğ‘£ + ğ‘¤) + ğ‘ˆ ğœ†(ğ‘£ + ğ‘ˆ) = (ğœ†ğ‘£) + ğ‘ˆ for all ğ‘£, ğ‘¤ âˆˆ ğ‘‰ and all ğœ† âˆˆ ğ…. As part of the proof of the next result, we will show that the definitions above make sense. Section 3E Products and Quotients of Vector Spaces 101 3.103 quotient space is a vector space Suppose ğ‘ˆ is a subspace of ğ‘‰. Then ğ‘‰/ğ‘ˆ, with the operations of addition and scalar multiplication as defined above, is a vector space. Proof The potential problem with the definitions above of addition and scalar multiplication on ğ‘‰/ğ‘ˆ is that the representation of a translate of ğ‘ˆ is not unique. Specifically, supposeğ‘£1, ğ‘£2, ğ‘¤1, ğ‘¤2 âˆˆ ğ‘‰ are such that ğ‘£1 + ğ‘ˆ = ğ‘£2 + ğ‘ˆ and ğ‘¤1 + ğ‘ˆ = ğ‘¤2 + ğ‘ˆ. To show that the definition of addition onğ‘‰/ğ‘ˆ given above makes sense, we must show that (ğ‘£1 + ğ‘¤1) + ğ‘ˆ = (ğ‘£2 + ğ‘¤2) + ğ‘ˆ. By 3.101, we have ğ‘£1 âˆ’ ğ‘£2 âˆˆ ğ‘ˆ and ğ‘¤1 âˆ’ ğ‘¤2 âˆˆ ğ‘ˆ. Because ğ‘ˆ is a subspace of ğ‘‰ and thus is closed under addition, this implies that (ğ‘£1 âˆ’ ğ‘£2) + (ğ‘¤1 âˆ’ ğ‘¤2) âˆˆ ğ‘ˆ. Thus (ğ‘£1 + ğ‘¤1) âˆ’ (ğ‘£2 + ğ‘¤2) âˆˆ ğ‘ˆ. Using 3.101 again, we see that (ğ‘£1 + ğ‘¤1) + ğ‘ˆ = (ğ‘£2 + ğ‘¤2) + ğ‘ˆ, as desired. Thus the definition of addition onğ‘‰/ğ‘ˆ makes sense. Similarly, suppose ğœ† âˆˆ ğ…. We are still assuming that ğ‘£1 + ğ‘ˆ = ğ‘£2 + ğ‘ˆ. Because ğ‘ˆ is a subspace of ğ‘‰ and thus is closed under scalar multiplication, we have ğœ†(ğ‘£1 âˆ’ ğ‘£2) âˆˆ ğ‘ˆ. Thus ğœ†ğ‘£1 âˆ’ ğœ†ğ‘£2 âˆˆ ğ‘ˆ. Hence 3.101 implies that (ğœ†ğ‘£1) + ğ‘ˆ = (ğœ†ğ‘£2) + ğ‘ˆ. Thus the definition of scalar multiplication onğ‘‰/ğ‘ˆ makes sense. Now that addition and scalar multiplication have been defined onğ‘‰/ğ‘ˆ, the verification that these operations makeğ‘‰/ğ‘ˆ into a vector space is straightforward and is left to the reader. Note that the additive identity of ğ‘‰/ğ‘ˆ is 0+ ğ‘ˆ (which equals ğ‘ˆ) and that the additive inverse of ğ‘£ + ğ‘ˆ is (âˆ’ğ‘£) + ğ‘ˆ. The next concept will lead to a computation of the dimension of ğ‘‰/ğ‘ˆ. 3.104 definition: quotient map, ğœ‹ Suppose ğ‘ˆ is a subspace of ğ‘‰. The quotient map ğœ‹âˆ¶ ğ‘‰ â†’ ğ‘‰/ğ‘ˆ is the linear map defined by ğœ‹(ğ‘£) = ğ‘£ + ğ‘ˆ for each ğ‘£ âˆˆ ğ‘‰. The reader should verify that ğœ‹ is indeed a linear map. Although ğœ‹ depends on ğ‘ˆ as well as ğ‘‰, these spaces are left out of the notation because they should be clear from the context. 102 Chapter 3 Linear Maps 3.105 dimension of quotient space Suppose ğ‘‰ is finite-dimensional andğ‘ˆ is a subspace of ğ‘‰. Then dim ğ‘‰/ğ‘ˆ = dim ğ‘‰ âˆ’ dim ğ‘ˆ. Proof Let ğœ‹ denote the quotient map from ğ‘‰ to ğ‘‰/ğ‘ˆ. If ğ‘£ âˆˆ ğ‘‰, then ğ‘£+ğ‘ˆ = 0+ğ‘ˆ if and only if ğ‘£ âˆˆ ğ‘ˆ (by 3.101), which implies that null ğœ‹ = ğ‘ˆ. The definition of ğœ‹ implies range ğœ‹ = ğ‘‰/ğ‘ˆ. The fundamental theorem of linear maps (3.21) now implies dim ğ‘‰ = dim ğ‘ˆ + dim ğ‘‰/ğ‘ˆ, which gives the desired result. Each linear map ğ‘‡ on ğ‘‰ induces a linear map Ìƒğ‘‡ on ğ‘‰/(null ğ‘‡), which we now define. 3.106 notation:Ìƒğ‘‡ Suppose ğ‘‡ âˆˆ â„’(ğ‘‰, ğ‘Š). DefineÌƒğ‘‡âˆ¶ ğ‘‰/(null ğ‘‡) â†’ ğ‘Š by Ìƒğ‘‡(ğ‘£ + null ğ‘‡) = ğ‘‡ğ‘£. To show that the definition ofÌƒğ‘‡ makes sense, suppose ğ‘¢, ğ‘£ âˆˆ ğ‘‰ are such that ğ‘¢ + null ğ‘‡ = ğ‘£ + null ğ‘‡. By 3.101, we have ğ‘¢ âˆ’ ğ‘£ âˆˆ null ğ‘‡. Thus ğ‘‡(ğ‘¢ âˆ’ ğ‘£) = 0. Hence ğ‘‡ğ‘¢ = ğ‘‡ğ‘£. Thus the definition ofÌƒğ‘‡ indeed makes sense. The routine verification thatÌƒğ‘‡ is a linear map from ğ‘‰/(null ğ‘‡) to ğ‘Š is left to the reader. The next result shows that we can think of Ìƒğ‘‡ as a modified version ofğ‘‡, with a domain that produces a one-to-one map. 3.107 null space and range of Ìƒğ‘‡ Suppose ğ‘‡ âˆˆ â„’(ğ‘‰, ğ‘Š). Then (a) Ìƒğ‘‡ âˆ˜ ğœ‹ = ğ‘‡, where ğœ‹ is the quotient map of ğ‘‰ onto ğ‘‰/(null ğ‘‡); (b) Ìƒğ‘‡ is injective; (c) range Ìƒğ‘‡ = range ğ‘‡; (d) ğ‘‰/(null ğ‘‡) and range ğ‘‡ are isomorphic vector spaces. Proof (a) If ğ‘£ âˆˆ ğ‘‰, then (Ìƒğ‘‡ âˆ˜ ğœ‹)(ğ‘£) = Ìƒğ‘‡(ğœ‹(ğ‘£))= Ìƒğ‘‡(ğ‘£ + null ğ‘‡) = ğ‘‡ğ‘£, as desired. (b) Suppose ğ‘£ âˆˆ ğ‘‰ and Ìƒğ‘‡(ğ‘£ + null ğ‘‡) = 0. Then ğ‘‡ğ‘£ = 0. Thus ğ‘£ âˆˆ null ğ‘‡. Hence 3.101 implies that ğ‘£ + null ğ‘‡ = 0+ null ğ‘‡. This implies that null Ìƒğ‘‡ = {0+ null ğ‘‡}. Hence Ìƒğ‘‡ is injective, as desired. (c) The definition ofÌƒğ‘‡ shows that range Ìƒğ‘‡ = range ğ‘‡. (d) Now (b) and (c) imply that if we think of Ìƒğ‘‡ as mapping into range ğ‘‡, then Ìƒğ‘‡ is an isomorphism from ğ‘‰/(null ğ‘‡) onto range ğ‘‡. Section 3E Products and Quotients of Vector Spaces 103 Exercises 3E 1 Suppose ğ‘‡ is a function from ğ‘‰ to ğ‘Š. The graph of ğ‘‡ is the subset of ğ‘‰Ã— ğ‘Š defined by graph of ğ‘‡ = {(ğ‘£, ğ‘‡ğ‘£) âˆˆ ğ‘‰ Ã— ğ‘Š âˆ¶ ğ‘£ âˆˆ ğ‘‰}. Prove that ğ‘‡ is a linear map if and only if the graph of ğ‘‡ is a subspace of ğ‘‰ Ã— ğ‘Š. Formally, a function ğ‘‡ from ğ‘‰ to ğ‘Š is a subset ğ‘‡ of ğ‘‰ Ã— ğ‘Š such that for each ğ‘£ âˆˆ ğ‘‰, there exists exactly one element (ğ‘£, ğ‘¤) âˆˆ ğ‘‡. In other words, formally a function is what is called above its graph. We do not usually think of functions in this formal manner. However, if we do become formal, then this exercise could be rephrased as follows: Prove that a function ğ‘‡ from ğ‘‰ to ğ‘Š is a linear map if and only if ğ‘‡ is a subspace of ğ‘‰ Ã— ğ‘Š. 2 Suppose that ğ‘‰1, â€¦, ğ‘‰ğ‘š are vector spaces such that ğ‘‰1 Ã— â‹¯ Ã— ğ‘‰ğ‘š is finite- dimensional. Prove that ğ‘‰ğ‘˜ is finite-dimensional for eachğ‘˜ = 1, â€¦, ğ‘š. 3 Suppose ğ‘‰1, â€¦, ğ‘‰ğ‘š are vector spaces. Prove that â„’(ğ‘‰1 Ã— â‹¯ Ã— ğ‘‰ğ‘š, ğ‘Š) and â„’(ğ‘‰1, ğ‘Š) Ã— â‹¯ Ã— â„’(ğ‘‰ğ‘š, ğ‘Š) are isomorphic vector spaces. 4 Suppose ğ‘Š1, â€¦, ğ‘Šğ‘š are vector spaces. Prove that â„’(ğ‘‰, ğ‘Š1 Ã— â‹¯ Ã— ğ‘Šğ‘š) and â„’(ğ‘‰, ğ‘Š1) Ã— â‹¯ Ã— â„’(ğ‘‰, ğ‘Šğ‘š) are isomorphic vector spaces. 5 For ğ‘š a positive integer, defineğ‘‰ğ‘š by ğ‘‰ğ‘š = ğ‘‰ Ã— â‹¯ Ã— ğ‘‰âŸ ğ‘š times . Prove that ğ‘‰ğ‘š and â„’(ğ…ğ‘š, ğ‘‰)are isomorphic vector spaces. 6 Suppose that ğ‘£, ğ‘¥ are vectors in ğ‘‰ and that ğ‘ˆ, ğ‘Š are subspaces of ğ‘‰ such that ğ‘£ + ğ‘ˆ = ğ‘¥ + ğ‘Š. Prove that ğ‘ˆ = ğ‘Š. 7 Let ğ‘ˆ = {(ğ‘¥, ğ‘¦, ğ‘§) âˆˆ ğ‘3 âˆ¶ 2ğ‘¥+ 3ğ‘¦+ 5ğ‘§ = 0}. Suppose ğ´ âŠ† ğ‘ 3. Prove that ğ´ is a translate of ğ‘ˆ if and only if there exists ğ‘ âˆˆ ğ‘ such that ğ´ = {(ğ‘¥, ğ‘¦, ğ‘§) âˆˆ ğ‘3 âˆ¶ 2ğ‘¥+ 3ğ‘¦+ 5ğ‘§ = ğ‘}. 8 (a) Suppose ğ‘‡ âˆˆ â„’(ğ‘‰, ğ‘Š) and ğ‘ âˆˆ ğ‘Š. Prove that {ğ‘¥ âˆˆ ğ‘‰ âˆ¶ ğ‘‡ğ‘¥ = ğ‘} is either the empty set or is a translate of null ğ‘‡. (b) Explain why the set of solutions to a system of linear equations such as 3.27 is either the empty set or is a translate of some subspace of ğ…ğ‘›. 9 Prove that a nonempty subset ğ´ of ğ‘‰ is a translate of some subspace of ğ‘‰ if and only if ğœ†ğ‘£ + (1 âˆ’ ğœ†)ğ‘¤ âˆˆ ğ´for all ğ‘£, ğ‘¤ âˆˆ ğ´ and all ğœ† âˆˆ ğ…. 10 Suppose ğ´1 = ğ‘£ + ğ‘ˆ1 and ğ´2 = ğ‘¤ + ğ‘ˆ2 for some ğ‘£, ğ‘¤ âˆˆ ğ‘‰ and some subspaces ğ‘ˆ1, ğ‘ˆ2 of ğ‘‰. Prove that the intersection ğ´1 âˆ© ğ´2 is either a translate of some subspace of ğ‘‰ or is the empty set. 104 Chapter 3 Linear Maps 11 Suppose ğ‘ˆ = {(ğ‘¥1, ğ‘¥2, â€¦ ) âˆˆ ğ…âˆ âˆ¶ ğ‘¥ğ‘˜ â‰  0for only finitely manyğ‘˜}. (a) Show that ğ‘ˆ is a subspace of ğ…âˆ. (b) Prove that ğ…âˆ/ğ‘ˆ is infinite-dimensional. 12 Suppose ğ‘£1, â€¦, ğ‘£ğ‘š âˆˆ ğ‘‰. Let ğ´ = {ğœ†1ğ‘£1 + â‹¯ + ğœ†ğ‘šğ‘£ğ‘š âˆ¶ ğœ†1, â€¦, ğœ†ğ‘š âˆˆ ğ… and ğœ†1 + â‹¯ + ğœ†ğ‘š = 1}. (a) Prove that ğ´ is a translate of some subspace of ğ‘‰. (b) Prove that if ğµ is a translate of some subspace of ğ‘‰ and {ğ‘£1, â€¦, ğ‘£ğ‘š} âŠ† ğµ, then ğ´ âŠ† ğµ. (c) Prove that ğ´ is a translate of some subspace of ğ‘‰ of dimension less than ğ‘š. 13 Suppose ğ‘ˆ is a subspace of ğ‘‰ such that ğ‘‰/ğ‘ˆ is finite-dimensional. Prove that ğ‘‰ is isomorphic to ğ‘ˆ Ã— (ğ‘‰/ğ‘ˆ). 14 Suppose ğ‘ˆ and ğ‘Š are subspaces of ğ‘‰ and ğ‘‰ = ğ‘ˆ âŠ• ğ‘Š. Suppose ğ‘¤1, â€¦, ğ‘¤ğ‘š is a basis of ğ‘Š. Prove that ğ‘¤1 + ğ‘ˆ, â€¦, ğ‘¤ğ‘š + ğ‘ˆ is a basis of ğ‘‰/ğ‘ˆ. 15 Suppose ğ‘ˆ is a subspace of ğ‘‰ and ğ‘£1 + ğ‘ˆ, â€¦, ğ‘£ğ‘š + ğ‘ˆ is a basis of ğ‘‰/ğ‘ˆ and ğ‘¢1, â€¦, ğ‘¢ğ‘› is a basis of ğ‘ˆ. Prove that ğ‘£1, â€¦, ğ‘£ğ‘š, ğ‘¢1, â€¦, ğ‘¢ğ‘› is a basis of ğ‘‰. 16 Suppose ğœ‘ âˆˆ â„’(ğ‘‰, ğ…) and ğœ‘ â‰  0. Prove that dim ğ‘‰/(null ğœ‘) = 1. 17 Suppose ğ‘ˆ is a subspace of ğ‘‰ such that dim ğ‘‰/ğ‘ˆ = 1. Prove that there exists ğœ‘ âˆˆ â„’(ğ‘‰, ğ…) such that null ğœ‘ = ğ‘ˆ. 18 Suppose that ğ‘ˆ is a subspace of ğ‘‰ such that ğ‘‰/ğ‘ˆ is finite-dimensional. (a) Show that if ğ‘Š is a finite-dimensional subspace ofğ‘‰ and ğ‘‰ = ğ‘ˆ + ğ‘Š, then dim ğ‘Š â‰¥dim ğ‘‰/ğ‘ˆ. (b) Prove that there exists a finite-dimensional subspaceğ‘Š of ğ‘‰ such that dim ğ‘Š = dim ğ‘‰/ğ‘ˆ and ğ‘‰ = ğ‘ˆ âŠ• ğ‘Š. 19 Suppose ğ‘‡ âˆˆ â„’(ğ‘‰, ğ‘Š) and ğ‘ˆ is a subspace of ğ‘‰. Let ğœ‹ denote the quotient map from ğ‘‰ onto ğ‘‰/ğ‘ˆ. Prove that there exists ğ‘† âˆˆ â„’(ğ‘‰/ğ‘ˆ, ğ‘Š) such that ğ‘‡ = ğ‘† âˆ˜ ğœ‹ if and only if ğ‘ˆ âŠ†null ğ‘‡. Section 3F Duality 105 3F Duality Dual Space and Dual Map Linear maps into the scalar fieldğ… play a special role in linear algebra, and thus they get a special name. 3.108 definition: linear functional A linear functional on ğ‘‰ is a linear map from ğ‘‰ to ğ…. In other words, a linear functional is an element of â„’(ğ‘‰, ğ…). 3.109 example: linear functionals â€¢ Defineğœ‘âˆ¶ ğ‘3 â†’ ğ‘ by ğœ‘(ğ‘¥, ğ‘¦, ğ‘§) = 4ğ‘¥ âˆ’ 5ğ‘¦+ 2ğ‘§. Then ğœ‘ is a linear functional on ğ‘3. â€¢ Fix (ğ‘1, â€¦, ğ‘ğ‘›) âˆˆ ğ…ğ‘›. Defineğœ‘âˆ¶ ğ…ğ‘› â†’ ğ… by ğœ‘(ğ‘¥1, â€¦, ğ‘¥ğ‘›) = ğ‘1ğ‘¥1 + â‹¯ + ğ‘ğ‘› ğ‘¥ğ‘›. Then ğœ‘ is a linear functional on ğ…ğ‘›. â€¢ Defineğœ‘âˆ¶ ğ’«(ğ‘) â†’ ğ‘ by ğœ‘(ğ‘) = 3ğ‘ â€³(5)+ 7ğ‘(4). Then ğœ‘ is a linear functional on ğ’«(ğ‘). â€¢ Defineğœ‘âˆ¶ ğ’«(ğ‘) â†’ ğ‘ by ğœ‘(ğ‘) = âˆ« 1 0 ğ‘ for each ğ‘ âˆˆ ğ’«(ğ‘). Then ğœ‘ is a linear functional on ğ’«(ğ‘). The vector space â„’(ğ‘‰, ğ…) also gets a special name and special notation. 3.110 definition: dual space, ğ‘‰â€² The dual space of ğ‘‰, denoted by ğ‘‰â€², is the vector space of all linear functionals on ğ‘‰. In other words, ğ‘‰â€² = â„’(ğ‘‰, ğ…). 3.111 dim ğ‘‰â€² = dim ğ‘‰ Suppose ğ‘‰ is finite-dimensional. Thenğ‘‰â€² is also finite-dimensional and dim ğ‘‰â€² = dim ğ‘‰. Proof By 3.72 we have dim ğ‘‰â€² = dim â„’(ğ‘‰, ğ…) = (dim ğ‘‰)(dim ğ…) = dim ğ‘‰, as desired. 106 Chapter 3 Linear Maps In the following definition, the linear map lemma (3.4) implies that each ğœ‘ğ‘— is well defined. 3.112 definition: dual basis If ğ‘£1, â€¦, ğ‘£ğ‘› is a basis of ğ‘‰, then the dual basis of ğ‘£1, â€¦, ğ‘£ğ‘› is the list ğœ‘1, â€¦, ğœ‘ğ‘› of elements of ğ‘‰â€², where each ğœ‘ğ‘— is the linear functional on ğ‘‰ such that ğœ‘ğ‘—(ğ‘£ğ‘˜) = â§{ â¨{â© 1 if ğ‘˜ = ğ‘—, 0 if ğ‘˜ â‰  ğ‘—. 3.113 example: the dual basis of the standard basis of ğ…ğ‘› Suppose ğ‘› is a positive integer. For 1 â‰¤ ğ‘— â‰¤ ğ‘›, defineğœ‘ğ‘— to be the linear functional on ğ…ğ‘› that selects the ğ‘—th coordinate of a vector in ğ…ğ‘›. Thus ğœ‘ğ‘—(ğ‘¥1, â€¦, ğ‘¥ğ‘›) = ğ‘¥ğ‘— for each (ğ‘¥1, â€¦, ğ‘¥ğ‘›) âˆˆ ğ…ğ‘›. Let ğ‘’1, â€¦, ğ‘’ğ‘› be the standard basis of ğ…ğ‘›. Then ğœ‘ğ‘—(ğ‘’ğ‘˜) = â§{ â¨{â© 1 if ğ‘˜ = ğ‘—, 0 if ğ‘˜ â‰  ğ‘—. Thus ğœ‘1, â€¦, ğœ‘ğ‘› is the dual basis of the standard basis ğ‘’1, â€¦, ğ‘’ğ‘› of ğ…ğ‘›. The next result shows that the dual basis of a basis of ğ‘‰ consists of the linear functionals on ğ‘‰ that give the coefficients for expressing a vector in ğ‘‰ as a linear combination of the basis vectors. 3.114 dual basis gives coefficients for linear combination Suppose ğ‘£1, â€¦, ğ‘£ğ‘› is a basis of ğ‘‰ and ğœ‘1, â€¦, ğœ‘ğ‘› is the dual basis. Then ğ‘£ = ğœ‘1(ğ‘£)ğ‘£1 + â‹¯ + ğœ‘ğ‘›(ğ‘£)ğ‘£ğ‘› for each ğ‘£ âˆˆ ğ‘‰. Proof Suppose ğ‘£ âˆˆ ğ‘‰. Then there exist ğ‘1, â€¦, ğ‘ğ‘› âˆˆ ğ… such that 3.115 ğ‘£ = ğ‘1ğ‘£1 + â‹¯ + ğ‘ğ‘›ğ‘£ğ‘›. If ğ‘— âˆˆ {1, â€¦, ğ‘›}, then applying ğœ‘ğ‘— to both sides of the equation above gives ğœ‘ğ‘—(ğ‘£) = ğ‘ğ‘—. Substituting the values for ğ‘1, â€¦, ğ‘ğ‘› given by the equation above into 3.115 shows that ğ‘£ = ğœ‘1(ğ‘£)ğ‘£1 + â‹¯ + ğœ‘ğ‘›(ğ‘£)ğ‘£ğ‘›. Section 3F Duality 107 The next result shows that the dual basis is indeed a basis of the dual space. Thus the terminology â€œdual basisâ€ is justified. 3.116 dual basis is a basis of the dual space Suppose ğ‘‰ is finite-dimensional. Then the dual basis of a basis ofğ‘‰ is a basis of ğ‘‰â€². Proof Suppose ğ‘£1, â€¦, ğ‘£ğ‘› is a basis of ğ‘‰. Let ğœ‘1, â€¦, ğœ‘ğ‘› denote the dual basis. To show that ğœ‘1, â€¦, ğœ‘ğ‘› is a linearly independent list of elements of ğ‘‰â€², suppose ğ‘1, â€¦, ğ‘ğ‘› âˆˆ ğ… are such that 3.117 ğ‘1ğœ‘1 + â‹¯ + ğ‘ğ‘›ğœ‘ğ‘› = 0. Now (ğ‘1ğœ‘1 + â‹¯ + ğ‘ğ‘›ğœ‘ğ‘›)(ğ‘£ğ‘˜) = ğ‘ğ‘˜ for each ğ‘˜ = 1, â€¦, ğ‘›. Thus 3.117 shows that ğ‘1 = â‹¯ = ğ‘ğ‘› = 0. Hence ğœ‘1, â€¦, ğœ‘ğ‘› is linearly independent. Because ğœ‘1, â€¦, ğœ‘ğ‘› is a linearly independent list in ğ‘‰â€² whose length equals dim ğ‘‰â€² (by 3.111), we can conclude that ğœ‘1, â€¦, ğœ‘ğ‘› is a basis of ğ‘‰â€² (see 2.38). In the definition below, note that ifğ‘‡ is a linear map from ğ‘‰ to ğ‘Š then ğ‘‡â€² is a linear map from ğ‘Šâ€² to ğ‘‰â€². 3.118 definition: dual map, ğ‘‡â€² Suppose ğ‘‡ âˆˆ â„’(ğ‘‰, ğ‘Š). The dual map of ğ‘‡ is the linear map ğ‘‡â€² âˆˆ â„’(ğ‘Šâ€², ğ‘‰â€²) defined for eachğœ‘ âˆˆ ğ‘Šâ€² by ğ‘‡â€²(ğœ‘) = ğœ‘ âˆ˜ ğ‘‡. If ğ‘‡ âˆˆ â„’(ğ‘‰, ğ‘Š) and ğœ‘ âˆˆ ğ‘Šâ€², then ğ‘‡â€²(ğœ‘) is defined above to be the composition of the linear maps ğœ‘ and ğ‘‡. Thus ğ‘‡â€²(ğœ‘) is indeed a linear map from ğ‘‰ to ğ…; in other words, ğ‘‡â€²(ğœ‘) âˆˆ ğ‘‰â€². The following two bullet points show that ğ‘‡â€² is a linear map from ğ‘Šâ€² to ğ‘‰â€². â€¢ If ğœ‘, ğœ“ âˆˆ ğ‘Šâ€², then ğ‘‡â€²(ğœ‘ + ğœ“) = (ğœ‘ + ğœ“) âˆ˜ ğ‘‡ = ğœ‘ âˆ˜ ğ‘‡ + ğœ“ âˆ˜ ğ‘‡ = ğ‘‡â€²(ğœ‘) + ğ‘‡â€²(ğœ“). â€¢ If ğœ† âˆˆ ğ… and ğœ‘ âˆˆ ğ‘Šâ€², then ğ‘‡â€²(ğœ†ğœ‘) = (ğœ†ğœ‘) âˆ˜ ğ‘‡ = ğœ†(ğœ‘ âˆ˜ ğ‘‡) = ğœ†ğ‘‡â€²(ğœ‘). The prime notation appears with two unrelated meanings in the next example: ğ·â€² denotes the dual of the linear map ğ·, and ğ‘ â€² denotes the derivative of a polynomial ğ‘. 108 Chapter 3 Linear Maps 3.119 example: dual map of the differentiation linear map Defineğ·âˆ¶ ğ’«(ğ‘) â†’ ğ’«(ğ‘) by ğ·ğ‘ = ğ‘â€². â€¢ Suppose ğœ‘ is the linear functional on ğ’«(ğ‘) defined byğœ‘(ğ‘) = ğ‘(3). Then ğ· â€²(ğœ‘) is the linear functional on ğ’«(ğ‘) given by (ğ·â€²(ğœ‘))(ğ‘) = (ğœ‘ âˆ˜ ğ·)(ğ‘) = ğœ‘(ğ·ğ‘) = ğœ‘(ğ‘ â€²)= ğ‘â€²(3). Thus ğ·â€²(ğœ‘) is the linear functional on ğ’«(ğ‘) taking ğ‘ to ğ‘ â€²(3). â€¢ Suppose ğœ‘ is the linear functional on ğ’«(ğ‘) defined byğœ‘(ğ‘) = âˆ« 1 0 ğ‘. Then ğ· â€²(ğœ‘) is the linear functional on ğ’«(ğ‘) given by (ğ· â€²(ğœ‘))(ğ‘) = (ğœ‘ âˆ˜ ğ·)(ğ‘) = ğœ‘(ğ·ğ‘) = ğœ‘(ğ‘ â€²) = âˆ«1 0 ğ‘â€² = ğ‘(1) âˆ’ ğ‘(0). Thus ğ·â€²(ğœ‘) is the linear functional on ğ’«(ğ‘) taking ğ‘ to ğ‘(1) âˆ’ ğ‘(0). In the next result, (a) and (b) imply that the function that takes ğ‘‡ to ğ‘‡â€² is a linear map from â„’(ğ‘‰, ğ‘Š) to â„’(ğ‘Šâ€², ğ‘‰â€²). In (c) below, note the reversal of order from ğ‘†ğ‘‡ on the left to ğ‘‡â€²ğ‘† â€² on the right. 3.120 algebraic properties of dual maps Suppose ğ‘‡ âˆˆ â„’(ğ‘‰, ğ‘Š). Then (a) (ğ‘† + ğ‘‡) â€² = ğ‘†â€² + ğ‘‡â€² for all ğ‘† âˆˆ â„’(ğ‘‰, ğ‘Š); (b) (ğœ†ğ‘‡)â€² = ğœ†ğ‘‡â€² for all ğœ† âˆˆ ğ…; (c) (ğ‘†ğ‘‡) â€² = ğ‘‡â€²ğ‘† â€² for all ğ‘† âˆˆ â„’(ğ‘Š, ğ‘ˆ). Proof The proofs of (a) and (b) are left to the reader. To prove (c), suppose ğœ‘ âˆˆ ğ‘ˆâ€². Then (ğ‘†ğ‘‡) â€²(ğœ‘) = ğœ‘ âˆ˜ (ğ‘†ğ‘‡) = (ğœ‘ âˆ˜ ğ‘†) âˆ˜ ğ‘‡ = ğ‘‡â€²(ğœ‘ âˆ˜ ğ‘†) = ğ‘‡â€²(ğ‘† â€²(ğœ‘))= (ğ‘‡â€²ğ‘†â€²)(ğœ‘), Some books use the notation ğ‘‰âˆ— and ğ‘‡âˆ— for duality instead of ğ‘‰â€² and ğ‘‡â€². However, here we reserve the notation ğ‘‡âˆ— for the adjoint, which will be intro- duced when we study linear maps on inner product spaces in Chapter 7. where the first, third, and fourth equal- ities above hold because of the defini- tion of the dual map, the second equality holds because composition of functions is associative, and the last equality fol- lows from the definition of composition. The equation above shows that (ğ‘†ğ‘‡) â€²(ğœ‘) = (ğ‘‡â€²ğ‘† â€²)(ğœ‘) for all ğœ‘ âˆˆ ğ‘ˆâ€². Thus (ğ‘†ğ‘‡)â€² = ğ‘‡â€²ğ‘† â€². Section 3F Duality 109 Null Space and Range of Dual of Linear Map Our goal in this subsection is to describe null ğ‘‡â€² and range ğ‘‡â€² in terms of range ğ‘‡ and null ğ‘‡. To do this, we will need the next definition. 3.121 definition: annihilator, ğ‘ˆ0 For ğ‘ˆ âŠ† ğ‘‰, the annihilator of ğ‘ˆ, denoted by ğ‘ˆ0, is defined by ğ‘ˆ0 = {ğœ‘ âˆˆ ğ‘‰â€² âˆ¶ ğœ‘(ğ‘¢) = 0for all ğ‘¢ âˆˆ ğ‘ˆ}. 3.122 example: element of an annihilator Suppose ğ‘ˆ is the subspace of ğ’«(ğ‘) consisting of polynomial multiples of ğ‘¥2. If ğœ‘ is the linear functional on ğ’«(ğ‘) defined byğœ‘(ğ‘) = ğ‘â€²(0), then ğœ‘ âˆˆ ğ‘ˆ0. For ğ‘ˆ âŠ† ğ‘‰, the annihilator ğ‘ˆ0 is a subset of the dual space ğ‘‰â€². Thus ğ‘ˆ0 depends on the vector space containing ğ‘ˆ, so a notation such as ğ‘ˆ0 ğ‘‰ would be more precise. However, the containing vector space will always be clear from the context, so we will use the simpler notation ğ‘ˆ0. 3.123 example: the annihilator of a two-dimensional subspace of ğ‘5 Let ğ‘’1, ğ‘’2, ğ‘’3, ğ‘’4, ğ‘’5 denote the standard basis of ğ‘5; let ğœ‘1, ğœ‘2, ğœ‘3, ğœ‘4, ğœ‘5 âˆˆ (ğ‘5) â€² denote the dual basis of ğ‘’1, ğ‘’2, ğ‘’3, ğ‘’4, ğ‘’5. Suppose ğ‘ˆ = span(ğ‘’1, ğ‘’2) = {(ğ‘¥1, ğ‘¥2, 0, 0, 0) âˆˆ ğ‘ 5 âˆ¶ ğ‘¥1, ğ‘¥2 âˆˆ ğ‘}. We want to show that ğ‘ˆ0 = span(ğœ‘3, ğœ‘4, ğœ‘5). Recall (see 3.113) that ğœ‘ğ‘— is the linear functional on ğ‘5 that selects the ğ‘—th coordinate: ğœ‘ğ‘—(ğ‘¥1, ğ‘¥2, ğ‘¥3, ğ‘¥4, ğ‘¥5) = ğ‘¥ğ‘—. First suppose ğœ‘ âˆˆ span(ğœ‘3, ğœ‘4, ğœ‘5). Then there exist ğ‘3, ğ‘4, ğ‘5 âˆˆ ğ‘ such that ğœ‘ = ğ‘3ğœ‘3 + ğ‘4ğœ‘4 + ğ‘5ğœ‘5. If (ğ‘¥1, ğ‘¥2, 0, 0, 0) âˆˆ ğ‘ˆ, then ğœ‘(ğ‘¥1, ğ‘¥2, 0, 0, 0) = (ğ‘3ğœ‘3 + ğ‘4ğœ‘4 + ğ‘5ğœ‘5)(ğ‘¥1, ğ‘¥2, 0, 0, 0) = 0. Thus ğœ‘ âˆˆ ğ‘ˆ0. Hence we have shown that span(ğœ‘3, ğœ‘4, ğœ‘5) âŠ† ğ‘ˆ 0. To show the inclusion in the other direction, suppose that ğœ‘ âˆˆ ğ‘ˆ0. Be- cause the dual basis is a basis of (ğ‘5) â€² , there exist ğ‘1, ğ‘2, ğ‘3, ğ‘4, ğ‘5 âˆˆ ğ‘ such that ğœ‘ = ğ‘1ğœ‘1 + ğ‘2ğœ‘2 + ğ‘3ğœ‘3 + ğ‘4ğœ‘4 + ğ‘5ğœ‘5. Because ğ‘’1 âˆˆ ğ‘ˆ and ğœ‘ âˆˆ ğ‘ˆ0, we have 0 = ğœ‘(ğ‘’1) = (ğ‘1ğœ‘1 + ğ‘2ğœ‘2 + ğ‘3ğœ‘3 + ğ‘4ğœ‘4 + ğ‘5ğœ‘5)(ğ‘’1) = ğ‘1. Similarly, ğ‘’2 âˆˆ ğ‘ˆ and thus ğ‘2 = 0. Hence ğœ‘ = ğ‘3ğœ‘3 + ğ‘4ğœ‘4 + ğ‘5ğœ‘5. Thus ğœ‘ âˆˆ span(ğœ‘3, ğœ‘4, ğœ‘5), which shows that ğ‘ˆ0 âŠ†span(ğœ‘3, ğœ‘4, ğœ‘5). Thus ğ‘ˆ0 = span(ğœ‘3, ğœ‘4, ğœ‘5). 110 Chapter 3 Linear Maps 3.124 the annihilator is a subspace Suppose ğ‘ˆ âŠ† ğ‘‰. Then ğ‘ˆ0 is a subspace of ğ‘‰â€². Proof Note that 0 âˆˆ ğ‘ˆ 0 (here 0is the zero linear functional on ğ‘‰) because the zero linear functional applied to every vector in ğ‘ˆ is the zero vector in ğ‘‰. Suppose ğœ‘, ğœ“ âˆˆ ğ‘ˆ0. Thus ğœ‘, ğœ“ âˆˆ ğ‘‰â€² and ğœ‘(ğ‘¢) = ğœ“(ğ‘¢) = 0for every ğ‘¢ âˆˆ ğ‘ˆ. If ğ‘¢ âˆˆ ğ‘ˆ, then (ğœ‘ + ğœ“)(ğ‘¢) = ğœ‘(ğ‘¢) + ğœ“(ğ‘¢) = 0+ 0 = 0. Thus ğœ‘ + ğœ“ âˆˆ ğ‘ˆ0. Similarly, ğ‘ˆ0 is closed under scalar multiplication. Thus 1.34 implies that ğ‘ˆ0 is a subspace of ğ‘‰â€². The next result shows that dim ğ‘ˆ0 is the difference of dim ğ‘‰ and dim ğ‘ˆ. For example, this shows that if ğ‘ˆ is a two-dimensional subspace of ğ‘5, then ğ‘ˆ0 is a three-dimensional subspace of (ğ‘5) â€² , as in Example 3.123. The next result can be proved following the pattern of Example 3.123: choose a basis ğ‘¢1, â€¦, ğ‘¢ğ‘š of ğ‘ˆ, extend to a basis ğ‘¢1, â€¦, ğ‘¢ğ‘š, â€¦, ğ‘¢ğ‘› of ğ‘‰, let ğœ‘1, â€¦, ğœ‘ğ‘š, â€¦, ğœ‘ğ‘› be the dual basis of ğ‘‰â€², and then show that ğœ‘ğ‘š + 1, â€¦, ğœ‘ğ‘› is a basis of ğ‘ˆ0, which implies the desired result. You should construct the proof just outlined, even though a slicker proof is presented here. 3.125 dimension of the annihilator Suppose ğ‘‰ is finite-dimensional andğ‘ˆ is a subspace of ğ‘‰. Then dim ğ‘ˆ0 = dim ğ‘‰ âˆ’ dim ğ‘ˆ. Proof Let ğ‘– âˆˆ â„’(ğ‘ˆ, ğ‘‰) be the inclusion map defined byğ‘–(ğ‘¢) = ğ‘¢ for each ğ‘¢ âˆˆ ğ‘ˆ. Thus ğ‘–â€² is a linear map from ğ‘‰â€² to ğ‘ˆâ€². The fundamental theorem of linear maps (3.21) applied to ğ‘–â€² shows that dim range ğ‘–â€² + dim null ğ‘–â€² = dim ğ‘‰â€². However, null ğ‘–â€² = ğ‘ˆ0 (as can be seen by thinking about the definitions) and dim ğ‘‰â€² = dim ğ‘‰ (by 3.111), so we can rewrite the equation above as 3.126 dim range ğ‘–â€² + dim ğ‘ˆ0 = dim ğ‘‰. If ğœ‘ âˆˆ ğ‘ˆâ€², then ğœ‘ can be extended to a linear functional ğœ“ on ğ‘‰ (see, for example, Exercise 13 in Section 3A). The definition ofğ‘–â€² shows that ğ‘–â€²(ğœ“) = ğœ‘. Thus ğœ‘ âˆˆ range ğ‘–â€², which implies that range ğ‘–â€² = ğ‘ˆâ€². Hence dim range ğ‘–â€² = dim ğ‘ˆâ€² = dim ğ‘ˆ, and then 3.126 becomes the equation dim ğ‘ˆ + dim ğ‘ˆ0 = dim ğ‘‰, as desired. Section 3F Duality 111 The next result can be a useful tool to show that a subspace is as big as possibleâ€”see (a)â€”or to show that a subspace is as small as possibleâ€”see (b). 3.127 condition for the annihilator to equal {0}or the whole space Suppose ğ‘‰ is finite-dimensional andğ‘ˆ is a subspace of ğ‘‰. Then (a) ğ‘ˆ0 = {0} âŸº ğ‘ˆ = ğ‘‰; (b) ğ‘ˆ0 = ğ‘‰â€² âŸº ğ‘ˆ = {0}. Proof To prove (a), we have ğ‘ˆ0 = {0} âŸºdim ğ‘ˆ0 = 0 âŸº dim ğ‘ˆ = dim ğ‘‰ âŸº ğ‘ˆ = ğ‘‰, where the second equivalence follows from 3.125 and the third equivalence follows from 2.39. Similarly, to prove (b) we have ğ‘ˆ0 = ğ‘‰â€² âŸº dim ğ‘ˆ0 = dim ğ‘‰â€² âŸº dim ğ‘ˆ0 = dim ğ‘‰ âŸº dim ğ‘ˆ = 0 âŸº ğ‘ˆ = {0}, where one direction of the first equivalence follows from2.39, the second equiva- lence follows from 3.111, and the third equivalence follows from 3.125. The proof of (a) in the next result does not use the hypothesis that ğ‘‰ and ğ‘Š are finite-dimensional. 3.128 the null space of ğ‘‡â€² Suppose ğ‘‰ and ğ‘Š are finite-dimensional andğ‘‡ âˆˆ â„’(ğ‘‰, ğ‘Š). Then (a) null ğ‘‡â€² = (range ğ‘‡)0; (b) dim null ğ‘‡â€² = dim null ğ‘‡ + dim ğ‘Š âˆ’ dim ğ‘‰. Proof (a) First suppose ğœ‘ âˆˆ null ğ‘‡â€². Thus 0 = ğ‘‡ â€²(ğœ‘) = ğœ‘ âˆ˜ ğ‘‡. Hence 0 = (ğœ‘ âˆ˜ ğ‘‡)(ğ‘£) = ğœ‘(ğ‘‡ğ‘£) for every ğ‘£ âˆˆ ğ‘‰. Thus ğœ‘ âˆˆ (range ğ‘‡) 0. This implies that null ğ‘‡â€² âŠ† (range ğ‘‡) 0. To prove the inclusion in the opposite direction, now suppose ğœ‘ âˆˆ (range ğ‘‡) 0. Thus ğœ‘(ğ‘‡ğ‘£) = 0for every vector ğ‘£ âˆˆ ğ‘‰. Hence 0 = ğœ‘ âˆ˜ ğ‘‡ = ğ‘‡ â€²(ğœ‘). In other words, ğœ‘ âˆˆ null ğ‘‡â€², which shows that (range ğ‘‡)0 âŠ† null ğ‘‡â€², completing the proof of (a). 112 Chapter 3 Linear Maps (b) We have dim null ğ‘‡â€² = dim(range ğ‘‡) 0 = dim ğ‘Š âˆ’ dim range ğ‘‡ = dim ğ‘Š âˆ’ (dim ğ‘‰ âˆ’ dim null ğ‘‡) = dim null ğ‘‡ + dim ğ‘Š âˆ’ dim ğ‘‰, where the first equality comes from (a), the second equality comes from 3.125, and the third equality comes from the fundamental theorem of linear maps (3.21). The next result can be useful because sometimes it is easier to verify that ğ‘‡â€² is injective than to show directly that ğ‘‡ is surjective. 3.129 ğ‘‡ surjective is equivalent to ğ‘‡â€² injective Suppose ğ‘‰ and ğ‘Š are finite-dimensional andğ‘‡ âˆˆ â„’(ğ‘‰, ğ‘Š). Then ğ‘‡ is surjective âŸº ğ‘‡â€² is injective. Proof We have ğ‘‡ âˆˆ â„’(ğ‘‰, ğ‘Š) is surjective âŸº range ğ‘‡ = ğ‘Š âŸº (range ğ‘‡) 0 = {0} âŸº null ğ‘‡â€² = {0} âŸº ğ‘‡â€² is injective, where the second equivalence comes from 3.127(a) and the third equivalence comes from 3.128(a). 3.130 the range of ğ‘‡â€² Suppose ğ‘‰ and ğ‘Š are finite-dimensional andğ‘‡ âˆˆ â„’(ğ‘‰, ğ‘Š). Then (a) dim range ğ‘‡â€² = dim range ğ‘‡; (b) range ğ‘‡â€² = (null ğ‘‡)0. Proof (a) We have dim range ğ‘‡â€² = dim ğ‘Šâ€² âˆ’ dim null ğ‘‡â€² = dim ğ‘Š âˆ’ dim(range ğ‘‡) 0 = dim range ğ‘‡, where the first equality comes from3.21, the second equality comes from 3.111 and 3.128(a), and the third equality comes from 3.125. Section 3F Duality 113 (b) First suppose ğœ‘ âˆˆ range ğ‘‡â€². Thus there exists ğœ“ âˆˆ ğ‘Šâ€² such that ğœ‘ = ğ‘‡â€²(ğœ“). If ğ‘£ âˆˆ null ğ‘‡, then ğœ‘(ğ‘£) = (ğ‘‡â€²(ğœ“))ğ‘£ = (ğœ“ âˆ˜ ğ‘‡)(ğ‘£) = ğœ“(ğ‘‡ğ‘£) = ğœ“(0) = 0. Hence ğœ‘ âˆˆ (null ğ‘‡)0. This implies that range ğ‘‡â€² âŠ† (null ğ‘‡)0. We will complete the proof by showing that range ğ‘‡â€² and (null ğ‘‡) 0 have the same dimension. To do this, note that dim range ğ‘‡â€² = dim range ğ‘‡ = dim ğ‘‰ âˆ’ dim null ğ‘‡ = dim(null ğ‘‡)0, where the first equality comes from (a), the second equality comes from3.21, and the third equality comes from 3.125. The next result should be compared to 3.129. 3.131 ğ‘‡ injective is equivalent to ğ‘‡â€² surjective Suppose ğ‘‰ and ğ‘Š are finite-dimensional andğ‘‡ âˆˆ â„’(ğ‘‰, ğ‘Š). Then ğ‘‡ is injective âŸº ğ‘‡â€² is surjective. Proof We have ğ‘‡ is injective âŸº null ğ‘‡ = {0} âŸº (null ğ‘‡) 0 = ğ‘‰â€² âŸº range ğ‘‡â€² = ğ‘‰â€², where the second equivalence follows from 3.127(b) and the third equivalence follows from 3.130(b). Matrix of Dual of Linear Map The setting for the next result is the assumption that we have a basis ğ‘£1, â€¦, ğ‘£ğ‘› of ğ‘‰, along with its dual basis ğœ‘1, â€¦, ğœ‘ğ‘› of ğ‘‰â€². We also have a basis ğ‘¤1, â€¦, ğ‘¤ğ‘š of ğ‘Š, along with its dual basis ğœ“1, â€¦, ğœ“ğ‘š of ğ‘Šâ€². Thus â„³(ğ‘‡) is computed with respect to the bases just mentioned of ğ‘‰ and ğ‘Š, and â„³(ğ‘‡â€²)is computed with respect to the dual bases just mentioned of ğ‘Šâ€² and ğ‘‰â€². Using these bases gives the following pretty result. 3.132 matrix of ğ‘‡â€² is transpose of matrix of ğ‘‡ Suppose ğ‘‰ and ğ‘Š are finite-dimensional andğ‘‡ âˆˆ â„’(ğ‘‰, ğ‘Š). Then â„³(ğ‘‡â€²)= (â„³(ğ‘‡)) t . 114 Chapter 3 Linear Maps Proof Let ğ´ = â„³(ğ‘‡) and ğ¶ = â„³(ğ‘‡â€²). Suppose 1 â‰¤ ğ‘— â‰¤ ğ‘šand 1 â‰¤ ğ‘˜ â‰¤ ğ‘›. From the definition ofâ„³(ğ‘‡â€²)we have ğ‘‡â€²(ğœ“ğ‘—) = ğ‘› âˆ‘ ğ‘Ÿ = 1 ğ¶ğ‘Ÿ, ğ‘—ğœ‘ğ‘Ÿ. The left side of the equation above equals ğœ“ğ‘— âˆ˜ ğ‘‡. Thus applying both sides of the equation above to ğ‘£ğ‘˜ gives (ğœ“ğ‘— âˆ˜ ğ‘‡)(ğ‘£ğ‘˜) = ğ‘› âˆ‘ ğ‘Ÿ = 1 ğ¶ğ‘Ÿ, ğ‘—ğœ‘ğ‘Ÿ(ğ‘£ğ‘˜) = ğ¶ğ‘˜, ğ‘—. We also have (ğœ“ğ‘— âˆ˜ ğ‘‡)(ğ‘£ğ‘˜) = ğœ“ğ‘—(ğ‘‡ğ‘£ğ‘˜) = ğœ“ğ‘—(ğ‘š âˆ‘ ğ‘Ÿ = 1 ğ´ğ‘Ÿ, ğ‘˜ğ‘¤ğ‘Ÿ) = ğ‘š âˆ‘ ğ‘Ÿ = 1 ğ´ğ‘Ÿ, ğ‘˜ğœ“ğ‘—(ğ‘¤ğ‘Ÿ) = ğ´ğ‘—, ğ‘˜. Comparing the last line of the last two sets of equations, we have ğ¶ğ‘˜, ğ‘— = ğ´ğ‘—, ğ‘˜. Thus ğ¶ = ğ´t. In other words, â„³(ğ‘‡â€²)= (â„³(ğ‘‡)) t, as desired. Now we use duality to give an alternative proof that the column rank of a matrix equals the row rank of the matrix. This result was previously proved using different toolsâ€”see 3.57. 3.133 column rank equals row rank Suppose ğ´ âˆˆ ğ…ğ‘š, ğ‘›. Then the column rank of ğ´ equals the row rank of ğ´. Proof Defineğ‘‡âˆ¶ ğ…ğ‘›, 1 â†’ ğ…ğ‘š, 1 by ğ‘‡ğ‘¥ = ğ´ğ‘¥. Thus â„³(ğ‘‡) = ğ´, where â„³(ğ‘‡) is computed with respect to the standard bases of ğ…ğ‘›, 1 and ğ…ğ‘š, 1. Now column rank of ğ´ = column rank of â„³(ğ‘‡) = dim range ğ‘‡ = dim range ğ‘‡â€² = column rank of â„³(ğ‘‡â€²) = column rank of ğ´ t = row rank of ğ´, where the second equality comes from 3.78, the third equality comes from 3.130(a), the fourth equality comes from 3.78, the fifth equality comes from3.132, and the last equality follows from the definitions of row and column rank. See Exercise 8 in Section 7A for another alternative proof of the result above. Section 3F Duality 115 Exercises 3F 1 Explain why each linear functional is surjective or is the zero map. 2 Give three distinct examples of linear functionals on ğ‘[0, 1]. 3 Suppose ğ‘‰ is finite-dimensional andğ‘£ âˆˆ ğ‘‰ with ğ‘£ â‰  0. Prove that there exists ğœ‘ âˆˆ ğ‘‰â€² such that ğœ‘(ğ‘£) = 1. 4 Suppose ğ‘‰ is finite-dimensional andğ‘ˆ is a subspace of ğ‘‰ such that ğ‘ˆ â‰  ğ‘‰. Prove that there exists ğœ‘ âˆˆ ğ‘‰â€² such that ğœ‘(ğ‘¢) = 0for every ğ‘¢ âˆˆ ğ‘ˆ but ğœ‘ â‰  0. 5 Suppose ğ‘‡ âˆˆ â„’(ğ‘‰, ğ‘Š) and ğ‘¤1, â€¦, ğ‘¤ğ‘š is a basis of range ğ‘‡. Hence for each ğ‘£ âˆˆ ğ‘‰, there exist unique numbers ğœ‘1(ğ‘£), â€¦, ğœ‘ğ‘š(ğ‘£) such that ğ‘‡ğ‘£ = ğœ‘1(ğ‘£)ğ‘¤1 + â‹¯ + ğœ‘ğ‘š(ğ‘£)ğ‘¤ğ‘š, thus defining functionsğœ‘1, â€¦, ğœ‘ğ‘š from ğ‘‰ to ğ…. Show that each of the func- tions ğœ‘1, â€¦, ğœ‘ğ‘š is a linear functional on ğ‘‰. 6 Suppose ğœ‘, ğ›½ âˆˆ ğ‘‰â€². Prove that null ğœ‘ âŠ† null ğ›½ if and only if there exists ğ‘ âˆˆ ğ… such that ğ›½ = ğ‘ğœ‘. 7 Suppose that ğ‘‰1, â€¦, ğ‘‰ğ‘š are vector spaces. Prove that (ğ‘‰1 Ã— â‹¯ Ã— ğ‘‰ğ‘š)â€² and ğ‘‰1â€² Ã— â‹¯ Ã— ğ‘‰ğ‘šâ€² are isomorphic vector spaces. 8 Suppose ğ‘£1, â€¦, ğ‘£ğ‘› is a basis of ğ‘‰ and ğœ‘1, â€¦, ğœ‘ğ‘› is the dual basis of ğ‘‰â€². Define Î“âˆ¶ ğ‘‰ â†’ ğ…ğ‘› and Î›âˆ¶ ğ…ğ‘› â†’ ğ‘‰ by Î“(ğ‘£) = (ğœ‘1(ğ‘£), â€¦, ğœ‘ğ‘›(ğ‘£)) and Î›(ğ‘1, â€¦, ğ‘ğ‘›) = ğ‘1ğ‘£1 + â‹¯ + ğ‘ğ‘›ğ‘£ğ‘›. Explain why Î“ and Î› are inverses of each other. 9 Suppose ğ‘š is a positive integer. Show that the dual basis of the basis 1, ğ‘¥, â€¦, ğ‘¥ğ‘š of ğ’«ğ‘š(ğ‘) is ğœ‘0, ğœ‘1, â€¦, ğœ‘ğ‘š, where ğœ‘ğ‘˜(ğ‘) = ğ‘(ğ‘˜)(0) ğ‘˜! . Here ğ‘(ğ‘˜)denotes the ğ‘˜th derivative of ğ‘, with the understanding that the 0 th derivative of ğ‘ is ğ‘. 10 Suppose ğ‘š is a positive integer. (a) Show that 1, ğ‘¥ âˆ’ 5, â€¦, (ğ‘¥ âˆ’ 5) ğ‘š is a basis of ğ’«ğ‘š(ğ‘). (b) What is the dual basis of the basis in (a)? 11 Suppose ğ‘£1, â€¦, ğ‘£ğ‘› is a basis of ğ‘‰ and ğœ‘1, â€¦, ğœ‘ğ‘› is the corresponding dual basis of ğ‘‰â€². Suppose ğœ“ âˆˆ ğ‘‰â€². Prove that ğœ“ = ğœ“(ğ‘£1)ğœ‘1 + â‹¯ + ğœ“(ğ‘£ğ‘›)ğœ‘ğ‘›. 116 Chapter 3 Linear Maps 12 Suppose ğ‘†, ğ‘‡ âˆˆ â„’(ğ‘‰, ğ‘Š). (a) Prove that (ğ‘† + ğ‘‡) â€² = ğ‘†â€² + ğ‘‡â€². (b) Prove that (ğœ†ğ‘‡)â€² = ğœ†ğ‘‡â€² for all ğœ† âˆˆ ğ…. This exercise asks you to verify (a) and (b) in 3.120. 13 Show that the dual map of the identity operator on ğ‘‰ is the identity operator on ğ‘‰â€². 14 Defineğ‘‡âˆ¶ ğ‘3 â†’ ğ‘2 by ğ‘‡(ğ‘¥, ğ‘¦, ğ‘§) = (4ğ‘¥+ 5ğ‘¦+ 6ğ‘§, 7ğ‘¥+ 8ğ‘¦+ 9ğ‘§). Suppose ğœ‘1, ğœ‘2 denotes the dual basis of the standard basis of ğ‘2 and ğœ“1, ğœ“2, ğœ“3 denotes the dual basis of the standard basis of ğ‘3. (a) Describe the linear functionals ğ‘‡â€²(ğœ‘1) and ğ‘‡â€²(ğœ‘2). (b) Write ğ‘‡â€²(ğœ‘1) and ğ‘‡â€²(ğœ‘2) as linear combinations of ğœ“1, ğœ“2, ğœ“3. 15 Defineğ‘‡âˆ¶ ğ’«(ğ‘) â†’ ğ’«(ğ‘) by (ğ‘‡ğ‘)(ğ‘¥) = ğ‘¥2ğ‘(ğ‘¥) + ğ‘ â€³(ğ‘¥) for each ğ‘¥ âˆˆ ğ‘. (a) Suppose ğœ‘ âˆˆ ğ’«(ğ‘)â€² is defined byğœ‘(ğ‘) = ğ‘â€²(4). Describe the linear functional ğ‘‡â€²(ğœ‘) on ğ’«(ğ‘). (b) Suppose ğœ‘ âˆˆ ğ’«(ğ‘)â€² is defined byğœ‘(ğ‘) = âˆ«1 0 ğ‘. Evaluate (ğ‘‡â€²(ğœ‘))(ğ‘¥3). 16 Suppose ğ‘Š is finite-dimensional andğ‘‡ âˆˆ â„’(ğ‘‰, ğ‘Š). Prove that ğ‘‡â€² = 0 âŸº ğ‘‡ = 0. 17 Suppose ğ‘‰ and ğ‘Š are finite-dimensional andğ‘‡ âˆˆ â„’(ğ‘‰, ğ‘Š). Prove that ğ‘‡ is invertible if and only if ğ‘‡â€² âˆˆ â„’(ğ‘Šâ€², ğ‘‰â€²)is invertible. 18 Suppose ğ‘‰ and ğ‘Š are finite-dimensional. Prove that the map that takes ğ‘‡ âˆˆ â„’(ğ‘‰, ğ‘Š) to ğ‘‡â€² âˆˆ â„’(ğ‘Šâ€², ğ‘‰â€²)is an isomorphism of â„’(ğ‘‰, ğ‘Š) onto â„’(ğ‘Šâ€², ğ‘‰â€²). 19 Suppose ğ‘ˆ âŠ† ğ‘‰. Explain why ğ‘ˆ0 = {ğœ‘ âˆˆ ğ‘‰â€² âˆ¶ ğ‘ˆ âŠ†null ğœ‘}. 20 Suppose ğ‘‰ is finite-dimensional andğ‘ˆ is a subspace of ğ‘‰. Show that ğ‘ˆ = {ğ‘£ âˆˆ ğ‘‰ âˆ¶ ğœ‘(ğ‘£) = 0for every ğœ‘ âˆˆ ğ‘ˆ0}. 21 Suppose ğ‘‰ is finite-dimensional andğ‘ˆ and ğ‘Š are subspaces of ğ‘‰. (a) Prove that ğ‘Š0 âŠ† ğ‘ˆ 0 if and only if ğ‘ˆ âŠ† ğ‘Š. (b) Prove that ğ‘Š0 = ğ‘ˆ0 if and only if ğ‘ˆ = ğ‘Š. Section 3F Duality 117 22 Suppose ğ‘‰ is finite-dimensional andğ‘ˆ and ğ‘Š are subspaces of ğ‘‰. (a) Show that (ğ‘ˆ + ğ‘Š) 0 = ğ‘ˆ0 âˆ© ğ‘Š 0. (b) Show that (ğ‘ˆ âˆ© ğ‘Š) 0 = ğ‘ˆ0 + ğ‘Š0. 23 Suppose ğ‘‰ is finite-dimensional andğœ‘1, â€¦, ğœ‘ğ‘š âˆˆ ğ‘‰â€². Prove that the follow- ing three sets are equal to each other. (a) span(ğœ‘1, â€¦, ğœ‘ğ‘š) (b) ((null ğœ‘1) âˆ© â‹¯ âˆ© (null ğœ‘ğ‘š)) 0 (c) {ğœ‘ âˆˆ ğ‘‰â€² âˆ¶ (null ğœ‘1) âˆ© â‹¯ âˆ© (null ğœ‘ğ‘š) âŠ†null ğœ‘} 24 Suppose ğ‘‰ is finite-dimensional andğ‘£1, â€¦, ğ‘£ğ‘š âˆˆ ğ‘‰. Define a linear map Î“âˆ¶ ğ‘‰â€² â†’ ğ…ğ‘š by Î“(ğœ‘) = (ğœ‘(ğ‘£1), â€¦, ğœ‘(ğ‘£ğ‘š)). (a) Prove that ğ‘£1, â€¦, ğ‘£ğ‘š spans ğ‘‰ if and only if Î“ is injective. (b) Prove that ğ‘£1, â€¦, ğ‘£ğ‘š is linearly independent if and only if Î“ is surjective. 25 Suppose ğ‘‰ is finite-dimensional andğœ‘1, â€¦, ğœ‘ğ‘š âˆˆ ğ‘‰â€². Define a linear map Î“âˆ¶ ğ‘‰ â†’ ğ…ğ‘š by Î“(ğ‘£) = (ğœ‘1(ğ‘£), â€¦, ğœ‘ğ‘š(ğ‘£)). (a) Prove that ğœ‘1, â€¦, ğœ‘ğ‘š spans ğ‘‰â€² if and only if Î“ is injective. (b) Prove that ğœ‘1, â€¦, ğœ‘ğ‘š is linearly independent if and only if Î“ is surjective. 26 Suppose ğ‘‰ is finite-dimensional andÎ© is a subspace of ğ‘‰â€². Prove that Î© = {ğ‘£ âˆˆ ğ‘‰ âˆ¶ ğœ‘(ğ‘£) = 0for every ğœ‘ âˆˆ Î©} 0 . 27 Suppose ğ‘‡ âˆˆ â„’(ğ’«5(ğ‘))and null ğ‘‡â€² = span(ğœ‘), where ğœ‘ is the linear functional on ğ’«5(ğ‘) defined byğœ‘(ğ‘) = ğ‘(8). Prove that range ğ‘‡ = {ğ‘ âˆˆ ğ’«5(ğ‘) âˆ¶ ğ‘(8) = 0}. 28 Suppose ğ‘‰ is finite-dimensional andğœ‘1, â€¦, ğœ‘ğ‘š is a linearly independent list in ğ‘‰â€². Prove that dim((null ğœ‘1) âˆ© â‹¯ âˆ© (null ğœ‘ğ‘š))= (dim ğ‘‰) âˆ’ ğ‘š. 29 Suppose ğ‘‰ and ğ‘Š are finite-dimensional andğ‘‡ âˆˆ â„’(ğ‘‰, ğ‘Š). (a) Prove that if ğœ‘ âˆˆ ğ‘Šâ€² and null ğ‘‡â€² = span(ğœ‘), then range ğ‘‡ = null ğœ‘. (b) Prove that if ğœ“ âˆˆ ğ‘‰â€² and range ğ‘‡â€² = span(ğœ“), then null ğ‘‡ = null ğœ“. 30 Suppose ğ‘‰ is finite-dimensional andğœ‘1, â€¦, ğœ‘ğ‘› is a basis of ğ‘‰â€². Show that there exists a basis of ğ‘‰ whose dual basis is ğœ‘1, â€¦, ğœ‘ğ‘›. 31 Suppose ğ‘ˆ is a subspace of ğ‘‰. Let ğ‘–âˆ¶ ğ‘ˆ â†’ ğ‘‰ be the inclusion map defined by ğ‘–(ğ‘¢) = ğ‘¢. Thus ğ‘–â€² âˆˆ â„’(ğ‘‰â€², ğ‘ˆâ€²). (a) Show that null ğ‘–â€² = ğ‘ˆ0. (b) Prove that if ğ‘‰ is finite-dimensional, thenrange ğ‘–â€² = ğ‘ˆâ€². (c) Prove that if ğ‘‰ is finite-dimensional, then Ìƒğ‘–â€² is an isomorphism from ğ‘‰â€²/ğ‘ˆ0 onto ğ‘ˆâ€². The isomorphism in (c) is natural in that it does not depend on a choice of basis in either vector space. 118 Chapter 3 Linear Maps 32 The double dual space of ğ‘‰, denoted by ğ‘‰â€³, is defined to be the dual space of ğ‘‰â€². In other words, ğ‘‰â€³ = (ğ‘‰â€²) â€². DefineÎ›âˆ¶ ğ‘‰ â†’ ğ‘‰â€³ by (Î›ğ‘£)(ğœ‘) = ğœ‘(ğ‘£) for each ğ‘£ âˆˆ ğ‘‰ and each ğœ‘ âˆˆ ğ‘‰â€². (a) Show that Î› is a linear map from ğ‘‰ to ğ‘‰â€³. (b) Show that if ğ‘‡ âˆˆ â„’(ğ‘‰), then ğ‘‡â€³ âˆ˜ Î› = Î› âˆ˜ ğ‘‡, where ğ‘‡â€³ = (ğ‘‡â€²) â€². (c) Show that if ğ‘‰ is finite-dimensional, thenÎ› is an isomorphism from ğ‘‰ onto ğ‘‰â€³. Suppose ğ‘‰ is finite-dimensional. Then ğ‘‰ and ğ‘‰â€² are isomorphic, but finding an isomorphism from ğ‘‰ onto ğ‘‰â€² generally requires choosing a basis of ğ‘‰. In contrast, the isomorphism Î› from ğ‘‰ onto ğ‘‰â€³ does not require a choice of basis and thus is considered more natural. 33 Suppose ğ‘ˆ is a subspace of ğ‘‰. Let ğœ‹âˆ¶ ğ‘‰ â†’ ğ‘‰/ğ‘ˆ be the usual quotient map. Thus ğœ‹â€² âˆˆ â„’((ğ‘‰/ğ‘ˆ) â€², ğ‘‰â€²). (a) Show that ğœ‹â€² is injective. (b) Show that range ğœ‹â€² = ğ‘ˆ0. (c) Conclude that ğœ‹â€² is an isomorphism from (ğ‘‰/ğ‘ˆ) â€² onto ğ‘ˆ0. The isomorphism in (c) is natural in that it does not depend on a choice of basis in either vector space. In fact, there is no assumption here that any of these vector spaces are finite-dimensional. Chapter 4 Polynomials This chapter contains material on polynomials that we will use to investigate linear maps from a vector space to itself. Many results in this chapter will already be familiar to you from other courses; they are included here for completeness. Because this chapter is not about linear algebra, your instructor may go through it rapidly. You may not be asked to scrutinize all the proofs. Make sure, however, that you at least read and understand the statements of all results in this chapterâ€” they will be used in later chapters. This chapter begins with a brief discussion of algebraic properties of the complex numbers. Then we prove that a nonconstant polynomial cannot have more zeros than its degree. We also give a linear-algebra-based proof of the division algorithm for polynomials, which is worth reading even if you are already familiar with a proof that does not use linear algebra. As we will see, the fundamental theorem of algebra leads to a factorization of every polynomial into degree-one factors if the scalar field isğ‚ or to factors of degree at most two if the scalar field isğ‘. standing assumption for this chapter â€¢ ğ… denotes ğ‘ or ğ‚.AlirezaJavaheriCCBY Statue of mathematician and poet Omar Khayyam (1048â€“1131), whose algebra book written in 1070 contained the first serious study of cubic polynomials. 119 S. Axler, Linear Algebra Done Right, Undergraduate Texts in Mathematics, https://doi.org/10.1007/978-3-031-41026-0_4 Â© Sheldon Axler 2024 120 Chapter 4 Polynomials Before discussing polynomials with complex or real coefficients, we need to learn a bit more about the complex numbers. 4.1 definition: real part, Re ğ‘§, imaginary part, Im ğ‘§ Suppose ğ‘§ = ğ‘ + ğ‘ğ‘–, where ğ‘ and ğ‘ are real numbers. â€¢ The real part of ğ‘§, denoted by Re ğ‘§, is defined byRe ğ‘§ = ğ‘. â€¢ The imaginary part of ğ‘§, denoted by Im ğ‘§, is defined byIm ğ‘§ = ğ‘. Thus for every complex number ğ‘§, we have ğ‘§ = Re ğ‘§ + (Im ğ‘§)ğ‘–. 4.2 definition: complex conjugate, ğ‘§, absolute value, |ğ‘§| Suppose ğ‘§ âˆˆ ğ‚. â€¢ The complex conjugate of ğ‘§ âˆˆ ğ‚, denoted by ğ‘§, is defined by ğ‘§ = Re ğ‘§ âˆ’ (Im ğ‘§)ğ‘–. â€¢ The absolute value of a complex number ğ‘§, denoted by |ğ‘§|, is defined by |ğ‘§| = âˆš(Re ğ‘§)2 + (Im ğ‘§)2. 4.3 example: real and imaginary part, complex conjugate, absolute value Suppose ğ‘§ = 3+ 2ğ‘–. Then â€¢ Re ğ‘§ = 3and Im ğ‘§ = 2; â€¢ ğ‘§ = 3 âˆ’ 2ğ‘–; â€¢ |ğ‘§| = âˆš32 + 22 = âˆš13. Identifying a complex number ğ‘§ âˆˆ ğ‚ with the ordered pair (Re ğ‘§, Im ğ‘§) âˆˆ ğ‘2 identifiesğ‚ with ğ‘2. Note that ğ‚ is a one-dimensional complex vector space, but we can also think of ğ‚ (identified withğ‘2) as a two-dimensional real vector space. The absolute value of each complex number is a nonnegative number. Specif- ically, if ğ‘§ âˆˆ ğ‚, then |ğ‘§| equals the distance from the origin in ğ‘2 to the point (Re ğ‘§, Im ğ‘§) âˆˆ ğ‘2. You should verify that ğ‘§ = ğ‘§ if and only if ğ‘§ is a real number. The real and imaginary parts, com- plex conjugate, and absolute value have the properties listed in the following multipart result. Chapter 4 Polynomials 121 4.4 properties of complex numbers Suppose ğ‘¤, ğ‘§ âˆˆ ğ‚. Then the following equalities and inequalities hold. sum of ğ‘§ and ğ‘§ ğ‘§ + ğ‘§ = 2Re ğ‘§. difference of ğ‘§ and ğ‘§ ğ‘§ âˆ’ ğ‘§ = 2(Im ğ‘§)ğ‘–. product of ğ‘§ and ğ‘§ ğ‘§ğ‘§ = |ğ‘§|2. additivity and multiplicativity of complex conjugate ğ‘¤ + ğ‘§ = ğ‘¤ + ğ‘§ and ğ‘¤ğ‘§ = ğ‘¤ ğ‘§. double complex conjugate ğ‘§ = ğ‘§. real and imaginary parts are bounded by |ğ‘§| | Re ğ‘§| â‰¤ |ğ‘§|and | Im ğ‘§| â‰¤ |ğ‘§|. absolute value of the complex conjugate âˆ£ğ‘§âˆ£ = |ğ‘§|. multiplicativity of absolute value |ğ‘¤ğ‘§| = |ğ‘¤| |ğ‘§|. triangle inequality |ğ‘¤ + ğ‘§| â‰¤ |ğ‘¤|+ |ğ‘§|. Geometric interpretation of triangle in- equality: The length of each side of a triangle is less than or equal to the sum of the lengths of the two other sides. Proof Except for the last item above, the routine verifications of the assertions above are left to the reader. To verify the triangle inequality, we have |ğ‘¤ + ğ‘§|2 = (ğ‘¤ + ğ‘§)(ğ‘¤ + ğ‘§) = ğ‘¤ğ‘¤ + ğ‘§ğ‘§ + ğ‘¤ğ‘§ + ğ‘§ğ‘¤ = |ğ‘¤|2 + |ğ‘§|2 + ğ‘¤ğ‘§ + ğ‘¤ğ‘§ = |ğ‘¤|2 + |ğ‘§|2 + 2Re(ğ‘¤ğ‘§) â‰¤ |ğ‘¤| 2 + |ğ‘§| 2 + 2âˆ£ğ‘¤ğ‘§âˆ£ = |ğ‘¤|2 + |ğ‘§|2 + 2|ğ‘¤| |ğ‘§| = (|ğ‘¤| + |ğ‘§|) 2. See Exercise 2 for the reverse triangle inequality. Taking square roots now gives the desired inequality |ğ‘¤ + ğ‘§| â‰¤ |ğ‘¤|+ |ğ‘§|. 122 Chapter 4 Polynomials Zeros of Polynomials Recall that a function ğ‘âˆ¶ ğ… â†’ ğ… is called a polynomial of degree ğ‘š if there exist ğ‘0, â€¦, ğ‘ğ‘š âˆˆ ğ… with ğ‘ğ‘š â‰  0such that ğ‘(ğ‘§) = ğ‘0 + ğ‘1ğ‘§ + â‹¯ + ğ‘ğ‘šğ‘§ ğ‘š for all ğ‘§ âˆˆ ğ…. A polynomial could have more than one degree if the representation of ğ‘ in the form above were not unique. Our first task is to show that this cannot happen. The solutions to the equation ğ‘(ğ‘§) = 0play a crucial role in the study of a polynomial ğ‘ âˆˆ ğ’«(ğ…). Thus these solutions have a special name. 4.5 definition: zero of a polynomial A number ğœ† âˆˆ ğ… is called a zero (or root) of a polynomial ğ‘ âˆˆ ğ’«(ğ…) if ğ‘(ğœ†) = 0. The next result is the key tool that we will use to show that the degree of a polynomial is unique. 4.6 each zero of a polynomial corresponds to a degree-one factor Suppose ğ‘š is a positive integer and ğ‘ âˆˆ ğ’«(ğ…) is a polynomial of degree ğ‘š. Suppose ğœ† âˆˆ ğ…. Then ğ‘(ğœ†) = 0if and only if there exists a polynomial ğ‘ âˆˆ ğ’«(ğ…) of degree ğ‘š âˆ’ 1such that ğ‘(ğ‘§) = (ğ‘§ âˆ’ ğœ†)ğ‘(ğ‘§) for every ğ‘§ âˆˆ ğ…. Proof First suppose ğ‘(ğœ†) = 0. Let ğ‘0, ğ‘1, â€¦, ğ‘ğ‘š âˆˆ ğ… be such that ğ‘(ğ‘§) = ğ‘0 + ğ‘1ğ‘§ + â‹¯ + ğ‘ğ‘šğ‘§ ğ‘š for all ğ‘§ âˆˆ ğ…. Then 4.7 ğ‘(ğ‘§) = ğ‘(ğ‘§) âˆ’ ğ‘(ğœ†) = ğ‘1(ğ‘§ âˆ’ ğœ†) + â‹¯ + ğ‘ğ‘š(ğ‘§ğ‘š âˆ’ ğœ†ğ‘š) for all ğ‘§ âˆˆ ğ…. For each ğ‘˜ âˆˆ {1, â€¦, ğ‘š}, the equation ğ‘§ ğ‘˜ âˆ’ ğœ†ğ‘˜ = (ğ‘§ âˆ’ ğœ†) ğ‘˜ âˆ‘ ğ‘— = 1 ğœ†ğ‘— âˆ’ 1ğ‘§ğ‘˜ âˆ’ ğ‘— shows that ğ‘§ğ‘˜ âˆ’ ğœ†ğ‘˜ equals ğ‘§ âˆ’ ğœ† times some polynomial of degree ğ‘˜ âˆ’ 1. Thus 4.7 shows that ğ‘ equals ğ‘§ âˆ’ ğœ† times some polynomial of degree ğ‘š âˆ’ 1, as desired. To prove the implication in the other direction, now suppose that there is a polynomial ğ‘ âˆˆ ğ’«(ğ…) such that ğ‘(ğ‘§) = (ğ‘§ âˆ’ ğœ†)ğ‘(ğ‘§) for every ğ‘§ âˆˆ ğ…. Then ğ‘(ğœ†) = (ğœ† âˆ’ ğœ†)ğ‘(ğœ†) = 0, as desired. Chapter 4 Polynomials 123 Now we can prove that polynomials do not have too many zeros. 4.8 degree ğ‘š implies at most ğ‘š zeros Suppose ğ‘š is a positive integer and ğ‘ âˆˆ ğ’«(ğ…) is a polynomial of degree ğ‘š. Then ğ‘ has at most ğ‘š zeros in ğ…. Proof We will use induction on ğ‘š. The desired result holds if ğ‘š = 1because if ğ‘1 â‰  0then the polynomial ğ‘0 + ğ‘1ğ‘§ has only one zero (which equals âˆ’ğ‘0/ğ‘1). Thus assume that ğ‘š > 1and the desired result holds for ğ‘š âˆ’ 1. If ğ‘ has no zeros in ğ…, then the desired result holds and we are done. Thus suppose ğ‘ has a zero ğœ† âˆˆ ğ…. By 4.6, there is polynomial ğ‘ âˆˆ ğ’«(ğ…) of degree ğ‘š âˆ’ 1such that ğ‘(ğ‘§) = (ğ‘§ âˆ’ ğœ†)ğ‘(ğ‘§) for every ğ‘§ âˆˆ ğ…. Our induction hypothesis implies that ğ‘ has at most ğ‘š âˆ’ 1zeros in ğ…. The equation above shows that the zeros of ğ‘ in ğ… are exactly the zeros of ğ‘ in ğ… along with ğœ†. Thus ğ‘ has at most ğ‘š zeros in ğ…. The result above implies that the coefficients of a polynomial are uniquely determined (because if a polynomial had two different sets of coefficients, then subtracting the two representations of the polynomial would give a polynomial with some nonzero coefficients but infinitely many zeros). In particular, the degree of a polynomial is uniquely defined. The 0polynomial is declared to have degree âˆ’âˆ so that exceptions are not needed for various reasonable results such as deg(ğ‘ğ‘) = deg ğ‘ + deg ğ‘. Recall that the degree of the 0poly- nomial is defined to beâˆ’âˆ. When necessary, use the expected arithmetic with âˆ’âˆ. For example, âˆ’âˆ < ğ‘š and âˆ’âˆ + ğ‘š = âˆ’âˆ for every integer ğ‘š. Division Algorithm for Polynomials If ğ‘ and ğ‘  are nonnegative integers, with ğ‘  â‰  0, then there exist nonnegative integers ğ‘ and ğ‘Ÿ such that ğ‘ = ğ‘ ğ‘ + ğ‘Ÿ and ğ‘Ÿ < ğ‘ . Think of dividing ğ‘ by ğ‘ , getting quotient ğ‘ with remainder ğ‘Ÿ. Our next result gives an analogous result for polynomials. Thus the next result is often called the division algorithm for polynomials, although as stated here it is not really an algorithm, just a useful result. Think of the division algorithm for poly- nomials as giving a remainder polyno- mial ğ‘Ÿ when the polynomial ğ‘ is divided by the polynomial ğ‘ . The division algorithm for polynomi- als could be proved without using any linear algebra. However, as is appropri- ate for a linear algebra textbook, the proof given here uses linear algebra techniques and makes nice use of a basis of ğ’«ğ‘›(ğ…), which is the (ğ‘› + 1)-dimensional vector space of polynomials with coefficients in ğ… and of degree at most ğ‘›. 124 Chapter 4 Polynomials 4.9 division algorithm for polynomials Suppose that ğ‘, ğ‘  âˆˆ ğ’«(ğ…), with ğ‘  â‰  0. Then there exist unique polynomials ğ‘, ğ‘Ÿ âˆˆ ğ’«(ğ…) such that ğ‘ = ğ‘ ğ‘ + ğ‘Ÿ and deg ğ‘Ÿ < deg ğ‘ . Proof Let ğ‘› = deg ğ‘ and let ğ‘š = deg ğ‘ . If ğ‘› < ğ‘š, then take ğ‘ = 0and ğ‘Ÿ = ğ‘ to get the desired equation ğ‘ = ğ‘ ğ‘ + ğ‘Ÿ with deg ğ‘Ÿ < deg ğ‘ . Thus we now assume that ğ‘› â‰¥ ğ‘š. The list 4.10 1, ğ‘§, â€¦, ğ‘§ ğ‘š âˆ’ 1, ğ‘ , ğ‘§ğ‘ , â€¦, ğ‘§ ğ‘› âˆ’ ğ‘šğ‘  is linearly independent in ğ’«ğ‘›(ğ…) because each polynomial in this list has a different degree. Also, the list 4.10 has length ğ‘› + 1, which equals dim ğ’«ğ‘›(ğ…). Hence 4.10 is a basis of ğ’«ğ‘›(ğ…) [by2.38]. Because ğ‘ âˆˆ ğ’«ğ‘›(ğ…) and 4.10 is a basis of ğ’«ğ‘›(ğ…), there exist unique constants ğ‘0, ğ‘1, â€¦, ğ‘ğ‘š âˆ’ 1 âˆˆ ğ… and ğ‘0, ğ‘1, â€¦, ğ‘ğ‘› âˆ’ ğ‘š âˆˆ ğ… such that ğ‘ = ğ‘0 + ğ‘1ğ‘§ + â‹¯ + ğ‘ğ‘š âˆ’ 1ğ‘§ ğ‘š âˆ’ 1 + ğ‘0ğ‘  + ğ‘1ğ‘§ğ‘  + â‹¯ + ğ‘ğ‘› âˆ’ ğ‘šğ‘§ğ‘› âˆ’ ğ‘šğ‘ 4.11 = ğ‘0 + ğ‘1ğ‘§ + â‹¯ + ğ‘ğ‘š âˆ’ 1ğ‘§ğ‘š âˆ’ 1âŸâŸâŸâŸâŸâŸâŸâŸâŸ ğ‘Ÿ + ğ‘ (ğ‘0 + ğ‘1ğ‘§ + â‹¯ + ğ‘ğ‘› âˆ’ ğ‘šğ‘§ğ‘› âˆ’ ğ‘šâŸâŸâŸâŸâŸâŸâŸâŸâŸ ğ‘ ). With ğ‘Ÿ and ğ‘ as defined above, we see thatğ‘ can be written as ğ‘ = ğ‘ ğ‘ + ğ‘Ÿ with deg ğ‘Ÿ < deg ğ‘ , as desired. The uniqueness of ğ‘, ğ‘Ÿ âˆˆ ğ’«(ğ…) satisfying these conditions follows from the uniqueness of the constants ğ‘0, ğ‘1, â€¦, ğ‘ğ‘š âˆ’ 1 âˆˆ ğ… and ğ‘0, ğ‘1, â€¦, ğ‘ğ‘› âˆ’ ğ‘š âˆˆ ğ… satisfy- ing 4.11. Factorization of Polynomials over ğ‚ The fundamental theorem of algebra is an existence theorem. Its proof does not lead to a method for finding zeros. The quadratic formula gives the zeros explicitly for polynomials of degree 2. Similar but more complicated formulas exist for polynomials of degree 3and 4. No such formulas exist for polynomials of degree 5and above. W have been handling polynomials with complex coefficients and polynomials with real coefficients simultaneously, let- ting ğ… denote ğ‘ or ğ‚. Now we will see differences between these two cases. First we treat polynomials with complex coefficients. Then we will use those re- sults to prove corresponding results for polynomials with real coefficients. Our proof of the fundamental theorem of algebra implicitly uses the result that a continuous real-valued function on a closed disk in ğ‘2 attains a minimum value. A web search can lead you to several Chapter 4 Polynomials 125 other proofs of the fundamental theorem of algebra. The proof using Liouvilleâ€™s theorem is particularly nice if you are comfortable with analytic functions. All proofs of the fundamental theorem of algebra need to use some analysis, because the result is not true if ğ‚ is replaced, for example, with the set of numbers of the form ğ‘ + ğ‘‘ğ‘– where ğ‘, ğ‘‘ are rational numbers. 4.12 fundamental theorem of algebra, first version Every nonconstant polynomial with complex coefficients has a zero in ğ‚. Proof De Moivreâ€™s theorem, which you can prove using induction on ğ‘˜ and the addition formulas for cosine and sine, states that if ğ‘˜ is a positive integer and ğœƒ âˆˆ ğ‘, then (cos ğœƒ + ğ‘– sin ğœƒ) ğ‘˜ = cos ğ‘˜ğœƒ + ğ‘– sin ğ‘˜ğœƒ. Suppose ğ‘¤ âˆˆ ğ‚ and ğ‘˜ is a positive integer. Using polar coordinates, we know that there exist ğ‘Ÿ â‰¥ 0and ğœƒ âˆˆ ğ‘ such that ğ‘Ÿ(cos ğœƒ + ğ‘– sin ğœƒ) = ğ‘¤. De Moivreâ€™s theorem implies that (ğ‘Ÿ1/ğ‘˜(cos ğœƒ ğ‘˜ + ğ‘– sin ğœƒ ğ‘˜ )) ğ‘˜ = ğ‘¤. Thus every complex number has a ğ‘˜th root, a fact that we will soon use. Suppose ğ‘ is a nonconstant polynomial with complex coefficients and highest- order nonzero term ğ‘ğ‘šğ‘§ğ‘š. Then |ğ‘(ğ‘§)| â†’ âˆ as |ğ‘§| â†’ âˆ (because |ğ‘(ğ‘§)|/âˆ£ğ‘§ ğ‘šâˆ£ â†’ |ğ‘ğ‘š| as |ğ‘§| â†’ âˆ). Thus the continuous function ğ‘§ â†¦ |ğ‘(ğ‘§)| has a global minimum at some point ğœ âˆˆ ğ‚. To show that ğ‘(ğœ ) = 0, suppose that ğ‘(ğœ ) â‰  0. Define a new polynomialğ‘ by ğ‘(ğ‘§) = ğ‘(ğ‘§ + ğœ ) ğ‘(ğœ ) . The function ğ‘§ â†¦ |ğ‘(ğ‘§)| has a global minimum value of 1at ğ‘§ = 0. Write ğ‘(ğ‘§) = 1+ ğ‘ğ‘˜ğ‘§ğ‘˜ + â‹¯ + ğ‘ğ‘šğ‘§ ğ‘š, where ğ‘˜ is the smallest positive integer such that the coefficient of ğ‘§ ğ‘˜ is nonzero; in other words, ğ‘ğ‘˜ â‰  0. Let ğ›½ âˆˆ ğ‚ be such that ğ›½ğ‘˜ = âˆ’ 1 ğ‘ğ‘˜ . There is a constant ğ‘ > 1such that if ğ‘¡ âˆˆ (0, 1), then |ğ‘(ğ‘¡ğ›½)| â‰¤âˆ£1+ ğ‘ğ‘˜ğ‘¡ğ‘˜ğ›½ğ‘˜âˆ£ + ğ‘¡ğ‘˜ + 1ğ‘ = 1 âˆ’ ğ‘¡ ğ‘˜(1 âˆ’ ğ‘¡ğ‘). Thus taking ğ‘¡ to be 1/(2ğ‘)in the inequality above, we have |ğ‘(ğ‘¡ğ›½)| < 1, which contradicts the assumption that the global minimum of ğ‘§ â†¦ |ğ‘(ğ‘§)| is 1. This contradiction implies that ğ‘(ğœ ) = 0, showing that ğ‘ has a zero, as desired. 126 Chapter 4 Polynomials Computers can use clever numerical methods to find good approximations to the zeros of any polynomial, even when exact zeros cannot be found. For example, no one will ever give an exact formula for a zero of the polynomial ğ‘ defined by ğ‘(ğ‘¥) = ğ‘¥5 âˆ’ 5ğ‘¥ 4 âˆ’ 6ğ‘¥ 3 + 17ğ‘¥ 2 + 4ğ‘¥ âˆ’ 7. However, a computer can find that the zeros ofğ‘ are approximately the five numbers âˆ’1.87, âˆ’0.74, 0.62, 1.47, 5.51. The first version of the fundamental theorem of algebra leads to the following factorization result for polynomials with complex coefficients. Note that in this factorization, the zeros of ğ‘ are the numbers ğœ†1, â€¦, ğœ†ğ‘š, which are the only values of ğ‘§ for which the right side of the equation in the next result equals 0. 4.13 fundamental theorem of algebra, second version If ğ‘ âˆˆ ğ’«(ğ‚) is a nonconstant polynomial, then ğ‘ has a unique factorization (except for the order of the factors) of the form ğ‘(ğ‘§) = ğ‘(ğ‘§ âˆ’ ğœ†1)â‹¯(ğ‘§ âˆ’ ğœ†ğ‘š), where ğ‘, ğœ†1, â€¦, ğœ†ğ‘š âˆˆ ğ‚. Proof Let ğ‘ âˆˆ ğ’«(ğ‚) and let ğ‘š = deg ğ‘. We will use induction on ğ‘š. If ğ‘š = 1, then the desired factorization exists and is unique. So assume that ğ‘š > 1and that the desired factorization exists and is unique for all polynomials of degree ğ‘š âˆ’ 1. First we will show that the desired factorization of ğ‘ exists. By the first version of the fundamental theorem of algebra (4.12), ğ‘ has a zero ğœ† âˆˆ ğ‚. By 4.6, there is a polynomial ğ‘ of degree ğ‘š âˆ’ 1such that ğ‘(ğ‘§) = (ğ‘§ âˆ’ ğœ†)ğ‘(ğ‘§) for all ğ‘§ âˆˆ ğ‚. Our induction hypothesis implies that ğ‘ has the desired factorization, which when plugged into the equation above gives the desired factorization of ğ‘. Now we turn to the question of uniqueness. The number ğ‘ is uniquely deter- mined as the coefficient of ğ‘§ ğ‘š in ğ‘. So we only need to show that except for the order, there is only one way to choose ğœ†1, â€¦, ğœ†ğ‘š. If (ğ‘§ âˆ’ ğœ†1)â‹¯(ğ‘§ âˆ’ ğœ†ğ‘š) = (ğ‘§ âˆ’ ğœ1)â‹¯(ğ‘§ âˆ’ ğœğ‘š) for all ğ‘§ âˆˆ ğ‚, then because the left side of the equation above equals 0when ğ‘§ = ğœ†1, one of the ğœâ€™s on the right side equals ğœ†1. Relabeling, we can assume that ğœ1 = ğœ†1. Now if ğ‘§ â‰  ğœ†1, we can divide both sides of the equation above by ğ‘§ âˆ’ ğœ†1, getting (ğ‘§ âˆ’ ğœ†2)â‹¯(ğ‘§ âˆ’ ğœ†ğ‘š) = (ğ‘§ âˆ’ ğœ2)â‹¯(ğ‘§ âˆ’ ğœğ‘š) for all ğ‘§ âˆˆ ğ‚ except possibly ğ‘§ = ğœ†1. Actually the equation above holds for all ğ‘§ âˆˆ ğ‚, because otherwise by subtracting the right side from the left side we would get a nonzero polynomial that has infinitely many zeros. The equation above and our induction hypothesis imply that except for the order, the ğœ†â€™s are the same as the ğœâ€™s, completing the proof of uniqueness. Chapter 4 Polynomials 127 Factorization of Polynomials over ğ‘ The failure of the fundamental theorem of algebra for ğ‘ accounts for the differ- ences between linear algebra on real and complex vector spaces, as we will see in later chapters. A polynomial with real coefficients may have no real zeros. For example, the poly- nomial 1+ ğ‘¥2 has no real zeros. To obtain a factorization theorem over ğ‘, we will use our factorization theorem over ğ‚. We begin with the next result. 4.14 polynomials with real coefficients have nonreal zeros in pairs Suppose ğ‘ âˆˆ ğ’«(ğ‚) is a polynomial with real coefficients. If ğœ† âˆˆ ğ‚ is a zero of ğ‘, then so is ğœ†. Proof Let ğ‘(ğ‘§) = ğ‘0 + ğ‘1ğ‘§ + â‹¯ + ğ‘ğ‘šğ‘§ ğ‘š, where ğ‘0, â€¦, ğ‘ğ‘š are real numbers. Suppose ğœ† âˆˆ ğ‚ is a zero of ğ‘. Then ğ‘0 + ğ‘1 ğœ† + â‹¯ + ğ‘ğ‘š ğœ† ğ‘š = 0. Take the complex conjugate of both sides of this equation, obtaining ğ‘0 + ğ‘1 ğœ† + â‹¯ + ğ‘ğ‘š ğœ†ğ‘š = 0, where we have used basic properties of the complex conjugate (see 4.4). The equation above shows that ğœ† is a zero of ğ‘. Think about the quadratic formula in connection with the result below. We want a factorization theorem for polynomials with real coefficients. We begin with the following result. 4.15 factorization of a quadratic polynomial Suppose ğ‘, ğ‘ âˆˆ ğ‘. Then there is a polynomial factorization of the form ğ‘¥2 + ğ‘ğ‘¥ + ğ‘ = (ğ‘¥ âˆ’ ğœ†1)(ğ‘¥ âˆ’ ğœ†2) with ğœ†1, ğœ†2 âˆˆ ğ‘ if and only if ğ‘ 2 â‰¥ 4ğ‘. Proof Notice that ğ‘¥2 + ğ‘ğ‘¥ + ğ‘ = (ğ‘¥ + ğ‘ 2 ) 2 + (ğ‘ âˆ’ ğ‘ 2 4 ). The equation above is the basis of the technique called completing the square. First suppose ğ‘2 < 4ğ‘. Then the right side of the equation above is positive for every ğ‘¥ âˆˆ ğ‘. Hence the polynomial ğ‘¥2 + ğ‘ğ‘¥ + ğ‘ has no real zeros and thus cannot be factored in the form (ğ‘¥ âˆ’ ğœ†1)(ğ‘¥ âˆ’ ğœ†2) with ğœ†1, ğœ†2 âˆˆ ğ‘. 128 Chapter 4 Polynomials Conversely, now suppose ğ‘ 2 â‰¥ 4ğ‘. Then there is a real number ğ‘‘ such that ğ‘‘2 = ğ‘2 4 âˆ’ ğ‘. From the displayed equation above, we have ğ‘¥2 + ğ‘ğ‘¥ + ğ‘ = (ğ‘¥ + ğ‘ 2 ) 2 âˆ’ ğ‘‘2 = (ğ‘¥ + ğ‘ 2 + ğ‘‘)(ğ‘¥ + ğ‘ 2 âˆ’ ğ‘‘), which gives the desired factorization. The next result gives a factorization of a polynomial over ğ‘. The idea of the proof is to use the second version of the fundamental theorem of algebra (4.13), which gives a factorization of ğ‘ as a polynomial with complex coefficients. Complex but nonreal zeros of ğ‘ come in pairs; see 4.14. Thus if the factorization of ğ‘ as an element of ğ’«(ğ‚) includes terms of the form (ğ‘¥ âˆ’ ğœ†) with ğœ† a nonreal complex number, then (ğ‘¥ âˆ’ ğœ†) is also a term in the factorization. Multiplying together these two terms, we get (ğ‘¥2 âˆ’ 2(Re ğœ†)ğ‘¥ + |ğœ†|2), which is a quadratic term of the required form. The idea sketched in the paragraph above almost provides a proof of the existence of our desired factorization. However, we need to be careful about one point. Suppose ğœ† is a nonreal complex number and (ğ‘¥ âˆ’ ğœ†) is a term in the factorization of ğ‘ as an element of ğ’«(ğ‚). We are guaranteed by 4.14 that (ğ‘¥ âˆ’ ğœ†) also appears as a term in the factorization, but 4.14 does not state that these two factors appear the same number of times, as needed to make the idea above work. However, the proof works around this point. In the next result, either ğ‘š or ğ‘€ may equal 0. The numbers ğœ†1, â€¦, ğœ†ğ‘š are precisely the real zeros of ğ‘, for these are the only real values of ğ‘¥ for which the right side of the equation in the next result equals 0. 4.16 factorization of a polynomial over ğ‘ Suppose ğ‘ âˆˆ ğ’«(ğ‘) is a nonconstant polynomial. Then ğ‘ has a unique factor- ization (except for the order of the factors) of the form ğ‘(ğ‘¥) = ğ‘(ğ‘¥ âˆ’ ğœ†1)â‹¯(ğ‘¥ âˆ’ ğœ†ğ‘š)(ğ‘¥2 + ğ‘1ğ‘¥ + ğ‘1)â‹¯(ğ‘¥2 + ğ‘ğ‘€ğ‘¥ + ğ‘ğ‘€), where ğ‘, ğœ†1, â€¦, ğœ†ğ‘š, ğ‘1, â€¦, ğ‘ğ‘€, ğ‘1, â€¦, ğ‘ğ‘€ âˆˆ ğ‘, with ğ‘ğ‘˜ 2 < 4ğ‘ğ‘˜ for each ğ‘˜. Proof First we will prove that the desired factorization exists, and after that we will prove the uniqueness. Think of ğ‘ as an element of ğ’«(ğ‚). If all (complex) zeros of ğ‘ are real, then we have the desired factorization by 4.13. Thus suppose ğ‘ has a zero ğœ† âˆˆ ğ‚ with ğœ† âˆ‰ ğ‘. By 4.14, ğœ† is a zero of ğ‘. Thus we can write Chapter 4 Polynomials 129 ğ‘(ğ‘¥) = (ğ‘¥ âˆ’ ğœ†)(ğ‘¥ âˆ’ ğœ†)ğ‘(ğ‘¥) = (ğ‘¥2 âˆ’ 2(Re ğœ†)ğ‘¥ + |ğœ†|2)ğ‘(ğ‘¥) for some polynomial ğ‘ âˆˆ ğ’«(ğ‚) of degree two less than the degree of ğ‘. If we can prove that ğ‘ has real coefficients, then using induction on the degree of ğ‘ completes the proof of the existence part of this result. To prove that ğ‘ has real coefficients, we solve the equation above for ğ‘, getting ğ‘(ğ‘¥) = ğ‘(ğ‘¥) ğ‘¥2 âˆ’ 2(Re ğœ†)ğ‘¥ + |ğœ†|2 for all ğ‘¥ âˆˆ ğ‘. The equation above implies that ğ‘(ğ‘¥) âˆˆ ğ‘ for all ğ‘¥ âˆˆ ğ‘. Writing ğ‘(ğ‘¥) = ğ‘0 + ğ‘1ğ‘¥ + â‹¯ + ğ‘ğ‘› âˆ’ 2ğ‘¥ğ‘› âˆ’ 2, where ğ‘› = deg ğ‘ and ğ‘0, â€¦, ğ‘ğ‘› âˆ’ 2 âˆˆ ğ‚, we thus have 0 =Im ğ‘(ğ‘¥) = (Im ğ‘0) + (Im ğ‘1)ğ‘¥ + â‹¯ + (Im ğ‘ğ‘› âˆ’ 2)ğ‘¥ğ‘› âˆ’ 2 for all ğ‘¥ âˆˆ ğ‘. This implies that Im ğ‘0, â€¦, Im ğ‘ğ‘› âˆ’ 2 all equal 0(by 4.8). Thus all coefficients of ğ‘ are real, as desired. Hence the desired factorization exists. Now we turn to the question of uniqueness of our factorization. A factor of ğ‘ of the form ğ‘¥2 + ğ‘ğ‘˜ğ‘¥+ ğ‘ğ‘˜ with ğ‘ğ‘˜ 2 < 4ğ‘ğ‘˜ can be uniquely written as (ğ‘¥ âˆ’ ğœ†ğ‘˜)(ğ‘¥ âˆ’ ğœ†ğ‘˜) with ğœ†ğ‘˜ âˆˆ ğ‚. A momentâ€™s thought shows that two different factorizations of ğ‘ as an element of ğ’«(ğ‘) would lead to two different factorizations of ğ‘ as an element of ğ’«(ğ‚), contradicting 4.13. Exercises 4 1 Suppose ğ‘¤, ğ‘§ âˆˆ ğ‚. Verify the following equalities and inequalities. (a) ğ‘§ + ğ‘§ = 2Re ğ‘§ (b) ğ‘§ âˆ’ ğ‘§ = 2(Im ğ‘§)ğ‘– (c) ğ‘§ğ‘§ = |ğ‘§|2 (d) ğ‘¤ + ğ‘§ = ğ‘¤ + ğ‘§ and ğ‘¤ğ‘§ = ğ‘¤ ğ‘§ (e) ğ‘§ = ğ‘§ (f) | Re ğ‘§| â‰¤ |ğ‘§|and | Im ğ‘§| â‰¤ |ğ‘§| (g) âˆ£ğ‘§âˆ£ = |ğ‘§| (h) |ğ‘¤ğ‘§| = |ğ‘¤| |ğ‘§| The results above are the parts of 4.4 that were left to the reader. 2 Prove that if ğ‘¤, ğ‘§ âˆˆ ğ‚, then âˆ£ |ğ‘¤| âˆ’ |ğ‘§| âˆ£ â‰¤ |ğ‘¤ âˆ’ ğ‘§|. The inequality above is called the reverse triangle inequality. 130 Chapter 4 Polynomials 3 Suppose ğ‘‰ is a complex vector space and ğœ‘ âˆˆ ğ‘‰â€². Defineğœâˆ¶ ğ‘‰ â†’ ğ‘ by ğœ(ğ‘£) = Re ğœ‘(ğ‘£) for each ğ‘£ âˆˆ ğ‘‰. Show that ğœ‘(ğ‘£) = ğœ(ğ‘£) âˆ’ ğ‘–ğœ(ğ‘–ğ‘£) for all ğ‘£ âˆˆ ğ‘‰. 4 Suppose ğ‘š is a positive integer. Is the set {0} âˆª{ğ‘ âˆˆ ğ’«(ğ…) âˆ¶ deg ğ‘ = ğ‘š} a subspace of ğ’«(ğ…)? 5 Is the set {0} âˆª{ğ‘ âˆˆ ğ’«(ğ…) âˆ¶ deg ğ‘ is even} a subspace of ğ’«(ğ…)? 6 Suppose that ğ‘š and ğ‘› are positive integers with ğ‘š â‰¤ ğ‘›, and suppose ğœ†1, â€¦, ğœ†ğ‘š âˆˆ ğ…. Prove that there exists a polynomial ğ‘ âˆˆ ğ’«(ğ…) with deg ğ‘ = ğ‘› such that 0 = ğ‘(ğœ†1) = â‹¯ = ğ‘(ğœ†ğ‘š) and such that ğ‘ has no other zeros. 7 Suppose that ğ‘š is a nonnegative integer, ğ‘§1, â€¦, ğ‘§ğ‘š + 1 are distinct elements of ğ…, and ğ‘¤1, â€¦, ğ‘¤ğ‘š + 1 âˆˆ ğ…. Prove that there exists a unique polynomial ğ‘ âˆˆ ğ’«ğ‘š(ğ…) such that ğ‘(ğ‘§ğ‘˜) = ğ‘¤ğ‘˜ for each ğ‘˜ = 1, â€¦, ğ‘š + 1. This result can be proved without using linear algebra. However, try to find the clearer, shorter proof that uses some linear algebra. 8 Suppose ğ‘ âˆˆ ğ’«(ğ‚) has degree ğ‘š. Prove that ğ‘ has ğ‘š distinct zeros if and only if ğ‘ and its derivative ğ‘ â€² have no zeros in common. 9 Prove that every polynomial of odd degree with real coefficients has a real zero. 10 For ğ‘ âˆˆ ğ’«(ğ‘), defineğ‘‡ğ‘âˆ¶ ğ‘ â†’ ğ‘ by (ğ‘‡ğ‘)(ğ‘¥) = â§{{ â¨{{â© ğ‘(ğ‘¥) âˆ’ ğ‘(3) ğ‘¥ âˆ’ 3 if ğ‘¥ â‰  3, ğ‘ â€²(3) if ğ‘¥ = 3 for each ğ‘¥ âˆˆ ğ‘. Show that ğ‘‡ğ‘ âˆˆ ğ’«(ğ‘) for every polynomial ğ‘ âˆˆ ğ’«(ğ‘) and also show that ğ‘‡âˆ¶ ğ’«(ğ‘) â†’ ğ’«(ğ‘) is a linear map. 11 Suppose ğ‘ âˆˆ ğ’«(ğ‚). Defineğ‘âˆ¶ ğ‚ â†’ ğ‚ by ğ‘(ğ‘§) = ğ‘(ğ‘§) ğ‘(ğ‘§). Prove that ğ‘ is a polynomial with real coefficients. Chapter 4 Polynomials 131 12 Suppose ğ‘š is a nonnegative integer and ğ‘ âˆˆ ğ’«ğ‘š(ğ‚) is such that there are distinct real numbers ğ‘¥0, ğ‘¥1, â€¦, ğ‘¥ğ‘š with ğ‘(ğ‘¥ğ‘˜) âˆˆ ğ‘ for each ğ‘˜ = 0, 1, â€¦, ğ‘š. Prove that all coefficients of ğ‘ are real. 13 Suppose ğ‘ âˆˆ ğ’«(ğ…) with ğ‘ â‰  0. Let ğ‘ˆ = {ğ‘ğ‘ âˆ¶ ğ‘ âˆˆ ğ’«(ğ…)}. (a) Show that dim ğ’«(ğ…)/ğ‘ˆ = deg ğ‘. (b) Find a basis of ğ’«(ğ…)/ğ‘ˆ. 14 Suppose ğ‘, ğ‘ âˆˆ ğ’«(ğ‚) are nonconstant polynomials with no zeros in common. Let ğ‘š = deg ğ‘ and ğ‘› = deg ğ‘. Use linear algebra as outlined below in (a)â€“(c) to prove that there exist ğ‘Ÿ âˆˆ ğ’«ğ‘› âˆ’ 1(ğ‚) and ğ‘  âˆˆ ğ’«ğ‘š âˆ’ 1(ğ‚) such that ğ‘Ÿğ‘ + ğ‘ ğ‘ = 1. (a) Defineğ‘‡âˆ¶ ğ’«ğ‘› âˆ’ 1(ğ‚) Ã— ğ’«ğ‘š âˆ’ 1(ğ‚) â†’ ğ’«ğ‘š + ğ‘› âˆ’ 1(ğ‚) by ğ‘‡(ğ‘Ÿ, ğ‘ ) = ğ‘Ÿğ‘ + ğ‘ ğ‘. Show that the linear map ğ‘‡ is injective. (b) Show that the linear map ğ‘‡ in (a) is surjective. (c) Use (b) to conclude that there exist ğ‘Ÿ âˆˆ ğ’«ğ‘› âˆ’ 1(ğ‚) and ğ‘  âˆˆ ğ’«ğ‘š âˆ’ 1(ğ‚) such that ğ‘Ÿğ‘ + ğ‘ ğ‘ = 1. Open Access This chapter is licensed under the terms of the Creative Commons Attribution-NonCommercial 4.0 International License (https://creativecommons.org/licenses/by-nc/4.0), which permits any noncommercial use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to original author and source, provide a link to the Creative Commons license, and indicate if changes were made. The images or other third party material in this chapter are included in the chapterâ€™s Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the chapterâ€™s Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. Chapter 5 Eigenvalues and Eigenvectors Linear maps from one vector space to another vector space were the objects of study in Chapter 3. Now we begin our investigation of operators, which are linear maps from a vector space to itself. Their study constitutes the most important part of linear algebra. To learn about an operator, we might try restricting it to a smaller subspace. Asking for that restriction to be an operator will lead us to the notion of invariant subspaces. Each one-dimensional invariant subspace arises from a vector that the operator maps into a scalar multiple of the vector. This path will lead us to eigenvectors and eigenvalues. We will then prove one of the most important results in linear algebra: every operator on a finite-dimensional nonzero complex vector space has an eigenvalue. This result will allow us to show that for each operator on a finite-dimensional complex vector space, there is a basis of the vector space with respect to which the matrix of the operator has at least almost half its entries equal to 0. standing assumptions for this chapter â€¢ ğ… denotes ğ‘ or ğ‚. â€¢ ğ‘‰ denotes a vector space over ğ….Hans-PeterPostelCCBY Statue of Leonardo of Pisa (1170â€“1250, approximate dates), also known as Fibonacci. Exercise 21 in Section 5D shows how linear algebra can be used to find the explicit formula for the Fibonacci sequence shown on the front cover. 132 S. Axler, Linear Algebra Done Right, Undergraduate Texts in Mathematics, https://doi.org/10.1007/978-3-031-41026-0_5 Â© Sheldon Axler 2024 Section 5A Invariant Subspaces 133 5A Invariant Subspaces Eigenvalues 5.1 definition: operator A linear map from a vector space to itself is called an operator. Recall that we defined the notation â„’(ğ‘‰) to mean â„’(ğ‘‰, ğ‘‰). Suppose ğ‘‡ âˆˆ â„’(ğ‘‰). If ğ‘š â‰¥ 2and ğ‘‰ = ğ‘‰1 âŠ• â‹¯ âŠ• ğ‘‰ğ‘š , where each ğ‘‰ğ‘˜ is a nonzero subspace of ğ‘‰, then to understand the behavior of ğ‘‡ we only need to understand the behavior of each ğ‘‡|ğ‘‰ğ‘˜; here ğ‘‡|ğ‘‰ğ‘˜ denotes the restriction of ğ‘‡ to the smaller domain ğ‘‰ğ‘˜. Dealing with ğ‘‡|ğ‘‰ğ‘˜ should be easier than dealing with ğ‘‡ because ğ‘‰ğ‘˜ is a smaller vector space than ğ‘‰. However, if we intend to apply tools useful in the study of operators (such as taking powers), then we have a problem: ğ‘‡|ğ‘‰ğ‘˜ may not map ğ‘‰ğ‘˜ into itself; in other words, ğ‘‡|ğ‘‰ğ‘˜ may not be an operator on ğ‘‰ğ‘˜. Thus we are led to consider only decompositions of ğ‘‰ of the form above in which ğ‘‡ maps each ğ‘‰ğ‘˜ into itself. Hence we now give a name to subspaces of ğ‘‰ that get mapped into themselves by ğ‘‡. 5.2 definition: invariant subspace Suppose ğ‘‡ âˆˆ â„’(ğ‘‰). A subspace ğ‘ˆ of ğ‘‰ is called invariant under ğ‘‡ if ğ‘‡ğ‘¢ âˆˆ ğ‘ˆ for every ğ‘¢ âˆˆ ğ‘ˆ. Thus ğ‘ˆ is invariant under ğ‘‡ if ğ‘‡|ğ‘ˆ is an operator on ğ‘ˆ. 5.3 example: subspace invariant under differentiation operator Suppose that ğ‘‡ âˆˆ â„’(ğ’«(ğ‘))is defined byğ‘‡ğ‘ = ğ‘ â€². Then ğ’«4(ğ‘), which is a subspace of ğ’«(ğ‘), is invariant under ğ‘‡ because if ğ‘ âˆˆ ğ’«(ğ‘) has degree at most 4, then ğ‘ â€² also has degree at most 4. 5.4 example: four invariant subspaces, not necessarily all different If ğ‘‡ âˆˆ â„’(ğ‘‰), then the following subspaces of ğ‘‰ are all invariant under ğ‘‡. {0} The subspace {0}is invariant under ğ‘‡ because if ğ‘¢ âˆˆ {0}, then ğ‘¢ = 0 and hence ğ‘‡ğ‘¢ = 0 âˆˆ {0}. ğ‘‰ The subspace ğ‘‰ is invariant under ğ‘‡ because if ğ‘¢ âˆˆ ğ‘‰, then ğ‘‡ğ‘¢ âˆˆ ğ‘‰. null ğ‘‡ The subspace null ğ‘‡ is invariant under ğ‘‡ because if ğ‘¢ âˆˆ null ğ‘‡, then ğ‘‡ğ‘¢ = 0, and hence ğ‘‡ğ‘¢ âˆˆ null ğ‘‡. range ğ‘‡ The subspace range ğ‘‡ is invariant under ğ‘‡ because if ğ‘¢ âˆˆ range ğ‘‡, then ğ‘‡ğ‘¢ âˆˆ range ğ‘‡. 134 Chapter 5 Eigenvalues and Eigenvectors Must an operator ğ‘‡ âˆˆ â„’(ğ‘‰) have any invariant subspaces other than {0} and ğ‘‰? Later we will see that this question has an affirmative answer if ğ‘‰ is finite-dimensional anddim ğ‘‰ > 1(for ğ… = ğ‚) or dim ğ‘‰ > 2(for ğ… = ğ‘); see 5.19 and Exercise 29 in Section 5B. The previous example noted that null ğ‘‡ and range ğ‘‡ are invariant under ğ‘‡. However, these subspaces do not necessarily provide easy answers to the question above about the existence of invariant subspaces other than {0}and ğ‘‰, because null ğ‘‡ may equal {0}and range ğ‘‡ may equal ğ‘‰ (this happens when ğ‘‡ is invertible). We will return later to a deeper study of invariant subspaces. Now we turn to an investigation of the simplest possible nontrivial invariant subspacesâ€”invariant subspaces of dimension one. Take any ğ‘£ âˆˆ ğ‘‰ with ğ‘£ â‰  0and let ğ‘ˆ equal the set of all scalar multiples of ğ‘£: ğ‘ˆ = {ğœ†ğ‘£ âˆ¶ ğœ† âˆˆ ğ…} = span(ğ‘£). Then ğ‘ˆ is a one-dimensional subspace of ğ‘‰ (and every one-dimensional subspace of ğ‘‰ is of this form for an appropriate choice of ğ‘£). If ğ‘ˆ is invariant under an operator ğ‘‡ âˆˆ â„’(ğ‘‰), then ğ‘‡ğ‘£ âˆˆ ğ‘ˆ, and hence there is a scalar ğœ† âˆˆ ğ… such that ğ‘‡ğ‘£ = ğœ†ğ‘£. Conversely, if ğ‘‡ğ‘£ = ğœ†ğ‘£ for some ğœ† âˆˆ ğ…, then span(ğ‘£) is a one-dimensional subspace of ğ‘‰ invariant under ğ‘‡. The equation ğ‘‡ğ‘£ = ğœ†ğ‘£, which we have just seen is intimately connected with one-dimensional invariant subspaces, is important enough that the scalars ğœ† and vectors ğ‘£ satisfying it are given special names. 5.5 definition: eigenvalue Suppose ğ‘‡ âˆˆ â„’(ğ‘‰). A number ğœ† âˆˆ ğ… is called an eigenvalue of ğ‘‡ if there exists ğ‘£ âˆˆ ğ‘‰ such that ğ‘£ â‰  0and ğ‘‡ğ‘£ = ğœ†ğ‘£. The word eigenvalue is half-German, half-English. The German prefix eigen means â€œownâ€ in the sense of charac- terizing an intrinsic property. In the definition above, we require that ğ‘£ â‰  0because every scalar ğœ† âˆˆ ğ… satisfiesğ‘‡0 = ğœ†0. The comments above show that ğ‘‰ has a one-dimensional subspace invariant under ğ‘‡ if and only if ğ‘‡ has an eigenvalue. 5.6 example:eigenvalue Define an operatorğ‘‡ âˆˆ â„’(ğ…3)by ğ‘‡(ğ‘¥, ğ‘¦, ğ‘§) = (7ğ‘¥+ 3ğ‘§, 3ğ‘¥+ 6ğ‘¦+ 9ğ‘§, âˆ’6ğ‘¦) for (ğ‘¥, ğ‘¦, ğ‘§) âˆˆ ğ…3. Then ğ‘‡(3, 1, âˆ’1) = (18, 6, âˆ’6) = 6(3, 1, âˆ’1). Thus 6is an eigenvalue of ğ‘‡. Section 5A Invariant Subspaces 135 The equivalences in the next result, along with many deep results in linear algebra, are valid only in the context of finite-dimensional vector spaces. 5.7 equivalent conditions to be an eigenvalue Suppose ğ‘‰ is finite-dimensional,ğ‘‡ âˆˆ â„’(ğ‘‰), and ğœ† âˆˆ ğ…. Then the following are equivalent. (a) ğœ† is an eigenvalue of ğ‘‡. (b) ğ‘‡ âˆ’ ğœ†ğ¼ is not injective. (c) ğ‘‡ âˆ’ ğœ†ğ¼ is not surjective. Reminder: ğ¼ âˆˆ â„’(ğ‘‰) is the identity operator. Thus ğ¼ğ‘£ = ğ‘£ for all ğ‘£ âˆˆ ğ‘‰. (d) ğ‘‡ âˆ’ ğœ†ğ¼ is not invertible. Proof Conditions (a) and (b) are equivalent because the equation ğ‘‡ğ‘£ = ğœ†ğ‘£ is equivalent to the equation (ğ‘‡ âˆ’ ğœ†ğ¼)ğ‘£ = 0. Conditions (b), (c), and (d) are equivalent by 3.65. 5.8 definition: eigenvector Suppose ğ‘‡ âˆˆ â„’(ğ‘‰) and ğœ† âˆˆ ğ… is an eigenvalue of ğ‘‡. A vector ğ‘£ âˆˆ ğ‘‰ is called an eigenvector of ğ‘‡ corresponding to ğœ† if ğ‘£ â‰  0and ğ‘‡ğ‘£ = ğœ†ğ‘£. In other words, a nonzero vector ğ‘£ âˆˆ ğ‘‰ is an eigenvector of an operator ğ‘‡ âˆˆ â„’(ğ‘‰) if and only if ğ‘‡ğ‘£ is a scalar multiple of ğ‘£. Because ğ‘‡ğ‘£ = ğœ†ğ‘£ if and only if (ğ‘‡ âˆ’ ğœ†ğ¼)ğ‘£ = 0, a vector ğ‘£ âˆˆ ğ‘‰ with ğ‘£ â‰  0is an eigenvector of ğ‘‡ corresponding to ğœ† if and only if ğ‘£ âˆˆ null(ğ‘‡ âˆ’ ğœ†ğ¼). 5.9 example: eigenvalues and eigenvectors Suppose ğ‘‡ âˆˆ â„’(ğ…2)is defined byğ‘‡(ğ‘¤, ğ‘§) = (âˆ’ğ‘§, ğ‘¤). (a) First consider the case ğ… = ğ‘. Then ğ‘‡ is a counterclockwise rotation by 90 âˆ˜ about the origin in ğ‘2. An operator has an eigenvalue if and only if there exists a nonzero vector in its domain that gets sent by the operator to a scalar multiple of itself. A 90 âˆ˜ counterclockwise rotation of a nonzero vector in ğ‘2 cannot equal a scalar multiple of itself. Conclusion: if ğ… = ğ‘, then ğ‘‡ has no eigenvalues (and thus has no eigenvectors). (b) Now consider the case ğ… = ğ‚. To find eigenvalues ofğ‘‡, we must find the scalars ğœ† such that ğ‘‡(ğ‘¤, ğ‘§) = ğœ†(ğ‘¤, ğ‘§) has some solution other than ğ‘¤ = ğ‘§ = 0. The equation ğ‘‡(ğ‘¤, ğ‘§) = ğœ†(ğ‘¤, ğ‘§) is equivalent to the simultaneous equations 5.10 âˆ’ğ‘§ = ğœ†ğ‘¤, ğ‘¤ = ğœ†ğ‘§. Substituting the value for ğ‘¤ given by the second equation into the first equation gives âˆ’ğ‘§ = ğœ†2ğ‘§. 136 Chapter 5 Eigenvalues and Eigenvectors Now ğ‘§ cannot equal 0[otherwise 5.10 implies that ğ‘¤ = 0; we are looking for solutions to 5.10 such that (ğ‘¤, ğ‘§) is not the 0vector], so the equation above leads to the equation âˆ’1 = ğœ† 2. The solutions to this equation are ğœ† = ğ‘– and ğœ† = âˆ’ğ‘–. You can verify that ğ‘– and âˆ’ğ‘– are eigenvalues of ğ‘‡. Indeed, the eigenvectors corresponding to the eigenvalue ğ‘– are the vectors of the form (ğ‘¤, âˆ’ğ‘¤ğ‘–), with ğ‘¤ âˆˆ ğ‚ and ğ‘¤ â‰  0. Furthermore, the eigenvectors corresponding to the eigenvalue âˆ’ğ‘– are the vectors of the form (ğ‘¤, ğ‘¤ğ‘–), with ğ‘¤ âˆˆ ğ‚ and ğ‘¤ â‰  0. In the next proof, we again use the equivalence ğ‘‡ğ‘£ = ğœ†ğ‘£ âŸº (ğ‘‡ âˆ’ ğœ†ğ¼)ğ‘£ = 0. 5.11 linearly independent eigenvectors Suppose ğ‘‡ âˆˆ â„’(ğ‘‰). Then every list of eigenvectors of ğ‘‡ corresponding to distinct eigenvalues of ğ‘‡ is linearly independent. Proof Suppose the desired result is false. Then there exists a smallest positive integer ğ‘š such that there exists a linearly dependent list ğ‘£1, â€¦, ğ‘£ğ‘š of eigenvectors of ğ‘‡ corresponding to distinct eigenvalues ğœ†1, â€¦, ğœ†ğ‘š of ğ‘‡ (note that ğ‘š â‰¥ 2because an eigenvector is, by definition, nonzero). Thus there existğ‘1, â€¦, ğ‘ğ‘š âˆˆ ğ…, none of which are 0(because of the minimality of ğ‘š), such that ğ‘1ğ‘£1 + â‹¯ + ğ‘ğ‘šğ‘£ğ‘š = 0. Apply ğ‘‡ âˆ’ ğœ†ğ‘šğ¼ to both sides of the equation above, getting ğ‘1(ğœ†1 âˆ’ ğœ†ğ‘š)ğ‘£1 + â‹¯ + ğ‘ğ‘š âˆ’ 1(ğœ†ğ‘š âˆ’ 1 âˆ’ ğœ†ğ‘š)ğ‘£ğ‘š âˆ’ 1 = 0. Because the eigenvalues ğœ†1, â€¦, ğœ†ğ‘š are distinct, none of the coefficients above equal 0. Thus ğ‘£1, â€¦, ğ‘£ğ‘š âˆ’ 1 is a linearly dependent list of ğ‘š âˆ’ 1eigenvectors of ğ‘‡ corresponding to distinct eigenvalues, contradicting the minimality of ğ‘š. This contradiction completes the proof. The result above leads to a short proof of the result below, which puts an upper bound on the number of distinct eigenvalues that an operator can have. 5.12 operator cannot have more eigenvalues than dimension of vector space Suppose ğ‘‰ is finite-dimensional. Then each operator onğ‘‰ has at most dim ğ‘‰ distinct eigenvalues. Proof Let ğ‘‡ âˆˆ â„’(ğ‘‰). Suppose ğœ†1, â€¦, ğœ†ğ‘š are distinct eigenvalues of ğ‘‡. Let ğ‘£1, â€¦, ğ‘£ğ‘š be corresponding eigenvectors. Then 5.11 implies that the list ğ‘£1, â€¦, ğ‘£ğ‘š is linearly independent. Thus ğ‘š â‰¤dim ğ‘‰ (see 2.22), as desired. Section 5A Invariant Subspaces 137 Polynomials Applied to Operators The main reason that a richer theory exists for operators (which map a vector space into itself) than for more general linear maps is that operators can be raised to powers. In this subsection we define that notion and the concept of applying a polynomial to an operator. This concept will be the key tool that we use in the next section when we prove that every operator on a nonzero finite-dimensional complex vector space has an eigenvalue. If ğ‘‡ is an operator, then ğ‘‡ğ‘‡ makes sense (see 3.7) and is also an operator on the same vector space as ğ‘‡. We usually write ğ‘‡2 instead of ğ‘‡ğ‘‡. More generally, we have the following definition ofğ‘‡ğ‘š. 5.13 notation: ğ‘‡ğ‘š Suppose ğ‘‡ âˆˆ â„’(ğ‘‰) and ğ‘š is a positive integer. â€¢ ğ‘‡ğ‘š âˆˆ â„’(ğ‘‰) is defined byğ‘‡ğ‘š = ğ‘‡â‹¯ğ‘‡âŸ ğ‘š times . â€¢ ğ‘‡0 is defined to be the identity operatorğ¼ on ğ‘‰. â€¢ If ğ‘‡ is invertible with inverse ğ‘‡âˆ’1, then ğ‘‡âˆ’ğ‘š âˆˆ â„’(ğ‘‰) is defined by ğ‘‡âˆ’ğ‘š = (ğ‘‡âˆ’1) ğ‘š . You should verify that if ğ‘‡ is an operator, then ğ‘‡ğ‘šğ‘‡ğ‘› = ğ‘‡ğ‘š + ğ‘› and (ğ‘‡ğ‘š) ğ‘› = ğ‘‡ğ‘šğ‘›, where ğ‘š and ğ‘› are arbitrary integers if ğ‘‡ is invertible and are nonnegative integers if ğ‘‡ is not invertible. Having defined powers of an operator, we can now define what it means to apply a polynomial to an operator. 5.14 notation: ğ‘(ğ‘‡) Suppose ğ‘‡ âˆˆ â„’(ğ‘‰) and ğ‘ âˆˆ ğ’«(ğ…) is a polynomial given by ğ‘(ğ‘§) = ğ‘0 + ğ‘1ğ‘§ + ğ‘2ğ‘§2 + â‹¯ + ğ‘ğ‘šğ‘§ğ‘š for all ğ‘§ âˆˆ ğ…. Then ğ‘(ğ‘‡) is the operator on ğ‘‰ defined by ğ‘(ğ‘‡) = ğ‘0ğ¼ + ğ‘1ğ‘‡ + ğ‘2ğ‘‡2 + â‹¯ + ğ‘ğ‘šğ‘‡ğ‘š. This is a new use of the symbol ğ‘ because we are applying ğ‘ to operators, not just elements of ğ…. The idea here is that to evaluate ğ‘(ğ‘‡), we simply replace ğ‘§ with ğ‘‡ in the expression definingğ‘. Note that the constant term ğ‘0 in ğ‘(ğ‘§) becomes the operator ğ‘0ğ¼ (which is a reasonable choice because ğ‘0 = ğ‘0ğ‘§ 0 and thus we should replace ğ‘0 with ğ‘0ğ‘‡0, which equals ğ‘0ğ¼). 138 Chapter 5 Eigenvalues and Eigenvectors 5.15 example: a polynomial applied to the differentiation operator Suppose ğ· âˆˆ â„’(ğ’«(ğ‘))is the differentiation operator defined byğ·ğ‘ = ğ‘â€² and ğ‘ is the polynomial defined byğ‘(ğ‘¥) = 7 âˆ’ 3ğ‘¥+ 5ğ‘¥ 2. Then ğ‘(ğ·) = 7ğ¼ âˆ’ 3ğ·+ 5ğ· 2. Thus (ğ‘(ğ·))ğ‘ = 7ğ‘ âˆ’ 3ğ‘ â€² + 5ğ‘ â€³ for every ğ‘ âˆˆ ğ’«(ğ‘). If we fix an operatorğ‘‡ âˆˆ â„’(ğ‘‰), then the function from ğ’«(ğ…) to â„’(ğ‘‰) given by ğ‘ â†¦ ğ‘(ğ‘‡) is linear, as you should verify. 5.16 definition:product of polynomials If ğ‘, ğ‘ âˆˆ ğ’«(ğ…), then ğ‘ğ‘ âˆˆ ğ’«(ğ…) is the polynomial defined by (ğ‘ğ‘)(ğ‘§) = ğ‘(ğ‘§)ğ‘(ğ‘§) for all ğ‘§ âˆˆ ğ…. The order does not matter in taking products of polynomials of a single operator, as shown by (b) in the next result. 5.17 multiplicative properties Suppose ğ‘, ğ‘ âˆˆ ğ’«(ğ…) and ğ‘‡ âˆˆ â„’(ğ‘‰). Then (a) (ğ‘ğ‘)(ğ‘‡) = ğ‘(ğ‘‡)ğ‘(ğ‘‡); (b) ğ‘(ğ‘‡)ğ‘(ğ‘‡) = ğ‘(ğ‘‡)ğ‘(ğ‘‡). Informal proof: When a product of polynomials is expanded using the dis- tributive property, it does not matter whether the symbol is ğ‘§ or ğ‘‡. Proof (a) Suppose ğ‘(ğ‘§) = ğ‘š âˆ‘ ğ‘— = 0 ğ‘ğ‘—ğ‘§ ğ‘— and ğ‘(ğ‘§) = ğ‘› âˆ‘ ğ‘˜ = 0 ğ‘ğ‘˜ğ‘§ğ‘˜ for all ğ‘§ âˆˆ ğ…. Then (ğ‘ğ‘)(ğ‘§) = ğ‘š âˆ‘ ğ‘— = 0 ğ‘› âˆ‘ ğ‘˜ = 0 ğ‘ğ‘—ğ‘ğ‘˜ğ‘§ ğ‘— + ğ‘˜. Thus (ğ‘ğ‘)(ğ‘‡) = ğ‘š âˆ‘ ğ‘— = 0 ğ‘› âˆ‘ ğ‘˜ = 0 ğ‘ğ‘—ğ‘ğ‘˜ğ‘‡ğ‘— + ğ‘˜ = (ğ‘š âˆ‘ ğ‘— = 0 ğ‘ğ‘—ğ‘‡ğ‘—)( ğ‘› âˆ‘ ğ‘˜ = 0 ğ‘ğ‘˜ğ‘‡ğ‘˜) = ğ‘(ğ‘‡)ğ‘(ğ‘‡). (b) Using (a) twice, we have ğ‘(ğ‘‡)ğ‘(ğ‘‡) = (ğ‘ğ‘)(ğ‘‡) = (ğ‘ğ‘)(ğ‘‡) = ğ‘(ğ‘‡)ğ‘(ğ‘‡). Section 5A Invariant Subspaces 139 We observed earlier that if ğ‘‡ âˆˆ â„’(ğ‘‰), then the subspaces null ğ‘‡ and range ğ‘‡ are invariant under ğ‘‡ (see 5.4). Now we show that the null space and the range of every polynomial of ğ‘‡ are also invariant under ğ‘‡. 5.18 null space and range of ğ‘(ğ‘‡) are invariant under ğ‘‡ Suppose ğ‘‡ âˆˆ â„’(ğ‘‰) and ğ‘ âˆˆ ğ’«(ğ…). Then null ğ‘(ğ‘‡) and range ğ‘(ğ‘‡) are invariant under ğ‘‡. Proof Suppose ğ‘¢ âˆˆ null ğ‘(ğ‘‡). Then ğ‘(ğ‘‡)ğ‘¢ = 0. Thus (ğ‘(ğ‘‡))(ğ‘‡ğ‘¢) = ğ‘‡(ğ‘(ğ‘‡)ğ‘¢)= ğ‘‡(0) = 0. Hence ğ‘‡ğ‘¢ âˆˆ null ğ‘(ğ‘‡). Thus null ğ‘(ğ‘‡) is invariant under ğ‘‡, as desired. Suppose ğ‘¢ âˆˆ range ğ‘(ğ‘‡). Then there exists ğ‘£ âˆˆ ğ‘‰ such that ğ‘¢ = ğ‘(ğ‘‡)ğ‘£. Thus ğ‘‡ğ‘¢ = ğ‘‡(ğ‘(ğ‘‡)ğ‘£)= ğ‘(ğ‘‡)(ğ‘‡ğ‘£). Hence ğ‘‡ğ‘¢ âˆˆ range ğ‘(ğ‘‡). Thus range ğ‘(ğ‘‡) is invariant under ğ‘‡, as desired. Exercises 5A 1 Suppose ğ‘‡ âˆˆ â„’(ğ‘‰) and ğ‘ˆ is a subspace of ğ‘‰. (a) Prove that if ğ‘ˆ âŠ†null ğ‘‡, then ğ‘ˆ is invariant under ğ‘‡. (b) Prove that if range ğ‘‡ âŠ† ğ‘ˆ, then ğ‘ˆ is invariant under ğ‘‡. 2 Suppose that ğ‘‡ âˆˆ â„’(ğ‘‰) and ğ‘‰1, â€¦, ğ‘‰ğ‘š are subspaces of ğ‘‰ invariant under ğ‘‡. Prove that ğ‘‰1 + â‹¯ + ğ‘‰ğ‘š is invariant under ğ‘‡. 3 Suppose ğ‘‡ âˆˆ â„’(ğ‘‰). Prove that the intersection of every collection of subspaces of ğ‘‰ invariant under ğ‘‡ is invariant under ğ‘‡. 4 Prove or give a counterexample: If ğ‘‰ is finite-dimensional andğ‘ˆ is a sub- space of ğ‘‰ that is invariant under every operator on ğ‘‰, then ğ‘ˆ = {0}or ğ‘ˆ = ğ‘‰. 5 Suppose ğ‘‡ âˆˆ â„’(ğ‘2)is defined byğ‘‡(ğ‘¥, ğ‘¦) = (âˆ’3ğ‘¦, ğ‘¥). Find the eigenvalues of ğ‘‡. 6 Defineğ‘‡ âˆˆ â„’(ğ…2)by ğ‘‡(ğ‘¤, ğ‘§) = (ğ‘§, ğ‘¤). Find all eigenvalues and eigenvec- tors of ğ‘‡. 7 Defineğ‘‡ âˆˆ â„’(ğ…3)by ğ‘‡(ğ‘§1, ğ‘§2, ğ‘§3) = (2ğ‘§2, 0, 5ğ‘§3). Find all eigenvalues and eigenvectors of ğ‘‡. 8 Suppose ğ‘ƒ âˆˆ â„’(ğ‘‰) is such that ğ‘ƒ2 = ğ‘ƒ. Prove that if ğœ† is an eigenvalue of ğ‘ƒ, then ğœ† = 0or ğœ† = 1. 140 Chapter 5 Eigenvalues and Eigenvectors 9 Defineğ‘‡âˆ¶ ğ’«(ğ‘) â†’ ğ’«(ğ‘) by ğ‘‡ğ‘ = ğ‘â€². Find all eigenvalues and eigenvectors of ğ‘‡. 10 Defineğ‘‡ âˆˆ â„’(ğ’«4(ğ‘))by (ğ‘‡ğ‘)(ğ‘¥) = ğ‘¥ğ‘â€²(ğ‘¥) for all ğ‘¥ âˆˆ ğ‘. Find all eigenval- ues and eigenvectors of ğ‘‡. 11 Suppose ğ‘‰ is finite-dimensional,ğ‘‡ âˆˆ â„’(ğ‘‰), and ğ›¼ âˆˆ ğ…. Prove that there ex- ists ğ›¿ > 0such that ğ‘‡âˆ’ ğœ†ğ¼ is invertible for all ğœ† âˆˆ ğ… such that 0 < |ğ›¼ âˆ’ ğœ†| < ğ›¿. 12 Suppose ğ‘‰ = ğ‘ˆ âŠ• ğ‘Š, where ğ‘ˆ and ğ‘Š are nonzero subspaces of ğ‘‰. Define ğ‘ƒ âˆˆ â„’(ğ‘‰) by ğ‘ƒ(ğ‘¢ + ğ‘¤) = ğ‘¢ for each ğ‘¢ âˆˆ ğ‘ˆ and each ğ‘¤ âˆˆ ğ‘Š. Find all eigenvalues and eigenvectors of ğ‘ƒ. 13 Suppose ğ‘‡ âˆˆ â„’(ğ‘‰). Suppose ğ‘† âˆˆ â„’(ğ‘‰) is invertible. (a) Prove that ğ‘‡ and ğ‘† âˆ’1ğ‘‡ğ‘† have the same eigenvalues. (b) What is the relationship between the eigenvectors of ğ‘‡ and the eigenvec- tors of ğ‘†âˆ’1ğ‘‡ğ‘†? 14 Give an example of an operator on ğ‘4 that has no (real) eigenvalues. 15 Suppose ğ‘‰ is finite-dimensional,ğ‘‡ âˆˆ â„’(ğ‘‰), and ğœ† âˆˆ ğ…. Show that ğœ† is an eigenvalue of ğ‘‡ if and only if ğœ† is an eigenvalue of the dual operator ğ‘‡â€² âˆˆ â„’(ğ‘‰â€²). 16 Suppose ğ‘£1, â€¦, ğ‘£ğ‘› is a basis of ğ‘‰ and ğ‘‡ âˆˆ â„’(ğ‘‰). Prove that if ğœ† is an eigenvalue of ğ‘‡, then |ğœ†| â‰¤ ğ‘›max{âˆ£â„³(ğ‘‡)ğ‘—, ğ‘˜âˆ£ âˆ¶ 1 â‰¤ ğ‘—, ğ‘˜ â‰¤ ğ‘›}, where â„³(ğ‘‡)ğ‘—, ğ‘˜ denotes the entry in row ğ‘—, column ğ‘˜ of the matrix of ğ‘‡ with respect to the basis ğ‘£1, â€¦, ğ‘£ğ‘›. See Exercise 19 in Section 6A for a different bound on |ğœ†|. 17 Suppose ğ… = ğ‘, ğ‘‡ âˆˆ â„’(ğ‘‰), and ğœ† âˆˆ ğ‘. Prove that ğœ† is an eigenvalue of ğ‘‡ if and only if ğœ† is an eigenvalue of the complexificationğ‘‡ğ‚. See Exercise 33 in Section 3B for the definition of ğ‘‡ğ‚. 18 Suppose ğ… = ğ‘, ğ‘‡ âˆˆ â„’(ğ‘‰), and ğœ† âˆˆ ğ‚. Prove that ğœ† is an eigenvalue of the complexificationğ‘‡ğ‚ if and only if ğœ† is an eigenvalue of ğ‘‡ğ‚. 19 Show that the forward shift operator ğ‘‡ âˆˆ â„’(ğ…âˆ)defined by ğ‘‡(ğ‘§1, ğ‘§2, â€¦ ) = (0, ğ‘§1, ğ‘§2, â€¦ ) has no eigenvalues. 20 Define the backward shift operatorğ‘† âˆˆ â„’(ğ…âˆ)by ğ‘†(ğ‘§1, ğ‘§2, ğ‘§3, â€¦ ) = (ğ‘§2, ğ‘§3, â€¦ ). (a) Show that every element of ğ… is an eigenvalue of ğ‘†. (b) Find all eigenvectors of ğ‘†. Section 5A Invariant Subspaces â‰ˆ 100âˆš2 21 Suppose ğ‘‡ âˆˆ â„’(ğ‘‰) is invertible. (a) Suppose ğœ† âˆˆ ğ… with ğœ† â‰  0. Prove that ğœ† is an eigenvalue of ğ‘‡ if and only if 1 ğœ† is an eigenvalue of ğ‘‡âˆ’1. (b) Prove that ğ‘‡ and ğ‘‡âˆ’1 have the same eigenvectors. 22 Suppose ğ‘‡ âˆˆ â„’(ğ‘‰) and there exist nonzero vectors ğ‘¢ and ğ‘¤ in ğ‘‰ such that ğ‘‡ğ‘¢ = 3ğ‘¤ and ğ‘‡ğ‘¤ = 3ğ‘¢. Prove that 3or âˆ’3is an eigenvalue of ğ‘‡. 23 Suppose ğ‘‰ is finite-dimensional andğ‘†, ğ‘‡ âˆˆ â„’(ğ‘‰). Prove that ğ‘†ğ‘‡ and ğ‘‡ğ‘† have the same eigenvalues. 24 Suppose ğ´ is an ğ‘›-by-ğ‘› matrix with entries in ğ…. Defineğ‘‡ âˆˆ â„’(ğ…ğ‘›)by ğ‘‡ğ‘¥ = ğ´ğ‘¥, where elements of ğ…ğ‘› are thought of as ğ‘›-by-1column vectors. (a) Suppose the sum of the entries in each row of ğ´ equals 1. Prove that 1 is an eigenvalue of ğ‘‡. (b) Suppose the sum of the entries in each column of ğ´ equals 1. Prove that 1is an eigenvalue of ğ‘‡. 25 Suppose ğ‘‡ âˆˆ â„’(ğ‘‰) and ğ‘¢, ğ‘¤ are eigenvectors of ğ‘‡ such that ğ‘¢ + ğ‘¤ is also an eigenvector of ğ‘‡. Prove that ğ‘¢ and ğ‘¤ are eigenvectors of ğ‘‡ corresponding to the same eigenvalue. 26 Suppose ğ‘‡ âˆˆ â„’(ğ‘‰) is such that every nonzero vector in ğ‘‰ is an eigenvector of ğ‘‡. Prove that ğ‘‡ is a scalar multiple of the identity operator. 27 Suppose that ğ‘‰ is finite-dimensional andğ‘˜ âˆˆ {1, â€¦, dim ğ‘‰ âˆ’ 1}. Suppose ğ‘‡ âˆˆ â„’(ğ‘‰) is such that every subspace of ğ‘‰ of dimension ğ‘˜ is invariant under ğ‘‡. Prove that ğ‘‡ is a scalar multiple of the identity operator. 28 Suppose ğ‘‰ is finite-dimensional andğ‘‡ âˆˆ â„’(ğ‘‰). Prove that ğ‘‡ has at most 1+ dim range ğ‘‡ distinct eigenvalues. 29 Suppose ğ‘‡ âˆˆ â„’(ğ‘3)and âˆ’4, 5, and âˆš7are eigenvalues of ğ‘‡. Prove that there exists ğ‘¥ âˆˆ ğ‘3 such that ğ‘‡ğ‘¥ âˆ’ 9ğ‘¥ =(âˆ’4, 5, âˆš7). 30 Suppose ğ‘‡ âˆˆ â„’(ğ‘‰) and (ğ‘‡ âˆ’ 2ğ¼)(ğ‘‡ âˆ’ 3ğ¼)(ğ‘‡ âˆ’ 4ğ¼) = 0. Suppose ğœ† is an eigenvalue of ğ‘‡. Prove that ğœ† = 2or ğœ† = 3or ğœ† = 4. 31 Give an example of ğ‘‡ âˆˆ â„’(ğ‘2)such that ğ‘‡4 = âˆ’ğ¼. 32 Suppose ğ‘‡ âˆˆ â„’(ğ‘‰) has no eigenvalues and ğ‘‡4 = ğ¼. Prove that ğ‘‡2 = âˆ’ğ¼. 33 Suppose ğ‘‡ âˆˆ â„’(ğ‘‰) and ğ‘š is a positive integer. (a) Prove that ğ‘‡ is injective if and only if ğ‘‡ğ‘š is injective. (b) Prove that ğ‘‡ is surjective if and only if ğ‘‡ğ‘š is surjective. 142 Chapter 5 Eigenvalues and Eigenvectors 34 Suppose ğ‘‰ is finite-dimensional andğ‘£1, â€¦, ğ‘£ğ‘š âˆˆ ğ‘‰. Prove that the list ğ‘£1, â€¦, ğ‘£ğ‘š is linearly independent if and only if there exists ğ‘‡ âˆˆ â„’(ğ‘‰) such that ğ‘£1, â€¦, ğ‘£ğ‘š are eigenvectors of ğ‘‡ corresponding to distinct eigenvalues. 35 Suppose that ğœ†1, â€¦, ğœ†ğ‘› is a list of distinct real numbers. Prove that the list ğ‘’ ğœ†1ğ‘¥, â€¦, ğ‘’ ğœ†ğ‘›ğ‘¥ is linearly independent in the vector space of real-valued functions on ğ‘. Hint: Let ğ‘‰ = span(ğ‘’ ğœ†1ğ‘¥, â€¦, ğ‘’ ğœ†ğ‘›ğ‘¥), and define an operator ğ· âˆˆ â„’(ğ‘‰) by ğ· ğ‘“ = ğ‘“ â€². Find eigenvalues and eigenvectors of ğ·. 36 Suppose that ğœ†1, â€¦, ğœ†ğ‘› is a list of distinct positive numbers. Prove that the list cos(ğœ†1ğ‘¥), â€¦, cos(ğœ†ğ‘›ğ‘¥) is linearly independent in the vector space of real-valued functions on ğ‘. 37 Suppose ğ‘‰ is finite-dimensional andğ‘‡ âˆˆ â„’(ğ‘‰). Defineğ’œ âˆˆ â„’(â„’(ğ‘‰))by ğ’œ(ğ‘†) = ğ‘‡ğ‘† for each ğ‘† âˆˆ â„’(ğ‘‰). Prove that the set of eigenvalues of ğ‘‡ equals the set of eigenvalues of ğ’œ. 38 Suppose ğ‘‰ is finite-dimensional,ğ‘‡ âˆˆ â„’(ğ‘‰), and ğ‘ˆ is a subspace of ğ‘‰ invariant under ğ‘‡. The quotient operator ğ‘‡/ğ‘ˆ âˆˆ â„’(ğ‘‰/ğ‘ˆ) is defined by (ğ‘‡/ğ‘ˆ)(ğ‘£ + ğ‘ˆ) = ğ‘‡ğ‘£ + ğ‘ˆ for each ğ‘£ âˆˆ ğ‘‰. (a) Show that the definition ofğ‘‡/ğ‘ˆ makes sense (which requires using the condition that ğ‘ˆ is invariant under ğ‘‡) and show that ğ‘‡/ğ‘ˆ is an operator on ğ‘‰/ğ‘ˆ. (b) Show that each eigenvalue of ğ‘‡/ğ‘ˆ is an eigenvalue of ğ‘‡. 39 Suppose ğ‘‰ is finite-dimensional andğ‘‡ âˆˆ â„’(ğ‘‰). Prove that ğ‘‡ has an eigen- value if and only if there exists a subspace of ğ‘‰ of dimension dim ğ‘‰ âˆ’ 1that is invariant under ğ‘‡. 40 Suppose ğ‘†, ğ‘‡ âˆˆ â„’(ğ‘‰) and ğ‘† is invertible. Suppose ğ‘ âˆˆ ğ’«(ğ…) is a polynomial. Prove that ğ‘(ğ‘†ğ‘‡ğ‘† âˆ’1)= ğ‘†ğ‘(ğ‘‡)ğ‘†âˆ’1. 41 Suppose ğ‘‡ âˆˆ â„’(ğ‘‰) and ğ‘ˆ is a subspace of ğ‘‰ invariant under ğ‘‡. Prove that ğ‘ˆ is invariant under ğ‘(ğ‘‡) for every polynomial ğ‘ âˆˆ ğ’«(ğ…). 42 Defineğ‘‡ âˆˆ â„’(ğ…ğ‘›)by ğ‘‡(ğ‘¥1, ğ‘¥2, ğ‘¥3, â€¦, ğ‘¥ğ‘›) = (ğ‘¥1, 2ğ‘¥2, 3ğ‘¥3, â€¦, ğ‘›ğ‘¥ğ‘›). (a) Find all eigenvalues and eigenvectors of ğ‘‡. (b) Find all subspaces of ğ…ğ‘› that are invariant under ğ‘‡. 43 Suppose that ğ‘‰ is finite-dimensional,dim ğ‘‰ > 1, and ğ‘‡ âˆˆ â„’(ğ‘‰). Prove that {ğ‘(ğ‘‡) âˆ¶ ğ‘ âˆˆ ğ’«(ğ…)}â‰  â„’(ğ‘‰). Section 5B The Minimal Polynomial 143 5B The Minimal Polynomial Existence of Eigenvalues on Complex Vector Spaces Now we come to one of the central results about operators on finite-dimensional complex vector spaces. 5.19 existence of eigenvalues Every operator on a finite-dimensional nonzero complex vector space has an eigenvalue. Proof Suppose ğ‘‰ is a finite-dimensional complex vector space of dimension ğ‘› > 0and ğ‘‡ âˆˆ â„’(ğ‘‰). Choose ğ‘£ âˆˆ ğ‘‰ with ğ‘£ â‰  0. Then ğ‘£, ğ‘‡ğ‘£, ğ‘‡2ğ‘£, â€¦, ğ‘‡ğ‘›ğ‘£ is not linearly independent, because ğ‘‰ has dimension ğ‘› and this list has length ğ‘› + 1. Hence some linear combination (with not all the coefficients equal to 0) of the vectors above equals 0. Thus there exists a nonconstant polynomial ğ‘ of smallest degree such that ğ‘(ğ‘‡)ğ‘£ = 0. By the first version of the fundamental theorem of algebra (see4.12), there exists ğœ† âˆˆ ğ‚ such that ğ‘(ğœ†) = 0. Hence there exists a polynomial ğ‘ âˆˆ ğ’«(ğ‚) such that ğ‘(ğ‘§) = (ğ‘§ âˆ’ ğœ†)ğ‘(ğ‘§) for every ğ‘§ âˆˆ ğ‚ (see 4.6). This implies (using 5.17) that 0 = ğ‘(ğ‘‡)ğ‘£ = (ğ‘‡ âˆ’ ğœ†ğ¼)(ğ‘(ğ‘‡)ğ‘£). Because ğ‘ has smaller degree than ğ‘, we know that ğ‘(ğ‘‡)ğ‘£ â‰  0. Thus the equation above implies that ğœ† is an eigenvalue of ğ‘‡ with eigenvector ğ‘(ğ‘‡)ğ‘£. The proof above makes crucial use of the fundamental theorem of algebra. The comment following Exercise 16 helps explain why the fundamental theorem of algebra is so tightly connected to the result above. The hypothesis in the result above that ğ… = ğ‚ cannot be replaced with the hypothesis that ğ… = ğ‘, as shown by Example 5.9. The next example shows that the finite-dimensional hypothesis in the result above also cannot be deleted. 5.20 example: an operator on a complex vector space with no eigenvalues Defineğ‘‡ âˆˆ â„’(ğ’«(ğ‚))by (ğ‘‡ğ‘)(ğ‘§) = ğ‘§ğ‘(ğ‘§). If ğ‘ âˆˆ ğ’«(ğ‚) is a nonzero poly- nomial, then the degree of ğ‘‡ğ‘ is one more than the degree of ğ‘, and thus ğ‘‡ğ‘ cannot equal a scalar multiple of ğ‘. Hence ğ‘‡ has no eigenvalues. Because ğ’«(ğ‚) is infinite-dimensional, this example does not contradict the result above. 144 Chapter 5 Eigenvalues and Eigenvectors Eigenvalues and the Minimal Polynomial In this subsection we introduce an important polynomial associated with each operator. We begin with the following definition. 5.21 definition: monic polynomial A monic polynomial is a polynomial whose highest-degree coefficient equals 1. For example, the polynomial 2+ 9ğ‘§ 2 + ğ‘§7 is a monic polynomial of degree 7. 5.22 existence, uniqueness, and degree of minimal polynomial Suppose ğ‘‰ is finite-dimensional andğ‘‡ âˆˆ â„’(ğ‘‰). Then there is a unique monic polynomial ğ‘ âˆˆ ğ’«(ğ…) of smallest degree such that ğ‘(ğ‘‡) = 0. Furthermore, deg ğ‘ â‰¤dim ğ‘‰. Proof If dim ğ‘‰ = 0, then ğ¼ is the zero operator on ğ‘‰ and thus we take ğ‘ to be the constant polynomial 1. Now use induction on dim ğ‘‰. Thus assume that dim ğ‘‰ > 0and that the desired result is true for all operators on all complex vector spaces of smaller dimension. Let ğ‘£ âˆˆ ğ‘‰ be such that ğ‘£ â‰  0. The list ğ‘£, ğ‘‡ğ‘£, â€¦, ğ‘‡dim ğ‘‰ğ‘£ has length 1+ dim ğ‘‰ and thus is linearly dependent. By the linear dependence lemma (2.19), there is a smallest positive integer ğ‘š â‰¤dim ğ‘‰ such that ğ‘‡ğ‘šğ‘£ is a linear combination of ğ‘£, ğ‘‡ğ‘£, â€¦, ğ‘‡ğ‘š âˆ’ 1ğ‘£. Thus there exist scalars ğ‘0, ğ‘1, ğ‘2, â€¦, ğ‘ğ‘š âˆ’ 1 âˆˆ ğ… such that 5.23 ğ‘0ğ‘£ + ğ‘1ğ‘‡ğ‘£ + â‹¯ + ğ‘ğ‘š âˆ’ 1ğ‘‡ğ‘š âˆ’ 1ğ‘£ + ğ‘‡ğ‘šğ‘£ = 0. Define a monic polynomialğ‘ âˆˆ ğ’«ğ‘š(ğ…) by ğ‘(ğ‘§) = ğ‘0 + ğ‘1ğ‘§ + â‹¯ + ğ‘ğ‘š âˆ’ 1ğ‘§ ğ‘š âˆ’ 1 + ğ‘§ ğ‘š. Then 5.23 implies that ğ‘(ğ‘‡)ğ‘£ = 0. If ğ‘˜ is a nonnegative integer, then ğ‘(ğ‘‡)(ğ‘‡ğ‘˜ğ‘£)= ğ‘‡ğ‘˜(ğ‘(ğ‘‡)ğ‘£)= ğ‘‡ğ‘˜(0) = 0. The linear dependence lemma (2.19) shows that ğ‘£, ğ‘‡ğ‘£, â€¦, ğ‘‡ğ‘š âˆ’ 1ğ‘£ is linearly inde- pendent. Thus the equation above implies that dim null ğ‘(ğ‘‡) â‰¥ ğ‘š. Hence dim range ğ‘(ğ‘‡) = dim ğ‘‰ âˆ’ dim null ğ‘(ğ‘‡) â‰¤dim ğ‘‰ âˆ’ ğ‘š. Because range ğ‘(ğ‘‡) is invariant under ğ‘‡ (by 5.18), we can apply our induction hypothesis to the operator ğ‘‡|range ğ‘(ğ‘‡)on the vector space range ğ‘(ğ‘‡). Thus there is a monic polynomial ğ‘  âˆˆ ğ’«(ğ…) with deg ğ‘  â‰¤dim ğ‘‰ âˆ’ ğ‘š and ğ‘ (ğ‘‡|range ğ‘(ğ‘‡)) = 0. Hence for all ğ‘£ âˆˆ ğ‘‰ we have (ğ‘ ğ‘)(ğ‘‡)(ğ‘£) = ğ‘ (ğ‘‡)(ğ‘(ğ‘‡)ğ‘£)= 0 because ğ‘(ğ‘‡)ğ‘£ âˆˆ range ğ‘(ğ‘‡) and ğ‘ (ğ‘‡)|range ğ‘(ğ‘‡)= ğ‘ (ğ‘‡|range ğ‘(ğ‘‡)) = 0. Thus ğ‘ ğ‘ is a monic polynomial such that deg ğ‘ ğ‘ â‰¤dim ğ‘‰ and (ğ‘ ğ‘)(ğ‘‡) = 0. Section 5B The Minimal Polynomial 145 The paragraph above shows that there is a monic polynomial of degree at most dim ğ‘‰ that when applied to ğ‘‡ gives the 0operator. Thus there is a monic polynomial of smallest degree with this property, completing the existence part of this result. Let ğ‘ âˆˆ ğ’«(ğ…) be a monic polynomial of smallest degree such that ğ‘(ğ‘‡) = 0. To prove the uniqueness part of the result, suppose ğ‘Ÿ âˆˆ ğ’«(ğ…) is a monic poly- nomial of the same degree as ğ‘ and ğ‘Ÿ(ğ‘‡) = 0. Then (ğ‘ âˆ’ ğ‘Ÿ)(ğ‘‡) = 0and also deg(ğ‘ âˆ’ ğ‘Ÿ) < deg ğ‘. If ğ‘ âˆ’ ğ‘Ÿ were not equal to 0, then we could divide ğ‘ âˆ’ ğ‘Ÿ by the coefficient of the highest-order term in ğ‘ âˆ’ ğ‘Ÿ to get a monic polynomial (of smaller degree than ğ‘) that when applied to ğ‘‡ gives the 0operator. Thus ğ‘ âˆ’ ğ‘Ÿ = 0, as desired. The previous result justifies the following definition. 5.24 definition: minimal polynomial Suppose ğ‘‰ is finite-dimensional andğ‘‡ âˆˆ â„’(ğ‘‰). Then the minimal polynomial of ğ‘‡ is the unique monic polynomial ğ‘ âˆˆ ğ’«(ğ…) of smallest degree such that ğ‘(ğ‘‡) = 0. To compute the minimal polynomial of an operator ğ‘‡ âˆˆ â„’(ğ‘‰), we need to find the smallest positive integerğ‘š such that the equation ğ‘0ğ¼ + ğ‘1ğ‘‡ + â‹¯ + ğ‘ğ‘š âˆ’ 1ğ‘‡ğ‘š âˆ’ 1 = âˆ’ğ‘‡ğ‘š has a solution ğ‘0, ğ‘1, â€¦, ğ‘ğ‘š âˆ’ 1 âˆˆ ğ…. If we pick a basis of ğ‘‰ and replace ğ‘‡ in the equation above with the matrix of ğ‘‡, then the equation above can be thought of as a system of (dim ğ‘‰) 2 linear equations in the ğ‘š unknowns ğ‘0, ğ‘1, â€¦, ğ‘ğ‘š âˆ’ 1 âˆˆ ğ…. Gaussian elimination or another fast method of solving systems of linear equations can tell us whether a solution exists, testing successive values ğ‘š = 1, 2, â€¦ until a solution exists. By 5.22, a solution exists for some smallest positive integer ğ‘š â‰¤dim ğ‘‰. The minimal polynomial of ğ‘‡ is then ğ‘0 + ğ‘1ğ‘§ + â‹¯ + ğ‘ğ‘š âˆ’ 1ğ‘§ ğ‘š âˆ’ 1 + ğ‘§ğ‘š. Even faster (usually), pick ğ‘£ âˆˆ ğ‘‰ and consider the equation 5.25 ğ‘0ğ‘£ + ğ‘1ğ‘‡ğ‘£ + â‹¯ + ğ‘dim ğ‘‰ âˆ’ 1ğ‘‡dim ğ‘‰ âˆ’ 1ğ‘£ = âˆ’ğ‘‡dim ğ‘‰ğ‘£. Use a basis of ğ‘‰ to convert the equation above to a system of dim ğ‘‰ linear equa- tions in dim ğ‘‰ unknowns ğ‘0, ğ‘1, â€¦, ğ‘dim ğ‘‰ âˆ’ 1. If this system of equations has a unique solution ğ‘0, ğ‘1, â€¦, ğ‘dim ğ‘‰ âˆ’ 1 (as happens most of the time), then the scalars ğ‘0, ğ‘1, â€¦, ğ‘dim ğ‘‰ âˆ’ 1, 1are the coefficients of the minimal polynomial of ğ‘‡ (because 5.22 states that the degree of the minimal polynomial is at most dim ğ‘‰). These estimates are based on testing millions of random matrices. Consider operators on ğ‘4 (thought of as 4-by-4matrices with respect to the standard basis), and take ğ‘£ = (1, 0, 0, 0) in the paragraph above. The faster method described above works on over 99.8% of the 4-by-4matrices with integer entries in the interval [âˆ’10, 10]and on over 99.999% of the 4-by-4matrices with integer entries in [âˆ’100, 100]. 146 Chapter 5 Eigenvalues and Eigenvectors The next example illustrates the faster procedure discussed above. 5.26 example:minimal polynomial of an operator on ğ…5 Suppose ğ‘‡ âˆˆ â„’(ğ…5)and â„³(ğ‘‡) = â›âœâœâœâœâœâœâœâœâœâœ â 0 0 0 0 âˆ’3 1 0 0 0 6 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 ââŸâŸâŸâŸâŸâŸâŸâŸâŸâŸ â  with respect to the standard basis ğ‘’1, ğ‘’2, ğ‘’3, ğ‘’4, ğ‘’5. Taking ğ‘£ = ğ‘’1 for 5.25, we have ğ‘‡ğ‘’1 = ğ‘’2, ğ‘‡4ğ‘’1 = ğ‘‡(ğ‘‡3ğ‘’1)= ğ‘‡ğ‘’4 = ğ‘’5, ğ‘‡2ğ‘’1 = ğ‘‡(ğ‘‡ğ‘’1) = ğ‘‡ğ‘’2 = ğ‘’3, ğ‘‡5ğ‘’1 = ğ‘‡(ğ‘‡4ğ‘’1)= ğ‘‡ğ‘’5 = âˆ’3ğ‘’1 + 6ğ‘’2. ğ‘‡3ğ‘’1 = ğ‘‡(ğ‘‡2ğ‘’1)= ğ‘‡ğ‘’3 = ğ‘’4, Thus 3ğ‘’1 âˆ’ 6ğ‘‡ğ‘’1 = âˆ’ğ‘‡5ğ‘’1. The list ğ‘’1, ğ‘‡ğ‘’1, ğ‘‡2ğ‘’1, ğ‘‡3ğ‘’1, ğ‘‡4ğ‘’1, which equals the list ğ‘’1, ğ‘’2, ğ‘’3, ğ‘’4, ğ‘’5, is linearly independent, so no other linear combination of this list equals âˆ’ğ‘‡5ğ‘’1. Hence the minimal polynomial of ğ‘‡ is 3 âˆ’ 6ğ‘§+ ğ‘§5. Recall that by definition, eigenvalues of operators onğ‘‰ and zeros of polyno- mials in ğ’«(ğ…) must be elements of ğ…. In particular, if ğ… = ğ‘, then eigenvalues and zeros must be real numbers. 5.27 eigenvalues are the zeros of the minimal polynomial Suppose ğ‘‰ is finite-dimensional andğ‘‡ âˆˆ â„’(ğ‘‰). (a) The zeros of the minimal polynomial of ğ‘‡ are the eigenvalues of ğ‘‡. (b) If ğ‘‰ is a complex vector space, then the minimal polynomial of ğ‘‡ has the form (ğ‘§ âˆ’ ğœ†1)â‹¯(ğ‘§ âˆ’ ğœ†ğ‘š), where ğœ†1, â€¦, ğœ†ğ‘š is a list of all eigenvalues of ğ‘‡, possibly with repetitions. Proof Let ğ‘ be the minimal polynomial of ğ‘‡. (a) First suppose ğœ† âˆˆ ğ… is a zero of ğ‘. Then ğ‘ can be written in the form ğ‘(ğ‘§) = (ğ‘§ âˆ’ ğœ†)ğ‘(ğ‘§), where ğ‘ is a monic polynomial with coefficients in ğ… (see 4.6). Because ğ‘(ğ‘‡) = 0, we have 0 = (ğ‘‡ âˆ’ ğœ†ğ¼)(ğ‘(ğ‘‡)ğ‘£) for all ğ‘£ âˆˆ ğ‘‰. Because the degree of ğ‘ is less than the degree of the minimal polynomial ğ‘, there exists at least one vector ğ‘£ âˆˆ ğ‘‰ such that ğ‘(ğ‘‡)ğ‘£ â‰  0. The equation above thus implies that ğœ† is an eigenvalue of ğ‘‡, as desired. Section 5B The Minimal Polynomial 147 To prove that every eigenvalue of ğ‘‡ is a zero of ğ‘, now suppose ğœ† âˆˆ ğ… is an eigenvalue of ğ‘‡. Thus there exists ğ‘£ âˆˆ ğ‘‰ with ğ‘£ â‰  0such that ğ‘‡ğ‘£ = ğœ†ğ‘£. Repeated applications of ğ‘‡ to both sides of this equation show that ğ‘‡ğ‘˜ğ‘£ = ğœ†ğ‘˜ğ‘£ for every nonnegative integer ğ‘˜. Thus ğ‘(ğ‘‡)ğ‘£ = ğ‘(ğœ†)ğ‘£. Because ğ‘ is the minimal polynomial of ğ‘‡, we have ğ‘(ğ‘‡)ğ‘£ = 0. Hence the equation above implies that ğ‘(ğœ†) = 0. Thus ğœ† is a zero of ğ‘, as desired. (b) To get the desired result, use (a) and the second version of the fundamental theorem of algebra (see 4.13). A nonzero polynomial has at most as many distinct zeros as its degree (see 4.8). Thus (a) of the previous result, along with the result that the minimal polynomial of an operator on ğ‘‰ has degree at most dim ğ‘‰, gives an alternative proof of 5.12, which states that an operator on ğ‘‰ has at most dim ğ‘‰ distinct eigenvalues. Every monic polynomial is the minimal polynomial of some operator, as shown by Exercise 16, which generalizes Example 5.26. Thus 5.27(a) shows that finding exact expressions for the eigenvalues of an operator is equivalent to the problem of finding exact expressions for the zeros of a polynomial (and thus is not possible for some operators). 5.28 example: An operator whose eigenvalues cannot be found exactly Let ğ‘‡ âˆˆ â„’(ğ‚ 5)be the operator defined by ğ‘‡(ğ‘§1, ğ‘§2, ğ‘§3, ğ‘§4, ğ‘§5) = (âˆ’3ğ‘§5, ğ‘§1 + 6ğ‘§5, ğ‘§2, ğ‘§3, ğ‘§4). The matrix of ğ‘‡ with respect to the standard basis of ğ‚ 5 is the 5-by-5matrix in Example 5.26. As we showed in that example, the minimal polynomial of ğ‘‡ is the polynomial 3 âˆ’ 6ğ‘§+ ğ‘§ 5. No zero of the polynomial above can be expressed using rational numbers, roots of rational numbers, and the usual rules of arithmetic (a proof of this would take us considerably beyond linear algebra). Because the zeros of the polynomial above are the eigenvalues of ğ‘‡ [by 5.27(a)], we cannot find an exact expression for any eigenvalue of ğ‘‡ in any familiar form. Numeric techniques, which we will not discuss here, show that the zeros of the polynomial above, and thus the eigenvalues of ğ‘‡, are approximately the following five complex numbers: âˆ’1.67, 0.51, 1.40, âˆ’0.12+ 1.59ğ‘–, âˆ’0.12 âˆ’ 1.59ğ‘–. Note that the two nonreal zeros of this polynomial are complex conjugates of each other, as we expect for a polynomial with real coefficients (see 4.14). 148 Chapter 5 Eigenvalues and Eigenvectors The next result completely characterizes the polynomials that when applied to an operator give the 0operator. 5.29 ğ‘(ğ‘‡) = 0 âŸº ğ‘is a polynomial multiple of the minimal polynomial Suppose ğ‘‰ is finite-dimensional,ğ‘‡ âˆˆ â„’(ğ‘‰), and ğ‘ âˆˆ ğ’«(ğ…). Then ğ‘(ğ‘‡) = 0 if and only if ğ‘ is a polynomial multiple of the minimal polynomial of ğ‘‡. Proof Let ğ‘ denote the minimal polynomial of ğ‘‡. First suppose ğ‘(ğ‘‡) = 0. By the division algorithm for polynomials (4.9), there exist polynomials ğ‘ , ğ‘Ÿ âˆˆ ğ’«(ğ…) such that 5.30 ğ‘ = ğ‘ğ‘  + ğ‘Ÿ and deg ğ‘Ÿ < deg ğ‘. We have 0 = ğ‘(ğ‘‡) = ğ‘(ğ‘‡)ğ‘ (ğ‘‡)+ ğ‘Ÿ(ğ‘‡) = ğ‘Ÿ(ğ‘‡). The equation above implies that ğ‘Ÿ = 0(otherwise, dividing ğ‘Ÿ by its highest-degree coefficient would produce a monic polynomial that when applied to ğ‘‡ gives 0; this polynomial would have a smaller degree than the minimal polynomial, which would be a contradiction). Thus 5.30 becomes the equation ğ‘ = ğ‘ğ‘ . Hence ğ‘ is a polynomial multiple of ğ‘, as desired. To prove the other direction, now suppose ğ‘ is a polynomial multiple of ğ‘. Thus there exists a polynomial ğ‘  âˆˆ ğ’«(ğ…) such that ğ‘ = ğ‘ğ‘ . We have ğ‘(ğ‘‡) = ğ‘(ğ‘‡)ğ‘ (ğ‘‡) = 0 ğ‘ (ğ‘‡) = 0, as desired. The next result is a nice consequence of the result above. 5.31 minimal polynomial of a restriction operator Suppose ğ‘‰ is finite-dimensional,ğ‘‡ âˆˆ â„’(ğ‘‰), and ğ‘ˆ is a subspace of ğ‘‰ that is invariant under ğ‘‡. Then the minimal polynomial of ğ‘‡ is a polynomial multiple of the minimal polynomial of ğ‘‡|ğ‘ˆ. Proof Suppose ğ‘ is the minimal polynomial of ğ‘‡. Thus ğ‘(ğ‘‡)ğ‘£ = 0for all ğ‘£ âˆˆ ğ‘‰. In particular, ğ‘(ğ‘‡)ğ‘¢ = 0for all ğ‘¢ âˆˆ ğ‘ˆ. Thus ğ‘(ğ‘‡|ğ‘ˆ) = 0. Now 5.29, applied to the operator ğ‘‡|ğ‘ˆ in place of ğ‘‡, implies that ğ‘ is a polynomial multiple of the minimal polynomial of ğ‘‡|ğ‘ˆ. See Exercise 25 for a result about quotient operators that is analogous to the result above. The next result shows that the constant term of the minimal polynomial of an operator determines whether the operator is invertible. Section 5B The Minimal Polynomial 149 5.32 ğ‘‡ not invertible âŸº constant term of minimal polynomial of ğ‘‡ is 0 Suppose ğ‘‰ is finite-dimensional andğ‘‡ âˆˆ â„’(ğ‘‰). Then ğ‘‡ is not invertible if and only if the constant term of the minimal polynomial of ğ‘‡ is 0. Proof Suppose ğ‘‡ âˆˆ â„’(ğ‘‰) and ğ‘ is the minimal polynomial of ğ‘‡. Then ğ‘‡ is not invertible âŸº 0is an eigenvalue of ğ‘‡ âŸº 0is a zero of ğ‘ âŸº the constant term of ğ‘ is 0, where the first equivalence holds by5.7, the second equivalence holds by 5.27(a), and the last equivalence holds because the constant term of ğ‘ equals ğ‘(0). Eigenvalues on Odd-Dimensional Real Vector Spaces The next result will be the key tool that we use to show that every operator on an odd-dimensional real vector space has an eigenvalue. 5.33 even-dimensional null space Suppose ğ… = ğ‘ and ğ‘‰ is finite-dimensional. Suppose also thatğ‘‡ âˆˆ â„’(ğ‘‰) and ğ‘, ğ‘ âˆˆ ğ‘ with ğ‘ 2 < 4ğ‘. Then dim null(ğ‘‡2 + ğ‘ğ‘‡ + ğ‘ğ¼)is an even number. Proof Recall that null(ğ‘‡2 + ğ‘ğ‘‡ + ğ‘ğ¼)is invariant under ğ‘‡ (by 5.18). By replacing ğ‘‰ with null(ğ‘‡2 + ğ‘ğ‘‡ + ğ‘ğ¼)and replacing ğ‘‡ with ğ‘‡ restricted to null(ğ‘‡2 + ğ‘ğ‘‡ + ğ‘ğ¼), we can assume that ğ‘‡2 + ğ‘ğ‘‡ + ğ‘ğ¼ = 0; we now need to prove that dim ğ‘‰ is even. Suppose ğœ† âˆˆ ğ‘ and ğ‘£ âˆˆ ğ‘‰ are such that ğ‘‡ğ‘£ = ğœ†ğ‘£. Then 0 =(ğ‘‡2 + ğ‘ğ‘‡ + ğ‘ğ¼)ğ‘£ = (ğœ†2 + ğ‘ğœ† + ğ‘)ğ‘£ = ((ğœ† + ğ‘ 2 ) 2 + ğ‘ âˆ’ ğ‘2 4 )ğ‘£. The term in large parentheses above is a positive number. Thus the equation above implies that ğ‘£ = 0. Hence we have shown that ğ‘‡ has no eigenvectors. Let ğ‘ˆ be a subspace of ğ‘‰ that is invariant under ğ‘‡ and has the largest dimension among all subspaces of ğ‘‰ that are invariant under ğ‘‡ and have even dimension. If ğ‘ˆ = ğ‘‰, then we are done; otherwise assume there exists ğ‘¤ âˆˆ ğ‘‰ such that ğ‘¤ âˆ‰ ğ‘ˆ. Let ğ‘Š = span(ğ‘¤, ğ‘‡ğ‘¤). Then ğ‘Š is invariant under ğ‘‡ because ğ‘‡(ğ‘‡ğ‘¤) = âˆ’ğ‘ğ‘‡ğ‘¤ âˆ’ ğ‘ğ‘¤. Furthermore, dim ğ‘Š = 2because otherwise ğ‘¤ would be an eigen- vector of ğ‘‡. Now dim(ğ‘ˆ + ğ‘Š) = dim ğ‘ˆ + dim ğ‘Š âˆ’ dim(ğ‘ˆ âˆ© ğ‘Š) =dim ğ‘ˆ + 2, where ğ‘ˆ âˆ© ğ‘Š = {0}because otherwise ğ‘ˆ âˆ© ğ‘Šwould be a one-dimensional subspace of ğ‘‰ that is invariant under ğ‘‡ (impossible because ğ‘‡ has no eigenvectors). Because ğ‘ˆ + ğ‘Š is invariant under ğ‘‡, the equation above shows that there exists a subspace of ğ‘‰ invariant under ğ‘‡ of even dimension larger than dim ğ‘ˆ. Thus the assumption that ğ‘ˆ â‰  ğ‘‰ was incorrect. Hence ğ‘‰ has even dimension. 150 Chapter 5 Eigenvalues and Eigenvectors The next result states that on odd-dimensional vector spaces, every operator has an eigenvalue. We already know this result for finite-dimensional complex vectors spaces (without the odd hypothesis). Thus in the proof below, we will assume that ğ… = ğ‘. 5.34 operators on odd-dimensional vector spaces have eigenvalues Every operator on an odd-dimensional vector space has an eigenvalue. Proof Suppose ğ… = ğ‘ and ğ‘‰ is finite-dimensional. Letğ‘› = dim ğ‘‰, and suppose ğ‘› is an odd number. Let ğ‘‡ âˆˆ â„’(ğ‘‰). We will use induction on ğ‘› in steps of size two to show that ğ‘‡ has an eigenvalue. To get started, note that the desired result holds if dim ğ‘‰ = 1because then every nonzero vector in ğ‘‰ is an eigenvector of ğ‘‡. Now suppose that ğ‘› â‰¥ 3and the desired result holds for all operators on all odd-dimensional vector spaces of dimension less than ğ‘›. Let ğ‘ denote the minimal polynomial of ğ‘‡. If ğ‘ is a polynomial multiple of ğ‘¥ âˆ’ ğœ† for some ğœ† âˆˆ ğ‘, then ğœ† is an eigenvalue of ğ‘‡ [by 5.27(a)]and we are done. Thus we can assume that there exist ğ‘, ğ‘ âˆˆ ğ‘ such that ğ‘2 < 4ğ‘and ğ‘ is a polynomial multiple of ğ‘¥2 + ğ‘ğ‘¥ + ğ‘ (see 4.16). There exists a monic polynomial ğ‘ âˆˆ ğ’«(ğ‘) such that ğ‘(ğ‘¥) = ğ‘(ğ‘¥)(ğ‘¥2 + ğ‘ğ‘¥ + ğ‘) for all ğ‘¥ âˆˆ ğ‘. Now 0 = ğ‘(ğ‘‡) =(ğ‘(ğ‘‡))(ğ‘‡2 + ğ‘ğ‘‡ + ğ‘ğ¼), which means that ğ‘(ğ‘‡) equals 0on range(ğ‘‡2 + ğ‘ğ‘‡ + ğ‘ğ¼). Because deg ğ‘ < deg ğ‘ and ğ‘ is the minimal polynomial of ğ‘‡, this implies that range(ğ‘‡2 + ğ‘ğ‘‡ + ğ‘ğ¼)â‰  ğ‘‰. The fundamental theorem of linear maps (3.21) tells us that dim ğ‘‰ = dim null(ğ‘‡2 + ğ‘ğ‘‡ + ğ‘ğ¼)+ dim range(ğ‘‡2 + ğ‘ğ‘‡ + ğ‘ğ¼). Because dim ğ‘‰ is odd (by hypothesis) and dim null(ğ‘‡2 + ğ‘ğ‘‡ + ğ‘ğ¼)is even (by 5.33), the equation above shows that dim range(ğ‘‡2 + ğ‘ğ‘‡ + ğ‘ğ¼)is odd. Hence range(ğ‘‡2 + ğ‘ğ‘‡ + ğ‘ğ¼)is a subspace of ğ‘‰ that is invariant under ğ‘‡ (by 5.18) and that has odd dimension less than dim ğ‘‰. Our induction hypothesis now implies that ğ‘‡ restricted to range(ğ‘‡2 + ğ‘ğ‘‡ + ğ‘ğ¼)has an eigenvalue, which means that ğ‘‡ has an eigenvalue. See Exercise 23 in Section 8B and Exercise 10 in Section 9C for alternative proofs of the result above. Exercises 5B 1 Suppose ğ‘‡ âˆˆ â„’(ğ‘‰). Prove that 9is an eigenvalue of ğ‘‡2 if and only if 3or âˆ’3is an eigenvalue of ğ‘‡. 2 Suppose ğ‘‰ is a complex vector space and ğ‘‡ âˆˆ â„’(ğ‘‰) has no eigenvalues. Prove that every subspace of ğ‘‰ invariant under ğ‘‡ is either {0}or infinite- dimensional. Section 5B The Minimal Polynomial 151 3 Suppose ğ‘› is a positive integer and ğ‘‡ âˆˆ â„’(ğ…ğ‘›)is defined by ğ‘‡(ğ‘¥1, â€¦, ğ‘¥ğ‘›) = (ğ‘¥1 + â‹¯ + ğ‘¥ğ‘›, â€¦, ğ‘¥1 + â‹¯ + ğ‘¥ğ‘›). (a) Find all eigenvalues and eigenvectors of ğ‘‡. (b) Find the minimal polynomial of ğ‘‡. The matrix of ğ‘‡ with respect to the standard basis of ğ…ğ‘› consists of all 1â€™s. 4 Suppose ğ… = ğ‚, ğ‘‡ âˆˆ â„’(ğ‘‰), ğ‘ âˆˆ ğ’«(ğ‚), and ğ›¼ âˆˆ ğ‚. Prove that ğ›¼ is an eigenvalue of ğ‘(ğ‘‡) if and only if ğ›¼ = ğ‘(ğœ†) for some eigenvalue ğœ† of ğ‘‡. 5 Give an example of an operator on ğ‘2 that shows the result in Exercise 4 does not hold if ğ‚ is replaced with ğ‘. 6 Suppose ğ‘‡ âˆˆ â„’(ğ…2)is defined byğ‘‡(ğ‘¤, ğ‘§) = (âˆ’ğ‘§, ğ‘¤). Find the minimal polynomial of ğ‘‡. 7 (a) Give an example of ğ‘†, ğ‘‡ âˆˆ â„’(ğ…2)such that the minimal polynomial of ğ‘†ğ‘‡ does not equal the minimal polynomial of ğ‘‡ğ‘†. (b) Suppose ğ‘‰ is finite-dimensional andğ‘†, ğ‘‡ âˆˆ â„’(ğ‘‰). Prove that if at least one of ğ‘†, ğ‘‡ is invertible, then the minimal polynomial of ğ‘†ğ‘‡ equals the minimal polynomial of ğ‘‡ğ‘†. Hint: Show that if ğ‘† is invertible and ğ‘ âˆˆ ğ’«(ğ…), then ğ‘(ğ‘‡ğ‘†) = ğ‘†âˆ’1ğ‘(ğ‘†ğ‘‡)ğ‘†. 8 Suppose ğ‘‡ âˆˆ â„’(ğ‘2)is the operator of counterclockwise rotation by 1 âˆ˜. Find the minimal polynomial of ğ‘‡. Because dim ğ‘2 = 2, the degree of the minimal polynomial of ğ‘‡ is at most 2. Thus the minimal polynomial of ğ‘‡ is not the tempting polynomial ğ‘¥180 + 1, even though ğ‘‡180 = âˆ’ğ¼. 9 Suppose ğ‘‡ âˆˆ â„’(ğ‘‰) is such that with respect to some basis of ğ‘‰, all entries of the matrix of ğ‘‡ are rational numbers. Explain why all coefficients of the minimal polynomial of ğ‘‡ are rational numbers. 10 Suppose ğ‘‰ is finite-dimensional,ğ‘‡ âˆˆ â„’(ğ‘‰), and ğ‘£ âˆˆ ğ‘‰. Prove that span(ğ‘£, ğ‘‡ğ‘£, â€¦, ğ‘‡ğ‘šğ‘£)= span(ğ‘£, ğ‘‡ğ‘£, â€¦, ğ‘‡dim ğ‘‰ âˆ’ 1ğ‘£) for all integers ğ‘š â‰¥dim ğ‘‰ âˆ’ 1. 11 Suppose ğ‘‰ is a two-dimensional vector space, ğ‘‡ âˆˆ â„’(ğ‘‰), and the matrix of ğ‘‡ with respect to some basis of ğ‘‰ is ( ğ‘ ğ‘ ğ‘ ğ‘‘ ). (a) Show that ğ‘‡2 âˆ’ (ğ‘ + ğ‘‘)ğ‘‡ + (ğ‘ğ‘‘ âˆ’ ğ‘ğ‘)ğ¼ = 0. (b) Show that the minimal polynomial of ğ‘‡ equals â§{ â¨{â© ğ‘§ âˆ’ ğ‘ if ğ‘ = ğ‘ = 0and ğ‘ = ğ‘‘, ğ‘§ 2 âˆ’ (ğ‘ + ğ‘‘)ğ‘§ + (ğ‘ğ‘‘ âˆ’ ğ‘ğ‘) otherwise. 152 Chapter 5 Eigenvalues and Eigenvectors 12 Defineğ‘‡ âˆˆ â„’(ğ…ğ‘›)by ğ‘‡(ğ‘¥1, ğ‘¥2, ğ‘¥3, â€¦, ğ‘¥ğ‘›) = (ğ‘¥1, 2ğ‘¥2, 3ğ‘¥3, â€¦, ğ‘›ğ‘¥ğ‘›). Find the minimal polynomial of ğ‘‡. 13 Suppose ğ‘‡ âˆˆ â„’(ğ‘‰) and ğ‘ âˆˆ ğ’«(ğ…). Prove that there exists a unique ğ‘Ÿ âˆˆ ğ’«(ğ…) such that ğ‘(ğ‘‡) = ğ‘Ÿ(ğ‘‡) and deg ğ‘Ÿ is less than the degree of the minimal polynomial of ğ‘‡. 14 Suppose ğ‘‰ is finite-dimensional andğ‘‡ âˆˆ â„’(ğ‘‰) has minimal polynomial 4+ 5ğ‘§ âˆ’ 6ğ‘§ 2 âˆ’ 7ğ‘§ 3 + 2ğ‘§ 4 + ğ‘§5. Find the minimal polynomial of ğ‘‡âˆ’1. 15 Suppose ğ‘‰ is a finite-dimensional complex vector space withdim ğ‘‰ > 0 and ğ‘‡ âˆˆ â„’(ğ‘‰). Defineğ‘“âˆ¶ ğ‚ â†’ ğ‘ by ğ‘“ (ğœ†) = dim range(ğ‘‡ âˆ’ ğœ†ğ¼). Prove that ğ‘“ is not a continuous function. 16 Suppose ğ‘0, â€¦, ğ‘ğ‘› âˆ’ 1 âˆˆ ğ…. Let ğ‘‡ be the operator on ğ…ğ‘› whose matrix (with respect to the standard basis) is â›âœâœâœâœâœâœâœâœâœâœâœâœâœ â 0 âˆ’ğ‘0 1 0 âˆ’ğ‘1 1 â‹± âˆ’ğ‘2 â‹± â‹® 0 âˆ’ğ‘ğ‘› âˆ’ 2 1 âˆ’ğ‘ğ‘› âˆ’ 1 ââŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸ â  . Here all entries of the matrix are 0except for all 1â€™s on the line under the diagonal and the entries in the last column (some of which might also be 0). Show that the minimal polynomial of ğ‘‡ is the polynomial ğ‘0 + ğ‘1ğ‘§ + â‹¯ + ğ‘ğ‘› âˆ’ 1ğ‘§ ğ‘› âˆ’ 1 + ğ‘§ğ‘›. The matrix above is called the companion matrix of the polynomial above. This exercise shows that every monic polynomial is the minimal polynomial of some operator. Hence a formula or an algorithm that could produce exact eigenvalues for each operator on each ğ…ğ‘› could then produce exact zeros for each polynomial [by 5.27(a)]. Thus there is no such formula or algorithm. However, efficient numeric methods exist for obtaining very good approximations for the eigenvalues of an operator. 17 Suppose ğ‘‰ is finite-dimensional,ğ‘‡ âˆˆ â„’(ğ‘‰), and ğ‘ is the minimal polynomial of ğ‘‡. Suppose ğœ† âˆˆ ğ…. Show that the minimal polynomial of ğ‘‡ âˆ’ ğœ†ğ¼ is the polynomial ğ‘ defined byğ‘(ğ‘§) = ğ‘(ğ‘§ + ğœ†). 18 Suppose ğ‘‰ is finite-dimensional,ğ‘‡ âˆˆ â„’(ğ‘‰), and ğ‘ is the minimal polynomial of ğ‘‡. Suppose ğœ† âˆˆ ğ…\\{0}. Show that the minimal polynomial of ğœ†ğ‘‡ is the polynomial ğ‘ defined byğ‘(ğ‘§) = ğœ†deg ğ‘ ğ‘( ğ‘§ ğœ† ). Section 5B The Minimal Polynomial 153 19 Suppose ğ‘‰ is finite-dimensional andğ‘‡ âˆˆ â„’(ğ‘‰). Let â„° be the subspace of â„’(ğ‘‰) defined by â„° = {ğ‘(ğ‘‡) âˆ¶ ğ‘ âˆˆ ğ’«(ğ…)}. Prove that dim â„° equals the degree of the minimal polynomial of ğ‘‡. 20 Suppose ğ‘‡ âˆˆ â„’(ğ…4)is such that the eigenvalues of ğ‘‡ are 3, 5, 8. Prove that (ğ‘‡ âˆ’ 3ğ¼) 2(ğ‘‡ âˆ’ 5ğ¼) 2(ğ‘‡ âˆ’ 8ğ¼) 2 = 0. 21 Suppose ğ‘‰ is finite-dimensional andğ‘‡ âˆˆ â„’(ğ‘‰). Prove that the minimal polynomial of ğ‘‡ has degree at most 1+ dim range ğ‘‡. If dim range ğ‘‡ < dim ğ‘‰ âˆ’ 1, then this exercise gives a better upper bound than 5.22 for the degree of the minimal polynomial of ğ‘‡. 22 Suppose ğ‘‰ is finite-dimensional andğ‘‡ âˆˆ â„’(ğ‘‰). Prove that ğ‘‡ is invertible if and only if ğ¼ âˆˆ span(ğ‘‡, ğ‘‡2, â€¦, ğ‘‡dim ğ‘‰). 23 Suppose ğ‘‰ is finite-dimensional andğ‘‡ âˆˆ â„’(ğ‘‰). Let ğ‘› = dim ğ‘‰. Prove that if ğ‘£ âˆˆ ğ‘‰, then span(ğ‘£, ğ‘‡ğ‘£, â€¦, ğ‘‡ğ‘› âˆ’ 1ğ‘£)is invariant under ğ‘‡. 24 Suppose ğ‘‰ is a finite-dimensional complex vector space. Supposeğ‘‡ âˆˆ â„’(ğ‘‰) is such that 5and 6are eigenvalues of ğ‘‡ and that ğ‘‡ has no other eigenvalues. Prove that (ğ‘‡ âˆ’ 5ğ¼) dim ğ‘‰ âˆ’ 1(ğ‘‡ âˆ’ 6ğ¼) dim ğ‘‰ âˆ’ 1 = 0. 25 Suppose ğ‘‰ is finite-dimensional,ğ‘‡ âˆˆ â„’(ğ‘‰), and ğ‘ˆ is a subspace of ğ‘‰ that is invariant under ğ‘‡. (a) Prove that the minimal polynomial of ğ‘‡ is a polynomial multiple of the minimal polynomial of the quotient operator ğ‘‡/ğ‘ˆ. (b) Prove that (minimal polynomial of ğ‘‡|ğ‘ˆ) Ã— (minimal polynomial of ğ‘‡/ğ‘ˆ) is a polynomial multiple of the minimal polynomial of ğ‘‡. The quotient operator ğ‘‡/ğ‘ˆ was defined in Exercise 38 in Section 5A. 26 Suppose ğ‘‰ is finite-dimensional,ğ‘‡ âˆˆ â„’(ğ‘‰), and ğ‘ˆ is a subspace of ğ‘‰ that is invariant under ğ‘‡. Prove that the set of eigenvalues of ğ‘‡ equals the union of the set of eigenvalues of ğ‘‡|ğ‘ˆ and the set of eigenvalues of ğ‘‡/ğ‘ˆ. 27 Suppose ğ… = ğ‘, ğ‘‰ is finite-dimensional, andğ‘‡ âˆˆ â„’(ğ‘‰). Prove that the minimal polynomial of ğ‘‡ğ‚ equals the minimal polynomial of ğ‘‡. The complexification ğ‘‡ğ‚ was defined in Exercise 33 of Section 3B. 28 Suppose ğ‘‰ is finite-dimensional andğ‘‡ âˆˆ â„’(ğ‘‰). Prove that the minimal polynomial of ğ‘‡â€² âˆˆ â„’(ğ‘‰â€²)equals the minimal polynomial of ğ‘‡. The dual map ğ‘‡â€² was defined in Section 3F. 29 Show that every operator on a finite-dimensional vector space of dimension at least two has an invariant subspace of dimension two. Exercise 6 in Section 5C will give an improvement of this result when ğ… = ğ‚. 154 Chapter 5 Eigenvalues and Eigenvectors 5C Upper-Triangular Matrices In Chapter 3 we defined the matrix of a linear map from a finite-dimensional vector space to another finite-dimensional vector space. That matrix depends on a choice of basis of each of the two vector spaces. Now that we are studying operators, which map a vector space to itself, the emphasis is on using only one basis. 5.35 definition: matrix of an operator, â„³(ğ‘‡) Suppose ğ‘‡ âˆˆ â„’(ğ‘‰). The matrix of ğ‘‡ with respect to a basis ğ‘£1, â€¦, ğ‘£ğ‘› of ğ‘‰ is the ğ‘›-by-ğ‘› matrix â„³(ğ‘‡) = â›âœâœâœ â ğ´1, 1 â‹¯ ğ´1, ğ‘› â‹® â‹® ğ´ğ‘›, 1 â‹¯ ğ´ğ‘›, ğ‘› ââŸâŸâŸ â  whose entries ğ´ğ‘—, ğ‘˜ are defined by ğ‘‡ğ‘£ğ‘˜ = ğ´1, ğ‘˜ğ‘£1 + â‹¯ + ğ´ğ‘›, ğ‘˜ğ‘£ğ‘›. The notation â„³(ğ‘‡, (ğ‘£1, â€¦, ğ‘£ğ‘›))is used if the basis is not clear from the context. Operators have square matrices (meaning that the number of rows equals the number of columns), rather than the more general rectangular matrices that we considered earlier for linear maps. The ğ‘˜th column of the matrix â„³(ğ‘‡) is formed from the coefficients used to write ğ‘‡ğ‘£ğ‘˜ as a linear combination of the basis ğ‘£1, â€¦, ğ‘£ğ‘›. If ğ‘‡ is an operator on ğ…ğ‘› and no ba- sis is specified, assume that the basis in question is the standard one (where the ğ‘˜th basis vector is 1in the ğ‘˜th slot and 0 in all other slots). You can then think of the ğ‘˜th column of â„³(ğ‘‡) as ğ‘‡ applied to the ğ‘˜th basis vector, where we identify ğ‘›-by-1column vectors with elements of ğ…ğ‘›. 5.36 example:matrix of an operator with respect to standard basis Defineğ‘‡ âˆˆ â„’(ğ…3)by ğ‘‡(ğ‘¥, ğ‘¦, ğ‘§) = (2ğ‘¥+ ğ‘¦, 5ğ‘¦+ 3ğ‘§, 8ğ‘§). Then the matrix of ğ‘‡ with respect to the standard basis of ğ…3 is â„³(ğ‘‡) = â›âœâœâœ â 2 1 0 0 5 3 0 0 8 ââŸâŸâŸ â  , as you should verify. A central goal of linear algebra is to show that given an operator ğ‘‡ on a finite- dimensional vector space ğ‘‰, there exists a basis of ğ‘‰ with respect to which ğ‘‡ has a reasonably simple matrix. To make this vague formulation a bit more precise, we might try to choose a basis of ğ‘‰ such that â„³(ğ‘‡) has many 0â€™s. Section 5C Upper-Triangular Matrices 155 If ğ‘‰ is a finite-dimensional complex vector space, then we already know enough to show that there is a basis of ğ‘‰ with respect to which the matrix of ğ‘‡ has 0â€™s everywhere in the first column, except possibly the first entry. In other words, there is a basis of ğ‘‰ with respect to which the matrix of ğ‘‡ looks like â›âœâœâœâœâœâœ â ğœ† 0 âˆ— â‹® 0 ââŸâŸâŸâŸâŸâŸ â  ; here âˆ— denotes the entries in all columns other than the first column. To prove this, let ğœ† be an eigenvalue of ğ‘‡ (one exists by 5.19) and let ğ‘£ be a corresponding eigenvector. Extend ğ‘£ to a basis of ğ‘‰. Then the matrix of ğ‘‡ with respect to this basis has the form above. Soon we will see that we can choose a basis of ğ‘‰ with respect to which the matrix of ğ‘‡ has even more 0â€™s. 5.37 definition:diagonal of a matrix The diagonal of a square matrix consists of the entries on the line from the upper left corner to the bottom right corner. For example, the diagonal of the matrix â„³(ğ‘‡) = â›âœâœâœ â 2 1 0 0 5 3 0 08 ââŸâŸâŸ â  from Example 5.36 consists of the entries 2, 5, 8, which are shown in red in the matrix above. 5.38 definition: upper-triangular matrix A square matrix is called upper triangular if all entries below the diagonal are 0. For example, the 3-by-3matrix above is upper triangular. Typically we represent an upper-triangular matrix in the form â›âœâœâœ â ğœ†1 âˆ— â‹± 0 ğœ†ğ‘› ââŸâŸâŸ â  ; We often use âˆ— to denote matrix entries that we do not know or that are irrele- vant to the questions being discussed. the 0in the matrix above indicates that all entries below the diagonal in this ğ‘›-by-ğ‘› matrix equal 0. Upper-triangular matrices can be considered reasonably simpleâ€”if ğ‘› is large, then at least almost half the entries in an ğ‘›-by-ğ‘› upper- triangular matrix are 0. 156 Chapter 5 Eigenvalues and Eigenvectors The next result provides a useful connection between upper-triangular matrices and invariant subspaces. 5.39 conditions for upper-triangular matrix Suppose ğ‘‡ âˆˆ â„’(ğ‘‰) and ğ‘£1, â€¦, ğ‘£ğ‘› is a basis of ğ‘‰. Then the following are equivalent. (a) The matrix of ğ‘‡ with respect to ğ‘£1, â€¦, ğ‘£ğ‘› is upper triangular. (b) span(ğ‘£1, â€¦, ğ‘£ğ‘˜) is invariant under ğ‘‡ for each ğ‘˜ = 1, â€¦, ğ‘›. (c) ğ‘‡ğ‘£ğ‘˜ âˆˆ span(ğ‘£1, â€¦, ğ‘£ğ‘˜) for each ğ‘˜ = 1, â€¦, ğ‘›. Proof First suppose (a) holds. To prove that (b) holds, suppose ğ‘˜ âˆˆ {1, â€¦, ğ‘›}. If ğ‘— âˆˆ {1, â€¦, ğ‘›}, then ğ‘‡ğ‘£ğ‘— âˆˆ span(ğ‘£1, â€¦, ğ‘£ğ‘—) because the matrix of ğ‘‡ with respect to ğ‘£1, â€¦, ğ‘£ğ‘› is upper triangular. Because span(ğ‘£1, â€¦, ğ‘£ğ‘—) âŠ†span(ğ‘£1, â€¦, ğ‘£ğ‘˜) if ğ‘— â‰¤ ğ‘˜, we see that ğ‘‡ğ‘£ğ‘— âˆˆ span(ğ‘£1, â€¦, ğ‘£ğ‘˜) for each ğ‘— âˆˆ {1, â€¦, ğ‘˜}. Thus span(ğ‘£1, â€¦, ğ‘£ğ‘˜) is invariant under ğ‘‡, completing the proof that (a) implies (b). Now suppose (b) holds, so span(ğ‘£1, â€¦, ğ‘£ğ‘˜) is invariant under ğ‘‡ for each ğ‘˜ = 1, â€¦, ğ‘›. In particular, ğ‘‡ğ‘£ğ‘˜ âˆˆ span(ğ‘£1, â€¦, ğ‘£ğ‘˜) for each ğ‘˜ = 1, â€¦, ğ‘›. Thus (b) implies (c). Now suppose (c) holds, so ğ‘‡ğ‘£ğ‘˜ âˆˆ span(ğ‘£1, â€¦, ğ‘£ğ‘˜) for each ğ‘˜ = 1, â€¦, ğ‘›. This means that when writing each ğ‘‡ğ‘£ğ‘˜ as a linear combination of the basis vectors ğ‘£1, â€¦, ğ‘£ğ‘›, we need to use only the vectors ğ‘£1, â€¦, ğ‘£ğ‘˜. Hence all entries under the diagonal of â„³(ğ‘‡) are 0. Thus â„³(ğ‘‡) is an upper-triangular matrix, completing the proof that (c) implies (a). We have shown that (a) âŸ¹ (b) âŸ¹ (c) âŸ¹ (a), which shows that (a), (b), and (c) are equivalent. The next result tells us that if ğ‘‡ âˆˆ â„’(ğ‘‰) and with respect to some basis of ğ‘‰ we have â„³(ğ‘‡) = â›âœâœâœ â ğœ†1 âˆ— â‹± 0 ğœ†ğ‘› ââŸâŸâŸ â  , then ğ‘‡ satisfies a simple equation depending onğœ†1, â€¦, ğœ†ğ‘›. 5.40 equation satisfied by operator with upper-triangular matrix Suppose ğ‘‡ âˆˆ â„’(ğ‘‰) and ğ‘‰ has a basis with respect to which ğ‘‡ has an upper- triangular matrix with diagonal entries ğœ†1, â€¦, ğœ†ğ‘›. Then (ğ‘‡ âˆ’ ğœ†1ğ¼)â‹¯(ğ‘‡ âˆ’ ğœ†ğ‘›ğ¼) = 0. Section 5C Upper-Triangular Matrices 157 Proof Let ğ‘£1, â€¦, ğ‘£ğ‘› denote a basis of ğ‘‰ with respect to which ğ‘‡ has an upper- triangular matrix with diagonal entries ğœ†1, â€¦, ğœ†ğ‘›. Then ğ‘‡ğ‘£1 = ğœ†1ğ‘£1, which means that (ğ‘‡ âˆ’ ğœ†1ğ¼)ğ‘£1 = 0, which implies that (ğ‘‡ âˆ’ ğœ†1ğ¼)â‹¯(ğ‘‡ âˆ’ ğœ†ğ‘šğ¼)ğ‘£1 = 0for ğ‘š = 1, â€¦, ğ‘› (using the commutativity of each ğ‘‡ âˆ’ ğœ†ğ‘—ğ¼ with each ğ‘‡ âˆ’ ğœ†ğ‘˜ğ¼). Note that (ğ‘‡ âˆ’ ğœ†2ğ¼)ğ‘£2 âˆˆ span(ğ‘£1). Thus (ğ‘‡ âˆ’ ğœ†1ğ¼)(ğ‘‡ âˆ’ ğœ†2ğ¼)ğ‘£2 = 0(by the previous paragraph), which implies that (ğ‘‡ âˆ’ ğœ†1ğ¼)â‹¯(ğ‘‡ âˆ’ ğœ†ğ‘šğ¼)ğ‘£1 = 0for ğ‘š = 2, â€¦, ğ‘› (using the commutativity of each ğ‘‡ âˆ’ ğœ†ğ‘—ğ¼ with each ğ‘‡ âˆ’ ğœ†ğ‘˜ğ¼). Note that (ğ‘‡ âˆ’ ğœ†3ğ¼)ğ‘£3 âˆˆ span(ğ‘£1, ğ‘£2). Thus by the previous paragraph, (ğ‘‡âˆ’ğœ†1ğ¼)(ğ‘‡âˆ’ğœ†2ğ¼)(ğ‘‡âˆ’ğœ†3ğ¼)ğ‘£3 = 0, which implies that (ğ‘‡âˆ’ğœ†1ğ¼)â‹¯(ğ‘‡âˆ’ğœ†ğ‘šğ¼)ğ‘£1 = 0 for ğ‘š = 3, â€¦, ğ‘› (using the commutativity of each ğ‘‡ âˆ’ ğœ†ğ‘—ğ¼ with each ğ‘‡ âˆ’ ğœ†ğ‘˜ğ¼). Continuing this pattern, we see that (ğ‘‡ âˆ’ ğœ†1ğ¼)â‹¯(ğ‘‡ âˆ’ ğœ†ğ‘›ğ¼)ğ‘£ğ‘˜ = 0for each ğ‘˜ = 1, â€¦, ğ‘›. Thus (ğ‘‡ âˆ’ ğœ†1ğ¼)â‹¯(ğ‘‡ âˆ’ ğœ†ğ‘›ğ¼) is the 0operator because it is 0on each vector in a basis of ğ‘‰. Unfortunately no method exists for exactly computing the eigenvalues of an operator from its matrix. However, if we are fortunate enough to find a basis with respect to which the matrix of the operator is upper triangular, then the problem of computing the eigenvalues becomes trivial, as the next result shows. 5.41 determination of eigenvalues from upper-triangular matrix Suppose ğ‘‡ âˆˆ â„’(ğ‘‰) has an upper-triangular matrix with respect to some basis of ğ‘‰. Then the eigenvalues of ğ‘‡ are precisely the entries on the diagonal of that upper-triangular matrix. Proof Suppose ğ‘£1, â€¦, ğ‘£ğ‘› is a basis of ğ‘‰ with respect to which ğ‘‡ has an upper- triangular matrix â„³(ğ‘‡) = â›âœâœâœ â ğœ†1 âˆ— â‹± 0 ğœ†ğ‘› ââŸâŸâŸ â  . Because ğ‘‡ğ‘£1 = ğœ†1ğ‘£1, we see that ğœ†1 is an eigenvalue of ğ‘‡. Suppose ğ‘˜ âˆˆ {2, â€¦, ğ‘›}. Then (ğ‘‡ âˆ’ ğœ†ğ‘˜ğ¼)ğ‘£ğ‘˜ âˆˆ span(ğ‘£1, â€¦, ğ‘£ğ‘˜ âˆ’ 1). Thus ğ‘‡ âˆ’ ğœ†ğ‘˜ğ¼ maps span(ğ‘£1, â€¦, ğ‘£ğ‘˜) into span(ğ‘£1, â€¦, ğ‘£ğ‘˜ âˆ’ 1). Because dim span(ğ‘£1, â€¦, ğ‘£ğ‘˜) = ğ‘˜ and dim span(ğ‘£1, â€¦, ğ‘£ğ‘˜ âˆ’ 1) = ğ‘˜ âˆ’ 1, this implies that ğ‘‡ âˆ’ ğœ†ğ‘˜ğ¼ restricted to span(ğ‘£1, â€¦, ğ‘£ğ‘˜) is not injective (by 3.22). Thus there exists ğ‘£ âˆˆ span(ğ‘£1, â€¦, ğ‘£ğ‘˜) such that ğ‘£ â‰  0and (ğ‘‡ âˆ’ ğœ†ğ‘˜ğ¼)ğ‘£ = 0. Thus ğœ†ğ‘˜ is an eigenvalue of ğ‘‡. Hence we have shown that every entry on the diagonal of â„³(ğ‘‡) is an eigenvalue of ğ‘‡. To prove ğ‘‡ has no other eigenvalues, let ğ‘ be the polynomial defined by ğ‘(ğ‘§) = (ğ‘§ âˆ’ ğœ†1)â‹¯(ğ‘§ âˆ’ ğœ†ğ‘›). Then ğ‘(ğ‘‡) = 0(by 5.40). Hence ğ‘ is a polynomial multiple of the minimal polynomial of ğ‘‡ (by 5.29). Thus every zero of the minimal polynomial of ğ‘‡ is a zero of ğ‘. Because the zeros of the minimal polynomial of ğ‘‡ are the eigenvalues of ğ‘‡ (by 5.27), this implies that every eigenvalue of ğ‘‡ is a zero of ğ‘. Hence the eigenvalues of ğ‘‡ are all contained in the list ğœ†1, â€¦, ğœ†ğ‘›. 158 Chapter 5 Eigenvalues and Eigenvectors 5.42 example: eigenvalues via an upper-triangular matrix Defineğ‘‡ âˆˆ â„’(ğ…3)by ğ‘‡(ğ‘¥, ğ‘¦, ğ‘§) = (2ğ‘¥+ ğ‘¦, 5ğ‘¦+ 3ğ‘§, 8ğ‘§). The matrix of ğ‘‡ with respect to the standard basis is â„³(ğ‘‡) = â›âœâœâœ â 2 1 0 0 5 3 0 0 8 ââŸâŸâŸ â  . Now 5.41 implies that the eigenvalues of ğ‘‡ are 2, 5, and 8. The next example illustrates 5.44: an operator has an upper-triangular matrix with respect to some basis if and only if the minimal polynomial of the operator is the product of polynomials of degree 1. 5.43 example: whether ğ‘‡ has an upper-triangular matrix can depend on ğ… Defineğ‘‡ âˆˆ â„’(ğ…4)by ğ‘‡(ğ‘§1, ğ‘§2, ğ‘§3, ğ‘§4) = (âˆ’ğ‘§2, ğ‘§1, 2ğ‘§1 + 3ğ‘§3, ğ‘§3 + 3ğ‘§4). Thus with respect to the standard basis of ğ…4, the matrix of ğ‘‡ is â›âœâœâœâœâœâœ â 0 âˆ’1 0 0 1 0 0 0 2 0 3 0 0 0 1 3 ââŸâŸâŸâŸâŸâŸ â  . You can ask a computer to verify that the minimal polynomial of ğ‘‡ is the polyno- mial ğ‘ defined by ğ‘(ğ‘§) = 9 âˆ’ 6ğ‘§+ 10ğ‘§ 2 âˆ’ 6ğ‘§ 3 + ğ‘§4. First consider the case ğ… = ğ‘. Then the polynomial ğ‘ factors as ğ‘(ğ‘§) = (ğ‘§ 2 + 1)(ğ‘§ âˆ’ 3)(ğ‘§ âˆ’ 3), with no further factorization of ğ‘§ 2 + 1as the product of two polynomials of degree 1with real coefficients. Thus 5.44 states that there does not exist a basis of ğ‘4 with respect to which ğ‘‡ has an upper-triangular matrix. Now consider the case ğ… = ğ‚. Then the polynomial ğ‘ factors as ğ‘(ğ‘§) = (ğ‘§ âˆ’ ğ‘–)(ğ‘§ + ğ‘–)(ğ‘§ âˆ’ 3)(ğ‘§ âˆ’ 3), where all factors above have the form ğ‘§âˆ’ ğœ†ğ‘˜. Thus 5.44 states that there is a basis of ğ‚ 4 with respect to which ğ‘‡ has an upper-triangular matrix. Indeed, you can verify that with respect to the basis (4 âˆ’ 3ğ‘–, âˆ’3 âˆ’ 4ğ‘–, âˆ’3+ ğ‘–, 1), (4+ 3ğ‘–, âˆ’3+ 4ğ‘–, âˆ’3 âˆ’ ğ‘–, 1), (0, 0, 0, 1), (0, 0, 1, 0)of ğ‚4, the operator ğ‘‡ has the upper-triangular matrix â›âœâœâœâœâœâœ â ğ‘– 0 0 0 0 âˆ’ğ‘– 0 0 0 0 3 1 0 0 0 3 ââŸâŸâŸâŸâŸâŸ â  . Section 5C Upper-Triangular Matrices 159 5.44 necessary and sufficient condition to have an upper-triangular matrix Suppose ğ‘‰ is finite-dimensional andğ‘‡ âˆˆ â„’(ğ‘‰). Then ğ‘‡ has an upper- triangular matrix with respect to some basis of ğ‘‰ if and only if the minimal polynomial of ğ‘‡ equals (ğ‘§ âˆ’ ğœ†1)â‹¯(ğ‘§ âˆ’ ğœ†ğ‘š) for some ğœ†1, â€¦, ğœ†ğ‘š âˆˆ ğ…. Proof First suppose ğ‘‡ has an upper-triangular matrix with respect to some basis of ğ‘‰. Let ğ›¼1, â€¦, ğ›¼ğ‘› denote the diagonal entries of that matrix. Define a polynomial ğ‘ âˆˆ ğ’«(ğ…) by ğ‘(ğ‘§) = (ğ‘§ âˆ’ ğ›¼1)â‹¯(ğ‘§ âˆ’ ğ›¼ğ‘›). Then ğ‘(ğ‘‡) = 0, by 5.40. Hence ğ‘ is a polynomial multiple of the minimal polyno- mial of ğ‘‡, by 5.29. Thus the minimal polynomial of ğ‘‡ equals (ğ‘§ âˆ’ ğœ†1)â‹¯(ğ‘§ âˆ’ ğœ†ğ‘š) for some ğœ†1, â€¦, ğœ†ğ‘š âˆˆ ğ… with {ğœ†1, â€¦, ğœ†ğ‘š} âŠ† {ğ›¼1, â€¦, ğ›¼ğ‘›}. To prove the implication in the other direction, now suppose the minimal polynomial of ğ‘‡ equals (ğ‘§ âˆ’ ğœ†1)â‹¯(ğ‘§ âˆ’ ğœ†ğ‘š) for some ğœ†1, â€¦, ğœ†ğ‘š âˆˆ ğ…. We will use induction on ğ‘š. To get started, if ğ‘š = 1then ğ‘§ âˆ’ ğœ†1 is the minimal polynomial of ğ‘‡, which implies that ğ‘‡ = ğœ†1ğ¼, which implies that the matrix of ğ‘‡ (with respect to any basis of ğ‘‰) is upper triangular. Now suppose ğ‘š > 1and the desired result holds for all smaller positive integers. Let ğ‘ˆ = range(ğ‘‡ âˆ’ ğœ†ğ‘šğ¼). Then ğ‘ˆ is invariant under ğ‘‡ [this is a special case of 5.18 with ğ‘(ğ‘§) = ğ‘§ âˆ’ ğœ†ğ‘š]. Thus ğ‘‡|ğ‘ˆ is an operator on ğ‘ˆ. If ğ‘¢ âˆˆ ğ‘ˆ, then ğ‘¢ = (ğ‘‡ âˆ’ ğœ†ğ‘šğ¼)ğ‘£ for some ğ‘£ âˆˆ ğ‘‰ and (ğ‘‡ âˆ’ ğœ†1ğ¼)â‹¯(ğ‘‡ âˆ’ ğœ†ğ‘š âˆ’ 1ğ¼)ğ‘¢ = (ğ‘‡ âˆ’ ğœ†1ğ¼)â‹¯(ğ‘‡ âˆ’ ğœ†ğ‘šğ¼)ğ‘£ = 0. Hence (ğ‘§ âˆ’ ğœ†1)â‹¯(ğ‘§ âˆ’ ğœ†ğ‘š âˆ’ 1) is a polynomial multiple of the minimal polynomial of ğ‘‡|ğ‘ˆ, by 5.29. Thus the minimal polynomial of ğ‘‡|ğ‘ˆ is the product of at most ğ‘š âˆ’ 1terms of the form ğ‘§ âˆ’ ğœ†ğ‘˜. By our induction hypothesis, there is a basis ğ‘¢1, â€¦, ğ‘¢ğ‘€ of ğ‘ˆ with respect to which ğ‘‡|ğ‘ˆ has an upper-triangular matrix. Thus for each ğ‘˜ âˆˆ {1, â€¦, ğ‘€}, we have (using 5.39) 5.45 ğ‘‡ğ‘¢ğ‘˜ = (ğ‘‡|ğ‘ˆ)(ğ‘¢ğ‘˜) âˆˆ span(ğ‘¢1, â€¦, ğ‘¢ğ‘˜). Extend ğ‘¢1, â€¦, ğ‘¢ğ‘€ to a basis ğ‘¢1, â€¦, ğ‘¢ğ‘€, ğ‘£1, â€¦, ğ‘£ğ‘ of ğ‘‰. For each ğ‘˜ âˆˆ {1, â€¦, ğ‘}, we have ğ‘‡ğ‘£ğ‘˜ = (ğ‘‡ âˆ’ ğœ†ğ‘šğ¼)ğ‘£ğ‘˜ + ğœ†ğ‘šğ‘£ğ‘˜. The definition ofğ‘ˆ shows that (ğ‘‡ âˆ’ ğœ†ğ‘šğ¼)ğ‘£ğ‘˜ âˆˆ ğ‘ˆ = span(ğ‘¢1, â€¦, ğ‘¢ğ‘€). Thus the equation above shows that 5.46 ğ‘‡ğ‘£ğ‘˜ âˆˆ span(ğ‘¢1, â€¦, ğ‘¢ğ‘€, ğ‘£1, â€¦, ğ‘£ğ‘˜). From 5.45 and 5.46, we conclude (using 5.39) that ğ‘‡ has an upper-triangular matrix with respect to the basis ğ‘¢1, â€¦, ğ‘¢ğ‘€, ğ‘£1, â€¦, ğ‘£ğ‘ of ğ‘‰, as desired. 160 Chapter 5 Eigenvalues and Eigenvectors The set of numbers {ğœ†1, â€¦, ğœ†ğ‘š} from the previous result equals the set of eigenvalues of ğ‘‡ (because the set of zeros of the minimal polynomial of ğ‘‡ equals the set of eigenvalues of ğ‘‡, by 5.27), although the list ğœ†1, â€¦, ğœ†ğ‘š in the previous result may contain repetitions. In Chapter 8 we will improve even the wonderful result below; see 8.37 and 8.46. 5.47 if ğ… = ğ‚, then every operator on ğ‘‰ has an upper-triangular matrix Suppose ğ‘‰ is a finite-dimensional complex vector space andğ‘‡ âˆˆ â„’(ğ‘‰). Then ğ‘‡ has an upper-triangular matrix with respect to some basis of ğ‘‰. Proof The desired result follows immediately from 5.44 and the second version of the fundamental theorem of algebra (see 4.13). For an extension of the result above to two operators ğ‘† and ğ‘‡ such that ğ‘†ğ‘‡ = ğ‘‡ğ‘†, see 5.80. Also, for an extension to more than two operators, see Exercise 9(b) in Section 5E. Caution: If an operator ğ‘‡ âˆˆ â„’(ğ‘‰) has a upper-triangular matrix with respect to some basis ğ‘£1, â€¦, ğ‘£ğ‘› of ğ‘‰, then the eigenvalues of ğ‘‡ are exactly the entries on the diagonal of â„³(ğ‘‡), as shown by 5.41, and furthermore ğ‘£1 is an eigenvector of ğ‘‡. However, ğ‘£2, â€¦, ğ‘£ğ‘› need not be eigenvectors of ğ‘‡. Indeed, a basis vector ğ‘£ğ‘˜ is an eigenvector of ğ‘‡ if and only if all entries in the ğ‘˜th column of the matrix of ğ‘‡ are 0, except possibly the ğ‘˜th entry. The row echelon form of the matrix of an operator does not give us a list of the eigenvalues of the operator. In contrast, an upper-triangular matrix with respect to some basis gives us a list of all the eigenvalues of the op- erator. However, there is no method for computing exactly such an upper- triangular matrix, even though 5.47 guarantees its existence if ğ… = ğ‚. You may recall from a previous course that every matrix of numbers can be changed to a matrix in what is called row echelon form. If one begins with a square matrix, the matrix in row echelon form will be an upper-triangular matrix. Do not confuse this upper-triangular ma- trix with the upper-triangular matrix of an operator with respect to some basis whose existence is proclaimed by 5.47 (if ğ… = ğ‚)â€”there is no connection between these upper-triangular matrices. Exercises 5C 1 Prove or give a counterexample: If ğ‘‡ âˆˆ â„’(ğ‘‰) and ğ‘‡2 has an upper-triangular matrix with respect to some basis of ğ‘‰, then ğ‘‡ has an upper-triangular matrix with respect to some basis of ğ‘‰. Section 5C Upper-Triangular Matrices 161 2 Suppose ğ´ and ğµ are upper-triangular matrices of the same size, with ğ›¼1, â€¦, ğ›¼ğ‘› on the diagonal of ğ´ and ğ›½1, â€¦, ğ›½ğ‘› on the diagonal of ğµ. (a) Show that ğ´ + ğµ is an upper-triangular matrix with ğ›¼1 + ğ›½1, â€¦, ğ›¼ğ‘› + ğ›½ğ‘› on the diagonal. (b) Show that ğ´ğµ is an upper-triangular matrix with ğ›¼1ğ›½1, â€¦, ğ›¼ğ‘›ğ›½ğ‘› on the diagonal. The results in this exercise are used in the proof of 5.81. 3 Suppose ğ‘‡ âˆˆ â„’(ğ‘‰) is invertible and ğ‘£1, â€¦, ğ‘£ğ‘› is a basis of ğ‘‰ with respect to which the matrix of ğ‘‡ is upper triangular, with ğœ†1, â€¦, ğœ†ğ‘› on the diagonal. Show that the matrix of ğ‘‡âˆ’1 is also upper triangular with respect to the basis ğ‘£1, â€¦, ğ‘£ğ‘›, with 1 ğœ†1 , â€¦, 1 ğœ†ğ‘› on the diagonal. 4 Give an example of an operator whose matrix with respect to some basis contains only 0â€™s on the diagonal, but the operator is invertible. This exercise and the exercise below show that 5.41 fails without the hypoth- esis that an upper-triangular matrix is under consideration. 5 Give an example of an operator whose matrix with respect to some basis contains only nonzero numbers on the diagonal, but the operator is not invertible. 6 Suppose ğ… = ğ‚, ğ‘‰ is finite-dimensional, andğ‘‡ âˆˆ â„’(ğ‘‰). Prove that if ğ‘˜ âˆˆ {1, â€¦, dim ğ‘‰}, then ğ‘‰ has a ğ‘˜-dimensional subspace invariant under ğ‘‡. 7 Suppose ğ‘‰ is finite-dimensional,ğ‘‡ âˆˆ â„’(ğ‘‰), and ğ‘£ âˆˆ ğ‘‰. (a) Prove that there exists a unique monic polynomial ğ‘ğ‘£ of smallest degree such that ğ‘ğ‘£(ğ‘‡)ğ‘£ = 0. (b) Prove that the minimal polynomial of ğ‘‡ is a polynomial multiple of ğ‘ğ‘£. 8 Suppose ğ‘‰ is finite-dimensional,ğ‘‡ âˆˆ â„’(ğ‘‰), and there exists a nonzero vector ğ‘£ âˆˆ ğ‘‰ such that ğ‘‡2ğ‘£ + 2ğ‘‡ğ‘£ = âˆ’2ğ‘£. (a) Prove that if ğ… = ğ‘, then there does not exist a basis of ğ‘‰ with respect to which ğ‘‡ has an upper-triangular matrix. (b) Prove that if ğ… = ğ‚ and ğ´ is an upper-triangular matrix that equals the matrix of ğ‘‡ with respect to some basis of ğ‘‰, then âˆ’1+ ğ‘– or âˆ’1 âˆ’ ğ‘– appears on the diagonal of ğ´. 9 Suppose ğµ is a square matrix with complex entries. Prove that there exists an invertible square matrix ğ´ with complex entries such that ğ´ âˆ’1ğµğ´ is an upper-triangular matrix. 162 Chapter 5 Eigenvalues and Eigenvectors 10 Suppose ğ‘‡ âˆˆ â„’(ğ‘‰) and ğ‘£1, â€¦, ğ‘£ğ‘› is a basis of ğ‘‰. Show that the following are equivalent. (a) The matrix of ğ‘‡ with respect to ğ‘£1, â€¦, ğ‘£ğ‘› is lower triangular. (b) span(ğ‘£ğ‘˜, â€¦, ğ‘£ğ‘›) is invariant under ğ‘‡ for each ğ‘˜ = 1, â€¦, ğ‘›. (c) ğ‘‡ğ‘£ğ‘˜ âˆˆ span(ğ‘£ğ‘˜, â€¦, ğ‘£ğ‘›) for each ğ‘˜ = 1, â€¦, ğ‘›. A square matrix is called lower triangular if all entries above the diagonal are 0. 11 Suppose ğ… = ğ‚ and ğ‘‰ is finite-dimensional. Prove that ifğ‘‡ âˆˆ â„’(ğ‘‰), then there exists a basis of ğ‘‰ with respect to which ğ‘‡ has a lower-triangular matrix. 12 Suppose ğ‘‰ is finite-dimensional,ğ‘‡ âˆˆ â„’(ğ‘‰) has an upper-triangular matrix with respect to some basis of ğ‘‰, and ğ‘ˆ is a subspace of ğ‘‰ that is invariant under ğ‘‡. (a) Prove that ğ‘‡|ğ‘ˆ has an upper-triangular matrix with respect to some basis of ğ‘ˆ. (b) Prove that the quotient operator ğ‘‡/ğ‘ˆ has an upper-triangular matrix with respect to some basis of ğ‘‰/ğ‘ˆ. The quotient operator ğ‘‡/ğ‘ˆ was defined in Exercise 38 in Section 5A. 13 Suppose ğ‘‰ is finite-dimensional andğ‘‡ âˆˆ â„’(ğ‘‰). Suppose there exists a subspace ğ‘ˆ of ğ‘‰ that is invariant under ğ‘‡ such that ğ‘‡|ğ‘ˆ has an upper- triangular matrix with respect to some basis of ğ‘ˆ and also ğ‘‡/ğ‘ˆ has an upper-triangular matrix with respect to some basis of ğ‘‰/ğ‘ˆ. Prove that ğ‘‡ has an upper-triangular matrix with respect to some basis of ğ‘‰. 14 Suppose ğ‘‰ is finite-dimensional andğ‘‡ âˆˆ â„’(ğ‘‰). Prove that ğ‘‡ has an upper- triangular matrix with respect to some basis of ğ‘‰ if and only if the dual operator ğ‘‡â€² has an upper-triangular matrix with respect to some basis of the dual space ğ‘‰â€². Open Access This chapter is licensed under the terms of the Creative Commons Attribution-NonCommercial 4.0 International License (https://creativecommons.org/licenses/by-nc/4.0), which permits any noncommercial use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to original author and source, provide a link to the Creative Commons license, and indicate if changes were made. The images or other third party material in this chapter are included in the chapterâ€™s Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the chapterâ€™s Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. Section 5D Diagonalizable Operators 163 5D Diagonalizable Operators Diagonal Matrices 5.48 definition: diagonal matrix A diagonal matrix is a square matrix that is 0everywhere except possibly on the diagonal. 5.49 example: diagonal matrix â›âœâœâœ â 8 0 0 0 5 0 0 0 5 ââŸâŸâŸ â  is a diagonal matrix. Every diagonal matrix is upper tri- angular. Diagonal matrices typically have many more 0â€™s than most upper- triangular matrices of the same size. If an operator has a diagonal matrix with respect to some basis, then the en- tries on the diagonal are precisely the eigenvalues of the operator; this follows from 5.41 (or find an easier direct proof for diagonal matrices). 5.50 definition: diagonalizable An operator on ğ‘‰ is called diagonalizable if the operator has a diagonal matrix with respect to some basis of ğ‘‰. 5.51 example: diagonalization may require a different basis Defineğ‘‡ âˆˆ â„’(ğ‘2)by ğ‘‡(ğ‘¥, ğ‘¦) = (41ğ‘¥+ 7ğ‘¦, âˆ’20ğ‘¥+ 74ğ‘¦). The matrix of ğ‘‡ with respect to the standard basis of ğ‘2 is ( 41 7 âˆ’20 74 ), which is not a diagonal matrix. However, ğ‘‡ is diagonalizable. Specifically, the matrix of ğ‘‡ with respect to the basis (1, 4), (7, 5)is ( 69 0 0 46 ) because ğ‘‡(1, 4) = (69, 276) = 69(1, 4)and ğ‘‡(7, 5) = (322, 230) = 46(7, 5). 164 Chapter 5 Eigenvalues and Eigenvectors For ğœ† âˆˆ ğ…, we will find it convenient to have a name and a notation for the set of vectors that an operator ğ‘‡ maps to ğœ† times the vector. 5.52 definition: eigenspace, ğ¸(ğœ†, ğ‘‡) Suppose ğ‘‡ âˆˆ â„’(ğ‘‰) and ğœ† âˆˆ ğ…. The eigenspace of ğ‘‡ corresponding to ğœ† is the subspace ğ¸(ğœ†, ğ‘‡) of ğ‘‰ defined by ğ¸(ğœ†, ğ‘‡) = null(ğ‘‡ âˆ’ ğœ†ğ¼) = {ğ‘£ âˆˆ ğ‘‰ âˆ¶ ğ‘‡ğ‘£ = ğœ†ğ‘£}. Hence ğ¸(ğœ†, ğ‘‡) is the set of all eigenvectors of ğ‘‡ corresponding to ğœ†, along with the 0vector. For ğ‘‡ âˆˆ â„’(ğ‘‰) and ğœ† âˆˆ ğ…, the set ğ¸(ğœ†, ğ‘‡) is a subspace of ğ‘‰ because the null space of each linear map on ğ‘‰ is a subspace of ğ‘‰. The definitions imply thatğœ† is an eigenvalue of ğ‘‡ if and only if ğ¸(ğœ†, ğ‘‡) â‰  {0}. 5.53 example: eigenspaces of an operator Suppose the matrix of an operator ğ‘‡ âˆˆ â„’(ğ‘‰) with respect to a basis ğ‘£1, ğ‘£2, ğ‘£3 of ğ‘‰ is the matrix in Example 5.49. Then ğ¸(8, ğ‘‡) = span(ğ‘£1), ğ¸(5, ğ‘‡) = span(ğ‘£2, ğ‘£3). If ğœ† is an eigenvalue of an operator ğ‘‡ âˆˆ â„’(ğ‘‰), then ğ‘‡ restricted to ğ¸(ğœ†, ğ‘‡) is just the operator of multiplication by ğœ†. 5.54 sum of eigenspaces is a direct sum Suppose ğ‘‡ âˆˆ â„’(ğ‘‰) and ğœ†1, â€¦, ğœ†ğ‘š are distinct eigenvalues of ğ‘‡. Then ğ¸(ğœ†1, ğ‘‡) + â‹¯ + ğ¸(ğœ†ğ‘š, ğ‘‡) is a direct sum. Furthermore, if ğ‘‰ is finite-dimensional, then dim ğ¸(ğœ†1, ğ‘‡) + â‹¯ + dim ğ¸(ğœ†ğ‘š, ğ‘‡) â‰¤dim ğ‘‰. Proof To show that ğ¸(ğœ†1, ğ‘‡) + â‹¯ + ğ¸(ğœ†ğ‘š, ğ‘‡) is a direct sum, suppose ğ‘£1 + â‹¯ + ğ‘£ğ‘š = 0, where each ğ‘£ğ‘˜ is in ğ¸(ğœ†ğ‘˜, ğ‘‡). Because eigenvectors corresponding to distinct eigenvalues are linearly independent (by 5.11), this implies that each ğ‘£ğ‘˜ equals 0. Thus ğ¸(ğœ†1, ğ‘‡) + â‹¯ + ğ¸(ğœ†ğ‘š, ğ‘‡) is a direct sum (by 1.45), as desired. Now suppose ğ‘‰ is finite-dimensional. Then dim ğ¸(ğœ†1, ğ‘‡) + â‹¯ + dim ğ¸(ğœ†ğ‘š, ğ‘‡) = dim(ğ¸(ğœ†1, ğ‘‡) âŠ• â‹¯ âŠ• ğ¸(ğœ†ğ‘š, ğ‘‡)) â‰¤dim ğ‘‰, where the first line follows from3.94 and the second line follows from 2.37. Section 5D Diagonalizable Operators 165 Conditions for Diagonalizability The following characterizations of diagonalizable operators will be useful. 5.55 conditions equivalent to diagonalizability Suppose ğ‘‰ is finite-dimensional andğ‘‡ âˆˆ â„’(ğ‘‰). Let ğœ†1, â€¦, ğœ†ğ‘š denote the distinct eigenvalues of ğ‘‡. Then the following are equivalent. (a) ğ‘‡ is diagonalizable. (b) ğ‘‰ has a basis consisting of eigenvectors of ğ‘‡. (c) ğ‘‰ = ğ¸(ğœ†1, ğ‘‡) âŠ• â‹¯ âŠ• ğ¸(ğœ†ğ‘š, ğ‘‡). (d) dim ğ‘‰ = dim ğ¸(ğœ†1, ğ‘‡) + â‹¯ + dim ğ¸(ğœ†ğ‘š, ğ‘‡). Proof An operator ğ‘‡ âˆˆ â„’(ğ‘‰) has a diagonal matrix â›âœâœâœ â ğœ†1 0 â‹± 0 ğœ†ğ‘› ââŸâŸâŸ â  with respect to a basis ğ‘£1, â€¦, ğ‘£ğ‘› of ğ‘‰ if and only if ğ‘‡ğ‘£ğ‘˜ = ğœ†ğ‘˜ğ‘£ğ‘˜ for each ğ‘˜. Thus (a) and (b) are equivalent. Suppose (b) holds; thus ğ‘‰ has a basis consisting of eigenvectors of ğ‘‡. Hence every vector in ğ‘‰ is a linear combination of eigenvectors of ğ‘‡, which implies that ğ‘‰ = ğ¸(ğœ†1, ğ‘‡) + â‹¯ + ğ¸(ğœ†ğ‘š, ğ‘‡). Now 5.54 shows that (c) holds, proving that (b) implies (c). That (c) implies (d) follows immediately from 3.94. Finally, suppose (d) holds; thus 5.56 dim ğ‘‰ = dim ğ¸(ğœ†1, ğ‘‡) + â‹¯ + dim ğ¸(ğœ†ğ‘š, ğ‘‡). Choose a basis of each ğ¸(ğœ†ğ‘˜, ğ‘‡); put all these bases together to form a list ğ‘£1, â€¦, ğ‘£ğ‘› of eigenvectors of ğ‘‡, where ğ‘› = dim ğ‘‰ (by 5.56). To show that this list is linearly independent, suppose ğ‘1ğ‘£1 + â‹¯ + ğ‘ğ‘›ğ‘£ğ‘› = 0, where ğ‘1, â€¦, ğ‘ğ‘› âˆˆ ğ…. For each ğ‘˜ = 1, â€¦, ğ‘š, let ğ‘¢ğ‘˜ denote the sum of all the terms ğ‘ğ‘—ğ‘£ğ‘— such that ğ‘£ğ‘— âˆˆ ğ¸(ğœ†ğ‘˜, ğ‘‡). Thus each ğ‘¢ğ‘˜ is in ğ¸(ğœ†ğ‘˜, ğ‘‡), and ğ‘¢1 + â‹¯ + ğ‘¢ğ‘š = 0. Because eigenvectors corresponding to distinct eigenvalues are linearly indepen- dent (see 5.11), this implies that each ğ‘¢ğ‘˜ equals 0. Because each ğ‘¢ğ‘˜ is a sum of terms ğ‘ğ‘—ğ‘£ğ‘—, where the ğ‘£ğ‘—â€™s were chosen to be a basis of ğ¸(ğœ†ğ‘˜, ğ‘‡), this implies that all ğ‘ğ‘—â€™s equal 0. Thus ğ‘£1, â€¦, ğ‘£ğ‘› is linearly independent and hence is a basis of ğ‘‰ (by 2.38). Thus (d) implies (b), completing the proof. For additional conditions equivalent to diagonalizability, see 5.62, Exercises 5 and 15 in this section, Exercise 24 in Section 7B, and Exercise 15 in Section 8A. 166 Chapter 5 Eigenvalues and Eigenvectors As we know, every operator on a finite-dimensional complex vector space has an eigenvalue. However, not every operator on a finite-dimensional complex vector space has enough eigenvectors to be diagonalizable, as shown by the next example. 5.57 example:an operator that is not diagonalizable Define an operatorğ‘‡ âˆˆ â„’(ğ…3)by ğ‘‡(ğ‘, ğ‘, ğ‘) = (ğ‘, ğ‘, 0). The matrix of ğ‘‡ with respect to the standard basis of ğ…3 is â›âœâœâœ â 0 1 0 0 0 1 0 0 0 ââŸâŸâŸ â  , which is an upper-triangular matrix but is not a diagonal matrix. As you should verify, 0is the only eigenvalue of ğ‘‡ and furthermore ğ¸(0, ğ‘‡) = {(ğ‘, 0, 0) âˆˆ ğ… 3 âˆ¶ ğ‘ âˆˆ ğ…}. Hence conditions (b), (c), and (d) of 5.55 fail (of course, because these conditions are equivalent, it is sufficient to check that only one of them fails). Thus condition (a) of 5.55 also fails. Hence ğ‘‡ is not diagonalizable, regardless of whether ğ… = ğ‘ or ğ… = ğ‚. The next result shows that if an operator has as many distinct eigenvalues as the dimension of its domain, then the operator is diagonalizable. 5.58 enough eigenvalues implies diagonalizability Suppose ğ‘‰ is finite-dimensional andğ‘‡ âˆˆ â„’(ğ‘‰) has dim ğ‘‰ distinct eigenvalues. Then ğ‘‡ is diagonalizable. Proof Suppose ğ‘‡ has distinct eigenvalues ğœ†1, â€¦, ğœ†dim ğ‘‰. For each ğ‘˜, let ğ‘£ğ‘˜ âˆˆ ğ‘‰ be an eigenvector corresponding to the eigenvalue ğœ†ğ‘˜. Because eigenvectors corre- sponding to distinct eigenvalues are linearly independent (see 5.11), ğ‘£1, â€¦, ğ‘£dim ğ‘‰ is linearly independent. A linearly independent list of dim ğ‘‰ vectors in ğ‘‰ is a basis of ğ‘‰ (see 2.38); thus ğ‘£1, â€¦, ğ‘£dim ğ‘‰ is a basis of ğ‘‰. With respect to this basis consisting of eigenvectors, ğ‘‡ has a diagonal matrix. In later chapters we will find additional conditions that imply that certain operators are diagonalizable. For example, see the real spectral theorem (7.29) and the complex spectral theorem (7.31). The result above gives a sufficient condition for an operator to be diagonal- izable. However, this condition is not necessary. For example, the operator ğ‘‡ on ğ…3 defined byğ‘‡(ğ‘¥, ğ‘¦, ğ‘§) = (6ğ‘¥, 6ğ‘¦, 7ğ‘§)has only two eigenvalues (6and 7) and dim ğ…3 = 3, but ğ‘‡ is diagonalizable (by the standard basis of ğ…3). Section 5D Diagonalizable Operators 167 For a spectacular application of these techniques, see Exercise 21, which shows how to use diagonalization to find an exact formula for the ğ‘›th term of the Fibonacci sequence. The next example illustrates the im- portance of diagonalization, which can be used to compute high powers of an operator, taking advantage of the equa- tion ğ‘‡ğ‘˜ğ‘£ = ğœ†ğ‘˜ğ‘£ if ğ‘£ is an eigenvector of ğ‘‡ with eigenvalue ğœ†. 5.59 example: using diagonalization to compute ğ‘‡100 Defineğ‘‡ âˆˆ â„’(ğ…3)by ğ‘‡(ğ‘¥, ğ‘¦, ğ‘§) = (2ğ‘¥+ ğ‘¦, 5ğ‘¦+ 3ğ‘§, 8ğ‘§). With respect to the standard basis, the matrix of ğ‘‡ is â›âœâœâœ â 2 1 0 0 5 3 0 0 8 ââŸâŸâŸ â  . The matrix above is an upper-triangular matrix but it is not a diagonal matrix. By 5.41, the eigenvalues of ğ‘‡ are 2, 5, and 8. Because ğ‘‡ is an operator on a vector space of dimension three and ğ‘‡ has three distinct eigenvalues, 5.58 assures us that there exists a basis of ğ…3 with respect to which ğ‘‡ has a diagonal matrix. To find this basis, we only have to find an eigenvector for each eigenvalue. In other words, we have to find a nonzero solution to the equation ğ‘‡(ğ‘¥, ğ‘¦, ğ‘§) = ğœ†(ğ‘¥, ğ‘¦, ğ‘§) for ğœ† = 2, then for ğœ† = 5, and then for ğœ† = 8. Solving these simple equations shows that for ğœ† = 2we have an eigenvector (1, 0, 0), for ğœ† = 5we have an eigenvector (1, 3, 0), and for ğœ† = 8we have an eigenvector (1, 6, 6). Thus (1, 0, 0), (1, 3, 0), (1, 6, 6)is a basis of ğ…3 consisting of eigenvectors of ğ‘‡, and with respect to this basis the matrix of ğ‘‡ is the diagonal matrix â›âœâœâœ â 2 0 0 0 5 0 0 0 8 ââŸâŸâŸ â  . To compute ğ‘‡100(0, 0, 1), for example, write (0, 0, 1)as a linear combination of our basis of eigenvectors: (0, 0, 1) = 1 6 (1, 0, 0) âˆ’ 1 3 (1, 3, 0)+ 1 6 (1, 6, 6). Now apply ğ‘‡100 to both sides of the equation above, getting ğ‘‡100(0, 0, 1) = 1 6 (ğ‘‡100(1, 0, 0))âˆ’ 1 3 (ğ‘‡100(1, 3, 0))+ 1 6 (ğ‘‡100(1, 6, 6)) = 1 6 (2 100(1, 0, 0) âˆ’ 2 â‹… 5 100(1, 3, 0)+ 8 100(1, 6, 6)) = 1 6 (2 100 âˆ’ 2 â‹… 5 100 + 8 100, 6 â‹… 8 100 âˆ’ 6 â‹… 5 100, 6 â‹… 8 100). 168 Chapter 5 Eigenvalues and Eigenvectors We saw earlier that an operator ğ‘‡ on a finite-dimensional vector spaceğ‘‰ has an upper-triangular matrix with respect to some basis of ğ‘‰ if and only if the minimal polynomial of ğ‘‡ equals (ğ‘§ âˆ’ ğœ†1)â‹¯(ğ‘§ âˆ’ ğœ†ğ‘š) for some ğœ†1, â€¦, ğœ†ğ‘š âˆˆ ğ… (see 5.44). As we previously noted (see 5.47), this condition is always satisfied ifğ… = ğ‚. Our next result 5.62 states that an operator ğ‘‡ âˆˆ â„’(ğ‘‰) has a diagonal matrix with respect to some basis of ğ‘‰ if and only if the minimal polynomial of ğ‘‡ equals (ğ‘§ âˆ’ ğœ†1)â‹¯(ğ‘§ âˆ’ ğœ†ğ‘š) for some distinct ğœ†1, â€¦, ğœ†ğ‘š âˆˆ ğ…. Before formally stating this result, we give two examples of using it. 5.60 example:diagonalizable, but with no known exact eigenvalues Defineğ‘‡ âˆˆ â„’(ğ‚ 5)by ğ‘‡(ğ‘§1, ğ‘§2, ğ‘§3, ğ‘§4, ğ‘§5) = (âˆ’3ğ‘§5, ğ‘§1 + 6ğ‘§5, ğ‘§2, ğ‘§3, ğ‘§4). The matrix of ğ‘‡ is shown in Example 5.26, where we showed that the minimal polynomial of ğ‘‡ is 3 âˆ’ 6ğ‘§+ ğ‘§ 5. As mentioned in Example 5.28, no exact expression is known for any of the zeros of this polynomial, but numeric techniques show that the zeros of this polynomial are approximately âˆ’1.67, 0.51, 1.40, âˆ’0.12+ 1.59ğ‘–, âˆ’0.12 âˆ’ 1.59ğ‘–. The software that produces these approximations is accurate to more than three digits. Thus these approximations are good enough to show that the five numbers above are distinct. The minimal polynomial of ğ‘‡ equals the fifth degree monic polynomial with these zeros. Now 5.62 shows that ğ‘‡ is diagonalizable. 5.61 example:showing that an operator is not diagonalizable Defineğ‘‡ âˆˆ â„’(ğ…3)by ğ‘‡(ğ‘§1, ğ‘§2, ğ‘§3) = (6ğ‘§1 + 3ğ‘§2 + 4ğ‘§3, 6ğ‘§2 + 2ğ‘§3, 7ğ‘§3). The matrix of ğ‘‡ with respect to the standard basis of ğ…3 is â›âœâœâœ â 6 3 4 0 6 2 0 0 7 ââŸâŸâŸ â  . The matrix above is an upper-triangular matrix but is not a diagonal matrix. Might ğ‘‡ have a diagonal matrix with respect to some other basis of ğ…3? To answer this question, we will find the minimal polynomial ofğ‘‡. First note that the eigenvalues of ğ‘‡ are the diagonal entries of the matrix above (by 5.41). Thus the zeros of the minimal polynomial of ğ‘‡ are 6, 7[by 5.27(a)]. The diagonal of the matrix above tells us that (ğ‘‡ âˆ’ 6ğ¼) 2(ğ‘‡ âˆ’ 7ğ¼) = 0(by 5.40). The minimal polynomial of ğ‘‡ has degree at most 3(by 5.22). Putting all this together, we see that the minimal polynomial of ğ‘‡ is either (ğ‘§ âˆ’ 6)(ğ‘§ âˆ’ 7)or (ğ‘§ âˆ’ 6) 2(ğ‘§ âˆ’ 7). A simple computation shows that (ğ‘‡ âˆ’ 6ğ¼)(ğ‘‡ âˆ’ 7ğ¼) â‰  0. Thus the minimal polynomial of ğ‘‡ is (ğ‘§ âˆ’ 6) 2(ğ‘§ âˆ’ 7). Now 5.62 shows that ğ‘‡ is not diagonalizable. Section 5D Diagonalizable Operators 169 5.62 necessary and sufficient condition for diagonalizability Suppose ğ‘‰ is finite-dimensional andğ‘‡ âˆˆ â„’(ğ‘‰). Then ğ‘‡ is diagonalizable if and only if the minimal polynomial of ğ‘‡ equals (ğ‘§ âˆ’ ğœ†1)â‹¯(ğ‘§ âˆ’ ğœ†ğ‘š) for some list of distinct numbers ğœ†1, â€¦, ğœ†ğ‘š âˆˆ ğ…. Proof First suppose ğ‘‡ is diagonalizable. Thus there is a basis ğ‘£1, â€¦, ğ‘£ğ‘› of ğ‘‰ consisting of eigenvectors of ğ‘‡. Let ğœ†1, â€¦, ğœ†ğ‘š be the distinct eigenvalues of ğ‘‡. Then for each ğ‘£ğ‘—, there exists ğœ†ğ‘˜ with (ğ‘‡ âˆ’ ğœ†ğ‘˜ğ¼)ğ‘£ğ‘— = 0. Thus (ğ‘‡ âˆ’ ğœ†1ğ¼)â‹¯(ğ‘‡ âˆ’ ğœ†ğ‘šğ¼)ğ‘£ğ‘— = 0, which implies that the minimal polynomial of ğ‘‡ equals (ğ‘§ âˆ’ ğœ†1)â‹¯(ğ‘§ âˆ’ ğœ†ğ‘š). To prove the implication in the other direction, now suppose the minimal polynomial of ğ‘‡ equals (ğ‘§ âˆ’ ğœ†1)â‹¯(ğ‘§ âˆ’ ğœ†ğ‘š) for some list of distinct numbers ğœ†1, â€¦, ğœ†ğ‘š âˆˆ ğ…. Thus 5.63 (ğ‘‡ âˆ’ ğœ†1ğ¼)â‹¯(ğ‘‡ âˆ’ ğœ†ğ‘šğ¼) = 0. We will prove that ğ‘‡ is diagonalizable by induction on ğ‘š. To get started, suppose ğ‘š = 1. Then ğ‘‡ âˆ’ ğœ†1ğ¼ = 0, which means that ğ‘‡ is a scalar multiple of the identity operator, which implies that ğ‘‡ is diagonalizable. Now suppose that ğ‘š > 1and the desired result holds for all smaller values of ğ‘š. The subspace range(ğ‘‡ âˆ’ ğœ†ğ‘šğ¼) is invariant under ğ‘‡ [this is a special case of 5.18 with ğ‘(ğ‘§) = ğ‘§ âˆ’ ğœ†ğ‘š]. Thus ğ‘‡ restricted to range(ğ‘‡ âˆ’ ğœ†ğ‘šğ¼) is an operator on range(ğ‘‡ âˆ’ ğœ†ğ‘šğ¼). If ğ‘¢ âˆˆ range(ğ‘‡ âˆ’ ğœ†ğ‘šğ¼), then ğ‘¢ = (ğ‘‡ âˆ’ ğœ†ğ‘šğ¼)ğ‘£ for some ğ‘£ âˆˆ ğ‘‰, and 5.63 implies 5.64 (ğ‘‡ âˆ’ ğœ†1ğ¼)â‹¯(ğ‘‡ âˆ’ ğœ†ğ‘š âˆ’ 1ğ¼)ğ‘¢ = (ğ‘‡ âˆ’ ğœ†1ğ¼)â‹¯(ğ‘‡ âˆ’ ğœ†ğ‘šğ¼)ğ‘£ = 0. Hence (ğ‘§ âˆ’ ğœ†1)â‹¯(ğ‘§ âˆ’ ğœ†ğ‘š âˆ’ 1) is a polynomial multiple of the minimal polynomial of ğ‘‡ restricted to range(ğ‘‡ âˆ’ ğœ†ğ‘šğ¼) [by5.29]. Thus by our induction hypothesis, there is a basis of range(ğ‘‡ âˆ’ ğœ†ğ‘šğ¼) consisting of eigenvectors of ğ‘‡. Suppose that ğ‘¢ âˆˆ range(ğ‘‡ âˆ’ ğœ†ğ‘šğ¼) âˆ©null(ğ‘‡ âˆ’ ğœ†ğ‘šğ¼). Then ğ‘‡ğ‘¢ = ğœ†ğ‘šğ‘¢. Now 5.64 implies that 0 = (ğ‘‡ âˆ’ ğœ†1ğ¼)â‹¯(ğ‘‡ âˆ’ ğœ†ğ‘š âˆ’ 1ğ¼)ğ‘¢ = (ğœ†ğ‘š âˆ’ ğœ†1)â‹¯(ğœ†ğ‘š âˆ’ ğœ†ğ‘š âˆ’ 1)ğ‘¢. Because ğœ†1, â€¦, ğœ†ğ‘š are distinct, the equation above implies that ğ‘¢ = 0. Hence range(ğ‘‡ âˆ’ ğœ†ğ‘šğ¼) âˆ©null(ğ‘‡ âˆ’ ğœ†ğ‘šğ¼) = {0}. Thus range(ğ‘‡âˆ’ ğœ†ğ‘šğ¼)+null(ğ‘‡âˆ’ ğœ†ğ‘šğ¼) is a direct sum (by 1.46) whose dimension is dim ğ‘‰ (by 3.94 and 3.21). Hence range(ğ‘‡ âˆ’ ğœ†ğ‘šğ¼) âŠ• null(ğ‘‡ âˆ’ ğœ†ğ‘šğ¼) = ğ‘‰. Every vector in null(ğ‘‡ âˆ’ ğœ†ğ‘šğ¼) is an eigenvector of ğ‘‡ with eigenvalue ğœ†ğ‘š. Earlier in this proof we saw that there is a basis of range(ğ‘‡ âˆ’ ğœ†ğ‘šğ¼) consisting of eigenvectors of ğ‘‡. Adjoining to that basis a basis of null(ğ‘‡ âˆ’ ğœ†ğ‘šğ¼) gives a basis of ğ‘‰ consisting of eigenvectors of ğ‘‡. The matrix of ğ‘‡ with respect to this basis is a diagonal matrix, as desired. 170 Chapter 5 Eigenvalues and Eigenvectors No formula exists for the zeros of polynomials of degree 5or greater. However, the previous result can be used to determine whether an operator on a complex vector space is diagonalizable without even finding approximations of the zeros of the minimal polynomialâ€”see Exercise 15. The next result will be a key tool when we prove a result about the simul- taneous diagonalization of two operators; see 5.76. Note how the use of the characterization of diagonalizable operators in terms of the minimal polynomial (see 5.62) leads to a short proof of the next result. 5.65 restriction of diagonalizable operator to invariant subspace Suppose ğ‘‡ âˆˆ â„’(ğ‘‰) is diagonalizable and ğ‘ˆ is a subspace of ğ‘‰ that is invariant under ğ‘‡. Then ğ‘‡|ğ‘ˆ is a diagonalizable operator on ğ‘ˆ. Proof Because the operator ğ‘‡ is diagonalizable, the minimal polynomial of ğ‘‡ equals (ğ‘§ âˆ’ ğœ†1)â‹¯(ğ‘§ âˆ’ ğœ†ğ‘š) for some list of distinct numbers ğœ†1, â€¦, ğœ†ğ‘š âˆˆ ğ… (by 5.62). The minimal polynomial of ğ‘‡ is a polynomial multiple of the minimal polynomial of ğ‘‡|ğ‘ˆ (by 5.31). Hence the minimal polynomial of ğ‘‡|ğ‘ˆ has the form required by 5.62, which shows that ğ‘‡|ğ‘ˆ is diagonalizable. Gershgorin Disk Theorem 5.66 definition:Gershgorin disks Suppose ğ‘‡ âˆˆ â„’(ğ‘‰) and ğ‘£1, â€¦, ğ‘£ğ‘› is a basis of ğ‘‰. Let ğ´ denote the matrix of ğ‘‡ with respect to this basis. A Gershgorin disk of ğ‘‡ with respect to the basis ğ‘£1, â€¦, ğ‘£ğ‘› is a set of the form {ğ‘§ âˆˆ ğ… âˆ¶ |ğ‘§ âˆ’ ğ´ğ‘—, ğ‘—| â‰¤ ğ‘› âˆ‘ ğ‘˜ = 1 ğ‘˜ â‰  ğ‘— |ğ´ğ‘—, ğ‘˜|}, where ğ‘— âˆˆ {1, â€¦, ğ‘›}. Because there are ğ‘› choices for ğ‘— in the definition above,ğ‘‡ has ğ‘› Gershgorin disks. If ğ… = ğ‚, then for each ğ‘— âˆˆ {1, â€¦, ğ‘›}, the corresponding Gershgorin disk is a closed disk in ğ‚ centered at ğ´ğ‘—, ğ‘—, which is the ğ‘—th entry on the diagonal of ğ´. The radius of this closed disk is the sum of the absolute values of the entries in row ğ‘— of ğ´, excluding the diagonal entry. If ğ… = ğ‘, then the Gershgorin disks are closed intervals in ğ‘. In the special case that the square matrix ğ´ above is a diagonal matrix, each Gershgorin disk consists of a single point that is a diagonal entry of ğ´ (and each eigenvalue of ğ‘‡ is one of those points, as required by the next result). One consequence of our next result is that if the nondiagonal entries of ğ´ are small, then each eigenvalue of ğ‘‡ is near a diagonal entry of ğ´. Section 5D Diagonalizable Operators 171 5.67 Gershgorin disk theorem Suppose ğ‘‡ âˆˆ â„’(ğ‘‰) and ğ‘£1, â€¦, ğ‘£ğ‘› is a basis of ğ‘‰. Then each eigenvalue of ğ‘‡ is contained in some Gershgorin disk of ğ‘‡ with respect to the basis ğ‘£1, â€¦, ğ‘£ğ‘›. Proof Suppose ğœ† âˆˆ ğ… is an eigenvalue of ğ‘‡. Let ğ‘¤ âˆˆ ğ‘‰ be a corresponding eigenvector. There exist ğ‘1, â€¦, ğ‘ğ‘› âˆˆ ğ… such that 5.68 ğ‘¤ = ğ‘1ğ‘£1 + â‹¯ + ğ‘ğ‘›ğ‘£ğ‘›. Let ğ´ denote the matrix of ğ‘‡ with respect to the basis ğ‘£1, â€¦, ğ‘£ğ‘›. Applying ğ‘‡ to both sides of the equation above gives ğœ†ğ‘¤ = ğ‘› âˆ‘ ğ‘˜ = 1 ğ‘ğ‘˜ğ‘‡ğ‘£ğ‘˜5.69 = ğ‘› âˆ‘ ğ‘˜ = 1 ğ‘ğ‘˜ ğ‘› âˆ‘ ğ‘— = 1 ğ´ğ‘—, ğ‘˜ğ‘£ğ‘— = ğ‘› âˆ‘ ğ‘— = 1( ğ‘› âˆ‘ ğ‘˜ = 1 ğ´ğ‘—, ğ‘˜ğ‘ğ‘˜)ğ‘£ğ‘—.5.70 Let ğ‘— âˆˆ {1, â€¦, ğ‘›} be such that |ğ‘ğ‘—| = max{|ğ‘1|, â€¦, |ğ‘ğ‘›|}. Using 5.68, we see that the coefficient of ğ‘£ğ‘— on the left side of 5.69 equals ğœ†ğ‘ğ‘—, which must equal the coefficient of ğ‘£ğ‘— on the right side of 5.70. In other words, ğœ†ğ‘ğ‘— = ğ‘› âˆ‘ ğ‘˜ = 1 ğ´ğ‘—, ğ‘˜ ğ‘ğ‘˜. Subtract ğ´ğ‘—, ğ‘— ğ‘ğ‘— from each side of the equation above and then divide both sides by ğ‘ğ‘— to get |ğœ† âˆ’ ğ´ğ‘—, ğ‘—| = âˆ£ ğ‘› âˆ‘ ğ‘˜ = 1 ğ‘˜ â‰  ğ‘— ğ´ğ‘—, ğ‘˜ ğ‘ğ‘˜ ğ‘ğ‘— âˆ£ â‰¤ ğ‘› âˆ‘ ğ‘˜ = 1 ğ‘˜ â‰  ğ‘— |ğ´ğ‘—, ğ‘˜|. Thus ğœ† is in the ğ‘—th Gershgorin disk with respect to the basis ğ‘£1, â€¦, ğ‘£ğ‘›. The Gershgorin disk theorem is named for Semyon Aronovich Gershgorin, who published this result in 1931. Exercise 22 gives a nice application of the Gershgorin disk theorem. Exercise 23 states that the radius of each Gershgorin disk could be changed to the sum of the absolute values of corresponding column entries (instead of row entries), excluding the diagonal entry, and the theorem above would still hold. 172 Chapter 5 Eigenvalues and Eigenvectors Exercises 5D 1 Suppose ğ‘‰ is a finite-dimensional complex vector space andğ‘‡ âˆˆ â„’(ğ‘‰). (a) Prove that if ğ‘‡4 = ğ¼, then ğ‘‡ is diagonalizable. (b) Prove that if ğ‘‡4 = ğ‘‡, then ğ‘‡ is diagonalizable. (c) Give an example of an operator ğ‘‡ âˆˆ â„’(ğ‚ 2)such that ğ‘‡4 = ğ‘‡2 and ğ‘‡ is not diagonalizable. 2 Suppose ğ‘‡ âˆˆ â„’(ğ‘‰) has a diagonal matrix ğ´ with respect to some basis of ğ‘‰. Prove that if ğœ† âˆˆ ğ…, then ğœ† appears on the diagonal of ğ´ precisely dim ğ¸(ğœ†, ğ‘‡) times. 3 Suppose ğ‘‰ is finite-dimensional andğ‘‡ âˆˆ â„’(ğ‘‰). Prove that if the operator ğ‘‡ is diagonalizable, then ğ‘‰ = null ğ‘‡ âŠ• range ğ‘‡. 4 Suppose ğ‘‰ is finite-dimensional andğ‘‡ âˆˆ â„’(ğ‘‰). Prove that the following are equivalent. (a) ğ‘‰ = null ğ‘‡ âŠ• range ğ‘‡. (b) ğ‘‰ = null ğ‘‡ + range ğ‘‡. (c) null ğ‘‡ âˆ©range ğ‘‡ = {0}. 5 Suppose ğ‘‰ is a finite-dimensional complex vector space andğ‘‡ âˆˆ â„’(ğ‘‰). Prove that ğ‘‡ is diagonalizable if and only if ğ‘‰ = null(ğ‘‡ âˆ’ ğœ†ğ¼) âŠ• range(ğ‘‡ âˆ’ ğœ†ğ¼) for every ğœ† âˆˆ ğ‚. 6 Suppose ğ‘‡ âˆˆ â„’(ğ…5)and dim ğ¸(8, ğ‘‡) = 4. Prove that ğ‘‡ âˆ’ 2ğ¼or ğ‘‡ âˆ’ 6ğ¼is invertible. 7 Suppose ğ‘‡ âˆˆ â„’(ğ‘‰) is invertible. Prove that ğ¸(ğœ†, ğ‘‡) = ğ¸( 1 ğœ† , ğ‘‡âˆ’1) for every ğœ† âˆˆ ğ… with ğœ† â‰  0. 8 Suppose ğ‘‰ is finite-dimensional andğ‘‡ âˆˆ â„’(ğ‘‰). Let ğœ†1, â€¦, ğœ†ğ‘š denote the distinct nonzero eigenvalues of ğ‘‡. Prove that dim ğ¸(ğœ†1, ğ‘‡) + â‹¯ + dim ğ¸(ğœ†ğ‘š, ğ‘‡) â‰¤dim range ğ‘‡. 9 Suppose ğ‘…, ğ‘‡ âˆˆ â„’(ğ…3)each have 2, 6, 7as eigenvalues. Prove that there exists an invertible operator ğ‘† âˆˆ â„’(ğ…3)such that ğ‘… = ğ‘†âˆ’1ğ‘‡ğ‘†. 10 Find ğ‘…, ğ‘‡ âˆˆ â„’(ğ…4)such that ğ‘… and ğ‘‡ each have 2, 6, 7as eigenvalues, ğ‘… and ğ‘‡ have no other eigenvalues, and there does not exist an invertible operator ğ‘† âˆˆ â„’(ğ…4)such that ğ‘… = ğ‘†âˆ’1ğ‘‡ğ‘†. Section 5D Diagonalizable Operators 173 11 Find ğ‘‡ âˆˆ â„’(ğ‚ 3)such that 6and 7are eigenvalues of ğ‘‡ and such that ğ‘‡ does not have a diagonal matrix with respect to any basis of ğ‚ 3. 12 Suppose ğ‘‡ âˆˆ â„’(ğ‚ 3)is such that 6and 7are eigenvalues of ğ‘‡. Furthermore, suppose ğ‘‡ does not have a diagonal matrix with respect to any basis of ğ‚ 3. Prove that there exists (ğ‘§1, ğ‘§2, ğ‘§3) âˆˆ ğ‚3 such that ğ‘‡(ğ‘§1, ğ‘§2, ğ‘§3) = (6+ 8ğ‘§1, 7+ 8ğ‘§2, 13+ 8ğ‘§3). 13 Suppose ğ´ is a diagonal matrix with distinct entries on the diagonal and ğµ is a matrix of the same size as ğ´. Show that ğ´ğµ = ğµğ´ if and only if ğµ is a diagonal matrix. 14 (a) Give an example of a finite-dimensional complex vector space and an operator ğ‘‡ on that vector space such that ğ‘‡2 is diagonalizable but ğ‘‡ is not diagonalizable. (b) Suppose ğ… = ğ‚, ğ‘˜ is a positive integer, and ğ‘‡ âˆˆ â„’(ğ‘‰) is invertible. Prove that ğ‘‡ is diagonalizable if and only if ğ‘‡ğ‘˜ is diagonalizable. 15 Suppose ğ‘‰ is a finite-dimensional complex vector space,ğ‘‡ âˆˆ â„’(ğ‘‰), and ğ‘ is the minimal polynomial of ğ‘‡. Prove that the following are equivalent. (a) ğ‘‡ is diagonalizable. (b) There does not exist ğœ† âˆˆ ğ‚ such that ğ‘ is a polynomial multiple of (ğ‘§ âˆ’ ğœ†)2. (c) ğ‘ and its derivative ğ‘ â€² have no zeros in common. (d) The greatest common divisor of ğ‘ and ğ‘ â€² is the constant polynomial 1. The greatest common divisor of ğ‘ and ğ‘ â€² is the monic polynomial ğ‘ of largest degree such that ğ‘ and ğ‘ â€² are both polynomial multiples of ğ‘. The Euclidean algorithm for polynomials (look it up) can quickly determine the greatest common divisor of two polynomials, without requiring any information about the zeros of the polynomials. Thus the equivalence of (a) and (d) above shows that we can determine whether ğ‘‡ is diagonalizable without knowing anything about the zeros of ğ‘. 16 Suppose that ğ‘‡ âˆˆ â„’(ğ‘‰) is diagonalizable. Let ğœ†1, â€¦, ğœ†ğ‘š denote the distinct eigenvalues of ğ‘‡. Prove that a subspace ğ‘ˆ of ğ‘‰ is invariant under ğ‘‡ if and only if there exist subspaces ğ‘ˆ1, â€¦, ğ‘ˆğ‘š of ğ‘‰ such that ğ‘ˆğ‘˜ âŠ† ğ¸(ğœ†ğ‘˜, ğ‘‡) for each ğ‘˜ and ğ‘ˆ = ğ‘ˆ1 âŠ• â‹¯ âŠ• ğ‘ˆğ‘š. 17 Suppose ğ‘‰ is finite-dimensional. Prove thatâ„’(ğ‘‰) has a basis consisting of diagonalizable operators. 18 Suppose that ğ‘‡ âˆˆ â„’(ğ‘‰) is diagonalizable and ğ‘ˆ is a subspace of ğ‘‰ that is invariant under ğ‘‡. Prove that the quotient operator ğ‘‡/ğ‘ˆ is a diagonalizable operator on ğ‘‰/ğ‘ˆ. The quotient operator ğ‘‡/ğ‘ˆ was defined in Exercise 38 in Section 5A. 174 Chapter 5 Eigenvalues and Eigenvectors 19 Prove or give a counterexample: If ğ‘‡ âˆˆ â„’(ğ‘‰) and there exists a subspace ğ‘ˆ of ğ‘‰ that is invariant under ğ‘‡ such that ğ‘‡|ğ‘ˆ and ğ‘‡/ğ‘ˆ are both diagonalizable, then ğ‘‡ is diagonalizable. See Exercise 13 in Section 5C for an analogous statement about upper- triangular matrices. 20 Suppose ğ‘‰ is finite-dimensional andğ‘‡ âˆˆ â„’(ğ‘‰). Prove that ğ‘‡ is diagonaliz- able if and only if the dual operator ğ‘‡â€² is diagonalizable. 21 The Fibonacci sequence ğ¹0, ğ¹1, ğ¹2, â€¦ is defined by ğ¹0 = 0, ğ¹1 = 1, and ğ¹ğ‘› = ğ¹ğ‘› âˆ’ 2 + ğ¹ğ‘› âˆ’ 1 for ğ‘› â‰¥ 2. Defineğ‘‡ âˆˆ â„’(ğ‘2)by ğ‘‡(ğ‘¥, ğ‘¦) = (ğ‘¦, ğ‘¥ + ğ‘¦). (a) Show that ğ‘‡ğ‘›(0, 1) = (ğ¹ğ‘›, ğ¹ğ‘› + 1) for each nonnegative integer ğ‘›. (b) Find the eigenvalues of ğ‘‡. (c) Find a basis of ğ‘2 consisting of eigenvectors of ğ‘‡. (d) Use the solution to (c) to compute ğ‘‡ğ‘›(0, 1). Conclude that ğ¹ğ‘› = 1 âˆš5 [( 1+ âˆš5 2 ) ğ‘› âˆ’ ( 1 âˆ’âˆš5 2 ) ğ‘›] for each nonnegative integer ğ‘›. (e) Use (d) to conclude that if ğ‘› is a nonnegative integer, then the Fibonacci number ğ¹ğ‘› is the integer that is closest to 1 âˆš5 ( 1+ âˆš5 2 ) ğ‘›. Each ğ¹ğ‘› is a nonnegative integer, even though the right side of the formula in (d) does not look like an integer. The number 1+ âˆš5 2 is called the golden ratio. 22 Suppose ğ‘‡ âˆˆ â„’(ğ‘‰) and ğ´ is an ğ‘›-by-ğ‘› matrix that is the matrix of ğ‘‡ with respect to some basis of ğ‘‰. Prove that if |ğ´ğ‘—, ğ‘—| > ğ‘› âˆ‘ ğ‘˜ = 1 ğ‘˜ â‰  ğ‘— |ğ´ğ‘—, ğ‘˜| for each ğ‘— âˆˆ {1, â€¦, ğ‘›}, then ğ‘‡ is invertible. This exercise states that if the diagonal entries of the matrix of ğ‘‡ are large compared to the nondiagonal entries, then ğ‘‡ is invertible. 23 Suppose the definition of the Gershgorin disks is changed so that the radius of the ğ‘˜th disk is the sum of the absolute values of the entries in column (instead of row) ğ‘˜ of ğ´, excluding the diagonal entry. Show that the Gershgorin disk theorem (5.67) still holds with this changed definition. Section 5E Commuting Operators 175 5E Commuting Operators 5.71 definition:commute â€¢ Two operators ğ‘† and ğ‘‡ on the same vector space commute if ğ‘†ğ‘‡ = ğ‘‡ğ‘†. â€¢ Two square matrices ğ´ and ğµ of the same size commute if ğ´ğµ = ğµğ´. For example, if ğ‘‡ is an operator and ğ‘, ğ‘ âˆˆ ğ’«(ğ…), then ğ‘(ğ‘‡) and ğ‘(ğ‘‡) commute [see 5.17(b)]. As another example, if ğ¼ is the identity operator on ğ‘‰, then ğ¼ commutes with every operator on ğ‘‰. 5.72 example:partial differentiation operators commute Suppose ğ‘š is a nonnegative integer. Let ğ’«ğ‘š(ğ‘2)denote the real vector space of polynomials (with real coefficients) in two real variables and of degree at most ğ‘š, with the usual operations of addition and scalar multiplication of real-valued functions. Thus the elements of ğ’«ğ‘š(ğ‘2)are functions ğ‘ on ğ‘2 of the form 5.73 ğ‘ = âˆ‘ ğ‘— + ğ‘˜ â‰¤ ğ‘š ğ‘ğ‘—, ğ‘˜ğ‘¥ğ‘—ğ‘¦ğ‘˜, where the indices ğ‘— and ğ‘˜ take on all nonnegative integer values such that ğ‘— + ğ‘˜ â‰¤ ğ‘š, each ğ‘ğ‘—, ğ‘˜ is in ğ‘, and ğ‘¥ğ‘—ğ‘¦ğ‘˜ denotes the function on ğ‘2 defined by(ğ‘¥, ğ‘¦) â†¦ ğ‘¥ğ‘—ğ‘¦ğ‘˜. Define operatorsğ·ğ‘¥, ğ·ğ‘¦ âˆˆ â„’(ğ’«ğ‘š(ğ‘2))by ğ·ğ‘¥ğ‘ = ğœ•ğ‘ ğœ•ğ‘¥ = âˆ‘ ğ‘— + ğ‘˜ â‰¤ ğ‘š ğ‘—ğ‘ğ‘—, ğ‘˜ğ‘¥ğ‘— âˆ’ 1ğ‘¦ğ‘˜ and ğ·ğ‘¦ğ‘ = ğœ•ğ‘ ğœ•ğ‘¦ = âˆ‘ ğ‘— + ğ‘˜ â‰¤ ğ‘š ğ‘˜ğ‘ğ‘—, ğ‘˜ğ‘¥ğ‘—ğ‘¦ğ‘˜ âˆ’ 1, where ğ‘ is as in 5.73. The operators ğ·ğ‘¥ and ğ·ğ‘¦ are called partial differentiation operators because each of these operators differentiates with respect to one of the variables while pretending that the other variable is a constant. The operators ğ·ğ‘¥ and ğ·ğ‘¦ commute because if ğ‘ is as in 5.73, then (ğ·ğ‘¥ğ·ğ‘¦)ğ‘ = âˆ‘ ğ‘— + ğ‘˜ â‰¤ ğ‘š ğ‘—ğ‘˜ğ‘ğ‘—, ğ‘˜ğ‘¥ğ‘— âˆ’ 1ğ‘¦ğ‘˜ âˆ’ 1 = (ğ·ğ‘¦ğ·ğ‘¥)ğ‘. The equation ğ·ğ‘¥ğ·ğ‘¦ = ğ·ğ‘¦ğ·ğ‘¥ on ğ’«ğ‘š(ğ‘2)illustrates a more general result that the order of partial differentiation does not matter for nice functions. All 214,358,881 (which equals 11 8) pairs of the 2-by-2matrices under con- sideration were checked by a computer to discover that only 674,609 of these pairs of matrices commute. Commuting matrices are unusual. For example, there are 214,358,881 pairs of 2-by-2matrices all of whose entries are integers in the interval [âˆ’5, 5]. Only about 0.3% of these pairs of matrices commute. 176 Chapter 5 Eigenvalues and Eigenvectors The next result shows that two operators commute if and only if their matrices (with respect to the same basis) commute. 5.74 commuting operators correspond to commuting matrices Suppose ğ‘†, ğ‘‡ âˆˆ â„’(ğ‘‰) and ğ‘£1, â€¦, ğ‘£ğ‘› is a basis of ğ‘‰. Then ğ‘† and ğ‘‡ commute if and only if â„³(ğ‘†, (ğ‘£1, â€¦, ğ‘£ğ‘›))and â„³(ğ‘‡, (ğ‘£1, â€¦, ğ‘£ğ‘›))commute. Proof We have ğ‘† and ğ‘‡ commute âŸº ğ‘†ğ‘‡ = ğ‘‡ğ‘† âŸº â„³(ğ‘†ğ‘‡) = â„³(ğ‘‡ğ‘†) âŸº â„³(ğ‘†)â„³(ğ‘‡) = â„³(ğ‘‡)â„³(ğ‘†) âŸº â„³(ğ‘†) and â„³(ğ‘‡) commute, as desired. The next result shows that if two operators commute, then every eigenspace for one operator is invariant under the other operator. This result, which we will use several times, is one of the main reasons why a pair of commuting operators behaves better than a pair of operators that does not commute. 5.75 eigenspace is invariant under commuting operator Suppose ğ‘†, ğ‘‡ âˆˆ â„’(ğ‘‰) commute and ğœ† âˆˆ ğ…. Then ğ¸(ğœ†, ğ‘†) is invariant under ğ‘‡. Proof Suppose ğ‘£ âˆˆ ğ¸(ğœ†, ğ‘†). Then ğ‘†(ğ‘‡ğ‘£) = (ğ‘†ğ‘‡)ğ‘£ = (ğ‘‡ğ‘†)ğ‘£ = ğ‘‡(ğ‘†ğ‘£) = ğ‘‡(ğœ†ğ‘£) = ğœ†ğ‘‡ğ‘£. The equation above shows that ğ‘‡ğ‘£ âˆˆ ğ¸(ğœ†, ğ‘†). Thus ğ¸(ğœ†, ğ‘†) is invariant under ğ‘‡. Suppose we have two operators, each of which is diagonalizable. If we want to do computations involving both operators (for example, involving their sum), then we want the two operators to be diagonalizable by the same basis, which according to the next result is possible when the two operators commute. 5.76 simultaneous diagonalizablity âŸº commutativity Two diagonalizable operators on the same vector space have diagonal matrices with respect to the same basis if and only if the two operators commute. Proof First suppose ğ‘†, ğ‘‡ âˆˆ â„’(ğ‘‰) have diagonal matrices with respect to the same basis. The product of two diagonal matrices of the same size is the diagonal matrix obtained by multiplying the corresponding elements of the two diagonals. Thus any two diagonal matrices of the same size commute. Thus ğ‘† and ğ‘‡ commute, by 5.74. Section 5E Commuting Operators 177 To prove the implication in the other direction, now suppose that ğ‘†, ğ‘‡ âˆˆ â„’(ğ‘‰) are diagonalizable operators that commute. Let ğœ†1, â€¦, ğœ†ğ‘š denote the distinct eigenvalues of ğ‘†. Because ğ‘† is diagonalizable, 5.55(c) shows that 5.77 ğ‘‰ = ğ¸(ğœ†1, ğ‘†) âŠ• â‹¯ âŠ• ğ¸(ğœ†ğ‘š, ğ‘†). For each ğ‘˜ = 1, â€¦, ğ‘š, the subspace ğ¸(ğœ†ğ‘˜, ğ‘†) is invariant under ğ‘‡ (by 5.75). Because ğ‘‡ is diagonalizable, 5.65 implies that ğ‘‡|ğ¸( ğœ†ğ‘˜, ğ‘†)is diagonalizable for each ğ‘˜. Hence for each ğ‘˜ = 1, â€¦, ğ‘š, there is a basis of ğ¸(ğœ†ğ‘˜, ğ‘†) consisting of eigenvectors of ğ‘‡. Putting these bases together gives a basis of ğ‘‰ (because of 5.77), with each vector in this basis being an eigenvector of both ğ‘† and ğ‘‡. Thus ğ‘† and ğ‘‡ both have diagonal matrices with respect to this basis, as desired. See Exercise 2 for an extension of the result above to more than two operators. Suppose ğ‘‰ is a finite-dimensional nonzero complex vector space. Then every operator on ğ‘‰ has an eigenvector (see 5.19). The next result shows that if two operators on ğ‘‰ commute, then there is a vector in ğ‘‰ that is an eigenvector for both operators (but the two commuting operators might not have a common eigenvalue). For an extension of the next result to more than two operators, see Exercise 9(a). 5.78 common eigenvector for commuting operators Every pair of commuting operators on a finite-dimensional nonzero complex vector space has a common eigenvector. Proof Suppose ğ‘‰ is a finite-dimensional nonzero complex vector space and ğ‘†, ğ‘‡ âˆˆ â„’(ğ‘‰) commute. Let ğœ† be an eigenvalue of ğ‘† (5.19 tells us that ğ‘† does indeed have an eigenvalue). Thus ğ¸(ğœ†, ğ‘†) â‰  {0}. Also, ğ¸(ğœ†, ğ‘†) is invariant under ğ‘‡ (by 5.75). Thus ğ‘‡|ğ¸( ğœ†, ğ‘†)has an eigenvector (again using 5.19), which is an eigenvector for both ğ‘† and ğ‘‡, completing the proof. 5.79 example:common eigenvector for partial differentiation operators Let ğ’«ğ‘š(ğ‘2)be as in Example 5.72 and let ğ·ğ‘¥, ğ·ğ‘¦ âˆˆ â„’(ğ’«ğ‘š(ğ‘2))be the commuting partial differentiation operators in that example. As you can verify, 0 is the only eigenvalue of each of these operators. Also ğ¸(0, ğ·ğ‘¥) = {ğ‘š âˆ‘ ğ‘˜ = 0 ğ‘ğ‘˜ğ‘¦ğ‘˜ âˆ¶ ğ‘0, â€¦, ğ‘ğ‘š âˆˆ ğ‘}, ğ¸(0, ğ·ğ‘¦) = { ğ‘š âˆ‘ ğ‘— = 0 ğ‘ğ‘—ğ‘¥ğ‘— âˆ¶ ğ‘0, â€¦, ğ‘ğ‘š âˆˆ ğ‘}. The intersection of these two eigenspaces is the set of common eigenvectors of the two operators. Because ğ¸(0, ğ·ğ‘¥) âˆ© ğ¸(0, ğ·ğ‘¦) is the set of constant functions, we see that ğ·ğ‘¥ and ğ·ğ‘¦ indeed have a common eigenvector, as promised by 5.78. 178 Chapter 5 Eigenvalues and Eigenvectors The next result extends 5.47 (the existence of a basis that gives an upper- triangular matrix) to two commuting operators. 5.80 commuting operators are simultaneously upper triangularizable Suppose ğ‘‰ is a finite-dimensional complex vector space andğ‘†, ğ‘‡ are commuting operators on ğ‘‰. Then there is a basis of ğ‘‰ with respect to which both ğ‘† and ğ‘‡ have upper-triangular matrices. Proof Let ğ‘› = dim ğ‘‰. We will use induction on ğ‘›. The desired result holds if ğ‘› = 1because all 1-by-1matrices are upper triangular. Now suppose ğ‘› > 1and the desired result holds for all complex vector spaces whose dimension is ğ‘› âˆ’ 1. Let ğ‘£1 be any common eigenvector of ğ‘† and ğ‘‡ (using 5.78). Hence ğ‘†ğ‘£1 âˆˆ span(ğ‘£1) and ğ‘‡ğ‘£1 âˆˆ span(ğ‘£1). Let ğ‘Š be a subspace of ğ‘‰ such that ğ‘‰ = span(ğ‘£1) âŠ• ğ‘Š; see 2.33 for the existence of ğ‘Š. Define a linear mapğ‘ƒâˆ¶ ğ‘‰ â†’ ğ‘Š by ğ‘ƒ(ğ‘ğ‘£1 + ğ‘¤) = ğ‘¤ for each ğ‘ âˆˆ ğ‚ and each ğ‘¤ âˆˆ ğ‘Š. Define Ì‚ğ‘†, Ì‚ğ‘‡ âˆˆ â„’(ğ‘Š) by Ì‚ğ‘†ğ‘¤ = ğ‘ƒ(ğ‘†ğ‘¤) and Ì‚ğ‘‡ğ‘¤ = ğ‘ƒ(ğ‘‡ğ‘¤) for each ğ‘¤ âˆˆ ğ‘Š. To apply our induction hypothesis to Ì‚ğ‘† and Ì‚ğ‘‡, we must first show that these two operators on ğ‘Š commute. To do this, suppose ğ‘¤ âˆˆ ğ‘Š. Then there exists ğ‘ âˆˆ ğ‚ such that (Ì‚ğ‘† Ì‚ğ‘‡)ğ‘¤ = Ì‚ğ‘†(ğ‘ƒ(ğ‘‡ğ‘¤))= Ì‚ğ‘†(ğ‘‡ğ‘¤ âˆ’ ğ‘ğ‘£1) = ğ‘ƒ(ğ‘†(ğ‘‡ğ‘¤ âˆ’ ğ‘ğ‘£1))= ğ‘ƒ((ğ‘†ğ‘‡)ğ‘¤), where the last equality holds because ğ‘£1 is an eigenvector of ğ‘† and ğ‘ƒğ‘£1 = 0. Similarly, (Ì‚ğ‘‡ Ì‚ğ‘†)ğ‘¤ = ğ‘ƒ((ğ‘‡ğ‘†)ğ‘¤). Because the operators ğ‘† and ğ‘‡ commute, the last two displayed equations show that (Ì‚ğ‘† Ì‚ğ‘‡)ğ‘¤ = (Ì‚ğ‘‡ Ì‚ğ‘†)ğ‘¤. Hence Ì‚ğ‘† and Ì‚ğ‘‡ commute. Thus we can use our induction hypothesis to state that there exists a basis ğ‘£2, â€¦, ğ‘£ğ‘› of ğ‘Š such that Ì‚ğ‘† and Ì‚ğ‘‡ both have upper-triangular matrices with respect to this basis. The list ğ‘£1, â€¦, ğ‘£ğ‘› is a basis of ğ‘‰. If ğ‘˜ âˆˆ {2, â€¦, ğ‘›}, then there exist ğ‘ğ‘˜, ğ‘ğ‘˜ âˆˆ ğ‚ such that ğ‘†ğ‘£ğ‘˜ = ğ‘ğ‘˜ğ‘£1 + Ì‚ğ‘†ğ‘£ğ‘˜ and ğ‘‡ğ‘£ğ‘˜ = ğ‘ğ‘˜ğ‘£1 + Ì‚ğ‘‡ğ‘£ğ‘˜. Because Ì‚ğ‘† and Ì‚ğ‘‡ have upper-triangular matrices with respect to ğ‘£2, â€¦, ğ‘£ğ‘›, we know that Ì‚ğ‘†ğ‘£ğ‘˜ âˆˆ span(ğ‘£2, â€¦, ğ‘£ğ‘˜) and Ì‚ğ‘‡ğ‘£ğ‘˜ âˆˆ span(ğ‘£2, â€¦, ğ‘£ğ‘˜). Hence the equations above imply that ğ‘†ğ‘£ğ‘˜ âˆˆ span(ğ‘£1, â€¦, ğ‘£ğ‘˜) and ğ‘‡ğ‘£ğ‘˜ âˆˆ span(ğ‘£1, â€¦, ğ‘£ğ‘˜). Thus ğ‘† and ğ‘‡ have upper-triangular matrices with respect to ğ‘£1, â€¦, ğ‘£ğ‘›, as desired. Exercise 9(b) extends the result above to more than two operators. Section 5E Commuting Operators 179 In general, it is not possible to determine the eigenvalues of the sum or product of two operators from the eigenvalues of the two operators. However, the next result shows that something nice happens when the two operators commute. 5.81 eigenvalues of sum and product of commuting operators Suppose ğ‘‰ is a finite-dimensional complex vector space andğ‘†, ğ‘‡ are commut- ing operators on ğ‘‰. Then â€¢ every eigenvalue of ğ‘† + ğ‘‡ is an eigenvalue of ğ‘† plus an eigenvalue of ğ‘‡, â€¢ every eigenvalue of ğ‘†ğ‘‡ is an eigenvalue of ğ‘† times an eigenvalue of ğ‘‡. Proof There is a basis of ğ‘‰ with respect to which both ğ‘† and ğ‘‡ have upper- triangular matrices (by 5.80). With respect to that basis, â„³(ğ‘† + ğ‘‡) = â„³(ğ‘†) + â„³(ğ‘‡) and â„³(ğ‘†ğ‘‡) = â„³(ğ‘†)â„³(ğ‘‡), as stated in 3.35 and 3.43. The definition of matrix addition shows that each entry on the diagonal of â„³(ğ‘† + ğ‘‡) equals the sum of the corresponding entries on the diagonals of â„³(ğ‘†) and â„³(ğ‘‡). Similarly, because â„³(ğ‘†) and â„³(ğ‘‡) are upper-triangular matrices, the definition of matrix multiplication shows that each entry on the diagonal of â„³(ğ‘†ğ‘‡) equals the product of the corresponding entries on the diagonals of â„³(ğ‘†) and â„³(ğ‘‡). Furthermore, â„³(ğ‘† + ğ‘‡) and â„³(ğ‘†ğ‘‡) are upper-triangular matrices (see Exercise 2 in Section 5B). Every entry on the diagonal of â„³(ğ‘†) is an eigenvalue of ğ‘†, and every entry on the diagonal of â„³(ğ‘‡) is an eigenvalue of ğ‘‡ (by 5.41). Every eigenvalue of ğ‘† + ğ‘‡ is on the diagonal of â„³(ğ‘† + ğ‘‡), and every eigenvalue of ğ‘†ğ‘‡ is on the diagonal of â„³(ğ‘†ğ‘‡) (these assertions follow from 5.41). Putting all this together, we conclude that every eigenvalue of ğ‘† + ğ‘‡ is an eigenvalue of ğ‘† plus an eigenvalue of ğ‘‡, and every eigenvalue of ğ‘†ğ‘‡ is an eigenvalue of ğ‘† times an eigenvalue of ğ‘‡. Exercises 5E 1 Give an example of two commuting operators ğ‘†, ğ‘‡ on ğ…4 such that there is a subspace of ğ…4 that is invariant under ğ‘† but not under ğ‘‡ and there is a subspace of ğ…4 that is invariant under ğ‘‡ but not under ğ‘†. 2 Suppose â„° is a subset of â„’(ğ‘‰) and every element of â„° is diagonalizable. Prove that there exists a basis of ğ‘‰ with respect to which every element of â„° has a diagonal matrix if and only if every pair of elements of â„° commutes. This exercise extends 5.76, which considers the case in which â„° contains only two elements. For this exercise, â„° may contain any number of elements, and â„° may even be an infinite set. 180 Chapter 5 Eigenvalues and Eigenvectors 3 Suppose ğ‘†, ğ‘‡ âˆˆ â„’(ğ‘‰) are such that ğ‘†ğ‘‡ = ğ‘‡ğ‘†. Suppose ğ‘ âˆˆ ğ’«(ğ…). (a) Prove that null ğ‘(ğ‘†) is invariant under ğ‘‡. (b) Prove that range ğ‘(ğ‘†) is invariant under ğ‘‡. See 5.18 for the special case ğ‘† = ğ‘‡. 4 Prove or give a counterexample: If ğ´ is a diagonal matrix and ğµ is an upper-triangular matrix of the same size as ğ´, then ğ´ and ğµ commute. 5 Prove that a pair of operators on a finite-dimensional vector space commute if and only if their dual operators commute. See 3.118 for the definition of the dual of an operator. 6 Suppose ğ‘‰ is a finite-dimensional complex vector space andğ‘†, ğ‘‡ âˆˆ â„’(ğ‘‰) commute. Prove that there exist ğ›¼, ğœ† âˆˆ ğ‚ such that range(ğ‘† âˆ’ ğ›¼ğ¼) + range(ğ‘‡ âˆ’ ğœ†ğ¼) â‰  ğ‘‰. 7 Suppose ğ‘‰ is a complex vector space, ğ‘† âˆˆ â„’(ğ‘‰) is diagonalizable, and ğ‘‡ âˆˆ â„’(ğ‘‰) commutes with ğ‘†. Prove that there is a basis of ğ‘‰ such that ğ‘† has a diagonal matrix with respect to this basis and ğ‘‡ has an upper-triangular matrix with respect to this basis. 8 Suppose ğ‘š = 3in Example 5.72 and ğ·ğ‘¥, ğ·ğ‘¦ are the commuting partial differentiation operators on ğ’«3(ğ‘2)from that example. Find a basis of ğ’«3(ğ‘2)with respect to which ğ·ğ‘¥ and ğ·ğ‘¦ each have an upper-triangular matrix. 9 Suppose ğ‘‰ is a finite-dimensional nonzero complex vector space. Suppose that â„° âŠ†â„’(ğ‘‰) is such that ğ‘† and ğ‘‡ commute for all ğ‘†, ğ‘‡ âˆˆ â„°. (a) Prove that there is a vector in ğ‘‰ that is an eigenvector for every element of â„°. (b) Prove that there is a basis of ğ‘‰ with respect to which every element of â„° has an upper-triangular matrix. This exercise extends 5.78 and 5.80, which consider the case in which â„° contains only two elements. For this exercise, â„° may contain any number of elements, and â„° may even be an infinite set. 10 Give an example of two commuting operators ğ‘†, ğ‘‡ on a finite-dimensional real vector space such that ğ‘† + ğ‘‡ has a eigenvalue that does not equal an eigenvalue of ğ‘† plus an eigenvalue of ğ‘‡ and ğ‘†ğ‘‡ has a eigenvalue that does not equal an eigenvalue of ğ‘† times an eigenvalue of ğ‘‡. This exercise shows that 5.81 does not hold on real vector spaces. Chapter 6 Inner Product Spaces In making the definition of a vector space, we generalized the linear structure (addition and scalar multiplication) of ğ‘2 and ğ‘3. We ignored geometric features such as the notions of length and angle. These ideas are embedded in the concept of inner products, which we will investigate in this chapter. Every inner product induces a norm, which you can think of as a length. This norm satisfies key properties such as the Pythagorean theorem, the triangle inequality, the parallelogram equality, and the Cauchyâ€“Schwarz inequality. The notion of perpendicular vectors in Euclidean geometry gets renamed to orthogonal vectors in the context of an inner product space. We will see that orthonormal bases are tremendously useful in inner product spaces. The Gramâ€“ Schmidt procedure constructs such bases. This chapter will conclude by putting together these tools to solve minimization problems. standing assumptions for this chapter â€¢ ğ… denotes ğ‘ or ğ‚. â€¢ ğ‘‰ and ğ‘Š denote vector spaces over ğ….MatthewPetroffCCBY-SA The George Peabody Library, now part of Johns Hopkins University, opened while James Sylvester (1814â€“1897) was the universityâ€™s first mathematics professor. Sylvesterâ€™s publications include the first use of the word matrix in mathematics. 181 S. Axler, Linear Algebra Done Right, Undergraduate Texts in Mathematics, https://doi.org/10.1007/978-3-031-41026-0_6 Â© Sheldon Axler 2024 182 Chapter 6 Inner Product Spaces 6A Inner Products and Norms Inner Products This vector ğ‘£ has norm âˆšğ‘2 + ğ‘2. To motivate the concept of inner product, think of vectors in ğ‘2 and ğ‘3 as arrows with initial point at the origin. The length of a vector ğ‘£ in ğ‘2 or ğ‘3 is called the norm of ğ‘£ and is denoted by â€–ğ‘£â€–. Thus for ğ‘£ = (ğ‘, ğ‘) âˆˆ ğ‘2, we have â€–ğ‘£â€– = âˆšğ‘2 + ğ‘2. Similarly, if ğ‘£ = (ğ‘, ğ‘, ğ‘) âˆˆ ğ‘3, then â€–ğ‘£â€– = âˆšğ‘2 + ğ‘2 + ğ‘2. Even though we cannot draw pictures in higher dimensions, the generalization to ğ‘ğ‘› is easy: we define the norm ofğ‘¥ = (ğ‘¥1, â€¦, ğ‘¥ğ‘›) âˆˆ ğ‘ğ‘› by â€–ğ‘¥â€– = âˆšğ‘¥1 2 + â‹¯ + ğ‘¥ğ‘› 2. The norm is not linear on ğ‘ğ‘›. To inject linearity into the discussion, we introduce the dot product. 6.1 definition:dot product For ğ‘¥, ğ‘¦ âˆˆ ğ‘ğ‘›, the dot product of ğ‘¥ and ğ‘¦, denoted by ğ‘¥ â‹… ğ‘¦, is defined by ğ‘¥ â‹… ğ‘¦ = ğ‘¥1ğ‘¦1 + â‹¯ + ğ‘¥ğ‘›ğ‘¦ğ‘›, where ğ‘¥ = (ğ‘¥1, â€¦, ğ‘¥ğ‘›) and ğ‘¦ = (ğ‘¦1, â€¦, ğ‘¦ğ‘›). If we think of a vector as a point instead of as an arrow, then â€–ğ‘¥â€– should be interpreted to mean the distance from the origin to the point ğ‘¥. The dot product of two vectors in ğ‘ğ‘› is a number, not a vector. Notice that ğ‘¥ â‹… ğ‘¥ = â€–ğ‘¥â€– 2 for all ğ‘¥ âˆˆ ğ‘ğ‘›. Furthermore, the dot product on ğ‘ğ‘› has the following properties. â€¢ ğ‘¥ â‹… ğ‘¥ â‰¥ 0for all ğ‘¥ âˆˆ ğ‘ğ‘›. â€¢ ğ‘¥ â‹… ğ‘¥ = 0if and only if ğ‘¥ = 0. â€¢ For ğ‘¦ âˆˆ ğ‘ğ‘› fixed, the map fromğ‘ğ‘› to ğ‘ that sends ğ‘¥ âˆˆ ğ‘ğ‘› to ğ‘¥ â‹… ğ‘¦ is linear. â€¢ ğ‘¥ â‹… ğ‘¦ = ğ‘¦ â‹… ğ‘¥ for all ğ‘¥, ğ‘¦ âˆˆ ğ‘ğ‘›. An inner product is a generalization of the dot product. At this point you may be tempted to guess that an inner product is defined by abstracting the properties of the dot product discussed in the last paragraph. For real vector spaces, that guess is correct. However, so that we can make a definition that will be useful for both real and complex vector spaces, we need to examine the complex case before making the definition. Section 6A Inner Products and Norms 183 Recall that if ğœ† = ğ‘ + ğ‘ğ‘–, where ğ‘, ğ‘ âˆˆ ğ‘, then â€¢ the absolute value of ğœ†, denoted by |ğœ†|, is defined by|ğœ†| = âˆšğ‘2 + ğ‘2; â€¢ the complex conjugate of ğœ†, denoted by ğœ†, is defined byğœ† = ğ‘ âˆ’ ğ‘ğ‘–; â€¢ |ğœ†|2 = ğœ†ğœ†. See Chapter 4 for the definitions and the basic properties of the absolute value and complex conjugate. For ğ‘§ = (ğ‘§1, â€¦, ğ‘§ğ‘›) âˆˆ ğ‚ğ‘›, we define the norm ofğ‘§ by â€–ğ‘§â€– = âˆš|ğ‘§1|2 + â‹¯ + |ğ‘§ğ‘›|2. The absolute values are needed because we want â€–ğ‘§â€– to be a nonnegative number. Note that â€–ğ‘§â€– 2 = ğ‘§1ğ‘§1 + â‹¯ + ğ‘§ğ‘›ğ‘§ğ‘›. We want to think of â€–ğ‘§â€– 2 as the inner product of ğ‘§ with itself, as we did in ğ‘ğ‘›. The equation above thus suggests that the inner product of the vector ğ‘¤ = (ğ‘¤1, â€¦, ğ‘¤ğ‘›) âˆˆ ğ‚ğ‘› with ğ‘§ should equal ğ‘¤1ğ‘§1 + â‹¯ + ğ‘¤ğ‘›ğ‘§ğ‘›. If the roles of the ğ‘¤ and ğ‘§ were interchanged, the expression above would be replaced with its complex conjugate. Thus we should expect that the inner product of ğ‘¤ with ğ‘§ equals the complex conjugate of the inner product of ğ‘§ with ğ‘¤. With that motivation, we are now ready to define an inner product onğ‘‰, which may be a real or a complex vector space. One comment about the notation used in the next definition: â€¢ For ğœ† âˆˆ ğ‚, the notation ğœ† â‰¥ 0means ğœ† is real and nonnegative. 6.2 definition:inner product An inner product on ğ‘‰ is a function that takes each ordered pair (ğ‘¢, ğ‘£) of elements of ğ‘‰ to a number âŸ¨ğ‘¢, ğ‘£âŸ© âˆˆ ğ…and has the following properties. positivity âŸ¨ğ‘£, ğ‘£âŸ© â‰¥ 0for all ğ‘£ âˆˆ ğ‘‰. definiteness âŸ¨ğ‘£, ğ‘£âŸ© = 0if and only if ğ‘£ = 0. additivity in first slot âŸ¨ğ‘¢+ ğ‘£, ğ‘¤âŸ© = âŸ¨ğ‘¢, ğ‘¤âŸ©+ âŸ¨ğ‘£, ğ‘¤âŸ©for all ğ‘¢, ğ‘£, ğ‘¤ âˆˆ ğ‘‰. homogeneity in first slot âŸ¨ğœ†ğ‘¢, ğ‘£âŸ© = ğœ†âŸ¨ğ‘¢, ğ‘£âŸ©for all ğœ† âˆˆ ğ… and all ğ‘¢, ğ‘£ âˆˆ ğ‘‰. conjugate symmetry âŸ¨ğ‘¢, ğ‘£âŸ© = âŸ¨ğ‘£, ğ‘¢âŸ©for all ğ‘¢, ğ‘£ âˆˆ ğ‘‰. 184 Chapter 6 Inner Product Spaces Most mathematicians define inner products as above, but many physicists use a definition that requires homo- geneity in the second slot instead of the first slot. Every real number equals its complex conjugate. Thus if we are dealing with a real vector space, then in the last con- dition above we can dispense with the complex conjugate and simply state that âŸ¨ğ‘¢, ğ‘£âŸ© = âŸ¨ğ‘£, ğ‘¢âŸ©for all ğ‘¢, ğ‘£ âˆˆ ğ‘‰. 6.3 example:inner products (a) The Euclidean inner product on ğ…ğ‘› is defined by âŸ¨(ğ‘¤1, â€¦, ğ‘¤ğ‘›), (ğ‘§1, â€¦, ğ‘§ğ‘›)âŸ©= ğ‘¤1ğ‘§1 + â‹¯ + ğ‘¤ğ‘›ğ‘§ğ‘› for all (ğ‘¤1, â€¦, ğ‘¤ğ‘›), (ğ‘§1, â€¦, ğ‘§ğ‘›) âˆˆ ğ…ğ‘›. (b) If ğ‘1, â€¦, ğ‘ğ‘› are positive numbers, then an inner product can be defined onğ…ğ‘› by âŸ¨(ğ‘¤1, â€¦, ğ‘¤ğ‘›), (ğ‘§1, â€¦, ğ‘§ğ‘›)âŸ©= ğ‘1ğ‘¤1ğ‘§1 + â‹¯ + ğ‘ğ‘›ğ‘¤ğ‘›ğ‘§ğ‘› for all (ğ‘¤1, â€¦, ğ‘¤ğ‘›), (ğ‘§1, â€¦, ğ‘§ğ‘›) âˆˆ ğ…ğ‘›. (c) An inner product can be defined on the vector space of continuous real-valued functions on the interval [âˆ’1, 1]by âŸ¨ ğ‘“, ğ‘”âŸ© =âˆ«1 âˆ’1 ğ‘“ ğ‘” for all ğ‘“, ğ‘” continuous real-valued functions on [âˆ’1, 1]. (d) An inner product can be defined onğ’«(ğ‘) by âŸ¨ğ‘, ğ‘âŸ© = ğ‘(0)ğ‘(0)+ âˆ«1 âˆ’1 ğ‘ â€²ğ‘ â€² for all ğ‘, ğ‘ âˆˆ ğ’«(ğ‘). (e) An inner product can be defined onğ’«(ğ‘) by âŸ¨ğ‘, ğ‘âŸ© =âˆ« âˆ 0 ğ‘(ğ‘¥)ğ‘(ğ‘¥)ğ‘’âˆ’ğ‘¥ ğ‘‘ğ‘¥ for all ğ‘, ğ‘ âˆˆ ğ’«(ğ‘). 6.4 definition:inner product space An inner product space is a vector space ğ‘‰ along with an inner product on ğ‘‰. The most important example of an inner product space is ğ…ğ‘› with the Euclidean inner product given by (a) in the example above. When ğ…ğ‘› is referred to as an inner product space, you should assume that the inner product is the Euclidean inner product unless explicitly told otherwise. Section 6A Inner Products and Norms 185 So that we do not have to keep repeating the hypothesis that ğ‘‰ and ğ‘Š are inner product spaces, we make the following assumption. 6.5 notation:ğ‘‰, ğ‘Š For the rest of this chapter and the next chapter, ğ‘‰ and ğ‘Š denote inner product spaces over ğ…. Note the slight abuse of language here. An inner product space is a vector space along with an inner product on that vector space. When we say that a vector space ğ‘‰ is an inner product space, we are also thinking that an inner product on ğ‘‰ is lurking nearby or is clear from the context (or is the Euclidean inner product if the vector space is ğ…ğ‘›). 6.6 basic properties of an inner product (a) For each fixedğ‘£ âˆˆ ğ‘‰, the function that takes ğ‘¢ âˆˆ ğ‘‰ to âŸ¨ğ‘¢, ğ‘£âŸ©is a linear map from ğ‘‰ to ğ…. (b) âŸ¨0, ğ‘£âŸ© = 0for every ğ‘£ âˆˆ ğ‘‰. (c) âŸ¨ğ‘£, 0âŸ© = 0for every ğ‘£ âˆˆ ğ‘‰. (d) âŸ¨ğ‘¢, ğ‘£ + ğ‘¤âŸ© = âŸ¨ğ‘¢, ğ‘£âŸ©+ âŸ¨ğ‘¢, ğ‘¤âŸ©for all ğ‘¢, ğ‘£, ğ‘¤ âˆˆ ğ‘‰. (e) âŸ¨ğ‘¢, ğœ†ğ‘£âŸ© = ğœ†âŸ¨ğ‘¢, ğ‘£âŸ©for all ğœ† âˆˆ ğ… and all ğ‘¢, ğ‘£ âˆˆ ğ‘‰. Proof (a) For ğ‘£ âˆˆ ğ‘‰, the linearity of ğ‘¢ â†¦ âŸ¨ğ‘¢, ğ‘£âŸ©follows from the conditions of additivity and homogeneity in the first slot in the definition of an inner product. (b) Every linear map takes 0to 0. Thus (b) follows from (a). (c) If ğ‘£ âˆˆ ğ‘‰, then the conjugate symmetry property in the definition of an inner product and (b) show that âŸ¨ğ‘£, 0âŸ© = âŸ¨0, ğ‘£âŸ©= 0= 0. (d) Suppose ğ‘¢, ğ‘£, ğ‘¤ âˆˆ ğ‘‰. Then âŸ¨ğ‘¢, ğ‘£ + ğ‘¤âŸ© = âŸ¨ğ‘£+ ğ‘¤, ğ‘¢âŸ© = âŸ¨ğ‘£, ğ‘¢âŸ©+ âŸ¨ğ‘¤, ğ‘¢âŸ© = âŸ¨ğ‘£, ğ‘¢âŸ©+ âŸ¨ğ‘¤, ğ‘¢âŸ© = âŸ¨ğ‘¢, ğ‘£âŸ©+ âŸ¨ğ‘¢, ğ‘¤âŸ©. (e) Suppose ğœ† âˆˆ ğ… and ğ‘¢, ğ‘£ âˆˆ ğ‘‰. Then âŸ¨ğ‘¢, ğœ†ğ‘£âŸ© = âŸ¨ğœ†ğ‘£, ğ‘¢âŸ© = ğœ†âŸ¨ğ‘£, ğ‘¢âŸ© = ğœ† âŸ¨ğ‘£, ğ‘¢âŸ© = ğœ†âŸ¨ğ‘¢, ğ‘£âŸ©. 186 Chapter 6 Inner Product Spaces Norms Our motivation for defining inner products came initially from the norms of vectors on ğ‘2 and ğ‘3. Now we see that each inner product determines a norm. 6.7 definition:norm, â€–ğ‘£â€– For ğ‘£ âˆˆ ğ‘‰, the norm of ğ‘£, denoted by â€–ğ‘£â€–, is defined by â€–ğ‘£â€– = âˆšâŸ¨ğ‘£, ğ‘£âŸ©. 6.8 example:norms (a) If (ğ‘§1, â€¦, ğ‘§ğ‘›) âˆˆ ğ…ğ‘› (with the Euclidean inner product), then â€–(ğ‘§1, â€¦, ğ‘§ğ‘›)â€– = âˆš|ğ‘§1|2 + â‹¯ + |ğ‘§ğ‘›|2. (b) For ğ‘“ in the vector space of continuous real-valued functions on [âˆ’1, 1]and with inner product given as in 6.3(c), we have â€– ğ‘“ â€– = âˆšâˆ«1 âˆ’1 ğ‘“ 2. 6.9 basic properties of the norm Suppose ğ‘£ âˆˆ ğ‘‰. (a) â€–ğ‘£â€– = 0if and only if ğ‘£ = 0. (b) â€–ğœ†ğ‘£â€– = |ğœ†| â€–ğ‘£â€– for all ğœ† âˆˆ ğ…. Proof (a) The desired result holds because âŸ¨ğ‘£, ğ‘£âŸ© = 0if and only if ğ‘£ = 0. (b) Suppose ğœ† âˆˆ ğ…. Then â€–ğœ†ğ‘£â€–2 = âŸ¨ğœ†ğ‘£, ğœ†ğ‘£âŸ© = ğœ†âŸ¨ğ‘£, ğœ†ğ‘£âŸ© = ğœ†ğœ†âŸ¨ğ‘£, ğ‘£âŸ© = |ğœ†|2 â€–ğ‘£â€– 2. Taking square roots now gives the desired equality. The proof of (b) in the result above illustrates a general principle: working with norms squared is usually easier than working directly with norms. Section 6A Inner Products and Norms 187 Now we come to a crucial definition. 6.10 definition:orthogonal Two vectors ğ‘¢, ğ‘£ âˆˆ ğ‘‰ are called orthogonal if âŸ¨ğ‘¢, ğ‘£âŸ© = 0. The word orthogonal comes from the Greek word orthogonios, which means right-angled. In the definition above, the order of the two vectors does not matter, because âŸ¨ğ‘¢, ğ‘£âŸ© = 0if and only if âŸ¨ğ‘£, ğ‘¢âŸ© = 0. In- stead of saying ğ‘¢ and ğ‘£ are orthogonal, sometimes we say ğ‘¢ is orthogonal to ğ‘£. Exercise 15 asks you to prove that if ğ‘¢, ğ‘£ are nonzero vectors in ğ‘2, then âŸ¨ğ‘¢, ğ‘£âŸ© = â€–ğ‘¢â€– â€–ğ‘£â€–cos ğœƒ, where ğœƒ is the angle between ğ‘¢ and ğ‘£ (thinking of ğ‘¢ and ğ‘£ as arrows with initial point at the origin). Thus two nonzero vectors in ğ‘2 are orthogonal (with respect to the Euclidean inner product) if and only if the cosine of the angle between them is 0, which happens if and only if the vectors are perpendicular in the usual sense of plane geometry. Thus you can think of the word orthogonal as a fancy word meaning perpendicular. We begin our study of orthogonality with an easy result. 6.11 orthogonality and 0 (a) 0is orthogonal to every vector in ğ‘‰. (b) 0is the only vector in ğ‘‰ that is orthogonal to itself. Proof (a) Recall that 6.6(b) states that âŸ¨0, ğ‘£âŸ© = 0for every ğ‘£ âˆˆ ğ‘‰. (b) If ğ‘£ âˆˆ ğ‘‰ and âŸ¨ğ‘£, ğ‘£âŸ© = 0, then ğ‘£ = 0(by definition of inner product). For the special case ğ‘‰ = ğ‘2, the next theorem was known over 3,500 years ago in Babylonia and then rediscovered and proved over 2,500 years ago in Greece. Of course, the proof below is not the original proof. 6.12 Pythagorean theorem Suppose ğ‘¢, ğ‘£ âˆˆ ğ‘‰. If ğ‘¢ and ğ‘£ are orthogonal, then â€–ğ‘¢ + ğ‘£â€– 2 = â€–ğ‘¢â€–2 + â€–ğ‘£â€– 2. Proof Suppose âŸ¨ğ‘¢, ğ‘£âŸ© = 0. Then â€–ğ‘¢ + ğ‘£â€– 2 = âŸ¨ğ‘¢+ ğ‘£, ğ‘¢ + ğ‘£âŸ© = âŸ¨ğ‘¢, ğ‘¢âŸ©+ âŸ¨ğ‘¢, ğ‘£âŸ©+ âŸ¨ğ‘£, ğ‘¢âŸ©+ âŸ¨ğ‘£, ğ‘£âŸ© = â€–ğ‘¢â€–2 + â€–ğ‘£â€– 2. 188 Chapter 6 Inner Product Spaces Suppose ğ‘¢, ğ‘£ âˆˆ ğ‘‰, with ğ‘£ â‰  0. We would like to write ğ‘¢ as a scalar multiple of ğ‘£ plus a vector ğ‘¤ orthogonal to ğ‘£, as suggested in the picture here. An orthogonal decomposition: ğ‘¢ expressed as a scalar multiple of ğ‘£ plus a vector orthogonal to ğ‘£. To discover how to write ğ‘¢ as a scalar multiple of ğ‘£ plus a vector orthogonal to ğ‘£, let ğ‘ âˆˆ ğ… denote a scalar. Then ğ‘¢ = ğ‘ğ‘£ + (ğ‘¢ âˆ’ ğ‘ğ‘£). Thus we need to choose ğ‘ so that ğ‘£ is orthogonal to (ğ‘¢ âˆ’ ğ‘ğ‘£). Hence we want 0 = âŸ¨ğ‘¢ âˆ’ ğ‘ğ‘£, ğ‘£âŸ© = âŸ¨ğ‘¢, ğ‘£âŸ© âˆ’ ğ‘â€–ğ‘£â€– 2. The equation above shows that we should choose ğ‘ to be âŸ¨ğ‘¢, ğ‘£âŸ©/â€–ğ‘£â€– 2. Making this choice of ğ‘, we can write ğ‘¢ = âŸ¨ğ‘¢, ğ‘£âŸ© â€–ğ‘£â€–2 ğ‘£ + (ğ‘¢ âˆ’ âŸ¨ğ‘¢, ğ‘£âŸ© â€–ğ‘£â€–2 ğ‘£). As you should verify, the equation displayed above explicitly writes ğ‘¢ as a scalar multiple of ğ‘£ plus a vector orthogonal to ğ‘£. Thus we have proved the following key result. 6.13 an orthogonal decomposition Suppose ğ‘¢, ğ‘£ âˆˆ ğ‘‰, with ğ‘£ â‰  0. Set ğ‘ = âŸ¨ğ‘¢, ğ‘£âŸ© â€–ğ‘£â€–2 and ğ‘¤ = ğ‘¢ âˆ’ âŸ¨ğ‘¢, ğ‘£âŸ© â€–ğ‘£â€–2 ğ‘£. Then ğ‘¢ = ğ‘ğ‘£ + ğ‘¤ and âŸ¨ğ‘¤, ğ‘£âŸ© = 0. The orthogonal decomposition 6.13 will be used in the proof of the Cauchyâ€“ Schwarz inequality, which is our next result and is one of the most important inequalities in mathematics. Section 6A Inner Products and Norms 189 6.14 Cauchyâ€“Schwarz inequality Suppose ğ‘¢, ğ‘£ âˆˆ ğ‘‰. Then |âŸ¨ğ‘¢, ğ‘£âŸ©| â‰¤ â€–ğ‘¢â€– â€–ğ‘£â€–. This inequality is an equality if and only if one of ğ‘¢, ğ‘£ is a scalar multiple of the other. Proof If ğ‘£ = 0, then both sides of the desired inequality equal 0. Thus we can assume that ğ‘£ â‰  0. Consider the orthogonal decomposition ğ‘¢ = âŸ¨ğ‘¢, ğ‘£âŸ© â€–ğ‘£â€–2 ğ‘£ + ğ‘¤ given by 6.13, where ğ‘¤ is orthogonal to ğ‘£. By the Pythagorean theorem, â€–ğ‘¢â€–2 = âˆ¥ âŸ¨ğ‘¢, ğ‘£âŸ© â€–ğ‘£â€–2 ğ‘£âˆ¥ 2 + â€–ğ‘¤â€– 2 = âˆ£âŸ¨ğ‘¢, ğ‘£âŸ©âˆ£2 â€–ğ‘£â€–2 + â€–ğ‘¤â€– 2 â‰¥ âˆ£âŸ¨ğ‘¢, ğ‘£âŸ©âˆ£2 â€–ğ‘£â€–2 .6.15 Multiplying both sides of this inequality by â€–ğ‘£â€– 2 and then taking square roots gives the desired inequality. Augustin-Louis Cauchy (1789â€“1857) proved 6.16(a) in 1821. In 1859, Cauchyâ€™s student Viktor Bunyakovsky (1804â€“1889) proved integral inequal- ities like the one in 6.16(b). A few decades later, similar discoveries by Hermann Schwarz (1843â€“1921) at- tracted more attention and led to the name of this inequality. The proof in the paragraph above shows that the Cauchyâ€“Schwarz inequal- ity is an equality if and only if 6.15 is an equality. This happens if and only if ğ‘¤ = 0. But ğ‘¤ = 0if and only if ğ‘¢ is a multiple of ğ‘£ (see 6.13). Thus the Cauchyâ€“Schwarz inequality is an equal- ity if and only if ğ‘¢ is a scalar multiple of ğ‘£ or ğ‘£ is a scalar multiple of ğ‘¢ (or both; the phrasing has been chosen to cover cases in which either ğ‘¢ or ğ‘£ equals 0). 6.16 example:Cauchyâ€“Schwarz inequality (a) If ğ‘¥1, â€¦, ğ‘¥ğ‘›, ğ‘¦1, â€¦, ğ‘¦ğ‘› âˆˆ ğ‘, then (ğ‘¥1ğ‘¦1 + â‹¯ + ğ‘¥ğ‘›ğ‘¦ğ‘›)2 â‰¤(ğ‘¥1 2 + â‹¯ + ğ‘¥ğ‘› 2)(ğ‘¦1 2 + â‹¯ + ğ‘¦ğ‘› 2), as follows from applying the Cauchyâ€“Schwarz inequality to the vectors (ğ‘¥1, â€¦, ğ‘¥ğ‘›), (ğ‘¦1, â€¦, ğ‘¦ğ‘›) âˆˆ ğ‘ğ‘›, using the usual Euclidean inner product. 190 Chapter 6 Inner Product Spaces (b) If ğ‘“, ğ‘” are continuous real-valued functions on [âˆ’1, 1], then âˆ£âˆ«1 âˆ’1 ğ‘“ ğ‘” âˆ£ 2 â‰¤(âˆ«1 âˆ’1 ğ‘“ 2)(âˆ« 1 âˆ’1 ğ‘”2), as follows from applying the Cauchyâ€“Schwarz inequality to Example 6.3(c). In this triangle, the length of ğ‘¢ + ğ‘£ is less than the length of ğ‘¢ plus the length of ğ‘£. The next result, called the triangle inequality, has the geometric interpretation that the length of each side of a triangle is less than the sum of the lengths of the other two sides. Note that the triangle inequality implies that the shortest polygonal path between two points is a single line segment (a polygonal path consists of line segments). 6.17 triangle inequality Suppose ğ‘¢, ğ‘£ âˆˆ ğ‘‰. Then â€–ğ‘¢ + ğ‘£â€– â‰¤ â€–ğ‘¢â€–+ â€–ğ‘£â€–. This inequality is an equality if and only if one of ğ‘¢, ğ‘£ is a nonnegative real multiple of the other. Proof We have â€–ğ‘¢ + ğ‘£â€– 2 = âŸ¨ğ‘¢+ ğ‘£, ğ‘¢ + ğ‘£âŸ© = âŸ¨ğ‘¢, ğ‘¢âŸ©+ âŸ¨ğ‘£, ğ‘£âŸ©+ âŸ¨ğ‘¢, ğ‘£âŸ©+ âŸ¨ğ‘£, ğ‘¢âŸ© = âŸ¨ğ‘¢, ğ‘¢âŸ©+ âŸ¨ğ‘£, ğ‘£âŸ©+ âŸ¨ğ‘¢, ğ‘£âŸ©+ âŸ¨ğ‘¢, ğ‘£âŸ© = â€–ğ‘¢â€–2 + â€–ğ‘£â€– 2 + 2ReâŸ¨ğ‘¢, ğ‘£âŸ© â‰¤ â€–ğ‘¢â€– 2 + â€–ğ‘£â€– 2 + 2âˆ£âŸ¨ğ‘¢, ğ‘£âŸ©âˆ£6.18 â‰¤ â€–ğ‘¢â€– 2 + â€–ğ‘£â€– 2 + 2â€–ğ‘¢â€– â€–ğ‘£â€–6.19 = (â€–ğ‘¢â€– + â€–ğ‘£â€–) 2, where 6.19 follows from the Cauchyâ€“Schwarz inequality (6.14). Taking square roots of both sides of the inequality above gives the desired inequality. The proof above shows that the triangle inequality is an equality if and only if we have equality in 6.18 and 6.19. Thus we have equality in the triangle inequality if and only if 6.20 âŸ¨ğ‘¢, ğ‘£âŸ© = â€–ğ‘¢â€– â€–ğ‘£â€–. If one of ğ‘¢, ğ‘£ is a nonnegative real multiple of the other, then 6.20 holds. Con- versely, suppose 6.20 holds. Then the condition for equality in the Cauchyâ€“ Schwarz inequality (6.14) implies that one of ğ‘¢, ğ‘£ is a scalar multiple of the other. This scalar must be a nonnegative real number, by 6.20, completing the proof. For the reverse triangle inequality, see Exercise 20. Section 6A Inner Products and Norms191 The diagonals of this parallelogram are ğ‘¢ + ğ‘£ and ğ‘¢ âˆ’ ğ‘£. The next result is called the parallel- ogram equality because of its geometric interpretation: in every parallelogram, the sum of the squares of the lengths of the diagonals equals the sum of the squares of the lengths of the four sides. Note that the proof here is more straightforward than the usual proof in Euclidean geometry. 6.21 parallelogram equality Suppose ğ‘¢, ğ‘£ âˆˆ ğ‘‰. Then â€–ğ‘¢ + ğ‘£â€– 2 + â€–ğ‘¢ âˆ’ ğ‘£â€–2 = 2(â€–ğ‘¢â€– 2 + â€–ğ‘£â€–2). Proof We have â€–ğ‘¢ + ğ‘£â€–2 + â€–ğ‘¢ âˆ’ ğ‘£â€–2 = âŸ¨ğ‘¢+ ğ‘£, ğ‘¢ + ğ‘£âŸ©+ âŸ¨ğ‘¢ âˆ’ ğ‘£, ğ‘¢ âˆ’ ğ‘£âŸ© = â€–ğ‘¢â€–2 + â€–ğ‘£â€–2 + âŸ¨ğ‘¢, ğ‘£âŸ©+ âŸ¨ğ‘£, ğ‘¢âŸ© + â€–ğ‘¢â€–2 + â€–ğ‘£â€– 2 âˆ’ âŸ¨ğ‘¢, ğ‘£âŸ© âˆ’ âŸ¨ğ‘£, ğ‘¢âŸ© = 2(â€–ğ‘¢â€– 2 + â€–ğ‘£â€–2), as desired. Exercises 6A 1 Prove or give a counterexample: If ğ‘£1, â€¦, ğ‘£ğ‘š âˆˆ ğ‘‰, then ğ‘š âˆ‘ ğ‘— = 1 ğ‘š âˆ‘ ğ‘˜ = 1âŸ¨ğ‘£ğ‘—, ğ‘£ğ‘˜âŸ© â‰¥ 0. 2 Suppose ğ‘† âˆˆ â„’(ğ‘‰). Define âŸ¨â‹…, â‹…âŸ©1 by âŸ¨ğ‘¢, ğ‘£âŸ©1 = âŸ¨ğ‘†ğ‘¢, ğ‘†ğ‘£âŸ© for all ğ‘¢, ğ‘£ âˆˆ ğ‘‰. Show that âŸ¨â‹…, â‹…âŸ©1 is an inner product on ğ‘‰ if and only if ğ‘† is injective. 3 (a) Show that the function taking an ordered pair ((ğ‘¥1, ğ‘¥2), (ğ‘¦1, ğ‘¦2))of elements of ğ‘2 to |ğ‘¥1ğ‘¦1| + |ğ‘¥2ğ‘¦2| is not an inner product on ğ‘2. (b) Show that the function taking an ordered pair ((ğ‘¥1, ğ‘¥2, ğ‘¥3), (ğ‘¦1, ğ‘¦2, ğ‘¦3)) of elements of ğ‘3 to ğ‘¥1ğ‘¦1 + ğ‘¥3ğ‘¦3 is not an inner product on ğ‘3. 4 Suppose ğ‘‡ âˆˆ â„’(ğ‘‰)is such that â€–ğ‘‡ğ‘£â€– â‰¤ â€–ğ‘£â€–for every ğ‘£ âˆˆ ğ‘‰. Prove that ğ‘‡ âˆ’ âˆš2 ğ¼is injective. 192 Chapter 6 Inner Product Spaces 5 Suppose ğ‘‰ is a real inner product space. (a) Show that âŸ¨ğ‘¢+ ğ‘£, ğ‘¢ âˆ’ ğ‘£âŸ© = â€–ğ‘¢â€– 2 âˆ’ â€–ğ‘£â€–2 for every ğ‘¢, ğ‘£ âˆˆ ğ‘‰. (b) Show that if ğ‘¢, ğ‘£ âˆˆ ğ‘‰ have the same norm, then ğ‘¢ + ğ‘£ is orthogonal to ğ‘¢ âˆ’ ğ‘£. (c) Use (b) to show that the diagonals of a rhombus are perpendicular to each other. 6 Suppose ğ‘¢, ğ‘£ âˆˆ ğ‘‰. Prove that âŸ¨ğ‘¢, ğ‘£âŸ© = 0 âŸº â€–ğ‘¢â€– â‰¤ â€–ğ‘¢+ ğ‘ğ‘£â€– for all ğ‘ âˆˆ ğ…. 7 Suppose ğ‘¢, ğ‘£ âˆˆ ğ‘‰. Prove that â€–ğ‘ğ‘¢ + ğ‘ğ‘£â€– = â€–ğ‘ğ‘¢ + ğ‘ğ‘£â€– for all ğ‘, ğ‘ âˆˆ ğ‘ if and only if â€–ğ‘¢â€– = â€–ğ‘£â€–. 8 Suppose ğ‘, ğ‘, ğ‘, ğ‘¥, ğ‘¦ âˆˆ ğ‘ and ğ‘2 + ğ‘ 2 + ğ‘2 + ğ‘¥2 + ğ‘¦2 â‰¤ 1. Prove that ğ‘ + ğ‘ + ğ‘ + 4ğ‘¥+ 9ğ‘¦ â‰¤ 10. 9 Suppose ğ‘¢, ğ‘£ âˆˆ ğ‘‰ and â€–ğ‘¢â€– = â€–ğ‘£â€– = 1and âŸ¨ğ‘¢, ğ‘£âŸ© = 1. Prove that ğ‘¢ = ğ‘£. 10 Suppose ğ‘¢, ğ‘£ âˆˆ ğ‘‰ and â€–ğ‘¢â€– â‰¤ 1and â€–ğ‘£â€– â‰¤ 1. Prove that âˆš1 âˆ’ â€–ğ‘¢â€–2âˆš1 âˆ’ â€–ğ‘£â€–2 â‰¤ 1 âˆ’âˆ£âŸ¨ğ‘¢, ğ‘£âŸ©âˆ£. 11 Find vectors ğ‘¢, ğ‘£ âˆˆ ğ‘2 such that ğ‘¢ is a scalar multiple of (1, 3), ğ‘£ is orthog- onal to (1, 3), and (1, 2) = ğ‘¢+ ğ‘£. 12 Suppose ğ‘, ğ‘, ğ‘, ğ‘‘ are positive numbers. (a) Prove that (ğ‘ + ğ‘ + ğ‘ + ğ‘‘)( 1 ğ‘ + 1 ğ‘ + 1 ğ‘ + 1 ğ‘‘ )â‰¥ 16. (b) For which positive numbers ğ‘, ğ‘, ğ‘, ğ‘‘ is the inequality above an equality? 13 Show that the square of an average is less than or equal to the average of the squares. More precisely, show that if ğ‘1, â€¦, ğ‘ğ‘› âˆˆ ğ‘, then the square of the average of ğ‘1, â€¦, ğ‘ğ‘› is less than or equal to the average of ğ‘1 2, â€¦, ğ‘ğ‘› 2. 14 Suppose ğ‘£ âˆˆ ğ‘‰ and ğ‘£ â‰  0. Prove that ğ‘£/â€–ğ‘£â€– is the unique closest element on the unit sphere of ğ‘‰ to ğ‘£. More precisely, prove that if ğ‘¢ âˆˆ ğ‘‰ and â€–ğ‘¢â€– = 1, then âˆ¥ğ‘£ âˆ’ ğ‘£ â€–ğ‘£â€– âˆ¥ â‰¤ â€–ğ‘£ âˆ’ ğ‘¢â€–, with equality only if ğ‘¢ = ğ‘£/â€–ğ‘£â€–. 15 Suppose ğ‘¢, ğ‘£ are nonzero vectors in ğ‘2. Prove that âŸ¨ğ‘¢, ğ‘£âŸ© = â€–ğ‘¢â€– â€–ğ‘£â€–cos ğœƒ, where ğœƒ is the angle between ğ‘¢ and ğ‘£ (thinking of ğ‘¢ and ğ‘£ as arrows with initial point at the origin). Hint: Use the law of cosines on the triangle formed by ğ‘¢, ğ‘£, and ğ‘¢ âˆ’ ğ‘£. Section 6A Inner Products and Norms 193 16 The angle between two vectors (thought of as arrows with initial point at the origin) in ğ‘2 or ğ‘3 can be defined geometrically. However, geometry is not as clear in ğ‘ğ‘› for ğ‘› > 3. Thus the angle between two nonzero vectors ğ‘¥, ğ‘¦ âˆˆ ğ‘ğ‘› is defined to be arccos âŸ¨ğ‘¥, ğ‘¦âŸ© â€–ğ‘¥â€– â€–ğ‘¦â€– , where the motivation for this definition comes from Exercise15. Explain why the Cauchyâ€“Schwarz inequality is needed to show that this definition makes sense. 17 Prove that ( ğ‘› âˆ‘ ğ‘˜ = 1 ğ‘ğ‘˜ğ‘ğ‘˜) 2 â‰¤( ğ‘› âˆ‘ ğ‘˜ = 1 ğ‘˜ğ‘ğ‘˜ 2)( ğ‘› âˆ‘ ğ‘˜ = 1 ğ‘ğ‘˜ 2 ğ‘˜ ) for all real numbers ğ‘1, â€¦, ğ‘ğ‘› and ğ‘1, â€¦, ğ‘ğ‘›. 18 (a) Suppose ğ‘“âˆ¶ [1, âˆ) â†’ [0, âˆ) is continuous. Show that (âˆ«âˆ 1 ğ‘“ ) 2 â‰¤âˆ« âˆ 1 ğ‘¥2(ğ‘“ (ğ‘¥)) 2 ğ‘‘ğ‘¥. (b) For which continuous functions ğ‘“âˆ¶ [1, âˆ) â†’ [0, âˆ) is the inequality in (a) an equality with both sides finite? 19 Suppose ğ‘£1, â€¦, ğ‘£ğ‘› is a basis of ğ‘‰ and ğ‘‡ âˆˆ â„’(ğ‘‰). Prove that if ğœ† is an eigenvalue of ğ‘‡, then |ğœ†|2 â‰¤ ğ‘› âˆ‘ ğ‘— = 1 ğ‘› âˆ‘ ğ‘˜ = 1|â„³(ğ‘‡)ğ‘—, ğ‘˜| 2, where â„³(ğ‘‡)ğ‘—, ğ‘˜ denotes the entry in row ğ‘—, column ğ‘˜ of the matrix of ğ‘‡ with respect to the basis ğ‘£1, â€¦, ğ‘£ğ‘›. 20 Prove that if ğ‘¢, ğ‘£ âˆˆ ğ‘‰, then âˆ£ â€–ğ‘¢â€– âˆ’ â€–ğ‘£â€– âˆ£ â‰¤ â€–ğ‘¢ âˆ’ ğ‘£â€–. The inequality above is called the reverse triangle inequality. For the reverse triangle inequality when ğ‘‰ = ğ‚, see Exercise 2 in Chapter 4. 21 Suppose ğ‘¢, ğ‘£ âˆˆ ğ‘‰ are such that â€–ğ‘¢â€– = 3, â€–ğ‘¢ + ğ‘£â€– = 4, â€–ğ‘¢ âˆ’ ğ‘£â€– = 6. What number does â€–ğ‘£â€– equal? 22 Show that if ğ‘¢, ğ‘£ âˆˆ ğ‘‰, then â€–ğ‘¢ + ğ‘£â€– â€–ğ‘¢ âˆ’ ğ‘£â€– â‰¤ â€–ğ‘¢â€– 2 + â€–ğ‘£â€– 2. 23 Suppose ğ‘£1, â€¦, ğ‘£ğ‘š âˆˆ ğ‘‰ are such that â€–ğ‘£ğ‘˜â€– â‰¤ 1for each ğ‘˜ = 1, â€¦, ğ‘š. Show that there exist ğ‘1, â€¦, ğ‘ğ‘š âˆˆ {1, âˆ’1}such that â€–ğ‘1ğ‘£1 + â‹¯ + ğ‘ğ‘šğ‘£ğ‘šâ€– â‰¤âˆš ğ‘š. 194 Chapter 6 Inner Product Spaces 24 Prove or give a counterexample: If â€– â‹… â€– is the norm associated with an inner product on ğ‘2, then there exists (ğ‘¥, ğ‘¦) âˆˆ ğ‘2 such that â€–(ğ‘¥, ğ‘¦)â€– â‰  max{|ğ‘¥|, |ğ‘¦|}. 25 Suppose ğ‘ > 0. Prove that there is an inner product on ğ‘2 such that the associated norm is given by â€–(ğ‘¥, ğ‘¦)â€– = (|ğ‘¥|ğ‘ + |ğ‘¦| ğ‘) 1/ğ‘ for all (ğ‘¥, ğ‘¦) âˆˆ ğ‘2 if and only if ğ‘ = 2. 26 Suppose ğ‘‰ is a real inner product space. Prove that âŸ¨ğ‘¢, ğ‘£âŸ© = â€–ğ‘¢ + ğ‘£â€– 2 âˆ’ â€–ğ‘¢ âˆ’ ğ‘£â€–2 4 for all ğ‘¢, ğ‘£ âˆˆ ğ‘‰. 27 Suppose ğ‘‰ is a complex inner product space. Prove that âŸ¨ğ‘¢, ğ‘£âŸ© = â€–ğ‘¢ + ğ‘£â€– 2 âˆ’ â€–ğ‘¢ âˆ’ ğ‘£â€–2 + â€–ğ‘¢ + ğ‘–ğ‘£â€– 2ğ‘– âˆ’ â€–ğ‘¢ âˆ’ ğ‘–ğ‘£â€–2ğ‘– 4 for all ğ‘¢, ğ‘£ âˆˆ ğ‘‰. 28 A norm on a vector space ğ‘ˆ is a function â€– â‹… â€–âˆ¶ ğ‘ˆ â†’ [0, âˆ) such that â€–ğ‘¢â€– = 0if and only if ğ‘¢ = 0, â€–ğ›¼ğ‘¢â€– = |ğ›¼|â€–ğ‘¢â€– for all ğ›¼ âˆˆ ğ… and all ğ‘¢ âˆˆ ğ‘ˆ, and â€–ğ‘¢ + ğ‘£â€– â‰¤ â€–ğ‘¢â€–+ â€–ğ‘£â€– for all ğ‘¢, ğ‘£ âˆˆ ğ‘ˆ. Prove that a norm satisfying the parallelogram equality comes from an inner product (in other words, show that if â€– â‹… â€– is a norm on ğ‘ˆ satisfying the parallelogram equality, then there is an inner product âŸ¨â‹…, â‹…âŸ©on ğ‘ˆ such that â€–ğ‘¢â€– = âŸ¨ğ‘¢, ğ‘¢âŸ© 1/2 for all ğ‘¢ âˆˆ ğ‘ˆ). 29 Suppose ğ‘‰1, â€¦, ğ‘‰ğ‘š are inner product spaces. Show that the equation âŸ¨(ğ‘¢1, â€¦, ğ‘¢ğ‘š), (ğ‘£1, â€¦, ğ‘£ğ‘š)âŸ©= âŸ¨ğ‘¢1, ğ‘£1âŸ©+ â‹¯ + âŸ¨ğ‘¢ğ‘š, ğ‘£ğ‘šâŸ© defines an inner product onğ‘‰1 Ã— â‹¯ Ã— ğ‘‰ğ‘š. In the expression above on the right, for each ğ‘˜ = 1, â€¦, ğ‘š, the inner product âŸ¨ğ‘¢ğ‘˜, ğ‘£ğ‘˜âŸ©denotes the inner product on ğ‘‰ğ‘˜. Each of the spaces ğ‘‰1, â€¦, ğ‘‰ğ‘š may have a different inner product, even though the same notation is used here. 30 Suppose ğ‘‰ is a real inner product space. For ğ‘¢, ğ‘£, ğ‘¤, ğ‘¥ âˆˆ ğ‘‰, define âŸ¨ğ‘¢+ ğ‘–ğ‘£, ğ‘¤ + ğ‘–ğ‘¥âŸ©ğ‚ = âŸ¨ğ‘¢, ğ‘¤âŸ©+ âŸ¨ğ‘£, ğ‘¥âŸ©+ (âŸ¨ğ‘£, ğ‘¤âŸ© âˆ’ âŸ¨ğ‘¢, ğ‘¥âŸ©)ğ‘–. (a) Show that âŸ¨â‹…, â‹…âŸ©ğ‚ makes ğ‘‰ğ‚ into a complex inner product space. (b) Show that if ğ‘¢, ğ‘£ âˆˆ ğ‘‰, then âŸ¨ğ‘¢, ğ‘£âŸ©ğ‚ = âŸ¨ğ‘¢, ğ‘£âŸ© and â€–ğ‘¢ + ğ‘–ğ‘£â€–ğ‚ 2 = â€–ğ‘¢â€–2 + â€–ğ‘£â€– 2. See Exercise 8 in Section 1B for the definition of the complexification ğ‘‰ğ‚. Section 6A Inner Products and Norms 195 31 Suppose ğ‘¢, ğ‘£, ğ‘¤ âˆˆ ğ‘‰. Prove that âˆ¥ğ‘¤ âˆ’ 1 2 (ğ‘¢ + ğ‘£)âˆ¥ 2 = â€–ğ‘¤ âˆ’ ğ‘¢â€–2 + â€–ğ‘¤ âˆ’ ğ‘£â€–2 2 âˆ’ â€–ğ‘¢ âˆ’ ğ‘£â€–2 4 . 32 Suppose that ğ¸ is a subset of ğ‘‰ with the property that ğ‘¢, ğ‘£ âˆˆ ğ¸ implies 1 2 (ğ‘¢ + ğ‘£) âˆˆ ğ¸. Let ğ‘¤ âˆˆ ğ‘‰. Show that there is at most one point in ğ¸ that is closest to ğ‘¤. In other words, show that there is at most one ğ‘¢ âˆˆ ğ¸ such that â€–ğ‘¤ âˆ’ ğ‘¢â€– â‰¤ â€–ğ‘¤ âˆ’ ğ‘¥â€– for all ğ‘¥ âˆˆ ğ¸. 33 Suppose ğ‘“, ğ‘” are differentiable functions from ğ‘ to ğ‘ğ‘›. (a) Show that âŸ¨ğ‘“ (ğ‘¡), ğ‘”(ğ‘¡)âŸ© â€² = âŸ¨ğ‘“ â€²(ğ‘¡), ğ‘”(ğ‘¡)âŸ©+ âŸ¨ğ‘“ (ğ‘¡), ğ‘”â€²(ğ‘¡)âŸ©. (b) Suppose ğ‘ is a positive number and âˆ¥ ğ‘“ (ğ‘¡)âˆ¥ = ğ‘ for every ğ‘¡ âˆˆ ğ‘. Show that âŸ¨ğ‘“ â€²(ğ‘¡), ğ‘“ (ğ‘¡)âŸ©= 0for every ğ‘¡ âˆˆ ğ‘. (c) Interpret the result in (b) geometrically in terms of the tangent vector to a curve lying on a sphere in ğ‘ğ‘› centered at the origin. A function ğ‘“âˆ¶ ğ‘ â†’ ğ‘ğ‘› is called differentiable if there exist differentiable functions ğ‘“1, â€¦, ğ‘“ğ‘› from ğ‘ to ğ‘ such that ğ‘“ (ğ‘¡) = (ğ‘“1(ğ‘¡), â€¦, ğ‘“ğ‘›(ğ‘¡))for each ğ‘¡ âˆˆ ğ‘. Furthermore, for each ğ‘¡ âˆˆ ğ‘, the derivative ğ‘“ â€²(ğ‘¡) âˆˆ ğ‘ğ‘› is defined by ğ‘“ â€²(ğ‘¡) = (ğ‘“1â€²(ğ‘¡), â€¦, ğ‘“ğ‘›â€²(ğ‘¡)). 34 Use inner products to prove Apolloniusâ€™s identity: In a triangle with sides of length ğ‘, ğ‘, and ğ‘, let ğ‘‘ be the length of the line segment from the midpoint of the side of length ğ‘ to the opposite vertex. Then ğ‘ 2 + ğ‘2 = 1 2 ğ‘2 + 2ğ‘‘ 2. 196 Chapter 6 Inner Product Spaces 35 Fix a positive integer ğ‘›. The Laplacian Î”ğ‘ of a twice differentiable real- valued function ğ‘ on ğ‘ğ‘› is the function on ğ‘ğ‘› defined by Î”ğ‘ = ğœ•2ğ‘ ğœ•ğ‘¥1 2 + â‹¯ + ğœ•2ğ‘ ğœ•ğ‘¥ğ‘› 2 . The function ğ‘ is called harmonic if Î”ğ‘ = 0. A polynomial on ğ‘ğ‘› is a linear combination (with coefficients in ğ‘) of functions of the form ğ‘¥1 ğ‘š1â‹¯ğ‘¥ğ‘› ğ‘šğ‘›, where ğ‘š1, â€¦, ğ‘šğ‘› are nonnegative integers. Suppose ğ‘ is a polynomial on ğ‘ğ‘›. Prove that there exists a harmonic polynomial ğ‘ on ğ‘ğ‘› such that ğ‘(ğ‘¥) = ğ‘(ğ‘¥) for every ğ‘¥ âˆˆ ğ‘ğ‘› with â€–ğ‘¥â€– = 1. The only fact about harmonic functions that you need for this exercise is that if ğ‘ is a harmonic function on ğ‘ğ‘› and ğ‘(ğ‘¥) = 0for all ğ‘¥ âˆˆ ğ‘ğ‘› with â€–ğ‘¥â€– = 1, then ğ‘ = 0. Hint: A reasonable guess is that the desired harmonic polynomial ğ‘ is of the form ğ‘ + (1 âˆ’ â€–ğ‘¥â€– 2)ğ‘Ÿ for some polynomial ğ‘Ÿ. Prove that there is a polynomial ğ‘Ÿ on ğ‘ğ‘› such that ğ‘ + (1 âˆ’ â€–ğ‘¥â€– 2)ğ‘Ÿ is harmonic by defining an operator ğ‘‡ on a suitable vector space by ğ‘‡ğ‘Ÿ = Î”((1 âˆ’ â€–ğ‘¥â€– 2)ğ‘Ÿ) and then showing that ğ‘‡ is injective and hence surjective. In realms of numbers, where the secrets lie, A noble truth emerges from the deep, Cauchy and Schwarz, their wisdom they apply, An inequality for all to keep. Two vectors, by this bond, are intertwined, As inner products weave a gilded thread, Their magnitude, by providence, confined, A bound to which their destiny is wed. Though shadows fall, and twilight dims the day, This inequality will stand the test, To guide us in our quest, to light the way, And in its truth, our understanding rest. So sing, ye muses, of this noble feat, Cauchyâ€“Schwarz, the bound that none can beat. â€”written by ChatGPT with input Shakespearean sonnet on Cauchyâ€“Schwarz inequality Section 6B Orthonormal Bases 197 6B Orthonormal Bases Orthonormal Lists and the Gramâ€“Schmidt Procedure 6.22 definition:orthonormal â€¢ A list of vectors is called orthonormal if each vector in the list has norm 1 and is orthogonal to all the other vectors in the list. â€¢ In other words, a list ğ‘’1, â€¦, ğ‘’ğ‘š of vectors in ğ‘‰ is orthonormal if âŸ¨ğ‘’ğ‘—, ğ‘’ğ‘˜âŸ© = â§{ â¨{â© 1 if ğ‘— = ğ‘˜, 0 if ğ‘— â‰  ğ‘˜ for all ğ‘—, ğ‘˜ âˆˆ {1, â€¦, ğ‘š}. 6.23 example:orthonormal lists (a) The standard basis of ğ…ğ‘› is an orthonormal list. (b) ( 1 âˆš3 , 1 âˆš3 , 1 âˆš3 ), (âˆ’ 1 âˆš2 , 1 âˆš2 , 0)is an orthonormal list in ğ…3. (c) ( 1 âˆš3 , 1 âˆš3 , 1 âˆš3 ), (âˆ’ 1 âˆš2 , 1 âˆš2 , 0), ( 1 âˆš6 , 1 âˆš6 , âˆ’ 2 âˆš6 )is an orthonormal list in ğ…3. (d) Suppose ğ‘› is a positive integer. Then, as Exercise 4 asks you to verify, 1 âˆš2ğœ‹ , cos ğ‘¥ âˆš ğœ‹ , cos 2ğ‘¥ âˆš ğœ‹ , â€¦, cos ğ‘›ğ‘¥ âˆš ğœ‹ , sin ğ‘¥ âˆš ğœ‹ , sin 2ğ‘¥ âˆš ğœ‹ , â€¦, sin ğ‘›ğ‘¥ âˆš ğœ‹ is an orthonormal list of vectors in ğ¶[âˆ’ğœ‹, ğœ‹], the vector space of continuous real-valued functions on [âˆ’ğœ‹, ğœ‹] with inner product âŸ¨ ğ‘“, ğ‘”âŸ© =âˆ«ğœ‹ âˆ’ğœ‹ ğ‘“ ğ‘”. The orthonormal list above is often used for modeling periodic phenomena, such as tides. (e) Suppose we make ğ’«2(ğ‘) into an inner product space using the inner product given by âŸ¨ğ‘, ğ‘âŸ© =âˆ« 1 âˆ’1 ğ‘ğ‘ for all ğ‘, ğ‘ âˆˆ ğ’«2(ğ‘). The standard basis 1, ğ‘¥, ğ‘¥2 of ğ’«2(ğ‘) is not an orthonor- mal list because the vectors in that list do not have norm 1. Dividing each vector by its norm gives the list 1/âˆš2, âˆš3/2ğ‘¥, âˆš5/2ğ‘¥ 2, in which each vector has norm 1, and the second vector is orthogonal to the first and third vectors. However, the first and third vectors are not orthogonal. Thus this is not an orthonormal list. Soon we will see how to construct an orthonormal list from the standard basis 1, ğ‘¥, ğ‘¥2 (see Example 6.34). 198 Chapter 6 Inner Product Spaces Orthonormal lists are particularly easy to work with, as illustrated by the next result. 6.24 norm of an orthonormal linear combination Suppose ğ‘’1, â€¦, ğ‘’ğ‘š is an orthonormal list of vectors in ğ‘‰. Then â€–ğ‘1ğ‘’1 + â‹¯ + ğ‘ğ‘šğ‘’ğ‘šâ€–2 = |ğ‘1| 2 + â‹¯ + |ğ‘ğ‘š|2 for all ğ‘1, â€¦, ğ‘ğ‘š âˆˆ ğ…. Proof Because each ğ‘’ğ‘˜ has norm 1, this follows from repeated applications of the Pythagorean theorem (6.12). The result above has the following important corollary. 6.25 orthonormal lists are linearly independent Every orthonormal list of vectors is linearly independent. Proof Suppose ğ‘’1, â€¦, ğ‘’ğ‘š is an orthonormal list of vectors in ğ‘‰ and ğ‘1, â€¦, ğ‘ğ‘š âˆˆ ğ… are such that ğ‘1ğ‘’1 + â‹¯ + ğ‘ğ‘šğ‘’ğ‘š = 0. Then |ğ‘1| 2 + â‹¯ + |ğ‘ğ‘š| 2 = 0(by 6.24), which means that all the ğ‘ğ‘˜â€™s are 0. Thus ğ‘’1, â€¦, ğ‘’ğ‘š is linearly independent. Now we come to an important inequality. 6.26 Besselâ€™s inequality Suppose ğ‘’1, â€¦, ğ‘’ğ‘š is an orthonormal list of vectors in ğ‘‰. If ğ‘£ âˆˆ ğ‘‰ then âˆ£âŸ¨ğ‘£, ğ‘’1âŸ©âˆ£2 + â‹¯ + âˆ£âŸ¨ğ‘£, ğ‘’ğ‘šâŸ©âˆ£2 â‰¤ â€–ğ‘£â€– 2. Proof Suppose ğ‘£ âˆˆ ğ‘‰. Then ğ‘£ = âŸ¨ğ‘£, ğ‘’1âŸ©ğ‘’1 + â‹¯ + âŸ¨ğ‘£, ğ‘’ğ‘šâŸ©ğ‘’ğ‘šâŸâŸâŸâŸâŸâŸâŸâŸâŸ ğ‘¢ + ğ‘£ âˆ’ âŸ¨ğ‘£, ğ‘’1âŸ©ğ‘’1 âˆ’ â‹¯ âˆ’ âŸ¨ğ‘£, ğ‘’ğ‘šâŸ©ğ‘’ğ‘šâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸ ğ‘¤ . Let ğ‘¢ and ğ‘¤ be defined as in the equation above. Ifğ‘˜ âˆˆ {1, â€¦, ğ‘š}, then âŸ¨ğ‘¤, ğ‘’ğ‘˜âŸ© = âŸ¨ğ‘£, ğ‘’ğ‘˜âŸ© âˆ’ âŸ¨ğ‘£, ğ‘’ğ‘˜âŸ©âŸ¨ğ‘’ğ‘˜, ğ‘’ğ‘˜âŸ© = 0. This implies that âŸ¨ğ‘¤, ğ‘¢âŸ© = 0. The Pythagorean theorem now implies that â€–ğ‘£â€– 2 = â€–ğ‘¢â€–2 + â€–ğ‘¤â€– 2 â‰¥ â€–ğ‘¢â€– 2 = âˆ£âŸ¨ğ‘£, ğ‘’1âŸ©âˆ£2 + â‹¯ + âˆ£âŸ¨ğ‘£, ğ‘’ğ‘šâŸ©âˆ£2, where the last line comes from 6.24. Section 6B Orthonormal Bases 199 The next definition introduces one of the most useful concepts in the study of inner product spaces. 6.27 definition:orthonormal basis An orthonormal basis of ğ‘‰ is an orthonormal list of vectors in ğ‘‰ that is also a basis of ğ‘‰. For example, the standard basis is an orthonormal basis of ğ…ğ‘›. 6.28 orthonormal lists of the right length are orthonormal bases Suppose ğ‘‰ is finite-dimensional. Then every orthonormal list of vectors inğ‘‰ of length dim ğ‘‰ is an orthonormal basis of ğ‘‰. Proof By 6.25, every orthonormal list of vectors in ğ‘‰ is linearly independent. Thus every such list of the right length is a basisâ€”see 2.38. 6.29 example:an orthonormal basis of ğ…4 As mentioned above, the standard basis is an orthonormal basis of ğ…4. We now show that ( 1 2 , 1 2 , 1 2 , 1 2 ), ( 1 2 , 1 2 , âˆ’ 1 2 , âˆ’ 1 2 ), ( 1 2 , âˆ’ 1 2 , âˆ’ 1 2 , 1 2 ), (âˆ’ 1 2 , 1 2 , âˆ’ 1 2 , 1 2 ) is also an orthonormal basis of ğ…4. We have âˆ¥( 1 2 , 1 2 , 1 2 , 1 2 )âˆ¥= âˆš 1 22 + 1 22 + 1 22 + 1 22 = 1. Similarly, the other three vectors in the list above also have norm 1. Note that âŸ¨( 1 2 , 1 2 , 1 2 , 1 2 ), ( 1 2 , 1 2 , âˆ’ 1 2 , âˆ’ 1 2 )âŸ©= 1 2 â‹… 1 2 + 1 2 â‹… 1 2 + 1 2 â‹… (âˆ’ 1 2 )+ 1 2 â‹… (âˆ’ 1 2 )= 0. Similarly, the inner product of any two distinct vectors in the list above also equals 0. Thus the list above is orthonormal. Because we have an orthonormal list of length four in the four-dimensional vector space ğ…4, this list is an orthonormal basis of ğ…4 (by 6.28). In general, given a basis ğ‘’1, â€¦, ğ‘’ğ‘› of ğ‘‰ and a vector ğ‘£ âˆˆ ğ‘‰, we know that there is some choice of scalars ğ‘1, â€¦, ğ‘ğ‘› âˆˆ ğ… such that ğ‘£ = ğ‘1ğ‘’1 + â‹¯ + ğ‘ğ‘›ğ‘’ğ‘›. Computing the numbers ğ‘1, â€¦, ğ‘ğ‘› that satisfy the equation above can be a long computation for an arbitrary basis of ğ‘‰. The next result shows, however, that this is easy for an orthonormal basisâ€”just take ğ‘ğ‘˜ = âŸ¨ğ‘£, ğ‘’ğ‘˜âŸ©. 200 Chapter 6 Inner Product Spaces The formula below for â€–ğ‘£â€– is called Parsevalâ€™s identity. It was published in 1799 in the context of Fourier series. Notice how the next result makes each inner product space of dimension ğ‘› behave like ğ…ğ‘›, with the role of the coordinates of a vector in ğ…ğ‘› played by âŸ¨ğ‘£, ğ‘’1âŸ©, â€¦, âŸ¨ğ‘£, ğ‘’ğ‘›âŸ©. 6.30 writing a vector as a linear combination of an orthonormal basis Suppose ğ‘’1, â€¦, ğ‘’ğ‘› is an orthonormal basis of ğ‘‰ and ğ‘¢, ğ‘£ âˆˆ ğ‘‰. Then (a) ğ‘£ = âŸ¨ğ‘£, ğ‘’1âŸ©ğ‘’1 + â‹¯ + âŸ¨ğ‘£, ğ‘’ğ‘›âŸ©ğ‘’ğ‘›, (b) â€–ğ‘£â€– 2 = âˆ£âŸ¨ğ‘£, ğ‘’1âŸ©âˆ£2 + â‹¯ + âˆ£âŸ¨ğ‘£, ğ‘’ğ‘›âŸ©âˆ£2, (c) âŸ¨ğ‘¢, ğ‘£âŸ© = âŸ¨ğ‘¢, ğ‘’1âŸ©âŸ¨ğ‘£, ğ‘’1âŸ©+ â‹¯ + âŸ¨ğ‘¢, ğ‘’ğ‘›âŸ©âŸ¨ğ‘£, ğ‘’ğ‘›âŸ©. Proof Because ğ‘’1, â€¦, ğ‘’ğ‘› is a basis of ğ‘‰, there exist scalars ğ‘1, â€¦, ğ‘ğ‘› such that ğ‘£ = ğ‘1ğ‘’1 + â‹¯ + ğ‘ğ‘›ğ‘’ğ‘›. Because ğ‘’1, â€¦, ğ‘’ğ‘› is orthonormal, taking the inner product of both sides of this equation with ğ‘’ğ‘˜ gives âŸ¨ğ‘£, ğ‘’ğ‘˜âŸ© = ğ‘ğ‘˜. Thus (a) holds. Now (b) follows immediately from (a) and 6.24. Taking the inner product of ğ‘¢ with each side of (a) and then using the conjugate symmetry of the inner product gives (c). 6.31 example:finding coefficients for a linear combination Suppose we want to write the vector (1, 2, 4, 7) âˆˆ ğ… 4 as a linear combination of the orthonormal basis ( 1 2 , 1 2 , 1 2 , 1 2 ), ( 1 2 , 1 2 , âˆ’ 1 2 , âˆ’ 1 2 ), ( 1 2 , âˆ’ 1 2 , âˆ’ 1 2 , 1 2 ), (âˆ’ 1 2 , 1 2 , âˆ’ 1 2 , 1 2 ) of ğ…4 from Example 6.29. Instead of solving a system of four linear equations in four unknowns, as typically would be required if we were working with a nonorthonormal basis, we simply evaluate four inner products and use 6.30(a), getting that (1, 2, 4, 7)equals 7( 1 2 , 1 2 , 1 2 , 1 2 )âˆ’ 4( 1 2 , 1 2 , âˆ’ 1 2 , âˆ’ 1 2 )+ ( 1 2 , âˆ’ 1 2 , âˆ’ 1 2 , 1 2 )+ 2(âˆ’ 1 2 , 1 2 , âˆ’ 1 2 , 1 2 ). Now that we understand the usefulness of orthonormal bases, how do we go about finding them? For example, doesğ’«ğ‘š(ğ‘) with inner product as in 6.3(c) have an orthonormal basis? The next result will lead to answers to these questions. JÃ¸rgen Gram (1850â€“1916) and Erhard Schmidt (1876â€“1959) popularized this algorithm that constructs orthonormal lists. The algorithm used in the next proof is called the Gramâ€“Schmidt procedure. It gives a method for turning a linearly independent list into an orthonormal list with the same span as the original list. Section 6B Orthonormal Bases 201 6.32 Gramâ€“Schmidt procedure Suppose ğ‘£1, â€¦, ğ‘£ğ‘š is a linearly independent list of vectors in ğ‘‰. Let ğ‘“1 = ğ‘£1. For ğ‘˜ = 2, â€¦, ğ‘š, defineğ‘“ğ‘˜ inductively by ğ‘“ğ‘˜ = ğ‘£ğ‘˜ âˆ’ âŸ¨ğ‘£ğ‘˜, ğ‘“1âŸ© â€– ğ‘“1â€–2 ğ‘“1 âˆ’ â‹¯ âˆ’ âŸ¨ğ‘£ğ‘˜, ğ‘“ğ‘˜ âˆ’ 1âŸ© â€– ğ‘“ğ‘˜ âˆ’ 1â€–2 ğ‘“ğ‘˜ âˆ’ 1. For each ğ‘˜ = 1, â€¦, ğ‘š, let ğ‘’ğ‘˜ = ğ‘“ğ‘˜ â€– ğ‘“ğ‘˜â€– . Then ğ‘’1, â€¦, ğ‘’ğ‘š is an orthonormal list of vectors in ğ‘‰ such that span(ğ‘£1, â€¦, ğ‘£ğ‘˜) = span(ğ‘’1, â€¦, ğ‘’ğ‘˜) for each ğ‘˜ = 1, â€¦, ğ‘š. Proof We will show by induction on ğ‘˜ that the desired conclusion holds. To get started with ğ‘˜ = 1, note that because ğ‘’1 = ğ‘“1/â€– ğ‘“1â€–, we have â€–ğ‘’1â€– = 1; also, span(ğ‘£1) = span(ğ‘’1) because ğ‘’1 is a nonzero multiple of ğ‘£1. Suppose 1 < ğ‘˜ â‰¤ ğ‘šand the list ğ‘’1, â€¦, ğ‘’ğ‘˜ âˆ’ 1 generated by 6.32 is an orthonormal list such that 6.33 span(ğ‘£1, â€¦, ğ‘£ğ‘˜ âˆ’ 1) = span(ğ‘’1, â€¦, ğ‘’ğ‘˜ âˆ’ 1). Because ğ‘£1, â€¦, ğ‘£ğ‘š is linearly independent, we have ğ‘£ğ‘˜ âˆ‰ span(ğ‘£1, â€¦, ğ‘£ğ‘˜ âˆ’ 1). Thus ğ‘£ğ‘˜ âˆ‰ span(ğ‘’1, â€¦, ğ‘’ğ‘˜ âˆ’ 1) = span( ğ‘“1, â€¦, ğ‘“ğ‘˜ âˆ’ 1), which implies that ğ‘“ğ‘˜ â‰  0. Hence we are not dividing by 0in the definition ofğ‘’ğ‘˜ given in 6.32. Dividing a vector by its norm produces a new vector with norm 1; thus â€–ğ‘’ğ‘˜â€– = 1. Let ğ‘— âˆˆ {1, â€¦, ğ‘˜ âˆ’ 1}. Then âŸ¨ğ‘’ğ‘˜, ğ‘’ğ‘—âŸ© = 1 â€– ğ‘“ğ‘˜â€– â€– ğ‘“ğ‘—â€– âŸ¨ ğ‘“ğ‘˜, ğ‘“ğ‘—âŸ© = 1 â€– ğ‘“ğ‘˜â€– â€– ğ‘“ğ‘—â€– âŸ¨ğ‘£ğ‘˜ âˆ’ âŸ¨ğ‘£ğ‘˜, ğ‘“1âŸ© â€– ğ‘“1â€–2 ğ‘“1 âˆ’ â‹¯ âˆ’ âŸ¨ğ‘£ğ‘˜, ğ‘“ğ‘˜ âˆ’ 1âŸ© â€– ğ‘“ğ‘˜ âˆ’ 1â€–2 ğ‘“ğ‘˜ âˆ’ 1, ğ‘“ğ‘—âŸ© = 1 â€– ğ‘“ğ‘˜â€– â€– ğ‘“ğ‘—â€– (âŸ¨ğ‘£ğ‘˜, ğ‘“ğ‘—âŸ© âˆ’ âŸ¨ğ‘£ğ‘˜, ğ‘“ğ‘—âŸ©) = 0. Thus ğ‘’1, â€¦, ğ‘’ğ‘˜ is an orthonormal list. From the definition ofğ‘’ğ‘˜ given in 6.32, we see that ğ‘£ğ‘˜ âˆˆ span(ğ‘’1, â€¦, ğ‘’ğ‘˜). Combining this information with 6.33 shows that span(ğ‘£1, â€¦, ğ‘£ğ‘˜) âŠ†span(ğ‘’1, â€¦, ğ‘’ğ‘˜). Both lists above are linearly independent (the ğ‘£â€™s by hypothesis, and the ğ‘’â€™s by orthonormality and 6.25). Thus both subspaces above have dimension ğ‘˜, and hence they are equal, completing the induction step and thus completing the proof. 202 Chapter 6 Inner Product Spaces 6.34 example:an orthonormal basis of ğ’«2(ğ‘) Suppose we make ğ’«2(ğ‘) into an inner product space using the inner product given by âŸ¨ğ‘, ğ‘âŸ© =âˆ« 1 âˆ’1 ğ‘ğ‘ for all ğ‘, ğ‘ âˆˆ ğ’«2(ğ‘). We know that 1, ğ‘¥, ğ‘¥2 is a basis of ğ’«2(ğ‘), but it is not an orthonormal basis. We will find an orthonormal basis ofğ’«2(ğ‘) by applying the Gramâ€“Schmidt procedure with ğ‘£1 = 1, ğ‘£2 = ğ‘¥, and ğ‘£3 = ğ‘¥2. To get started, take ğ‘“1 = ğ‘£1 = 1. Thus â€– ğ‘“1â€–2 = âˆ« 1 âˆ’1 1 = 2. Hence the formula in 6.32 tells us that ğ‘“2 = ğ‘£2 âˆ’ âŸ¨ğ‘£2, ğ‘“1âŸ© â€– ğ‘“1â€–2 ğ‘“1 = ğ‘¥ âˆ’ âŸ¨ğ‘¥, 1âŸ© â€– ğ‘“1â€–2 = ğ‘¥, where the last equality holds because âŸ¨ğ‘¥, 1âŸ© =âˆ« 1 âˆ’1 ğ‘¡ ğ‘‘ğ‘¡ = 0. The formula above for ğ‘“2 implies that â€– ğ‘“2â€– 2 = âˆ«1 âˆ’1 ğ‘¡2 ğ‘‘ğ‘¡ = 2 3 . Now the formula in 6.32 tells us that ğ‘“3 = ğ‘£3 âˆ’ âŸ¨ğ‘£3, ğ‘“1âŸ© â€– ğ‘“1â€–2 ğ‘“1 âˆ’ âŸ¨ğ‘£3, ğ‘“2âŸ© â€– ğ‘“2â€–2 ğ‘“2 = ğ‘¥2 âˆ’ 1 2 âŸ¨ğ‘¥2, 1âŸ©âˆ’ 3 2 âŸ¨ğ‘¥2, ğ‘¥âŸ©ğ‘¥ = ğ‘¥2 âˆ’ 1 3 . The formula above for ğ‘“3 implies that â€– ğ‘“3â€–2 = âˆ« 1 âˆ’1(ğ‘¡2 âˆ’ 1 3 ) 2 ğ‘‘ğ‘¡ = âˆ« 1 âˆ’1(ğ‘¡4 âˆ’ 2 3 ğ‘¡2 + 1 9 )ğ‘‘ğ‘¡ = 8 45 . Now dividing each of ğ‘“1, ğ‘“2, ğ‘“3 by its norm gives us the orthonormal list âˆš 1 2 , âˆš 3 2 ğ‘¥, âˆš 45 8 (ğ‘¥2 âˆ’ 1 3 ). The orthonormal list above has length three, which is the dimension of ğ’«2(ğ‘). Hence this orthonormal list is an orthonormal basis of ğ’«2(ğ‘) [by6.28]. Now we can answer the question about the existence of orthonormal bases. 6.35 existence of orthonormal basis Every finite-dimensional inner product space has an orthonormal basis. Proof Suppose ğ‘‰ is finite-dimensional. Choose a basis ofğ‘‰. Apply the Gramâ€“ Schmidt procedure (6.32) to it, producing an orthonormal list of length dim ğ‘‰. By 6.28, this orthonormal list is an orthonormal basis of ğ‘‰. Sometimes we need to know not only that an orthonormal basis exists, but also that every orthonormal list can be extended to an orthonormal basis. In the next corollary, the Gramâ€“Schmidt procedure shows that such an extension is always possible. Section 6B Orthonormal Bases 203 6.36 every orthonormal list extends to an orthonormal basis Suppose ğ‘‰ is finite-dimensional. Then every orthonormal list of vectors inğ‘‰ can be extended to an orthonormal basis of ğ‘‰. Proof Suppose ğ‘’1, â€¦, ğ‘’ğ‘š is an orthonormal list of vectors in ğ‘‰. Then ğ‘’1, â€¦, ğ‘’ğ‘š is linearly independent (by 6.25). Hence this list can be extended to a basis ğ‘’1, â€¦, ğ‘’ğ‘š, ğ‘£1, â€¦, ğ‘£ğ‘› of ğ‘‰ (see 2.32). Now apply the Gramâ€“Schmidt procedure (6.32) to ğ‘’1, â€¦, ğ‘’ğ‘š, ğ‘£1, â€¦, ğ‘£ğ‘›, producing an orthonormal list ğ‘’1, â€¦, ğ‘’ğ‘š, ğ‘“1, â€¦, ğ‘“ğ‘›; here the formula given by the Gramâ€“Schmidt procedure leaves the firstğ‘š vectors unchanged because they are already orthonormal. The list above is an orthonormal basis of ğ‘‰ by 6.28. Recall that a matrix is called upper triangular if it looks like this: â›âœâœâœ â âˆ— âˆ— â‹± 0 âˆ— ââŸâŸâŸ â  , where the 0in the matrix above indicates that all entries below the diagonal equal 0, and asterisks are used to denote entries on and above the diagonal. In the last chapter, we gave a necessary and sufficient condition for an operator to have an upper-triangular matrix with respect to some basis (see 5.44). Now that we are dealing with inner product spaces, we would like to know whether there exists an orthonormal basis with respect to which we have an upper-triangular matrix. The next result shows that the condition for an operator to have an upper- triangular matrix with respect to some orthonormal basis is the same as the condition to have an upper-triangular matrix with respect to an arbitrary basis. 6.37 upper-triangular matrix with respect to some orthonormal basis Suppose ğ‘‰ is finite-dimensional andğ‘‡ âˆˆ â„’(ğ‘‰). Then ğ‘‡ has an upper- triangular matrix with respect to some orthonormal basis of ğ‘‰ if and only if the minimal polynomial of ğ‘‡ equals (ğ‘§ âˆ’ ğœ†1)â‹¯(ğ‘§ âˆ’ ğœ†ğ‘š) for some ğœ†1, â€¦, ğœ†ğ‘š âˆˆ ğ…. Proof Suppose ğ‘‡ has an upper-triangular matrix with respect to some basis ğ‘£1, â€¦, ğ‘£ğ‘› of ğ‘‰. Thus span(ğ‘£1, â€¦, ğ‘£ğ‘˜) is invariant under ğ‘‡ for each ğ‘˜ = 1, â€¦, ğ‘› (see 5.39). Apply the Gramâ€“Schmidt procedure to ğ‘£1, â€¦, ğ‘£ğ‘›, producing an orthonormal basis ğ‘’1, â€¦, ğ‘’ğ‘› of ğ‘‰. Because span(ğ‘’1, â€¦, ğ‘’ğ‘˜) = span(ğ‘£1, â€¦, ğ‘£ğ‘˜) for each ğ‘˜ (see 6.32), we conclude that span(ğ‘’1, â€¦, ğ‘’ğ‘˜) is invariant under ğ‘‡ for each ğ‘˜ = 1, â€¦, ğ‘›. Thus, by 5.39, ğ‘‡ has an upper-triangular matrix with respect to the orthonormal basis ğ‘’1, â€¦, ğ‘’ğ‘›. Now use 5.44 to complete the proof. 204 Chapter 6 Inner Product Spaces Issai Schur (1875â€“1941) published a proof of the next result in 1909. For complex vector spaces, the next result is an important application of the result above. See Exercise 20 for a ver- sion of Schurâ€™s theorem that applies simultaneously to more than one operator. 6.38 Schurâ€™s theorem Every operator on a finite-dimensional complex inner product space has an upper-triangular matrix with respect to some orthonormal basis. Proof The desired result follows from the second version of the fundamental theorem of algebra (4.13) and 6.37. Linear Functionals on Inner Product Spaces Because linear maps into the scalar fieldğ… play a special role, we defined a special name for them and their vector space in Section 3F. Those definitions are repeated below in case you skipped Section 3F. 6.39 definition:linear functional, dual space, ğ‘‰â€² â€¢ A linear functional on ğ‘‰ is a linear map from ğ‘‰ to ğ…. â€¢ The dual space of ğ‘‰, denoted by ğ‘‰â€², is the vector space of all linear functionals on ğ‘‰. In other words, ğ‘‰â€² = â„’(ğ‘‰, ğ…). 6.40 example:linear functional on ğ…3 The function ğœ‘âˆ¶ ğ…3 â†’ ğ… defined by ğœ‘(ğ‘§1, ğ‘§2, ğ‘§3) = 2ğ‘§1 âˆ’ 5ğ‘§2 + ğ‘§3 is a linear functional on ğ…3. We could write this linear functional in the form ğœ‘(ğ‘§) = âŸ¨ğ‘§, ğ‘¤âŸ© for every ğ‘§ âˆˆ ğ…3, where ğ‘¤ = (2, âˆ’5, 1). 6.41 example:linear functional on ğ’«5(ğ‘) The function ğœ‘âˆ¶ ğ’«5(ğ‘) â†’ ğ‘ defined by ğœ‘(ğ‘) = âˆ«1 âˆ’1 ğ‘(ğ‘¡)(cos(ğœ‹ğ‘¡))ğ‘‘ğ‘¡ is a linear functional on ğ’«5(ğ‘). Section 6B Orthonormal Bases 205 The next result is named in honor of Frigyes Riesz (1880â€“1956), who proved several theorems early in the twentieth century that look very much like the result below. If ğ‘£ âˆˆ ğ‘‰, then the map that sends ğ‘¢ to âŸ¨ğ‘¢, ğ‘£âŸ©is a linear functional on ğ‘‰. The next result states that every linear func- tional on ğ‘‰ is of this form. For example, we can take ğ‘£ = (2, âˆ’5, 1)in Example 6.40. Suppose we make the vector space ğ’«5(ğ‘) into an inner product space by definingâŸ¨ğ‘, ğ‘âŸ© =âˆ« 1 âˆ’1 ğ‘ğ‘. Let ğœ‘ be as in Example 6.41. It is not obvious that there exists ğ‘ âˆˆ ğ’«5(ğ‘) such that âˆ«1 âˆ’1 ğ‘(ğ‘¡)(cos(ğœ‹ğ‘¡))ğ‘‘ğ‘¡ = âŸ¨ğ‘, ğ‘âŸ© for every ğ‘ âˆˆ ğ’«5(ğ‘) [we cannot take ğ‘(ğ‘¡) = cos(ğœ‹ğ‘¡) because that choice of ğ‘ is not an element of ğ’«5(ğ‘)]. The next result tells us the somewhat surprising result that there indeed exists a polynomial ğ‘ âˆˆ ğ’«5(ğ‘) such that the equation above holds for all ğ‘ âˆˆ ğ’«5(ğ‘). 6.42 Riesz representation theorem Suppose ğ‘‰ is finite-dimensional andğœ‘ is a linear functional on ğ‘‰. Then there is a unique vector ğ‘£ âˆˆ ğ‘‰ such that ğœ‘(ğ‘¢) = âŸ¨ğ‘¢, ğ‘£âŸ© for every ğ‘¢ âˆˆ ğ‘‰. Proof First we show that there exists a vector ğ‘£ âˆˆ ğ‘‰ such that ğœ‘(ğ‘¢) = âŸ¨ğ‘¢, ğ‘£âŸ©for every ğ‘¢ âˆˆ ğ‘‰. Let ğ‘’1, â€¦, ğ‘’ğ‘› be an orthonormal basis of ğ‘‰. Then ğœ‘(ğ‘¢) = ğœ‘(âŸ¨ğ‘¢, ğ‘’1âŸ©ğ‘’1 + â‹¯ + âŸ¨ğ‘¢, ğ‘’ğ‘›âŸ©ğ‘’ğ‘›) = âŸ¨ğ‘¢, ğ‘’1âŸ©ğœ‘(ğ‘’1) + â‹¯ + âŸ¨ğ‘¢, ğ‘’ğ‘›âŸ©ğœ‘(ğ‘’ğ‘›) = âŸ¨ğ‘¢, ğœ‘(ğ‘’1)ğ‘’1 + â‹¯ + ğœ‘(ğ‘’ğ‘›)ğ‘’ğ‘›âŸ© for every ğ‘¢ âˆˆ ğ‘‰, where the first equality comes from6.30(a). Thus setting 6.43 ğ‘£ = ğœ‘(ğ‘’1)ğ‘’1 + â‹¯ + ğœ‘(ğ‘’ğ‘›)ğ‘’ğ‘›, we have ğœ‘(ğ‘¢) = âŸ¨ğ‘¢, ğ‘£âŸ©for every ğ‘¢ âˆˆ ğ‘‰, as desired. Now we prove that only one vector ğ‘£ âˆˆ ğ‘‰ has the desired behavior. Suppose ğ‘£1, ğ‘£2 âˆˆ ğ‘‰ are such that ğœ‘(ğ‘¢) = âŸ¨ğ‘¢, ğ‘£1âŸ© = âŸ¨ğ‘¢, ğ‘£2âŸ© for every ğ‘¢ âˆˆ ğ‘‰. Then 0 = âŸ¨ğ‘¢, ğ‘£1âŸ© âˆ’ âŸ¨ğ‘¢, ğ‘£2âŸ© = âŸ¨ğ‘¢, ğ‘£1 âˆ’ ğ‘£2âŸ© for every ğ‘¢ âˆˆ ğ‘‰. Taking ğ‘¢ = ğ‘£1 âˆ’ ğ‘£2 shows that ğ‘£1 âˆ’ ğ‘£2 = 0. Thus ğ‘£1 = ğ‘£2, completing the proof of the uniqueness part of the result. 206 Chapter 6 Inner Product Spaces 6.44 example:computation illustrating Riesz representation theorem Suppose we want to find a polynomialğ‘ âˆˆ ğ’«2(ğ‘) such that 6.45 âˆ« 1 âˆ’1 ğ‘(ğ‘¡)(cos(ğœ‹ğ‘¡))ğ‘‘ğ‘¡ = âˆ« 1 âˆ’1 ğ‘ğ‘ for every polynomial ğ‘ âˆˆ ğ’«2(ğ‘). To do this, we make ğ’«2(ğ‘) into an inner product space by definingâŸ¨ğ‘, ğ‘âŸ©to be the right side of the equation above for ğ‘, ğ‘ âˆˆ ğ’«2(ğ‘). Note that the left side of the equation above does not equal the inner product in ğ’«2(ğ‘) of ğ‘ and the function ğ‘¡ â†¦ cos(ğœ‹ğ‘¡) because this last function is not a polynomial. Define a linear functionalğœ‘ on ğ’«2(ğ‘) by letting ğœ‘(ğ‘) = âˆ«1 âˆ’1 ğ‘(ğ‘¡)(cos(ğœ‹ğ‘¡))ğ‘‘ğ‘¡ for each ğ‘ âˆˆ ğ’«2(ğ‘). Now use the orthonormal basis from Example 6.34 and apply formula 6.43 from the proof of the Riesz representation theorem to see that if ğ‘ âˆˆ ğ’«2(ğ‘), then ğœ‘(ğ‘) = âŸ¨ğ‘, ğ‘âŸ©, where ğ‘(ğ‘¥) = (âˆ«1 âˆ’1 âˆš 1 2 cos(ğœ‹ğ‘¡) ğ‘‘ğ‘¡)âˆš 1 2 + (âˆ«1 âˆ’1 âˆš 3 2 ğ‘¡ cos(ğœ‹ğ‘¡) ğ‘‘ğ‘¡)âˆš 3 2 ğ‘¥ + (âˆ«1 âˆ’1 âˆš 45 8 (ğ‘¡2 âˆ’ 1 3 )cos(ğœ‹ğ‘¡) ğ‘‘ğ‘¡)âˆš 45 8 (ğ‘¥2 âˆ’ 1 3 ). A bit of calculus applied to the equation above shows that ğ‘(ğ‘¥) = 15 2ğœ‹2 (1 âˆ’ 3ğ‘¥ 2). The same procedure shows that if we want to findğ‘ âˆˆ ğ’«5(ğ‘) such that 6.45 holds for all ğ‘ âˆˆ ğ’«5(ğ‘), then we should take ğ‘(ğ‘¥) = 105 8ğœ‹4 ((27 âˆ’ 2ğœ‹ 2)+ (24ğœ‹ 2 âˆ’ 270)ğ‘¥2 + (315 âˆ’ 30ğœ‹ 2)ğ‘¥4). Suppose ğ‘‰ is finite-dimensional andğœ‘ a linear functional on ğ‘‰. Then 6.43 gives a formula for the vector ğ‘£ that satisfies ğœ‘(ğ‘¢) = âŸ¨ğ‘¢, ğ‘£âŸ© for all ğ‘¢ âˆˆ ğ‘‰. Specifically, we have ğ‘£ = ğœ‘(ğ‘’1)ğ‘’1 + â‹¯ + ğœ‘(ğ‘’ğ‘›)ğ‘’ğ‘›. The right side of the equation above seems to depend on the orthonormal basis ğ‘’1, â€¦, ğ‘’ğ‘› as well as on ğœ‘. However, 6.42 tells us that ğ‘£ is uniquely determined by ğœ‘. Thus the right side of the equation above is the same regardless of which orthonormal basis ğ‘’1, â€¦, ğ‘’ğ‘› of ğ‘‰ is chosen. For two additional different proofs of the Riesz representation theorem, see 6.58 and also Exercise 13 in Section 6C. Section 6B Orthonormal Bases 207 Exercises 6B 1 Suppose ğ‘’1, â€¦, ğ‘’ğ‘š is a list of vectors in ğ‘‰ such that â€–ğ‘1ğ‘’1 + â‹¯ + ğ‘ğ‘šğ‘’ğ‘šâ€–2 = |ğ‘1| 2 + â‹¯ + |ğ‘ğ‘š|2 for all ğ‘1, â€¦, ğ‘ğ‘š âˆˆ ğ…. Show that ğ‘’1, â€¦, ğ‘’ğ‘š is an orthonormal list. This exercise provides a converse to 6.24. 2 (a) Suppose ğœƒ âˆˆ ğ‘. Show that both (cos ğœƒ, sin ğœƒ), (âˆ’ sin ğœƒ, cos ğœƒ) and (cos ğœƒ, sin ğœƒ), (sin ğœƒ, âˆ’ cos ğœƒ) are orthonormal bases of ğ‘2. (b) Show that each orthonormal basis of ğ‘2 is of the form given by one of the two possibilities in (a). 3 Suppose ğ‘’1, â€¦, ğ‘’ğ‘š is an orthonormal list in ğ‘‰ and ğ‘£ âˆˆ ğ‘‰. Prove that â€–ğ‘£â€– 2 = âˆ£âŸ¨ğ‘£, ğ‘’1âŸ©âˆ£2 + â‹¯ + âˆ£âŸ¨ğ‘£, ğ‘’ğ‘šâŸ©âˆ£2 âŸº ğ‘£ âˆˆ span(ğ‘’1, â€¦, ğ‘’ğ‘š). 4 Suppose ğ‘› is a positive integer. Prove that 1 âˆš2ğœ‹ , cos ğ‘¥ âˆš ğœ‹ , cos 2ğ‘¥ âˆš ğœ‹ , â€¦, cos ğ‘›ğ‘¥ âˆš ğœ‹ , sin ğ‘¥ âˆš ğœ‹ , sin 2ğ‘¥ âˆš ğœ‹ , â€¦, sin ğ‘›ğ‘¥ âˆš ğœ‹ is an orthonormal list of vectors in ğ¶[âˆ’ğœ‹, ğœ‹], the vector space of continuous real-valued functions on [âˆ’ğœ‹, ğœ‹] with inner product âŸ¨ ğ‘“, ğ‘”âŸ© =âˆ«ğœ‹ âˆ’ğœ‹ ğ‘“ ğ‘”. Hint: The following formulas should help. (sin ğ‘¥)(cos ğ‘¦) = sin(ğ‘¥ âˆ’ ğ‘¦) + sin(ğ‘¥ + ğ‘¦) 2 (sin ğ‘¥)(sin ğ‘¦) = cos(ğ‘¥ âˆ’ ğ‘¦) âˆ’ cos(ğ‘¥ + ğ‘¦) 2 (cos ğ‘¥)(cos ğ‘¦) = cos(ğ‘¥ âˆ’ ğ‘¦) + cos(ğ‘¥ + ğ‘¦) 2 5 Suppose ğ‘“âˆ¶ [âˆ’ğœ‹, ğœ‹] â†’ ğ‘ is continuous. For each nonnegative integer ğ‘˜, define ğ‘ğ‘˜ = 1 âˆšğœ‹ âˆ«ğœ‹ âˆ’ğœ‹ ğ‘“ (ğ‘¥) cos(ğ‘˜ğ‘¥) ğ‘‘ğ‘¥ and ğ‘ğ‘˜ = 1 âˆš ğœ‹ âˆ«ğœ‹ âˆ’ğœ‹ ğ‘“ (ğ‘¥) sin(ğ‘˜ğ‘¥) ğ‘‘ğ‘¥. Prove that ğ‘0 2 2 + âˆ âˆ‘ ğ‘˜ = 1(ğ‘ğ‘˜ 2 + ğ‘ğ‘˜ 2)â‰¤âˆ« ğœ‹ âˆ’ğœ‹ ğ‘“ 2. The inequality above is actually an equality for all continuous functions ğ‘“âˆ¶ [âˆ’ğœ‹, ğœ‹] â†’ ğ‘. However, proving that this inequality is an equality involves Fourier series techniques beyond the scope of this book. 208 Chapter 6 Inner Product Spaces 6 Suppose ğ‘’1, â€¦, ğ‘’ğ‘› is an orthonormal basis of ğ‘‰. (a) Prove that if ğ‘£1, â€¦, ğ‘£ğ‘› are vectors in ğ‘‰ such that â€–ğ‘’ğ‘˜ âˆ’ ğ‘£ğ‘˜â€– < 1 âˆš ğ‘› for each ğ‘˜, then ğ‘£1, â€¦, ğ‘£ğ‘› is a basis of ğ‘‰. (b) Show that there exist ğ‘£1, â€¦, ğ‘£ğ‘› âˆˆ ğ‘‰ such that â€–ğ‘’ğ‘˜ âˆ’ ğ‘£ğ‘˜â€– â‰¤ 1 âˆš ğ‘› for each ğ‘˜, but ğ‘£1, â€¦, ğ‘£ğ‘› is not linearly independent. This exercise states in (a) that an appropriately small perturbation of an orthonormal basis is a basis. Then (b) shows that the number 1/ âˆšğ‘› on the right side of the inequality in (a) cannot be improved upon. 7 Suppose ğ‘‡ âˆˆ â„’(ğ‘3)has an upper-triangular matrix with respect to the basis (1, 0, 0), (1, 1, 1), (1, 1, 2). Find an orthonormal basis of ğ‘3 with respect to which ğ‘‡ has an upper-triangular matrix. 8 Make ğ’«2(ğ‘) into an inner product space by definingâŸ¨ğ‘, ğ‘âŸ© =âˆ« 1 0 ğ‘ğ‘ for all ğ‘, ğ‘ âˆˆ ğ’«2(ğ‘). (a) Apply the Gramâ€“Schmidt procedure to the basis 1, ğ‘¥, ğ‘¥2 to produce an orthonormal basis of ğ’«2(ğ‘). (b) The differentiation operator (the operator that takes ğ‘ to ğ‘ â€²)on ğ’«2(ğ‘) has an upper-triangular matrix with respect to the basis 1, ğ‘¥, ğ‘¥2, which is not an orthonormal basis. Find the matrix of the differentiation operator on ğ’«2(ğ‘) with respect to the orthonormal basis produced in (a) and verify that this matrix is upper triangular, as expected from the proof of 6.37. 9 Suppose ğ‘’1, â€¦, ğ‘’ğ‘š is the result of applying the Gramâ€“Schmidt procedure to a linearly independent list ğ‘£1, â€¦, ğ‘£ğ‘š in ğ‘‰. Prove that âŸ¨ğ‘£ğ‘˜, ğ‘’ğ‘˜âŸ© > 0for each ğ‘˜ = 1, â€¦, ğ‘š. 10 Suppose ğ‘£1, â€¦, ğ‘£ğ‘š is a linearly independent list in ğ‘‰. Explain why the orthonormal list produced by the formulas of the Gramâ€“Schmidt procedure (6.32) is the only orthonormal list ğ‘’1, â€¦, ğ‘’ğ‘š in ğ‘‰ such that âŸ¨ğ‘£ğ‘˜, ğ‘’ğ‘˜âŸ© > 0and span(ğ‘£1, â€¦, ğ‘£ğ‘˜) = span(ğ‘’1, â€¦, ğ‘’ğ‘˜) for each ğ‘˜ = 1, â€¦, ğ‘š. The result in this exercise is used in the proof of 7.58. 11 Find a polynomial ğ‘ âˆˆ ğ’«2(ğ‘) such that ğ‘( 1 2 )= âˆ«1 0 ğ‘ğ‘ for every ğ‘ âˆˆ ğ’«2(ğ‘). 12 Find a polynomial ğ‘ âˆˆ ğ’«2(ğ‘) such that âˆ« 1 0 ğ‘(ğ‘¥) cos(ğœ‹ğ‘¥) ğ‘‘ğ‘¥ = âˆ« 1 0 ğ‘ğ‘ for every ğ‘ âˆˆ ğ’«2(ğ‘). Section 6B Orthonormal Bases 209 13 Show that a list ğ‘£1, â€¦, ğ‘£ğ‘š of vectors in ğ‘‰ is linearly dependent if and only if the Gramâ€“Schmidt formula in 6.32 produces ğ‘“ğ‘˜ = 0for some ğ‘˜ âˆˆ {1, â€¦, ğ‘š}. This exercise gives an alternative to Gaussian elimination techniques for determining whether a list of vectors in an inner product space is linearly dependent. 14 Suppose ğ‘‰ is a real inner product space and ğ‘£1, â€¦, ğ‘£ğ‘š is a linearly indepen- dent list of vectors in ğ‘‰. Prove that there exist exactly 2 ğ‘š orthonormal lists ğ‘’1, â€¦, ğ‘’ğ‘š of vectors in ğ‘‰ such that span(ğ‘£1, â€¦, ğ‘£ğ‘˜) = span(ğ‘’1, â€¦, ğ‘’ğ‘˜) for all ğ‘˜ âˆˆ {1, â€¦, ğ‘š}. 15 Suppose âŸ¨â‹…, â‹…âŸ©1 and âŸ¨â‹…, â‹…âŸ©2 are inner products on ğ‘‰ such that âŸ¨ğ‘¢, ğ‘£âŸ©1 = 0if and only if âŸ¨ğ‘¢, ğ‘£âŸ©2 = 0. Prove that there is a positive number ğ‘ such that âŸ¨ğ‘¢, ğ‘£âŸ©1 = ğ‘âŸ¨ğ‘¢, ğ‘£âŸ©2 for every ğ‘¢, ğ‘£ âˆˆ ğ‘‰. This exercise shows that if two inner products have the same pairs of orthogonal vectors, then each of the inner products is a scalar multiple of the other inner product. 16 Suppose ğ‘‰ is finite-dimensional. SupposeâŸ¨â‹…, â‹…âŸ©1, âŸ¨â‹…, â‹…âŸ©2 are inner products on ğ‘‰ with corresponding norms â€– â‹… â€–1 and â€– â‹… â€–2. Prove that there exists a positive number ğ‘ such that â€–ğ‘£â€–1 â‰¤ ğ‘â€–ğ‘£â€–2 for every ğ‘£ âˆˆ ğ‘‰. 17 Suppose ğ… = ğ‚ and ğ‘‰ is finite-dimensional. Prove that ifğ‘‡ is an operator on ğ‘‰ such that 1is the only eigenvalue of ğ‘‡ and â€–ğ‘‡ğ‘£â€– â‰¤ â€–ğ‘£â€–for all ğ‘£ âˆˆ ğ‘‰, then ğ‘‡ is the identity operator. 18 Suppose ğ‘¢1, â€¦, ğ‘¢ğ‘š is a linearly independent list in ğ‘‰. Show that there exists ğ‘£ âˆˆ ğ‘‰ such that âŸ¨ğ‘¢ğ‘˜, ğ‘£âŸ© = 1for all ğ‘˜ âˆˆ {1, â€¦, ğ‘š}. 19 Suppose ğ‘£1, â€¦, ğ‘£ğ‘› is a basis of ğ‘‰. Prove that there exists a basis ğ‘¢1, â€¦, ğ‘¢ğ‘› of ğ‘‰ such that âŸ¨ğ‘£ğ‘—, ğ‘¢ğ‘˜âŸ© = â§{ â¨{â© 0 if ğ‘— â‰  ğ‘˜, 1 if ğ‘— = ğ‘˜. 20 Suppose ğ… = ğ‚, ğ‘‰ is finite-dimensional, andâ„° âŠ†â„’(ğ‘‰) is such that ğ‘†ğ‘‡ = ğ‘‡ğ‘† for all ğ‘†, ğ‘‡ âˆˆ â„°. Prove that there is an orthonormal basis of ğ‘‰ with respect to which every element of â„° has an upper-triangular matrix. This exercise strengthens Exercise 9(b) in Section 5E (in the context of inner product spaces) by asserting that the basis in that exercise can be chosen to be orthonormal. 21 Suppose ğ… = ğ‚, ğ‘‰ is finite-dimensional,ğ‘‡ âˆˆ â„’(ğ‘‰), and all eigenvalues of ğ‘‡ have absolute value less than 1. Let ğœ– > 0. Prove that there exists a positive integer ğ‘š such that âˆ¥ğ‘‡ğ‘šğ‘£âˆ¥ â‰¤ ğœ–â€–ğ‘£â€–for every ğ‘£ âˆˆ ğ‘‰. 210 Chapter 6 Inner Product Spaces 22 Suppose ğ¶[âˆ’1, 1]is the vector space of continuous real-valued functions on the interval [âˆ’1, 1]with inner product given by âŸ¨ ğ‘“, ğ‘”âŸ© =âˆ«1 âˆ’1 ğ‘“ ğ‘” for all ğ‘“, ğ‘” âˆˆ ğ¶[âˆ’1, 1]. Let ğœ‘ be the linear functional on ğ¶[âˆ’1, 1]defined by ğœ‘( ğ‘“ ) = ğ‘“ (0). Show that there does not exist ğ‘” âˆˆ ğ¶[âˆ’1, 1]such that ğœ‘( ğ‘“ ) = âŸ¨ ğ‘“, ğ‘”âŸ© for every ğ‘“ âˆˆ ğ¶[âˆ’1, 1]. This exercise shows that the Riesz representation theorem (6.42) does not hold on infinite-dimensional vector spaces without additional hypotheses on ğ‘‰ and ğœ‘. 23 For all ğ‘¢, ğ‘£ âˆˆ ğ‘‰, defineğ‘‘(ğ‘¢, ğ‘£) = â€–ğ‘¢ âˆ’ ğ‘£â€–. (a) Show that ğ‘‘ is a metric on ğ‘‰. (b) Show that if ğ‘‰ is finite-dimensional, thenğ‘‘ is a complete metric on ğ‘‰ (meaning that every Cauchy sequence converges). (c) Show that every finite-dimensional subspace ofğ‘‰ is a closed subset of ğ‘‰ (with respect to the metric ğ‘‘ ). This exercise requires familiarity with metric spaces. orthogonality at the Supreme Court Law professor Richard Friedman presenting a case before the U.S. Supreme Court in 2010: Mr. Friedman: I think that issue is entirely orthogonal to the issue here because the Commonwealth is acknowledgingâ€” Chief Justice Roberts: Iâ€™m sorry. Entirely what? Mr. Friedman: Orthogonal. Right angle. Unrelated. Irrelevant. Chief Justice Roberts: Oh. Justice Scalia: What was that adjective? I liked that. Mr. Friedman: Orthogonal. Chief Justice Roberts: Orthogonal. Mr. Friedman: Right, right. Justice Scalia: Orthogonal, ooh. (Laughter.) Justice Kennedy: I knew this case presented us a problem. (Laughter.) Section 6C Orthogonal Complements and Minimization Problems 211 6C Orthogonal Complements and Minimization Problems Orthogonal Complements 6.46 definition:orthogonal complement, ğ‘ˆâŸ‚ If ğ‘ˆ is a subset of ğ‘‰, then the orthogonal complement of ğ‘ˆ, denoted by ğ‘ˆâŸ‚, is the set of all vectors in ğ‘‰ that are orthogonal to every vector in ğ‘ˆ: ğ‘ˆâŸ‚ = {ğ‘£ âˆˆ ğ‘‰ âˆ¶ âŸ¨ğ‘¢, ğ‘£âŸ© = 0for every ğ‘¢ âˆˆ ğ‘ˆ}. The orthogonal complement ğ‘ˆâŸ‚ depends on ğ‘‰ as well as on ğ‘ˆ. However, the inner product space ğ‘‰ should always be clear from the context and thus it can be omitted from the notation. 6.47 example:orthogonal complements â€¢ If ğ‘‰ = ğ‘3 and ğ‘ˆ is the subset of ğ‘‰ consisting of the single point (2, 3, 5), then ğ‘ˆâŸ‚ is the plane {(ğ‘¥, ğ‘¦, ğ‘§) âˆˆ ğ‘3 âˆ¶ 2ğ‘¥+ 3ğ‘¦+ 5ğ‘§ = 0}. â€¢ If ğ‘‰ = ğ‘3 and ğ‘ˆ is the plane {(ğ‘¥, ğ‘¦, ğ‘§) âˆˆ ğ‘3 âˆ¶ 2ğ‘¥+ 3ğ‘¦+ 5ğ‘§ = 0}, then ğ‘ˆâŸ‚ is the line {(2ğ‘¡, 3ğ‘¡, 5ğ‘¡)âˆ¶ ğ‘¡ âˆˆ ğ‘}. â€¢ More generally, if ğ‘ˆ is a plane in ğ‘3 containing the origin, then ğ‘ˆâŸ‚ is the line containing the origin that is perpendicular to ğ‘ˆ. â€¢ If ğ‘ˆ is a line in ğ‘3 containing the origin, then ğ‘ˆâŸ‚ is the plane containing the origin that is perpendicular to ğ‘ˆ. â€¢ If ğ‘‰ = ğ…5 and ğ‘ˆ = {(ğ‘, ğ‘, 0, 0, 0) âˆˆ ğ… 5 âˆ¶ ğ‘, ğ‘ âˆˆ ğ…}, then ğ‘ˆâŸ‚ = {(0, 0, ğ‘¥, ğ‘¦, ğ‘§) âˆˆ ğ…5 âˆ¶ ğ‘¥, ğ‘¦, ğ‘§ âˆˆ ğ…}. â€¢ If ğ‘’1, â€¦, ğ‘’ğ‘š, ğ‘“1, â€¦, ğ‘“ğ‘› is an orthonormal basis of ğ‘‰, then (span(ğ‘’1, â€¦, ğ‘’ğ‘š)) âŸ‚ = span( ğ‘“1, â€¦, ğ‘“ğ‘›). We begin with some straightforward consequences of the definition. 6.48 properties of orthogonal complement (a) If ğ‘ˆ is a subset of ğ‘‰, then ğ‘ˆâŸ‚ is a subspace of ğ‘‰. (b) {0} âŸ‚ = ğ‘‰. (c) ğ‘‰âŸ‚ = {0}. (d) If ğ‘ˆ is a subset of ğ‘‰, then ğ‘ˆ âˆ© ğ‘ˆ âŸ‚ âŠ† {0}. (e) If ğº and ğ» are subsets of ğ‘‰ and ğº âŠ† ğ», then ğ»âŸ‚ âŠ† ğº âŸ‚. 212 Chapter 6 Inner Product Spaces Proof (a) Suppose ğ‘ˆ is a subset of ğ‘‰. Then âŸ¨ğ‘¢, 0âŸ© = 0for every ğ‘¢ âˆˆ ğ‘ˆ; thus 0 âˆˆ ğ‘ˆ âŸ‚. Suppose ğ‘£, ğ‘¤ âˆˆ ğ‘ˆâŸ‚. If ğ‘¢ âˆˆ ğ‘ˆ, then âŸ¨ğ‘¢, ğ‘£ + ğ‘¤âŸ© = âŸ¨ğ‘¢, ğ‘£âŸ©+ âŸ¨ğ‘¢, ğ‘¤âŸ© = 0+ 0 = 0. Thus ğ‘£ + ğ‘¤ âˆˆ ğ‘ˆâŸ‚, which shows that ğ‘ˆâŸ‚ is closed under addition. Similarly, suppose ğœ† âˆˆ ğ… and ğ‘£ âˆˆ ğ‘ˆâŸ‚. If ğ‘¢ âˆˆ ğ‘ˆ, then âŸ¨ğ‘¢, ğœ†ğ‘£âŸ© = ğœ†âŸ¨ğ‘¢, ğ‘£âŸ© = ğœ†â‹… 0 = 0. Thus ğœ†ğ‘£ âˆˆ ğ‘ˆâŸ‚, which shows that ğ‘ˆâŸ‚ is closed under scalar multiplication. Thus ğ‘ˆâŸ‚ is a subspace of ğ‘‰. (b) Suppose that ğ‘£ âˆˆ ğ‘‰. Then âŸ¨0, ğ‘£âŸ© = 0, which implies that ğ‘£ âˆˆ {0} âŸ‚. Thus {0} âŸ‚ = ğ‘‰. (c) Suppose that ğ‘£ âˆˆ ğ‘‰âŸ‚. Then âŸ¨ğ‘£, ğ‘£âŸ© = 0, which implies that ğ‘£ = 0. Thus ğ‘‰âŸ‚ = {0}. (d) Suppose ğ‘ˆ is a subset of ğ‘‰ and ğ‘¢ âˆˆ ğ‘ˆ âˆ© ğ‘ˆ âŸ‚. Then âŸ¨ğ‘¢, ğ‘¢âŸ© = 0, which implies that ğ‘¢ = 0. Thus ğ‘ˆ âˆ© ğ‘ˆ âŸ‚ âŠ† {0}. (e) Suppose ğº and ğ» are subsets of ğ‘‰ and ğº âŠ† ğ». Suppose ğ‘£ âˆˆ ğ»âŸ‚. Then âŸ¨ğ‘¢, ğ‘£âŸ© = 0for every ğ‘¢ âˆˆ ğ», which implies that âŸ¨ğ‘¢, ğ‘£âŸ© = 0for every ğ‘¢ âˆˆ ğº. Hence ğ‘£ âˆˆ ğºâŸ‚. Thus ğ»âŸ‚ âŠ† ğº âŸ‚. Recall that if ğ‘ˆ and ğ‘Š are subspaces of ğ‘‰, then ğ‘‰ is the direct sum of ğ‘ˆ and ğ‘Š (written ğ‘‰ = ğ‘ˆ âŠ• ğ‘Š) if each element of ğ‘‰ can be written in exactly one way as a vector in ğ‘ˆ plus a vector in ğ‘Š (see 1.41). Furthermore, this happens if and only if ğ‘‰ = ğ‘ˆ + ğ‘Š and ğ‘ˆ âˆ© ğ‘Š = {0}(see 1.46). The next result shows that every finite-dimensional subspace ofğ‘‰ leads to a natural direct sum decomposition of ğ‘‰. See Exercise 16 for an example showing that the result below can fail without the hypothesis that the subspace ğ‘ˆ is finite- dimensional. 6.49 direct sum of a subspace and its orthogonal complement Suppose ğ‘ˆ is a finite-dimensional subspace ofğ‘‰. Then ğ‘‰ = ğ‘ˆ âŠ• ğ‘ˆâŸ‚. Proof First we will show that ğ‘‰ = ğ‘ˆ + ğ‘ˆâŸ‚. To do this, suppose that ğ‘£ âˆˆ ğ‘‰. Let ğ‘’1, â€¦, ğ‘’ğ‘š be an orthonormal basis of ğ‘ˆ. We want to write ğ‘£ as the sum of a vector in ğ‘ˆ and a vector orthogonal to ğ‘ˆ. Section 6C Orthogonal Complements and Minimization Problems 213 We have 6.50 ğ‘£ = âŸ¨ğ‘£, ğ‘’1âŸ©ğ‘’1 + â‹¯ + âŸ¨ğ‘£, ğ‘’ğ‘šâŸ©ğ‘’ğ‘šâŸâŸâŸâŸâŸâŸâŸâŸâŸ ğ‘¢ + ğ‘£ âˆ’ âŸ¨ğ‘£, ğ‘’1âŸ©ğ‘’1 âˆ’ â‹¯ âˆ’ âŸ¨ğ‘£, ğ‘’ğ‘šâŸ©ğ‘’ğ‘šâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸ ğ‘¤ . Let ğ‘¢ and ğ‘¤ be defined as in the equation above (as was done in the proof of6.26). Because each ğ‘’ğ‘˜ âˆˆ ğ‘ˆ, we see that ğ‘¢ âˆˆ ğ‘ˆ. Because ğ‘’1, â€¦, ğ‘’ğ‘š is an orthonormal list, for each ğ‘˜ = 1, â€¦, ğ‘š we have âŸ¨ğ‘¤, ğ‘’ğ‘˜âŸ© = âŸ¨ğ‘£, ğ‘’ğ‘˜âŸ© âˆ’ âŸ¨ğ‘£, ğ‘’ğ‘˜âŸ© = 0. Thus ğ‘¤ is orthogonal to every vector in span(ğ‘’1, â€¦, ğ‘’ğ‘š), which shows that ğ‘¤ âˆˆ ğ‘ˆâŸ‚. Hence we have written ğ‘£ = ğ‘¢ + ğ‘¤, where ğ‘¢ âˆˆ ğ‘ˆ and ğ‘¤ âˆˆ ğ‘ˆâŸ‚, completing the proof that ğ‘‰ = ğ‘ˆ + ğ‘ˆâŸ‚. From 6.48(d), we know that ğ‘ˆ âˆ© ğ‘ˆ âŸ‚ = {0}. Now equation ğ‘‰ = ğ‘ˆ + ğ‘ˆâŸ‚ implies that ğ‘‰ = ğ‘ˆ âŠ• ğ‘ˆâŸ‚ (see 1.46). Now we can see how to compute dim ğ‘ˆâŸ‚ from dim ğ‘ˆ. 6.51 dimension of orthogonal complement Suppose ğ‘‰ is finite-dimensional andğ‘ˆ is a subspace of ğ‘‰. Then dim ğ‘ˆâŸ‚ = dim ğ‘‰ âˆ’ dim ğ‘ˆ. Proof The formula for dim ğ‘ˆâŸ‚ follows immediately from 6.49 and 3.94. The next result is an important consequence of 6.49. 6.52 orthogonal complement of the orthogonal complement Suppose ğ‘ˆ is a finite-dimensional subspace ofğ‘‰. Then ğ‘ˆ = (ğ‘ˆâŸ‚) âŸ‚ . Proof First we will show that 6.53 ğ‘ˆ âŠ†(ğ‘ˆâŸ‚) âŸ‚ . To do this, suppose ğ‘¢ âˆˆ ğ‘ˆ. Then âŸ¨ğ‘¢, ğ‘¤âŸ© = 0for every ğ‘¤ âˆˆ ğ‘ˆâŸ‚ (by the definition of ğ‘ˆâŸ‚). Because ğ‘¢ is orthogonal to every vector in ğ‘ˆâŸ‚, we have ğ‘¢ âˆˆ (ğ‘ˆâŸ‚) âŸ‚, completing the proof of 6.53. To prove the inclusion in the other direction, suppose ğ‘£ âˆˆ (ğ‘ˆâŸ‚) âŸ‚. By 6.49, we can write ğ‘£ = ğ‘¢ + ğ‘¤, where ğ‘¢ âˆˆ ğ‘ˆ and ğ‘¤ âˆˆ ğ‘ˆâŸ‚. We have ğ‘£ âˆ’ ğ‘¢ = ğ‘¤ âˆˆ ğ‘ˆâŸ‚. Because ğ‘£ âˆˆ (ğ‘ˆâŸ‚) âŸ‚ and ğ‘¢ âˆˆ (ğ‘ˆâŸ‚) âŸ‚ (from 6.53), we have ğ‘£ âˆ’ ğ‘¢ âˆˆ (ğ‘ˆâŸ‚) âŸ‚. Thus ğ‘£ âˆ’ ğ‘¢ âˆˆ ğ‘ˆâŸ‚ âˆ©(ğ‘ˆâŸ‚) âŸ‚, which implies that ğ‘£ âˆ’ ğ‘¢ = 0[by6.48(d)], which implies that ğ‘£ = ğ‘¢, which implies that ğ‘£ âˆˆ ğ‘ˆ. Thus (ğ‘ˆâŸ‚) âŸ‚ âŠ† ğ‘ˆ, which along with 6.53 completes the proof. 214 Chapter 6 Inner Product Spaces Exercise 16(a) shows that the result below is not true without the hypothesis that ğ‘ˆ is finite-dimensional. Suppose ğ‘ˆ is a subspace of ğ‘‰ and we want to show that ğ‘ˆ = ğ‘‰. In some situations, the easiest way to do this is to show that the only vector orthogonal to ğ‘ˆ is 0, and then use the result below. For example, the result below is useful for Exercise 4. 6.54 ğ‘ˆâŸ‚ = {0} âŸº ğ‘ˆ = ğ‘‰(for ğ‘ˆ a finite-dimensional subspace of ğ‘‰) Suppose ğ‘ˆ is a finite-dimensional subspace ofğ‘‰. Then ğ‘ˆâŸ‚ = {0} âŸº ğ‘ˆ = ğ‘‰. Proof First suppose ğ‘ˆâŸ‚ = {0}. Then by 6.52, ğ‘ˆ = (ğ‘ˆâŸ‚) âŸ‚ = {0} âŸ‚ = ğ‘‰, as desired. Conversely, if ğ‘ˆ = ğ‘‰, then ğ‘ˆâŸ‚ = ğ‘‰âŸ‚ = {0}by 6.48(c). We now define an operatorğ‘ƒğ‘ˆ for each finite-dimensional subspaceğ‘ˆ of ğ‘‰. 6.55 definition:orthogonal projection, ğ‘ƒğ‘ˆ Suppose ğ‘ˆ is a finite-dimensional subspace ofğ‘‰. The orthogonal projection of ğ‘‰ onto ğ‘ˆ is the operator ğ‘ƒğ‘ˆ âˆˆ â„’(ğ‘‰) defined as follows: For eachğ‘£ âˆˆ ğ‘‰, write ğ‘£ = ğ‘¢ + ğ‘¤, where ğ‘¢ âˆˆ ğ‘ˆ and ğ‘¤ âˆˆ ğ‘ˆâŸ‚. Then let ğ‘ƒğ‘ˆğ‘£ = ğ‘¢. The direct sum decomposition ğ‘‰ = ğ‘ˆ âŠ• ğ‘ˆâŸ‚ given by 6.49 shows that each ğ‘£ âˆˆ ğ‘‰ can be uniquely written in the form ğ‘£ = ğ‘¢ + ğ‘¤ with ğ‘¢ âˆˆ ğ‘ˆ and ğ‘¤ âˆˆ ğ‘ˆâŸ‚. Thus ğ‘ƒğ‘ˆğ‘£ is well defined. See the figure that accompanies the proof of6.61 for the picture describing ğ‘ƒğ‘ˆğ‘£ that you should keep in mind. 6.56 example:orthogonal projection onto one-dimensional subspace Suppose ğ‘¢ âˆˆ ğ‘‰ with ğ‘¢ â‰  0and ğ‘ˆ is the one-dimensional subspace of ğ‘‰ defined byğ‘ˆ = span(ğ‘¢). If ğ‘£ âˆˆ ğ‘‰, then ğ‘£ = âŸ¨ğ‘£, ğ‘¢âŸ© â€–ğ‘¢â€–2 ğ‘¢ + (ğ‘£ âˆ’ âŸ¨ğ‘£, ğ‘¢âŸ© â€–ğ‘¢â€–2 ğ‘¢), where the first term on the right is inspan(ğ‘¢) (and thus is in ğ‘ˆ) and the second term on the right is orthogonal to ğ‘¢ (and thus is in ğ‘ˆâŸ‚). Thus ğ‘ƒğ‘ˆğ‘£ equals the first term on the right. In other words, we have the formula ğ‘ƒğ‘ˆğ‘£ = âŸ¨ğ‘£, ğ‘¢âŸ© â€–ğ‘¢â€–2 ğ‘¢ for every ğ‘£ âˆˆ ğ‘‰. Taking ğ‘£ = ğ‘¢, the formula above becomes ğ‘ƒğ‘ˆğ‘¢ = ğ‘¢, as expected. Furthermore, taking ğ‘£ âˆˆ {ğ‘¢}âŸ‚, the formula above becomes ğ‘ƒğ‘ˆğ‘£ = 0, also as expected. Section 6C Orthogonal Complements and Minimization Problems 215 6.57 properties of orthogonal projection ğ‘ƒğ‘ˆ Suppose ğ‘ˆ is a finite-dimensional subspace ofğ‘‰. Then (a) ğ‘ƒğ‘ˆ âˆˆ â„’(ğ‘‰); (b) ğ‘ƒğ‘ˆğ‘¢ = ğ‘¢ for every ğ‘¢ âˆˆ ğ‘ˆ; (c) ğ‘ƒğ‘ˆğ‘¤ = 0for every ğ‘¤ âˆˆ ğ‘ˆâŸ‚; (d) range ğ‘ƒğ‘ˆ = ğ‘ˆ; (e) null ğ‘ƒğ‘ˆ = ğ‘ˆâŸ‚; (f) ğ‘£ âˆ’ ğ‘ƒğ‘ˆğ‘£ âˆˆ ğ‘ˆâŸ‚ for every ğ‘£ âˆˆ ğ‘‰; (g) ğ‘ƒğ‘ˆ 2 = ğ‘ƒğ‘ˆ; (h) â€–ğ‘ƒğ‘ˆğ‘£â€– â‰¤ â€–ğ‘£â€–for every ğ‘£ âˆˆ ğ‘‰; (i) if ğ‘’1, â€¦, ğ‘’ğ‘š is an orthonormal basis of ğ‘ˆ and ğ‘£ âˆˆ ğ‘‰, then ğ‘ƒğ‘ˆğ‘£ = âŸ¨ğ‘£, ğ‘’1âŸ©ğ‘’1 + â‹¯ + âŸ¨ğ‘£, ğ‘’ğ‘šâŸ©ğ‘’ğ‘š. Proof (a) To show that ğ‘ƒğ‘ˆ is a linear map on ğ‘‰, suppose ğ‘£1, ğ‘£2 âˆˆ ğ‘‰. Write ğ‘£1 = ğ‘¢1 + ğ‘¤1 and ğ‘£2 = ğ‘¢2 + ğ‘¤2 with ğ‘¢1, ğ‘¢2 âˆˆ ğ‘ˆ and ğ‘¤1, ğ‘¤2 âˆˆ ğ‘ˆâŸ‚. Thus ğ‘ƒğ‘ˆğ‘£1 = ğ‘¢1 and ğ‘ƒğ‘ˆğ‘£2 = ğ‘¢2. Now ğ‘£1 + ğ‘£2 = (ğ‘¢1 + ğ‘¢2) + (ğ‘¤1 + ğ‘¤2), where ğ‘¢1 + ğ‘¢2 âˆˆ ğ‘ˆ and ğ‘¤1 + ğ‘¤2 âˆˆ ğ‘ˆâŸ‚. Thus ğ‘ƒğ‘ˆ(ğ‘£1 + ğ‘£2) = ğ‘¢1 + ğ‘¢2 = ğ‘ƒğ‘ˆğ‘£1 + ğ‘ƒğ‘ˆğ‘£2. Similarly, suppose ğœ† âˆˆ ğ… and ğ‘£ âˆˆ ğ‘‰. Write ğ‘£ = ğ‘¢ + ğ‘¤, where ğ‘¢ âˆˆ ğ‘ˆ and ğ‘¤ âˆˆ ğ‘ˆâŸ‚. Then ğœ†ğ‘£ = ğœ†ğ‘¢ + ğœ†ğ‘¤ with ğœ†ğ‘¢ âˆˆ ğ‘ˆ and ğœ†ğ‘¤ âˆˆ ğ‘ˆâŸ‚. Thus ğ‘ƒğ‘ˆ(ğœ†ğ‘£) = ğœ†ğ‘¢ = ğœ†ğ‘ƒğ‘ˆğ‘£. Hence ğ‘ƒğ‘ˆ is a linear map from ğ‘‰ to ğ‘‰. (b) Suppose ğ‘¢ âˆˆ ğ‘ˆ. We can write ğ‘¢ = ğ‘¢ + 0, where ğ‘¢ âˆˆ ğ‘ˆ and 0 âˆˆ ğ‘ˆ âŸ‚. Thus ğ‘ƒğ‘ˆğ‘¢ = ğ‘¢. (c) Suppose ğ‘¤ âˆˆ ğ‘ˆâŸ‚. We can write ğ‘¤ = 0+ ğ‘¤, where 0 âˆˆ ğ‘ˆand ğ‘¤ âˆˆ ğ‘ˆâŸ‚. Thus ğ‘ƒğ‘ˆğ‘¤ = 0. (d) The definition ofğ‘ƒğ‘ˆ implies that range ğ‘ƒğ‘ˆ âŠ† ğ‘ˆ. Furthermore, (b) implies that ğ‘ˆ âŠ†range ğ‘ƒğ‘ˆ. Thus range ğ‘ƒğ‘ˆ = ğ‘ˆ. (e) The inclusion ğ‘ˆâŸ‚ âŠ†null ğ‘ƒğ‘ˆ follows from (c). To prove the inclusion in the other direction, note that if ğ‘£ âˆˆ null ğ‘ƒğ‘ˆ then the decomposition given by 6.49 must be ğ‘£ = 0+ ğ‘£, where 0 âˆˆ ğ‘ˆand ğ‘£ âˆˆ ğ‘ˆâŸ‚. Thus null ğ‘ƒğ‘ˆ âŠ† ğ‘ˆ âŸ‚. 216 Chapter 6 Inner Product Spaces (f) If ğ‘£ âˆˆ ğ‘‰ and ğ‘£ = ğ‘¢ + ğ‘¤ with ğ‘¢ âˆˆ ğ‘ˆ and ğ‘¤ âˆˆ ğ‘ˆâŸ‚, then ğ‘£ âˆ’ ğ‘ƒğ‘ˆğ‘£ = ğ‘£ âˆ’ ğ‘¢ = ğ‘¤ âˆˆ ğ‘ˆâŸ‚. (g) If ğ‘£ âˆˆ ğ‘‰ and ğ‘£ = ğ‘¢ + ğ‘¤ with ğ‘¢ âˆˆ ğ‘ˆ and ğ‘¤ âˆˆ ğ‘ˆâŸ‚, then (ğ‘ƒğ‘ˆ 2)ğ‘£ = ğ‘ƒğ‘ˆ(ğ‘ƒğ‘ˆğ‘£) = ğ‘ƒğ‘ˆğ‘¢ = ğ‘¢ = ğ‘ƒğ‘ˆğ‘£. (h) If ğ‘£ âˆˆ ğ‘‰ and ğ‘£ = ğ‘¢ + ğ‘¤ with ğ‘¢ âˆˆ ğ‘ˆ and ğ‘¤ âˆˆ ğ‘ˆâŸ‚, then â€–ğ‘ƒğ‘ˆğ‘£â€–2 = â€–ğ‘¢â€–2 â‰¤ â€–ğ‘¢â€– 2 + â€–ğ‘¤â€– 2 = â€–ğ‘£â€–2, where the last equality comes from the Pythagorean theorem. (i) The formula for ğ‘ƒğ‘ˆğ‘£ follows from equation 6.50 in the proof of 6.49. In the previous section we proved the Riesz representation theorem (6.42), whose key part states that every linear functional on a finite-dimensional inner product space is given by taking the inner product with some fixed vector. Seeing a different proof often provides new insight. Thus we now give a new proof of the key part of the Riesz representation theorem using orthogonal complements instead of orthonormal bases as in our previous proof. The restatement below of the Riesz representation theorem provides an iden- tification ofğ‘‰ with ğ‘‰â€². We will prove only the â€œontoâ€ part of the result below because the more routine â€œone-to-oneâ€ part of the result can be proved as in6.42. Intuition behind this new proof: If ğœ‘ âˆˆ ğ‘‰â€², ğ‘£ âˆˆ ğ‘‰, and ğœ‘(ğ‘¢) = âŸ¨ğ‘¢, ğ‘£âŸ©for all ğ‘¢ âˆˆ ğ‘‰, then ğ‘£ âˆˆ (null ğœ‘) âŸ‚. However, (null ğœ‘) âŸ‚ is a one-dimensional subspace of ğ‘‰ (except for the trivial case in which ğœ‘ = 0), as follows from 6.51 and 3.21. Thus we can obtain ğ‘£ be choosing any nonzero element of (null ğœ‘) âŸ‚ and then multiplying by an appropriate scalar, as is done in the proof below. 6.58 Riesz representation theorem, revisited Suppose ğ‘‰ is finite-dimensional. For eachğ‘£ âˆˆ ğ‘‰, defineğœ‘ğ‘£ âˆˆ ğ‘‰â€² by ğœ‘ğ‘£(ğ‘¢) = âŸ¨ğ‘¢, ğ‘£âŸ© for each ğ‘¢ âˆˆ ğ‘‰. Then ğ‘£ â†¦ ğœ‘ğ‘£ is a one-to-one function from ğ‘‰ onto ğ‘‰â€². Caution: The function ğ‘£ â†¦ ğœ‘ğ‘£ is a linear mapping from ğ‘‰ to ğ‘‰â€² if ğ… = ğ‘. However, this function is not linear if ğ… = ğ‚ because ğœ‘ ğœ†ğ‘£ = ğœ†ğœ‘ğ‘£ if ğœ† âˆˆ ğ‚. Proof To show that ğ‘£ â†¦ ğœ‘ğ‘£ is surjective, suppose ğœ‘ âˆˆ ğ‘‰â€². If ğœ‘ = 0, then ğœ‘ = ğœ‘0. Thus assume ğœ‘ â‰  0. Hence null ğœ‘ â‰  ğ‘‰, which implies that (null ğœ‘) âŸ‚ â‰  {0}(by 6.49 with ğ‘ˆ = null ğœ‘). Let ğ‘¤ âˆˆ (null ğœ‘) âŸ‚ be such that ğ‘¤ â‰  0. Let 6.59 ğ‘£ = ğœ‘(ğ‘¤) â€–ğ‘¤â€–2 ğ‘¤. Then ğ‘£ âˆˆ (null ğœ‘) âŸ‚. Also, ğ‘£ â‰  0(because ğ‘¤ âˆ‰ null ğœ‘). Section 6C Orthogonal Complements and Minimization Problems 217 Taking the norm of both sides of 6.59 gives 6.60 â€–ğ‘£â€– = |ğœ‘(ğ‘¤)| â€–ğ‘¤â€– . Applying ğœ‘ to both sides of 6.59 and then using 6.60, we have ğœ‘(ğ‘£) = |ğœ‘(ğ‘¤)| 2 â€–ğ‘¤â€–2 = â€–ğ‘£â€–2. Now suppose ğ‘¢ âˆˆ ğ‘‰. Using the equation above, we have ğ‘¢ = (ğ‘¢ âˆ’ ğœ‘(ğ‘¢) ğœ‘(ğ‘£) ğ‘£)+ ğœ‘(ğ‘¢) â€–ğ‘£â€–2 ğ‘£. The first term in parentheses above is innull ğœ‘ and hence is orthogonal to ğ‘£. Thus taking the inner product of both sides of the equation above with ğ‘£ shows that âŸ¨ğ‘¢, ğ‘£âŸ© = ğœ‘(ğ‘¢) â€–ğ‘£â€–2 âŸ¨ğ‘£, ğ‘£âŸ© = ğœ‘(ğ‘¢). Thus ğœ‘ = ğœ‘ğ‘£, showing that ğ‘£ â†¦ ğœ‘ğ‘£ is surjective, as desired. See Exercise 13 for yet another proof of the Riesz representation theorem. Minimization Problems The remarkable simplicity of the solu- tion to this minimization problem has led to many important applications of inner product spaces outside of pure mathematics. The following problem often arises: Given a subspace ğ‘ˆ of ğ‘‰ and a point ğ‘£ âˆˆ ğ‘‰, find a pointğ‘¢ âˆˆ ğ‘ˆ such that â€–ğ‘£ âˆ’ ğ‘¢â€– is as small as possible. The next result shows that ğ‘¢ = ğ‘ƒğ‘ˆğ‘£ is the unique solution of this minimization problem. 6.61 minimizing distance to a subspace Suppose ğ‘ˆ is a finite-dimensional subspace ofğ‘‰, ğ‘£ âˆˆ ğ‘‰, and ğ‘¢ âˆˆ ğ‘ˆ. Then â€–ğ‘£ âˆ’ ğ‘ƒğ‘ˆğ‘£â€– â‰¤ â€–ğ‘£ âˆ’ ğ‘¢â€–. Furthermore, the inequality above is an equality if and only if ğ‘¢ = ğ‘ƒğ‘ˆğ‘£. Proof We have â€–ğ‘£ âˆ’ ğ‘ƒğ‘ˆğ‘£â€– 2 â‰¤ â€–ğ‘£ âˆ’ ğ‘ƒğ‘ˆğ‘£â€– 2 + â€–ğ‘ƒğ‘ˆğ‘£ âˆ’ ğ‘¢â€–26.62 = âˆ¥(ğ‘£ âˆ’ ğ‘ƒğ‘ˆğ‘£) + (ğ‘ƒğ‘ˆğ‘£ âˆ’ ğ‘¢)âˆ¥2 = â€–ğ‘£ âˆ’ ğ‘¢â€–2, 218 Chapter 6 Inner Product Spaces ğ‘ƒğ‘ˆğ‘£ is the closest point in ğ‘ˆ to ğ‘£. where the first line above holds because0 â‰¤ â€–ğ‘ƒğ‘ˆğ‘£ âˆ’ ğ‘¢â€–2, the second line above comes from the Pythagorean the- orem [which applies because ğ‘£ âˆ’ ğ‘ƒğ‘ˆğ‘£ âˆˆ ğ‘ˆâŸ‚ by 6.57(f), and ğ‘ƒğ‘ˆğ‘£ âˆ’ ğ‘¢ âˆˆ ğ‘ˆ], and the third line above holds by simple algebra. Taking square roots gives the desired inequality. The inequality proved above is an equality if and only if 6.62 is an equality, which happens if and only if â€–ğ‘ƒğ‘ˆğ‘£ âˆ’ ğ‘¢â€– = 0, which happens if and only if ğ‘¢ = ğ‘ƒğ‘ˆğ‘£. The last result is often combined with the formula 6.57(i) to compute explicit solutions to minimization problems, as in the following example. 6.63 example:using linear algebra to approximate the sine function Suppose we want to find a polynomialğ‘¢ with real coefficients and of degree at most 5that approximates the sine function as well as possible on the interval [âˆ’ğœ‹, ğœ‹], in the sense that âˆ« ğœ‹ âˆ’ğœ‹âˆ£sin ğ‘¥ âˆ’ ğ‘¢(ğ‘¥)âˆ£2 ğ‘‘ğ‘¥ is as small as possible. Let ğ¶[âˆ’ğœ‹, ğœ‹] denote the real inner product space of continuous real-valued functions on [âˆ’ğœ‹, ğœ‹] with inner product 6.64 âŸ¨ ğ‘“, ğ‘”âŸ© =âˆ«ğœ‹ âˆ’ğœ‹ ğ‘“ ğ‘”. Let ğ‘£ âˆˆ ğ¶[âˆ’ğœ‹, ğœ‹] be the function defined byğ‘£(ğ‘¥) = sin ğ‘¥. Let ğ‘ˆ denote the subspace of ğ¶[âˆ’ğœ‹, ğœ‹] consisting of the polynomials with real coefficients and of degree at most 5. Our problem can now be reformulated as follows: Find ğ‘¢ âˆˆ ğ‘ˆ such that â€–ğ‘£ âˆ’ ğ‘¢â€– is as small as possible. A computer that can integrate is useful here. To compute the solution to our ap- proximation problem, first apply the Gramâ€“Schmidt procedure (using the in- ner product given by 6.64) to the basis 1, ğ‘¥, ğ‘¥2, ğ‘¥3, ğ‘¥4, ğ‘¥5 of ğ‘ˆ, producing an ortho- normal basis ğ‘’1, ğ‘’2, ğ‘’3, ğ‘’4, ğ‘’5, ğ‘’6 of ğ‘ˆ. Then, again using the inner product given by 6.64, compute ğ‘ƒğ‘ˆğ‘£ using 6.57(i) (with ğ‘š = 6). Doing this computation shows that ğ‘ƒğ‘ˆğ‘£ is the function ğ‘¢ defined by 6.65 ğ‘¢(ğ‘¥) = 0.987862ğ‘¥ âˆ’ 0.155271ğ‘¥ 3 + 0.00564312ğ‘¥ 5, where the ğœ‹â€™s that appear in the exact answer have been replaced with a good decimal approximation. By 6.61, the polynomial ğ‘¢ above is the best approximation to the sine function on [âˆ’ğœ‹, ğœ‹] using polynomials of degree at most 5(here â€œbest approximationâ€ means in the sense of minimizingâˆ«ğœ‹ âˆ’ğœ‹ | sin ğ‘¥ âˆ’ ğ‘¢(ğ‘¥)|2 ğ‘‘ğ‘¥). Section 6C Orthogonal Complements and Minimization Problems 219 To see how good this approximation is, the next figure shows the graphs of both the sine function and our approximation ğ‘¢ given by 6.65 over the interval [âˆ’ğœ‹, ğœ‹]. Graphs on [âˆ’ğœ‹, ğœ‹] of the sine function (red) and its best fifth degree polynomial approximation ğ‘¢ (blue) from 6.65. Our approximation 6.65 is so accurate that the two graphs are almost identicalâ€” our eyes may see only one graph! Here the red graph is placed almost exactly over the blue graph. If you are viewing this on an electronic device, enlarge the picture above by 400% near ğœ‹ or âˆ’ğœ‹ to see a small gap between the two graphs. Another well-known approximation to the sine function by a polynomial of degree 5is given by the Taylor polynomial ğ‘ defined by 6.66 ğ‘(ğ‘¥) = ğ‘¥ âˆ’ ğ‘¥3 3! + ğ‘¥5 5! . To see how good this approximation is, the next picture shows the graphs of both the sine function and the Taylor polynomial ğ‘ over the interval [âˆ’ğœ‹, ğœ‹]. Graphs on [âˆ’ğœ‹, ğœ‹] of the sine function (red) and the Taylor polynomial (blue) from 6.66. The Taylor polynomial is an excellent approximation to sin ğ‘¥ for ğ‘¥ near 0. But the picture above shows that for |ğ‘¥| > 2, the Taylor polynomial is not so accurate, especially compared to 6.65. For example, taking ğ‘¥ = 3, our approximation 6.65 estimates sin 3with an error of approximately 0.001, but the Taylor series 6.66 estimates sin 3with an error of approximately 0.4. Thus at ğ‘¥ = 3, the error in the Taylor series is hundreds of times larger than the error given by 6.65. Linear algebra has helped us discover an approximation to the sine function that improves upon what we learned in calculus! 220 Chapter 6 Inner Product Spaces Pseudoinverse Suppose ğ‘‡ âˆˆ â„’(ğ‘‰, ğ‘Š) and ğ‘ âˆˆ ğ‘Š. Consider the problem of findingğ‘¥ âˆˆ ğ‘‰ such that ğ‘‡ğ‘¥ = ğ‘. For example, if ğ‘‰ = ğ…ğ‘› and ğ‘Š = ğ…ğ‘š, then the equation above could represent a system of ğ‘š linear equations in ğ‘› unknowns. If ğ‘‡ is invertible, then the unique solution to the equation above is ğ‘¥ = ğ‘‡âˆ’1ğ‘. However, if ğ‘‡ is not invertible, then for some ğ‘ âˆˆ ğ‘Š there may not exist any solutions of the equation above, and for some ğ‘ âˆˆ ğ‘Š there may exist infinitely many solutions of the equation above. If ğ‘‡ is not invertible, then we can still try to do as well as possible with the equation above. For example, if the equation above has no solutions, then instead of solving the equation ğ‘‡ğ‘¥ âˆ’ ğ‘ = 0, we can try to findğ‘¥ âˆˆ ğ‘‰ such that â€–ğ‘‡ğ‘¥ âˆ’ ğ‘â€– is as small as possible. As another example, if the equation above has infinitely many solutions ğ‘¥ âˆˆ ğ‘‰, then among all those solutions we can try to find one such that â€–ğ‘¥â€– is as small as possible. The pseudoinverse will provide the tool to solve the equation above as well as possible, even when ğ‘‡ is not invertible. We need the next result to define the pseudoinverse. In the next two proofs, we will use without further comment the result that if ğ‘‰ is finite-dimensional andğ‘‡ âˆˆ â„’(ğ‘‰, ğ‘Š), then null ğ‘‡, (null ğ‘‡) âŸ‚, and range ğ‘‡ are all finite-dimensional. 6.67restriction of a linear map to obtain a one-to-one and onto map Suppose ğ‘‰ is finite-dimensional andğ‘‡ âˆˆ â„’(ğ‘‰, ğ‘Š). Then ğ‘‡|(null ğ‘‡)âŸ‚ is a one- to-one map of (null ğ‘‡) âŸ‚ onto range ğ‘‡. Proof Suppose that ğ‘£ âˆˆ (null ğ‘‡) âŸ‚ and ğ‘‡|(null ğ‘‡)âŸ‚ğ‘£ = 0. Hence ğ‘‡ğ‘£ = 0and thus ğ‘£ âˆˆ (null ğ‘‡) âˆ© (null ğ‘‡) âŸ‚, which implies that ğ‘£ = 0[by 6.48(d)]. Hence null ğ‘‡|(null ğ‘‡)âŸ‚ = {0}, which implies that ğ‘‡|(null ğ‘‡)âŸ‚ is injective, as desired. Clearly range ğ‘‡|(null ğ‘‡)âŸ‚ âŠ†range ğ‘‡. To prove the inclusion in the other direction, suppose ğ‘¤ âˆˆ range ğ‘‡. Hence there exists ğ‘£ âˆˆ ğ‘‰ such that ğ‘¤ = ğ‘‡ğ‘£. There exist ğ‘¢ âˆˆ null ğ‘‡ and ğ‘¥ âˆˆ (null ğ‘‡) âŸ‚ such that ğ‘£ = ğ‘¢ + ğ‘¥ (by 6.49). Now ğ‘‡|(null ğ‘‡)âŸ‚ğ‘¥ = ğ‘‡ğ‘¥ = ğ‘‡ğ‘£ âˆ’ ğ‘‡ğ‘¢ = ğ‘¤ âˆ’ 0 = ğ‘¤, which shows that ğ‘¤ âˆˆ range ğ‘‡|(null ğ‘‡)âŸ‚. Hence range ğ‘‡ âŠ†range ğ‘‡|(null ğ‘‡)âŸ‚, complet- ing the proof that range ğ‘‡|(null ğ‘‡)âŸ‚ = range ğ‘‡. To produce the pseudoinverse notation ğ‘‡â€  in TEX, type T Ì‚\\dagger. Now we can define the pseudoinverse ğ‘‡â€  (pronounced â€œğ‘‡ daggerâ€) of a linear map ğ‘‡. In the next definition (and from now on), think of ğ‘‡|(null ğ‘‡)âŸ‚ as an invertible linear map from (null ğ‘‡) âŸ‚ onto range ğ‘‡, as is justified by the result above. Section 6C Orthogonal Complements and Minimization Problems 221 6.68 definition:pseudoinverse, ğ‘‡â€  Suppose that ğ‘‰ is finite-dimensional andğ‘‡ âˆˆ â„’(ğ‘‰, ğ‘Š). The pseudoinverse ğ‘‡â€  âˆˆ â„’(ğ‘Š, ğ‘‰) of ğ‘‡ is the linear map from ğ‘Š to ğ‘‰ defined by ğ‘‡â€ ğ‘¤ = (ğ‘‡|(null ğ‘‡)âŸ‚)âˆ’1ğ‘ƒrange ğ‘‡ ğ‘¤ for each ğ‘¤ âˆˆ ğ‘Š. Recall that ğ‘ƒrange ğ‘‡ ğ‘¤ = 0if ğ‘¤ âˆˆ (range ğ‘‡) âŸ‚ and ğ‘ƒrange ğ‘‡ ğ‘¤ = ğ‘¤ if ğ‘¤ âˆˆ range ğ‘‡. Thus if ğ‘¤ âˆˆ (range ğ‘‡) âŸ‚, then ğ‘‡â€ ğ‘¤ = 0, and if ğ‘¤ âˆˆ range ğ‘‡, then ğ‘‡â€ ğ‘¤ is the unique element of (null ğ‘‡) âŸ‚ such that ğ‘‡(ğ‘‡â€ ğ‘¤)= ğ‘¤. The pseudoinverse behaves much like an inverse, as we will see. 6.69 algebraic properties of the pseudoinverse Suppose ğ‘‰ is finite-dimensional andğ‘‡ âˆˆ â„’(ğ‘‰, ğ‘Š). (a) If ğ‘‡ is invertible, then ğ‘‡â€  = ğ‘‡âˆ’1. (b) ğ‘‡ğ‘‡â€  = ğ‘ƒrange ğ‘‡ = the orthogonal projection of ğ‘Š onto range ğ‘‡. (c) ğ‘‡â€ ğ‘‡ = ğ‘ƒ(null ğ‘‡)âŸ‚ = the orthogonal projection of ğ‘‰ onto (null ğ‘‡)âŸ‚. Proof (a) Suppose ğ‘‡ is invertible. Then (null ğ‘‡) âŸ‚ = ğ‘‰ and range ğ‘‡ = ğ‘Š. Thus ğ‘‡|(null ğ‘‡)âŸ‚ = ğ‘‡ and ğ‘ƒrange ğ‘‡ is the identity operator on ğ‘Š. Hence ğ‘‡â€  = ğ‘‡âˆ’1. (b) Suppose ğ‘¤ âˆˆ range ğ‘‡. Thus ğ‘‡ğ‘‡â€ ğ‘¤ = ğ‘‡(ğ‘‡|(null ğ‘‡)âŸ‚)âˆ’1ğ‘¤ = ğ‘¤ = ğ‘ƒrange ğ‘‡ ğ‘¤. If ğ‘¤ âˆˆ (range ğ‘‡) âŸ‚, then ğ‘‡â€ ğ‘¤ = 0. Hence ğ‘‡ğ‘‡â€ ğ‘¤ = 0 = ğ‘ƒrange ğ‘‡ ğ‘¤. Thus ğ‘‡ğ‘‡â€  and ğ‘ƒrange ğ‘‡ agree on range ğ‘‡ and on (range ğ‘‡) âŸ‚. Hence these two linear maps are equal (by 6.49). (c) Suppose ğ‘£ âˆˆ (null ğ‘‡) âŸ‚. Because ğ‘‡ğ‘£ âˆˆ range ğ‘‡, the definition ofğ‘‡â€  shows that ğ‘‡â€ (ğ‘‡ğ‘£) = (ğ‘‡|(null ğ‘‡)âŸ‚)âˆ’1(ğ‘‡ğ‘£) = ğ‘£ = ğ‘ƒ(null ğ‘‡)âŸ‚ğ‘£. If ğ‘£ âˆˆ null ğ‘‡, then ğ‘‡â€ ğ‘‡ğ‘£ = 0 = ğ‘ƒ(null ğ‘‡)âŸ‚ğ‘£. Thus ğ‘‡â€ ğ‘‡ and ğ‘ƒ(null ğ‘‡)âŸ‚ agree on (null ğ‘‡) âŸ‚ and on null ğ‘‡. Hence these two linear maps are equal (by 6.49). The pseudoinverse is also called the Mooreâ€“Penrose inverse. Suppose that ğ‘‡ âˆˆ â„’(ğ‘‰, ğ‘Š). If ğ‘‡ is surjective, then ğ‘‡ğ‘‡â€  is the identity opera- tor on ğ‘Š, as follows from (b) in the result above. If ğ‘‡ is injective, then ğ‘‡â€ ğ‘‡ is the identity operator on ğ‘‰, as follows from (c) in the result above. For additional algebraic properties of the pseudoinverse, see Exercises 19â€“23. 222 Chapter 6 Inner Product Spaces Suppose ğ‘‡ âˆˆ â„’(ğ‘‰, ğ‘Š), ğ‘ âˆˆ ğ‘Š, and we want to findğ‘¥ âˆˆ ğ‘‰ that solves the equation ğ‘‡ğ‘¥ = ğ‘. If ğ‘‡ is invertible, then ğ‘¥ = ğ‘‡âˆ’1ğ‘ is the unique solution. If ğ‘‡ is not invertible, then ğ‘‡âˆ’1 is not defined. However, the pseudoinverseğ‘‡â€  is defined. Takingğ‘¥ = ğ‘‡â€ ğ‘ makes ğ‘‡ğ‘¥ as close to ğ‘ as possible, as shown by (a) of the next result. Thus the pseudoinverse provides what is called a best fit to the equation above. Among all vectors ğ‘¥ âˆˆ ğ‘‰ that make ğ‘‡ğ‘¥ as close as possible to ğ‘, the vector ğ‘‡â€ ğ‘ has the smallest norm, as shown by combining (b) in the next result with the condition for equality in (a). 6.70 pseudoinverse provides best approximate solution or best solution Suppose ğ‘‰ is finite-dimensional,ğ‘‡ âˆˆ â„’(ğ‘‰, ğ‘Š), and ğ‘ âˆˆ ğ‘Š. (a) If ğ‘¥ âˆˆ ğ‘‰, then âˆ¥ğ‘‡(ğ‘‡â€ ğ‘) âˆ’ ğ‘âˆ¥ â‰¤ â€–ğ‘‡ğ‘¥ âˆ’ ğ‘â€–, with equality if and only if ğ‘¥ âˆˆ ğ‘‡â€ ğ‘ + null ğ‘‡. (b) If ğ‘¥ âˆˆ ğ‘‡â€ ğ‘ + null ğ‘‡, then âˆ¥ğ‘‡â€ ğ‘âˆ¥ â‰¤ â€–ğ‘¥â€–, with equality if and only if ğ‘¥ = ğ‘‡â€ ğ‘. Proof (a) Suppose ğ‘¥ âˆˆ ğ‘‰. Then ğ‘‡ğ‘¥ âˆ’ ğ‘ = (ğ‘‡ğ‘¥ âˆ’ ğ‘‡ğ‘‡â€ ğ‘) + (ğ‘‡ğ‘‡â€ ğ‘ âˆ’ ğ‘). The first term in parentheses above is inrange ğ‘‡. Because the operator ğ‘‡ğ‘‡â€  is the orthogonal projection of ğ‘Š onto range ğ‘‡ [by 6.69(b)], the second term in parentheses above is in (range ğ‘‡) âŸ‚ [see 6.57(f)]. Thus the Pythagorean theorem implies the desired inequality that the norm of the second term in parentheses above is less than or equal to â€–ğ‘‡ğ‘¥ âˆ’ ğ‘â€–, with equality if and only if the first term in parentheses above equals0. Hence we have equality if and only if ğ‘¥ âˆ’ ğ‘‡â€ ğ‘ âˆˆ null ğ‘‡, which is equivalent to the statement that ğ‘¥ âˆˆ ğ‘‡â€ ğ‘ + null ğ‘‡, completing the proof of (a). (b) Suppose ğ‘¥ âˆˆ ğ‘‡â€ ğ‘ + null ğ‘‡. Hence ğ‘¥ âˆ’ ğ‘‡â€ ğ‘ âˆˆ null ğ‘‡. Now ğ‘¥ = (ğ‘¥ âˆ’ ğ‘‡â€ ğ‘) + ğ‘‡â€ ğ‘. The definition ofğ‘‡â€  implies that ğ‘‡â€ ğ‘ âˆˆ (null ğ‘‡) âŸ‚. Thus the Pythagorean theorem implies that âˆ¥ğ‘‡â€ ğ‘âˆ¥ â‰¤ â€–ğ‘¥â€–, with equality if and only if ğ‘¥ = ğ‘‡â€ ğ‘. A formula for ğ‘‡â€  will be given in the next chapter (see 7.78). Section 6C Orthogonal Complements and Minimization Problems 223 6.71 example:pseudoinverse of a linear map from ğ…4 to ğ…3 Suppose ğ‘‡ âˆˆ â„’(ğ…4, ğ…3)is defined by ğ‘‡(ğ‘, ğ‘, ğ‘, ğ‘‘) = (ğ‘ + ğ‘ + ğ‘, 2ğ‘+ ğ‘‘, 0). This linear map is neither injective nor surjective, but we can compute its pseudo- inverse. To do this, first note thatrange ğ‘‡ = {(ğ‘¥, ğ‘¦, 0)âˆ¶ ğ‘¥, ğ‘¦ âˆˆ ğ…}. Thus ğ‘ƒrange ğ‘‡(ğ‘¥, ğ‘¦, ğ‘§) = (ğ‘¥, ğ‘¦, 0) for each (ğ‘¥, ğ‘¦, ğ‘§) âˆˆ ğ…3. Also, null ğ‘‡ = {(ğ‘, ğ‘, ğ‘, ğ‘‘) âˆˆ ğ…4 âˆ¶ ğ‘ + ğ‘ + ğ‘ = 0and 2ğ‘+ ğ‘‘ = 0}. The list (âˆ’1, 1, 0, 0), (âˆ’1, 0, 1, âˆ’2)of two vectors in null ğ‘‡ spans null ğ‘‡ because if (ğ‘, ğ‘, ğ‘, ğ‘‘) âˆˆ null ğ‘‡ then (ğ‘, ğ‘, ğ‘, ğ‘‘) = ğ‘(âˆ’1, 1, 0, 0)+ ğ‘(âˆ’1, 0, 1, âˆ’2). Because the list (âˆ’1, 1, 0, 0), (âˆ’1, 0, 1, âˆ’2)is linearly independent, this list is a basis of null ğ‘‡. Now suppose (ğ‘¥, ğ‘¦, ğ‘§) âˆˆ ğ…3. Then 6.72 ğ‘‡â€ (ğ‘¥, ğ‘¦, ğ‘§) = (ğ‘‡|(null ğ‘‡)âŸ‚) âˆ’1ğ‘ƒrange ğ‘‡(ğ‘¥, ğ‘¦, ğ‘§) = (ğ‘‡|(null ğ‘‡)âŸ‚)âˆ’1(ğ‘¥, ğ‘¦, 0). The right side of the equation above is the vector (ğ‘, ğ‘, ğ‘, ğ‘‘) âˆˆ ğ…4 such that ğ‘‡(ğ‘, ğ‘, ğ‘, ğ‘‘) = (ğ‘¥, ğ‘¦, 0)and (ğ‘, ğ‘, ğ‘, ğ‘‘) âˆˆ (null ğ‘‡)âŸ‚. In other words, ğ‘, ğ‘, ğ‘, ğ‘‘ must satisfy the following equations: ğ‘ + ğ‘ + ğ‘ = ğ‘¥ 2ğ‘+ ğ‘‘ = ğ‘¦ âˆ’ğ‘ + ğ‘ = 0 âˆ’ğ‘ + ğ‘ âˆ’ 2ğ‘‘ = 0, where the first two equations are equivalent to the equationğ‘‡(ğ‘, ğ‘, ğ‘, ğ‘‘) = (ğ‘¥, ğ‘¦, 0) and the last two equations come from the condition for (ğ‘, ğ‘, ğ‘, ğ‘‘) to be orthogo- nal to each of the basis vectors (âˆ’1, 1, 0, 0), (âˆ’1, 0, 1, âˆ’2)in this basis of null ğ‘‡. Thinking of ğ‘¥ and ğ‘¦ as constants and ğ‘, ğ‘, ğ‘, ğ‘‘ as unknowns, we can solve the system above of four equations in four unknowns, getting ğ‘ = 1 11 (5ğ‘¥ âˆ’ 2ğ‘¦), ğ‘ = 1 11 (5ğ‘¥ âˆ’ 2ğ‘¦), ğ‘ = 1 11 (ğ‘¥ + 4ğ‘¦), ğ‘‘ = 1 11 (âˆ’2ğ‘¥+ 3ğ‘¦). Hence 6.72 tells us that ğ‘‡â€ (ğ‘¥, ğ‘¦, ğ‘§) = 1 11 (5ğ‘¥ âˆ’ 2ğ‘¦, 5ğ‘¥ âˆ’ 2ğ‘¦, ğ‘¥ + 4ğ‘¦, âˆ’2ğ‘¥+ 3ğ‘¦). The formula above for ğ‘‡â€  shows that ğ‘‡ğ‘‡â€ (ğ‘¥, ğ‘¦, ğ‘§) = (ğ‘¥, ğ‘¦, 0)for all (ğ‘¥, ğ‘¦, ğ‘§) âˆˆ ğ…3, which illustrates the equation ğ‘‡ğ‘‡â€  = ğ‘ƒrange ğ‘‡ from 6.69(b). 224 Chapter 6 Inner Product Spaces Exercises 6C 1 Suppose ğ‘£1, â€¦, ğ‘£ğ‘š âˆˆ ğ‘‰. Prove that {ğ‘£1, â€¦, ğ‘£ğ‘š} âŸ‚ = (span(ğ‘£1, â€¦, ğ‘£ğ‘š)) âŸ‚ . 2 Suppose ğ‘ˆ is a subspace of ğ‘‰ with basis ğ‘¢1, â€¦, ğ‘¢ğ‘š and ğ‘¢1, â€¦, ğ‘¢ğ‘š, ğ‘£1, â€¦, ğ‘£ğ‘› is a basis of ğ‘‰. Prove that if the Gramâ€“Schmidt procedure is applied to the basis of ğ‘‰ above, producing a list ğ‘’1, â€¦, ğ‘’ğ‘š, ğ‘“1, â€¦, ğ‘“ğ‘›, then ğ‘’1, â€¦, ğ‘’ğ‘š is an orthonormal basis of ğ‘ˆ and ğ‘“1, â€¦, ğ‘“ğ‘› is an orthonormal basis of ğ‘ˆâŸ‚. 3 Suppose ğ‘ˆ is the subspace of ğ‘4 defined by ğ‘ˆ = span((1, 2, 3, âˆ’4), (âˆ’5, 4, 3, 2)). Find an orthonormal basis of ğ‘ˆ and an orthonormal basis of ğ‘ˆâŸ‚. 4 Suppose ğ‘’1, â€¦, ğ‘’ğ‘› is a list of vectors in ğ‘‰ with â€–ğ‘’ğ‘˜â€– = 1for each ğ‘˜ = 1, â€¦, ğ‘› and â€–ğ‘£â€– 2 = âˆ£âŸ¨ğ‘£, ğ‘’1âŸ©âˆ£2 + â‹¯ + âˆ£âŸ¨ğ‘£, ğ‘’ğ‘›âŸ©âˆ£2 for all ğ‘£ âˆˆ ğ‘‰. Prove that ğ‘’1, â€¦, ğ‘’ğ‘› is an orthonormal basis of ğ‘‰. This exercise provides a converse to 6.30(b). 5 Suppose that ğ‘‰ is finite-dimensional andğ‘ˆ is a subspace of ğ‘‰. Show that ğ‘ƒğ‘ˆâŸ‚ = ğ¼ âˆ’ ğ‘ƒğ‘ˆ, where ğ¼ is the identity operator on ğ‘‰. 6 Suppose ğ‘‰ is finite-dimensional andğ‘‡ âˆˆ â„’(ğ‘‰, ğ‘Š). Show that ğ‘‡ = ğ‘‡ğ‘ƒ(null ğ‘‡)âŸ‚ = ğ‘ƒrange ğ‘‡ğ‘‡. 7 Suppose that ğ‘‹ and ğ‘Œ are finite-dimensional subspaces ofğ‘‰. Prove that ğ‘ƒğ‘‹ğ‘ƒğ‘Œ = 0if and only if âŸ¨ğ‘¥, ğ‘¦âŸ© = 0for all ğ‘¥ âˆˆ ğ‘‹ and all ğ‘¦ âˆˆ ğ‘Œ. 8 Suppose ğ‘ˆ is a finite-dimensional subspace ofğ‘‰ and ğ‘£ âˆˆ ğ‘‰. Define a linear functional ğœ‘âˆ¶ ğ‘ˆ â†’ ğ… by ğœ‘(ğ‘¢) = âŸ¨ğ‘¢, ğ‘£âŸ© for all ğ‘¢ âˆˆ ğ‘ˆ. By the Riesz representation theorem (6.42) as applied to the inner product space ğ‘ˆ, there exists a unique vector ğ‘¤ âˆˆ ğ‘ˆ such that ğœ‘(ğ‘¢) = âŸ¨ğ‘¢, ğ‘¤âŸ© for all ğ‘¢ âˆˆ ğ‘ˆ. Show that ğ‘¤ = ğ‘ƒğ‘ˆğ‘£. 9 Suppose ğ‘‰ is finite-dimensional. Supposeğ‘ƒ âˆˆ â„’(ğ‘‰) is such that ğ‘ƒ2 = ğ‘ƒ and every vector in null ğ‘ƒ is orthogonal to every vector in range ğ‘ƒ. Prove that there exists a subspace ğ‘ˆ of ğ‘‰ such that ğ‘ƒ = ğ‘ƒğ‘ˆ. Section 6C Orthogonal Complements and Minimization Problems 225 10 Suppose ğ‘‰ is finite-dimensional andğ‘ƒ âˆˆ â„’(ğ‘‰) is such that ğ‘ƒ2 = ğ‘ƒ and â€–ğ‘ƒğ‘£â€– â‰¤ â€–ğ‘£â€– for every ğ‘£ âˆˆ ğ‘‰. Prove that there exists a subspace ğ‘ˆ of ğ‘‰ such that ğ‘ƒ = ğ‘ƒğ‘ˆ. 11 Suppose ğ‘‡ âˆˆ â„’(ğ‘‰) and ğ‘ˆ is a finite-dimensional subspace ofğ‘‰. Prove that ğ‘ˆ is invariant under ğ‘‡ âŸº ğ‘ƒğ‘ˆğ‘‡ğ‘ƒğ‘ˆ = ğ‘‡ğ‘ƒğ‘ˆ. 12 Suppose ğ‘‰ is finite-dimensional,ğ‘‡ âˆˆ â„’(ğ‘‰), and ğ‘ˆ is a subspace of ğ‘‰. Prove that ğ‘ˆ and ğ‘ˆâŸ‚ are both invariant under ğ‘‡ âŸº ğ‘ƒğ‘ˆğ‘‡ = ğ‘‡ğ‘ƒğ‘ˆ. 13 Suppose ğ… = ğ‘ and ğ‘‰ is finite-dimensional. For eachğ‘£ âˆˆ ğ‘‰, let ğœ‘ğ‘£ denote the linear functional on ğ‘‰ defined by ğœ‘ğ‘£(ğ‘¢) = âŸ¨ğ‘¢, ğ‘£âŸ© for all ğ‘¢ âˆˆ ğ‘‰. (a) Show that ğ‘£ â†¦ ğœ‘ğ‘£ is an injective linear map from ğ‘‰ to ğ‘‰â€². (b) Use (a) and a dimension-counting argument to show that ğ‘£ â†¦ ğœ‘ğ‘£ is an isomorphism from ğ‘‰ onto ğ‘‰â€². The purpose of this exercise is to give an alternative proof of the Riesz representation theorem (6.42 and 6.58) when ğ… = ğ‘. Thus you should not use the Riesz representation theorem as a tool in your solution. 14 Suppose that ğ‘’1, â€¦, ğ‘’ğ‘› is an orthonormal basis of ğ‘‰. Explain why the dual basis (see 3.112) of ğ‘’1, â€¦, ğ‘’ğ‘› is ğ‘’1, â€¦, ğ‘’ğ‘› under the identification ofğ‘‰â€² with ğ‘‰ provided by the Riesz representation theorem (6.58). 15 In ğ‘4, let ğ‘ˆ = span((1, 1, 0, 0), (1, 1, 1, 2)). Find ğ‘¢ âˆˆ ğ‘ˆ such that â€–ğ‘¢ âˆ’ (1, 2, 3, 4)â€–is as small as possible. 16 Suppose ğ¶[âˆ’1, 1]is the vector space of continuous real-valued functions on the interval [âˆ’1, 1]with inner product given by âŸ¨ ğ‘“, ğ‘”âŸ© =âˆ«1 âˆ’1 ğ‘“ ğ‘” for all ğ‘“, ğ‘” âˆˆ ğ¶[âˆ’1, 1]. Let ğ‘ˆ be the subspace of ğ¶[âˆ’1, 1]defined by ğ‘ˆ = {ğ‘“ âˆˆ ğ¶[âˆ’1, 1]âˆ¶ ğ‘“ (0) = 0}. (a) Show that ğ‘ˆâŸ‚ = {0}. (b) Show that 6.49 and 6.52 do not hold without the finite-dimensional hypothesis. 226 Chapter 6 Inner Product Spaces 17 Find ğ‘ âˆˆ ğ’«3(ğ‘) such that ğ‘(0) = 0, ğ‘ â€²(0) = 0, and âˆ«1 0 âˆ£2+ 3ğ‘¥ âˆ’ ğ‘(ğ‘¥)âˆ£ 2 ğ‘‘ğ‘¥ is as small as possible. 18 Find ğ‘ âˆˆ ğ’«5(ğ‘) that makes âˆ«ğœ‹ âˆ’ğœ‹ âˆ£sin ğ‘¥ âˆ’ ğ‘(ğ‘¥)âˆ£ 2 ğ‘‘ğ‘¥ as small as possible. The polynomial 6.65 is an excellent approximation to the answer to this exercise, but here you are asked to find the exact solution, which involves powers of ğœ‹. A computer that can perform symbolic integration should help. 19 Suppose ğ‘‰ is finite-dimensional andğ‘ƒ âˆˆ â„’(ğ‘‰) is an orthogonal projection of ğ‘‰ onto some subspace of ğ‘‰. Prove that ğ‘ƒâ€  = ğ‘ƒ. 20 Suppose ğ‘‰ is finite-dimensional andğ‘‡ âˆˆ â„’(ğ‘‰, ğ‘Š). Show that null ğ‘‡â€  = (range ğ‘‡) âŸ‚ and range ğ‘‡â€  = (null ğ‘‡)âŸ‚. 21 Suppose ğ‘‡ âˆˆ â„’(ğ…3, ğ…2)is defined by ğ‘‡(ğ‘, ğ‘, ğ‘) = (ğ‘ + ğ‘ + ğ‘, 2ğ‘+ 3ğ‘). (a) For (ğ‘¥, ğ‘¦) âˆˆ ğ…2, find a formula forğ‘‡â€ (ğ‘¥, ğ‘¦). (b) Verify that the equation ğ‘‡ğ‘‡â€  = ğ‘ƒrange ğ‘‡ from 6.69(b) holds with the formula for ğ‘‡â€  obtained in (a). (c) Verify that the equation ğ‘‡â€ ğ‘‡ = ğ‘ƒ(null ğ‘‡)âŸ‚ from 6.69(c) holds with the formula for ğ‘‡â€  obtained in (a). 22 Suppose ğ‘‰ is finite-dimensional andğ‘‡ âˆˆ â„’(ğ‘‰, ğ‘Š). Prove that ğ‘‡ğ‘‡â€ ğ‘‡ = ğ‘‡ and ğ‘‡â€ ğ‘‡ğ‘‡â€  = ğ‘‡â€ . Both formulas above clearly hold if ğ‘‡ is invertible because in that case we can replace ğ‘‡â€  with ğ‘‡âˆ’1. 23 Suppose ğ‘‰ and ğ‘Š are finite-dimensional andğ‘‡ âˆˆ â„’(ğ‘‰, ğ‘Š). Prove that (ğ‘‡â€ ) â€  = ğ‘‡. The equation above is analogous to the equation (ğ‘‡âˆ’1) âˆ’1 = ğ‘‡ that holds if ğ‘‡ is invertible. Open Access This chapter is licensed under the terms of the Creative Commons Attribution-NonCommercial 4.0 International License (https://creativecommons.org/licenses/by-nc/4.0), which permits any noncommercial use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to original author and source, provide a link to the Creative Commons license, and indicate if changes were made. The images or other third party material in this chapter are included in the chapterâ€™s Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the chapterâ€™s Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. Chapter 7 Operators on Inner Product Spaces The deepest results related to inner product spaces deal with the subject to which we now turnâ€”linear maps and operators on inner product spaces. As we will see, good theorems can be proved by exploiting properties of the adjoint. The hugely important spectral theorem will provide a complete description of self-adjoint operators on real inner product spaces and of normal operators on complex inner product spaces. We will then use the spectral theorem to help understand positive operators and unitary operators, which will lead to unitary matrices and matrix factorizations. The spectral theorem will also lead to the popular singular value decomposition, which will lead to the polar decomposition. The most important results in the rest of this book are valid only in finite dimensions. Thus from now on we assume that ğ‘‰ and ğ‘Š are finite-dimensional. standing assumptions for this chapter â€¢ ğ… denotes ğ‘ or ğ‚. â€¢ ğ‘‰ and ğ‘Š are nonzero finite-dimensional inner product spaces overğ….PetarMiloÅ¡eviÄ‡CCBY-SA Market square in Lviv, a city that has had several names and has been in several countries because of changing international borders. From 1772 until 1918, the city was in Austria and was called Lemberg. Between World War I and World War II, the city was in Poland and was called LwÃ³w. During this time, mathematicians in LwÃ³w, particularly Stefan Banach (1892â€“1945) and his colleagues, developed the basic results of modern functional analysis, using tools of analysis to study infinite-dimensional vector spaces. Since the end of World War II, Lviv has been in Ukraine, which was part of the Soviet Union until Ukraine became an independent country in 1991. 227 S. Axler, Linear Algebra Done Right, Undergraduate Texts in Mathematics, https://doi.org/10.1007/978-3-031-41026-0_7 Â© Sheldon Axler 2024 228 Chapter 7 Operators on Inner Product Spaces 7A Self-Adjoint and Normal Operators Adjoints 7.1 definition:adjoint, ğ‘‡âˆ— Suppose ğ‘‡ âˆˆ â„’(ğ‘‰, ğ‘Š). The adjoint of ğ‘‡ is the function ğ‘‡âˆ— âˆ¶ ğ‘Š â†’ ğ‘‰ such that âŸ¨ğ‘‡ğ‘£, ğ‘¤âŸ© =âŸ¨ğ‘£, ğ‘‡âˆ—ğ‘¤âŸ© for every ğ‘£ âˆˆ ğ‘‰ and every ğ‘¤ âˆˆ ğ‘Š. The word adjoint has another meaning in linear algebra. In case you en- counter the second meaning elsewhere, be warned that the two meanings for adjoint are unrelated to each other. To see why the definition above makes sense, suppose ğ‘‡ âˆˆ â„’(ğ‘‰, ğ‘Š). Fix ğ‘¤ âˆˆ ğ‘Š. Consider the linear functional ğ‘£ â†¦ âŸ¨ğ‘‡ğ‘£, ğ‘¤âŸ© on ğ‘‰ that maps ğ‘£ âˆˆ ğ‘‰ to âŸ¨ğ‘‡ğ‘£, ğ‘¤âŸ©; this linear functional depends on ğ‘‡ and ğ‘¤. By the Riesz representation theorem (6.42), there exists a unique vector in ğ‘‰ such that this linear functional is given by taking the inner product with it. We call this unique vector ğ‘‡âˆ—ğ‘¤. In other words, ğ‘‡âˆ—ğ‘¤ is the unique vector in ğ‘‰ such that âŸ¨ğ‘‡ğ‘£, ğ‘¤âŸ© =âŸ¨ğ‘£, ğ‘‡âˆ—ğ‘¤âŸ© for every ğ‘£ âˆˆ ğ‘‰. In the equation above, the inner product on the left takes place in ğ‘Š and the inner product on the right takes place in ğ‘‰. However, we use the same notation âŸ¨â‹…, â‹…âŸ©for both inner products. 7.2 example:adjoint of a linear map from ğ‘3 to ğ‘2 Defineğ‘‡âˆ¶ ğ‘3 â†’ ğ‘2 by ğ‘‡(ğ‘¥1, ğ‘¥2, ğ‘¥3) = (ğ‘¥2 + 3ğ‘¥3, 2ğ‘¥1). To compute ğ‘‡âˆ—, suppose (ğ‘¥1, ğ‘¥2, ğ‘¥3) âˆˆ ğ‘3 and (ğ‘¦1, ğ‘¦2) âˆˆ ğ‘2. Then âŸ¨ğ‘‡(ğ‘¥1, ğ‘¥2, ğ‘¥3), (ğ‘¦1, ğ‘¦2)âŸ©= âŸ¨(ğ‘¥2 + 3ğ‘¥3, 2ğ‘¥1), (ğ‘¦1, ğ‘¦2)âŸ© = ğ‘¥2ğ‘¦1 + 3ğ‘¥3ğ‘¦1 + 2ğ‘¥1ğ‘¦2 = âŸ¨(ğ‘¥1, ğ‘¥2, ğ‘¥3), (2ğ‘¦2, ğ‘¦1, 3ğ‘¦1)âŸ©. The equation above and the definition of the adjoint imply that ğ‘‡âˆ—(ğ‘¦1, ğ‘¦2) = (2ğ‘¦2, ğ‘¦1, 3ğ‘¦1). Section 7A Self-Adjoint and Normal Operators 229 7.3 example:adjoint of a linear map with range of dimension at most 1 Fix ğ‘¢ âˆˆ ğ‘‰ and ğ‘¥ âˆˆ ğ‘Š. Defineğ‘‡ âˆˆ â„’(ğ‘‰, ğ‘Š) by ğ‘‡ğ‘£ = âŸ¨ğ‘£, ğ‘¢âŸ©ğ‘¥ for each ğ‘£ âˆˆ ğ‘‰. To compute ğ‘‡âˆ—, suppose ğ‘£ âˆˆ ğ‘‰ and ğ‘¤ âˆˆ ğ‘Š. Then âŸ¨ğ‘‡ğ‘£, ğ‘¤âŸ© =âŸ¨âŸ¨ğ‘£, ğ‘¢âŸ©ğ‘¥, ğ‘¤âŸ© = âŸ¨ğ‘£, ğ‘¢âŸ©âŸ¨ğ‘¥, ğ‘¤âŸ© = âŸ¨ğ‘£, âŸ¨ğ‘¤, ğ‘¥âŸ©ğ‘¢âŸ©. Thus ğ‘‡âˆ—ğ‘¤ = âŸ¨ğ‘¤, ğ‘¥âŸ©ğ‘¢. The two examples above and the proof below use a common technique for computing ğ‘‡âˆ—: start with a formula for âŸ¨ğ‘‡ğ‘£, ğ‘¤âŸ©then manipulate it to get just ğ‘£ in the first slot; the entry in the second slot will then be ğ‘‡âˆ—ğ‘¤. In the two examples above, ğ‘‡âˆ— turned out to be not just a function from ğ‘‰ to ğ‘Š but a linear map from ğ‘‰ to ğ‘Š. This behavior is true in general, as shown by the next result. 7.4 adjoint of a linear map is a linear map If ğ‘‡ âˆˆ â„’(ğ‘‰, ğ‘Š), then ğ‘‡âˆ— âˆˆ â„’(ğ‘Š, ğ‘‰). Proof Suppose ğ‘‡ âˆˆ â„’(ğ‘‰, ğ‘Š). If ğ‘£ âˆˆ ğ‘‰ and ğ‘¤1, ğ‘¤2 âˆˆ ğ‘Š, then âŸ¨ğ‘‡ğ‘£, ğ‘¤1 + ğ‘¤2âŸ© = âŸ¨ğ‘‡ğ‘£, ğ‘¤1âŸ©+ âŸ¨ğ‘‡ğ‘£, ğ‘¤2âŸ© = âŸ¨ğ‘£, ğ‘‡âˆ—ğ‘¤1âŸ©+ âŸ¨ğ‘£, ğ‘‡âˆ—ğ‘¤2âŸ© = âŸ¨ğ‘£, ğ‘‡âˆ—ğ‘¤1 + ğ‘‡âˆ—ğ‘¤2âŸ©. The equation above shows that ğ‘‡âˆ—(ğ‘¤1 + ğ‘¤2) = ğ‘‡âˆ—ğ‘¤1 + ğ‘‡âˆ—ğ‘¤2. If ğ‘£ âˆˆ ğ‘‰, ğœ† âˆˆ ğ…, and ğ‘¤ âˆˆ ğ‘Š, then âŸ¨ğ‘‡ğ‘£, ğœ†ğ‘¤âŸ© = ğœ†âŸ¨ğ‘‡ğ‘£, ğ‘¤âŸ© = ğœ†âŸ¨ğ‘£, ğ‘‡âˆ—ğ‘¤âŸ© = âŸ¨ğ‘£, ğœ†ğ‘‡âˆ—ğ‘¤âŸ©. The equation above shows that ğ‘‡âˆ—(ğœ†ğ‘¤) = ğœ†ğ‘‡âˆ—ğ‘¤. Thus ğ‘‡âˆ— is a linear map, as desired. 230 Chapter 7 Operators on Inner Product Spaces 7.5 properties of the adjoint Suppose ğ‘‡ âˆˆ â„’(ğ‘‰, ğ‘Š). Then (a) (ğ‘† + ğ‘‡)âˆ— = ğ‘†âˆ— + ğ‘‡âˆ— for all ğ‘† âˆˆ â„’(ğ‘‰, ğ‘Š); (b) (ğœ†ğ‘‡)âˆ— = ğœ†ğ‘‡âˆ— for all ğœ† âˆˆ ğ…; (c) (ğ‘‡âˆ—) âˆ— = ğ‘‡; (d) (ğ‘†ğ‘‡)âˆ— = ğ‘‡âˆ—ğ‘†âˆ— for all ğ‘† âˆˆ â„’(ğ‘Š, ğ‘ˆ) (here ğ‘ˆ is a finite-dimensional inner product space over ğ…); (e) ğ¼âˆ— = ğ¼, where ğ¼ is the identity operator on ğ‘‰; (f) if ğ‘‡ is invertible, then ğ‘‡âˆ— is invertible and (ğ‘‡âˆ—) âˆ’1 = (ğ‘‡âˆ’1) âˆ—. Proof Suppose ğ‘£ âˆˆ ğ‘‰ and ğ‘¤ âˆˆ ğ‘Š. (a) If ğ‘† âˆˆ â„’(ğ‘‰, ğ‘Š), then âŸ¨(ğ‘† + ğ‘‡)ğ‘£, ğ‘¤âŸ©= âŸ¨ğ‘†ğ‘£, ğ‘¤âŸ©+ âŸ¨ğ‘‡ğ‘£, ğ‘¤âŸ© = âŸ¨ğ‘£, ğ‘†âˆ—ğ‘¤âŸ©+ âŸ¨ğ‘£, ğ‘‡âˆ—ğ‘¤âŸ© = âŸ¨ğ‘£, ğ‘†âˆ—ğ‘¤ + ğ‘‡âˆ—ğ‘¤âŸ©. Thus (ğ‘† + ğ‘‡)âˆ—ğ‘¤ = ğ‘†âˆ—ğ‘¤ + ğ‘‡âˆ—ğ‘¤, as desired. (b) If ğœ† âˆˆ ğ…, then âŸ¨(ğœ†ğ‘‡)ğ‘£, ğ‘¤âŸ©= ğœ†âŸ¨ğ‘‡ğ‘£, ğ‘¤âŸ© = ğœ†âŸ¨ğ‘£, ğ‘‡âˆ—ğ‘¤âŸ©= âŸ¨ğ‘£, ğœ†ğ‘‡âˆ—ğ‘¤âŸ©. Thus (ğœ†ğ‘‡)âˆ—ğ‘¤ = ğœ†ğ‘‡âˆ—ğ‘¤, as desired. (c) We have âŸ¨ğ‘‡âˆ—ğ‘¤, ğ‘£âŸ©= âŸ¨ğ‘£, ğ‘‡âˆ—ğ‘¤âŸ©= âŸ¨ğ‘‡ğ‘£, ğ‘¤âŸ©= âŸ¨ğ‘¤, ğ‘‡ğ‘£âŸ©. Thus (ğ‘‡âˆ—) âˆ—ğ‘£ = ğ‘‡ğ‘£, as desired. (d) Suppose ğ‘† âˆˆ â„’(ğ‘Š, ğ‘ˆ) and ğ‘¢ âˆˆ ğ‘ˆ. Then âŸ¨(ğ‘†ğ‘‡)ğ‘£, ğ‘¢âŸ©= âŸ¨ğ‘†(ğ‘‡ğ‘£), ğ‘¢âŸ©= âŸ¨ğ‘‡ğ‘£, ğ‘†âˆ—ğ‘¢âŸ©= âŸ¨ğ‘£, ğ‘‡âˆ—(ğ‘†âˆ—ğ‘¢)âŸ©. Thus (ğ‘†ğ‘‡)âˆ—ğ‘¢ = ğ‘‡âˆ—(ğ‘†âˆ—ğ‘¢), as desired. (e) Suppose ğ‘¢ âˆˆ ğ‘‰. Then âŸ¨ğ¼ğ‘¢, ğ‘£âŸ© = âŸ¨ğ‘¢, ğ‘£âŸ©. Thus ğ¼âˆ—ğ‘£ = ğ‘£, as desired. (f) Suppose ğ‘‡ is invertible. Take adjoints of both sides of the equation ğ‘‡âˆ’1ğ‘‡ = ğ¼, then use (d) and (e) to show that ğ‘‡âˆ—(ğ‘‡âˆ’1) âˆ— = ğ¼. Similarly, the equation ğ‘‡ğ‘‡âˆ’1 = ğ¼ implies (ğ‘‡âˆ’1) âˆ—ğ‘‡âˆ— = ğ¼. Thus (ğ‘‡âˆ’1) âˆ— is the inverse of ğ‘‡âˆ—, as desired. If ğ… = ğ‘, then the map ğ‘‡ â†¦ ğ‘‡âˆ— is a linear map from â„’(ğ‘‰, ğ‘Š) to â„’(ğ‘Š, ğ‘‰), as follows from (a) and (b) of the result above. However, if ğ… = ğ‚, then this map is not linear because of the complex conjugate that appears in (b). Section 7A Self-Adjoint and Normal Operators 231 The next result shows the relationship between the null space and the range of a linear map and its adjoint. 7.6 null space and range of ğ‘‡âˆ— Suppose ğ‘‡ âˆˆ â„’(ğ‘‰, ğ‘Š). Then (a) null ğ‘‡âˆ— = (range ğ‘‡) âŸ‚; (b) range ğ‘‡âˆ— = (null ğ‘‡) âŸ‚; (c) null ğ‘‡ = (range ğ‘‡âˆ—) âŸ‚; (d) range ğ‘‡ = (null ğ‘‡âˆ—) âŸ‚. Proof We begin by proving (a). Let ğ‘¤ âˆˆ ğ‘Š. Then ğ‘¤ âˆˆ null ğ‘‡âˆ— âŸº ğ‘‡âˆ—ğ‘¤ = 0 âŸº âŸ¨ğ‘£, ğ‘‡âˆ—ğ‘¤âŸ©= 0for all ğ‘£ âˆˆ ğ‘‰ âŸº âŸ¨ğ‘‡ğ‘£, ğ‘¤âŸ© = 0for all ğ‘£ âˆˆ ğ‘‰ âŸº ğ‘¤ âˆˆ (range ğ‘‡)âŸ‚. Thus null ğ‘‡âˆ— = (range ğ‘‡)âŸ‚, proving (a). If we take the orthogonal complement of both sides of (a), we get (d), where we have used 6.52. Replacing ğ‘‡ with ğ‘‡âˆ— in (a) gives (c), where we have used 7.5(c). Finally, replacing ğ‘‡ with ğ‘‡âˆ— in (d) gives (b). As we will soon see, the next definition is intimately connected to the matrix of the adjoint of a linear map. 7.7 definition:conjugate transpose, ğ´âˆ— The conjugate transpose of an ğ‘š-by-ğ‘› matrix ğ´ is the ğ‘›-by-ğ‘š matrix ğ´âˆ— obtained by interchanging the rows and columns and then taking the complex conjugate of each entry. In other words, if ğ‘— âˆˆ {1, â€¦, ğ‘›} and ğ‘˜ âˆˆ {1, â€¦, ğ‘š}, then (ğ´âˆ—)ğ‘—, ğ‘˜ = ğ´ğ‘˜, ğ‘—. 7.8 example:conjugate transpose of a 2-by-3matrix If a matrix ğ´ has only real entries, then ğ´âˆ— = ğ´t, where ğ´ t denotes the transpose of ğ´ (the matrix obtained by interchanging the rows and the columns). The conjugate transpose of the 2-by-3 matrix ( 2 3+ 4ğ‘– 7 6 5 8ğ‘– )is the 3-by-2 matrix â›âœâœâœ â 2 6 3 âˆ’ 4ğ‘– 5 7 âˆ’8ğ‘– ââŸâŸâŸ â  . 232 Chapter 7 Operators on Inner Product Spaces The adjoint of a linear map does not depend on a choice of basis. Thus we frequently emphasize adjoints of linear maps instead of transposes or conjugate transposes of matrices. The next result shows how to compute the matrix of ğ‘‡âˆ— from the matrix of ğ‘‡. Caution: With respect to nonorthonor- mal bases, the matrix of ğ‘‡âˆ— does not nec- essarily equal the conjugate transpose of the matrix of ğ‘‡. 7.9 matrix of ğ‘‡âˆ— equals conjugate transpose of matrix of ğ‘‡ Let ğ‘‡ âˆˆ â„’(ğ‘‰, ğ‘Š). Suppose ğ‘’1, â€¦, ğ‘’ğ‘› is an orthonormal basis of ğ‘‰ and ğ‘“1, â€¦, ğ‘“ğ‘š is an orthonormal basis of ğ‘Š. Then â„³(ğ‘‡âˆ—, ( ğ‘“1, â€¦, ğ‘“ğ‘š), (ğ‘’1, â€¦, ğ‘’ğ‘›)) is the conjugate transpose of â„³(ğ‘‡, (ğ‘’1, â€¦, ğ‘’ğ‘›), ( ğ‘“1, â€¦, ğ‘“ğ‘š)). In other words, â„³(ğ‘‡âˆ—)= (â„³(ğ‘‡)) âˆ— . Proof In this proof, we will write â„³(ğ‘‡) and â„³(ğ‘‡âˆ—)instead of the longer expressions â„³(ğ‘‡, (ğ‘’1, â€¦, ğ‘’ğ‘›), ( ğ‘“1, â€¦, ğ‘“ğ‘š))and â„³(ğ‘‡âˆ—, ( ğ‘“1, â€¦, ğ‘“ğ‘š), (ğ‘’1, â€¦, ğ‘’ğ‘›)). Recall that we obtain the ğ‘˜th column of â„³(ğ‘‡) by writing ğ‘‡ğ‘’ğ‘˜ as a linear combination of the ğ‘“ğ‘—â€™s; the scalars used in this linear combination then become the ğ‘˜th column of â„³(ğ‘‡). Because ğ‘“1, â€¦, ğ‘“ğ‘š is an orthonormal basis of ğ‘Š, we know how to write ğ‘‡ğ‘’ğ‘˜ as a linear combination of the ğ‘“ğ‘—â€™s [see 6.30(a)]: ğ‘‡ğ‘’ğ‘˜ = âŸ¨ğ‘‡ğ‘’ğ‘˜, ğ‘“1âŸ© ğ‘“1 + â‹¯ + âŸ¨ğ‘‡ğ‘’ğ‘˜, ğ‘“ğ‘šâŸ© ğ‘“ğ‘š. Thus the entry in row ğ‘—, column ğ‘˜, of â„³(ğ‘‡) is âŸ¨ğ‘‡ğ‘’ğ‘˜, ğ‘“ğ‘—âŸ©. In the statement above, replace ğ‘‡ with ğ‘‡âˆ— and interchange ğ‘’1, â€¦, ğ‘’ğ‘› and ğ‘“1, â€¦, ğ‘“ğ‘š. This shows that the entry in row ğ‘—, column ğ‘˜, of â„³(ğ‘‡âˆ—)is âŸ¨ğ‘‡âˆ—ğ‘“ğ‘˜, ğ‘’ğ‘—âŸ©, which equals âŸ¨ ğ‘“ğ‘˜, ğ‘‡ğ‘’ğ‘—âŸ©, which equals âŸ¨ğ‘‡ğ‘’ğ‘—, ğ‘“ğ‘˜âŸ©, which equals the complex conjugate of the entry in row ğ‘˜, column ğ‘—, of â„³(ğ‘‡). Thus â„³(ğ‘‡âˆ—)= (â„³(ğ‘‡)) âˆ—. The Riesz representation theorem as stated in 6.58 provides an identification of ğ‘‰ with its dual space ğ‘‰â€² defined in3.110. Under this identification, the orthogonal complement ğ‘ˆâŸ‚ of a subset ğ‘ˆ âŠ† ğ‘‰corresponds to the annihilator ğ‘ˆ0 of ğ‘ˆ. If ğ‘ˆ is a subspace of ğ‘‰, then the formulas for the dimensions of ğ‘ˆâŸ‚ and ğ‘ˆ0 become identical under this identificationâ€”see3.125 and 6.51. Because orthogonal complements and adjoints are easier to deal with than annihilators and dual maps, there is no need to work with annihilators and dual maps in the context of inner product spaces. Suppose ğ‘‡âˆ¶ ğ‘‰ â†’ ğ‘Š is a linear map. Under the identification ofğ‘‰ with ğ‘‰â€² and the identification ofğ‘Š with ğ‘Šâ€², the ad- joint map ğ‘‡âˆ— âˆ¶ ğ‘Š â†’ ğ‘‰ corresponds to the dual map ğ‘‡â€² âˆ¶ ğ‘Šâ€² â†’ ğ‘‰â€² defined in 3.118, as Exercise 32 asks you to verify. Under this identification, the formulas for null ğ‘‡âˆ— and range ğ‘‡âˆ— [7.6(a) and (b)]then become identical to the formulas for null ğ‘‡â€² and range ğ‘‡â€² [3.128(a) and 3.130(b)]. Furthermore, the theorem about the matrix of ğ‘‡âˆ— (7.9) is analogous to the theorem about the matrix of ğ‘‡â€² (3.132). Section 7A Self-Adjoint and Normal Operators 233 Self-Adjoint Operators Now we switch our attention to operators on inner product spaces. Instead of considering linear maps from ğ‘‰ to ğ‘Š, we will focus on linear maps from ğ‘‰ to ğ‘‰; recall that such linear maps are called operators. 7.10 definition:self-adjoint An operator ğ‘‡ âˆˆ â„’(ğ‘‰) is called self-adjoint if ğ‘‡ = ğ‘‡âˆ—. If ğ‘‡ âˆˆ â„’(ğ‘‰) and ğ‘’1, â€¦, ğ‘’ğ‘› is an orthonormal basis of ğ‘‰, then ğ‘‡ is self-adjoint if and only if â„³(ğ‘‡, (ğ‘’1, â€¦, ğ‘’ğ‘›))= â„³(ğ‘‡, (ğ‘’1, â€¦, ğ‘’ğ‘›)) âˆ—, as follows from 7.9. 7.11 example:determining whether ğ‘‡ is self-adjoint from its matrix Suppose ğ‘ âˆˆ ğ… and ğ‘‡ is the operator on ğ…2 whose matrix (with respect to the standard basis) is â„³(ğ‘‡) = ( 2 ğ‘ 3 7 ). The matrix of ğ‘‡âˆ— (with respect to the standard basis) is â„³(ğ‘‡âˆ—)= ( 2 3 ğ‘ 7). Thus â„³(ğ‘‡) = â„³(ğ‘‡âˆ—)if and only if ğ‘ = 3. Hence the operator ğ‘‡ is self-adjoint if and only if ğ‘ = 3. A good analogy to keep in mind is that the adjoint on â„’(ğ‘‰) plays a role similar to that of the complex conjugate on ğ‚. A complex number ğ‘§ is real if and only if ğ‘§ = ğ‘§; thus a self-adjoint operator (ğ‘‡ = ğ‘‡âˆ—) is analogous to a real number. An operator ğ‘‡ âˆˆ â„’(ğ‘‰) is self-adjoint if and only if âŸ¨ğ‘‡ğ‘£, ğ‘¤âŸ© = âŸ¨ğ‘£, ğ‘‡ğ‘¤âŸ© for all ğ‘£, ğ‘¤ âˆˆ ğ‘‰. We will see that the analogy discussed above is reflected in some important prop- erties of self-adjoint operators, beginning with eigenvalues in the next result. If ğ… = ğ‘, then by definition every eigenvalue is real, so the next result is interesting only when ğ… = ğ‚. 7.12 eigenvalues of self-adjoint operators Every eigenvalue of a self-adjoint operator is real. Proof Suppose ğ‘‡ is a self-adjoint operator on ğ‘‰. Let ğœ† be an eigenvalue of ğ‘‡, and let ğ‘£ be a nonzero vector in ğ‘‰ such that ğ‘‡ğ‘£ = ğœ†ğ‘£. Then ğœ†â€–ğ‘£â€– 2 = âŸ¨ğœ†ğ‘£, ğ‘£âŸ© = âŸ¨ğ‘‡ğ‘£, ğ‘£âŸ© = âŸ¨ğ‘£, ğ‘‡ğ‘£âŸ© = âŸ¨ğ‘£, ğœ†ğ‘£âŸ© = ğœ†â€–ğ‘£â€– 2. Thus ğœ† = ğœ†, which means that ğœ† is real, as desired. 234 Chapter 7 Operators on Inner Product Spaces The next result is false for real inner product spaces. As an example, consider the operator ğ‘‡ âˆˆ â„’(ğ‘2)that is a counterclockwise rotation of 90 âˆ˜ around the origin; thus ğ‘‡(ğ‘¥, ğ‘¦) = (âˆ’ğ‘¦, ğ‘¥). Notice that ğ‘‡ğ‘£ is orthogonal to ğ‘£ for every ğ‘£ âˆˆ ğ‘2, even though ğ‘‡ â‰  0. 7.13 ğ‘‡ğ‘£ is orthogonal to ğ‘£ for all ğ‘£ âŸº ğ‘‡ = 0(assuming ğ… = ğ‚) Suppose ğ‘‰ is a complex inner product space and ğ‘‡ âˆˆ â„’(ğ‘‰). Then âŸ¨ğ‘‡ğ‘£, ğ‘£âŸ© = 0for every ğ‘£ âˆˆ ğ‘‰ âŸº ğ‘‡ = 0. Proof If ğ‘¢, ğ‘¤ âˆˆ ğ‘‰, then âŸ¨ğ‘‡ğ‘¢, ğ‘¤âŸ© = âŸ¨ğ‘‡(ğ‘¢ + ğ‘¤), ğ‘¢ + ğ‘¤âŸ©âˆ’ âŸ¨ğ‘‡(ğ‘¢ âˆ’ ğ‘¤), ğ‘¢ âˆ’ ğ‘¤âŸ© 4 + âŸ¨ğ‘‡(ğ‘¢ + ğ‘–ğ‘¤), ğ‘¢ + ğ‘–ğ‘¤âŸ©âˆ’ âŸ¨ğ‘‡(ğ‘¢ âˆ’ ğ‘–ğ‘¤), ğ‘¢ âˆ’ ğ‘–ğ‘¤âŸ© 4 ğ‘–, as can be verified by computing the right side. Note that each term on the right side is of the form âŸ¨ğ‘‡ğ‘£, ğ‘£âŸ©for appropriate ğ‘£ âˆˆ ğ‘‰. Now suppose âŸ¨ğ‘‡ğ‘£, ğ‘£âŸ© = 0for every ğ‘£ âˆˆ ğ‘‰. Then the equation above implies that âŸ¨ğ‘‡ğ‘¢, ğ‘¤âŸ© = 0for all ğ‘¢, ğ‘¤ âˆˆ ğ‘‰, which then implies that ğ‘‡ğ‘¢ = 0for every ğ‘¢ âˆˆ ğ‘ˆ (take ğ‘¤ = ğ‘‡ğ‘¢). Hence ğ‘‡ = 0, as desired. The next result provides another good example of how self-adjoint operators behave like real numbers. The next result is false for real inner product spaces, as shown by considering any operator on a real inner product space that is not self-adjoint. 7.14 âŸ¨ğ‘‡ğ‘£, ğ‘£âŸ©is real for all ğ‘£ âŸº ğ‘‡ is self-adjoint (assuming ğ… = ğ‚) Suppose ğ‘‰ is a complex inner product space and ğ‘‡ âˆˆ â„’(ğ‘‰). Then ğ‘‡ is self-adjoint âŸº âŸ¨ğ‘‡ğ‘£, ğ‘£âŸ© âˆˆ ğ‘for every ğ‘£ âˆˆ ğ‘‰. Proof If ğ‘£ âˆˆ ğ‘‰, then 7.15 âŸ¨ğ‘‡âˆ—ğ‘£, ğ‘£âŸ©= âŸ¨ğ‘£, ğ‘‡âˆ—ğ‘£âŸ©= âŸ¨ğ‘‡ğ‘£, ğ‘£âŸ©. Now ğ‘‡ is self-adjoint âŸº ğ‘‡ âˆ’ ğ‘‡âˆ— = 0 âŸº âŸ¨(ğ‘‡ âˆ’ ğ‘‡âˆ—)ğ‘£, ğ‘£âŸ©= 0for every ğ‘£ âˆˆ ğ‘‰ âŸº âŸ¨ğ‘‡ğ‘£, ğ‘£âŸ© âˆ’ âŸ¨ğ‘‡ğ‘£, ğ‘£âŸ©= 0for every ğ‘£ âˆˆ ğ‘‰ âŸº âŸ¨ğ‘‡ğ‘£, ğ‘£âŸ© âˆˆ ğ‘for every ğ‘£ âˆˆ ğ‘‰, where the second equivalence follows from 7.13 as applied to ğ‘‡ âˆ’ ğ‘‡âˆ— and the third equivalence follows from 7.15. Section 7A Self-Adjoint and Normal Operators 235 On a real inner product space ğ‘‰, a nonzero operator ğ‘‡ might satisfy âŸ¨ğ‘‡ğ‘£, ğ‘£âŸ© = 0 for all ğ‘£ âˆˆ ğ‘‰. However, the next result shows that this cannot happen for a self- adjoint operator. 7.16 ğ‘‡ self-adjoint and âŸ¨ğ‘‡ğ‘£, ğ‘£âŸ© = 0for all ğ‘£ âŸº ğ‘‡ = 0 Suppose ğ‘‡ is a self-adjoint operator on ğ‘‰. Then âŸ¨ğ‘‡ğ‘£, ğ‘£âŸ© = 0for every ğ‘£ âˆˆ ğ‘‰ âŸº ğ‘‡ = 0. Proof We have already proved this (without the hypothesis that ğ‘‡ is self-adjoint) when ğ‘‰ is a complex inner product space (see 7.13). Thus we can assume that ğ‘‰ is a real inner product space. If ğ‘¢, ğ‘¤ âˆˆ ğ‘‰, then 7.17 âŸ¨ğ‘‡ğ‘¢, ğ‘¤âŸ© = âŸ¨ğ‘‡(ğ‘¢ + ğ‘¤), ğ‘¢ + ğ‘¤âŸ©âˆ’ âŸ¨ğ‘‡(ğ‘¢ âˆ’ ğ‘¤), ğ‘¢ âˆ’ ğ‘¤âŸ© 4 , as can be proved by computing the right side using the equation âŸ¨ğ‘‡ğ‘¤, ğ‘¢âŸ© = âŸ¨ğ‘¤, ğ‘‡ğ‘¢âŸ© = âŸ¨ğ‘‡ğ‘¢, ğ‘¤âŸ©, where the first equality holds becauseğ‘‡ is self-adjoint and the second equality holds because we are working in a real inner product space. Now suppose âŸ¨ğ‘‡ğ‘£, ğ‘£âŸ© = 0for every ğ‘£ âˆˆ ğ‘‰. Because each term on the right side of 7.17 is of the form âŸ¨ğ‘‡ğ‘£, ğ‘£âŸ©for appropriate ğ‘£, this implies that âŸ¨ğ‘‡ğ‘¢, ğ‘¤âŸ© = 0 for all ğ‘¢, ğ‘¤ âˆˆ ğ‘‰. This implies that ğ‘‡ğ‘¢ = 0for every ğ‘¢ âˆˆ ğ‘‰ (take ğ‘¤ = ğ‘‡ğ‘¢). Hence ğ‘‡ = 0, as desired. Normal Operators 7.18 definition:normal â€¢ An operator on an inner product space is called normal if it commutes with its adjoint. â€¢ In other words, ğ‘‡ âˆˆ â„’(ğ‘‰) is normal if ğ‘‡ğ‘‡âˆ— = ğ‘‡âˆ—ğ‘‡. Every self-adjoint operator is normal, because if ğ‘‡ is self-adjoint then ğ‘‡âˆ— = ğ‘‡ and hence ğ‘‡ commutes with ğ‘‡âˆ—. 7.19 example:an operator that is normal but not self-adjoint Let ğ‘‡ be the operator on ğ…2 whose matrix (with respect to the standard basis) is ( 2 âˆ’3 3 2 ). Thus ğ‘‡(ğ‘¤, ğ‘§) = (2ğ‘¤ âˆ’ 3ğ‘§, 3ğ‘¤+ 2ğ‘§). 236 Chapter 7 Operators on Inner Product Spaces This operator ğ‘‡ is not self-adjoint because the entry in row 2, column 1 (which equals 3) does not equal the complex conjugate of the entry in row 1, column 2 (which equals âˆ’3). The matrix of ğ‘‡ğ‘‡âˆ— equals ( 2 âˆ’3 3 2 )( 2 3 âˆ’3 2 ), which equals ( 13 0 0 13 ). Similarly, the matrix of ğ‘‡âˆ—ğ‘‡ equals ( 2 3 âˆ’3 2 )( 2 âˆ’3 3 2 ), which equals ( 13 0 0 13 ). Because ğ‘‡ğ‘‡âˆ— and ğ‘‡âˆ—ğ‘‡ have the same matrix, we see that ğ‘‡ğ‘‡âˆ— = ğ‘‡âˆ—ğ‘‡. Thus ğ‘‡ is normal. In the next section we will see why normal operators are worthy of special attention. The next result provides a useful characterization of normal operators. 7.20 ğ‘‡ is normal if and only if ğ‘‡ğ‘£ and ğ‘‡âˆ—ğ‘£ have the same norm Suppose ğ‘‡ âˆˆ â„’(ğ‘‰). Then ğ‘‡ is normal âŸº â€–ğ‘‡ğ‘£â€– = â€–ğ‘‡âˆ—ğ‘£â€– for every ğ‘£ âˆˆ ğ‘‰. Proof We have ğ‘‡ is normal âŸº ğ‘‡âˆ—ğ‘‡ âˆ’ ğ‘‡ğ‘‡âˆ— = 0 âŸº âŸ¨(ğ‘‡âˆ—ğ‘‡ âˆ’ ğ‘‡ğ‘‡âˆ—)ğ‘£, ğ‘£âŸ©= 0for every ğ‘£ âˆˆ ğ‘‰ âŸº âŸ¨ğ‘‡âˆ—ğ‘‡ğ‘£, ğ‘£âŸ©= âŸ¨ğ‘‡ğ‘‡âˆ—ğ‘£, ğ‘£âŸ©for every ğ‘£ âˆˆ ğ‘‰ âŸº âŸ¨ğ‘‡ğ‘£, ğ‘‡ğ‘£âŸ© =âŸ¨ğ‘‡âˆ—ğ‘£, ğ‘‡âˆ—ğ‘£âŸ©for every ğ‘£ âˆˆ ğ‘‰ âŸº â€–ğ‘‡ğ‘£â€–2 = âˆ¥ğ‘‡âˆ—ğ‘£âˆ¥ 2 for every ğ‘£ âˆˆ ğ‘‰ âŸº â€–ğ‘‡ğ‘£â€– = âˆ¥ğ‘‡âˆ—ğ‘£âˆ¥ for every ğ‘£ âˆˆ ğ‘‰, where we used 7.16 to establish the second equivalence (note that the operator ğ‘‡âˆ—ğ‘‡ âˆ’ ğ‘‡ğ‘‡âˆ— is self-adjoint). The next result presents several consequences of the result above. Compare (e) of the next result to Exercise 3. That exercise states that the eigenvalues of the adjoint of each operator are equal (as a set) to the complex conjugates of the eigenvalues of the operator. The exercise says nothing about eigenvectors, because an operator and its adjoint may have different eigenvectors. However, (e) of the next result implies that a normal operator and its adjoint have the same eigenvectors. Section 7A Self-Adjoint and Normal Operators 237 7.21 range, null space, and eigenvectors of a normal operator Suppose ğ‘‡ âˆˆ â„’(ğ‘‰) is normal. Then (a) null ğ‘‡ = null ğ‘‡âˆ—; (b) range ğ‘‡ = range ğ‘‡âˆ—; (c) ğ‘‰ = null ğ‘‡ âŠ• range ğ‘‡; (d) ğ‘‡ âˆ’ ğœ†ğ¼ is normal for every ğœ† âˆˆ ğ…; (e) if ğ‘£ âˆˆ ğ‘‰ and ğœ† âˆˆ ğ…, then ğ‘‡ğ‘£ = ğœ†ğ‘£ if and only if ğ‘‡âˆ—ğ‘£ = ğœ†ğ‘£. Proof (a) Suppose ğ‘£ âˆˆ ğ‘‰. Then ğ‘£ âˆˆ null ğ‘‡ âŸº â€–ğ‘‡ğ‘£â€– = 0 âŸºâˆ¥ğ‘‡âˆ—ğ‘£âˆ¥ = 0 âŸº ğ‘£ âˆˆnull ğ‘‡âˆ—, where the middle equivalence above follows from 7.20. Thus null ğ‘‡ = null ğ‘‡âˆ—. (b) We have range ğ‘‡ = (null ğ‘‡âˆ—) âŸ‚ = (null ğ‘‡) âŸ‚ = range ğ‘‡âˆ—, where the first equality comes from7.6(d), the second equality comes from (a) in this result, and the third equality comes from 7.6(b). (c) We have ğ‘‰ = (null ğ‘‡) âŠ• (null ğ‘‡) âŸ‚ = null ğ‘‡ âŠ• range ğ‘‡âˆ— = null ğ‘‡ âŠ• range ğ‘‡, where the first equality comes from6.49, the second equality comes from 7.6(b), and the third equality comes from (b) in this result. (d) Suppose ğœ† âˆˆ ğ…. Then (ğ‘‡ âˆ’ ğœ†ğ¼)(ğ‘‡ âˆ’ ğœ†ğ¼)âˆ— = (ğ‘‡ âˆ’ ğœ†ğ¼)(ğ‘‡âˆ— âˆ’ ğœ†ğ¼) = ğ‘‡ğ‘‡âˆ— âˆ’ ğœ†ğ‘‡ âˆ’ ğœ†ğ‘‡âˆ— + |ğœ†|2ğ¼ = ğ‘‡âˆ—ğ‘‡ âˆ’ ğœ†ğ‘‡ âˆ’ ğœ†ğ‘‡âˆ— + |ğœ†|2ğ¼ = (ğ‘‡âˆ— âˆ’ ğœ†ğ¼)(ğ‘‡ âˆ’ ğœ†ğ¼) = (ğ‘‡ âˆ’ ğœ†ğ¼)âˆ—(ğ‘‡ âˆ’ ğœ†ğ¼). Thus ğ‘‡ âˆ’ ğœ†ğ¼ commutes with its adjoint. Hence ğ‘‡ âˆ’ ğœ†ğ¼ is normal. (e) Suppose ğ‘£ âˆˆ ğ‘‰ and ğœ† âˆˆ ğ…. Then (d) and 7.20 imply that â€–(ğ‘‡ âˆ’ ğœ†ğ¼)ğ‘£â€– = âˆ¥(ğ‘‡ âˆ’ ğœ†ğ¼)âˆ—ğ‘£âˆ¥ = âˆ¥(ğ‘‡âˆ— âˆ’ ğœ†ğ¼)ğ‘£âˆ¥. Thus â€–(ğ‘‡ âˆ’ ğœ†ğ¼)ğ‘£â€– = 0if and only if âˆ¥(ğ‘‡âˆ— âˆ’ ğœ†ğ¼)ğ‘£âˆ¥ = 0. Hence ğ‘‡ğ‘£ = ğœ†ğ‘£ if and only if ğ‘‡âˆ—ğ‘£ = ğœ†ğ‘£. 238 Chapter 7 Operators on Inner Product Spaces Because every self-adjoint operator is normal, the next result applies in partic- ular to self-adjoint operators. 7.22 orthogonal eigenvectors for normal operators Suppose ğ‘‡ âˆˆ â„’(ğ‘‰) is normal. Then eigenvectors of ğ‘‡ corresponding to distinct eigenvalues are orthogonal. Proof Suppose ğ›¼, ğ›½ are distinct eigenvalues of ğ‘‡, with corresponding eigen- vectors ğ‘¢, ğ‘£. Thus ğ‘‡ğ‘¢ = ğ›¼ğ‘¢ and ğ‘‡ğ‘£ = ğ›½ğ‘£. From 7.21(e) we have ğ‘‡âˆ—ğ‘£ = ğ›½ğ‘£. Thus (ğ›¼ âˆ’ ğ›½)âŸ¨ğ‘¢, ğ‘£âŸ© = âŸ¨ğ›¼ğ‘¢, ğ‘£âŸ© âˆ’âŸ¨ğ‘¢, ğ›½ğ‘£âŸ© = âŸ¨ğ‘‡ğ‘¢, ğ‘£âŸ© âˆ’âŸ¨ğ‘¢, ğ‘‡âˆ—ğ‘£âŸ© = 0. Because ğ›¼ â‰  ğ›½, the equation above implies that âŸ¨ğ‘¢, ğ‘£âŸ© = 0. Thus ğ‘¢ and ğ‘£ are orthogonal, as desired. As stated here, the next result makes sense only when ğ… = ğ‚. However, see Exercise 12 for a version that makes sense when ğ… = ğ‚ and when ğ… = ğ‘. Suppose ğ… = ğ‚ and ğ‘‡ âˆˆ â„’(ğ‘‰). Under the analogy between â„’(ğ‘‰) and ğ‚, with the adjoint on â„’(ğ‘‰) playing a similar role to that of the complex conjugate on ğ‚, the operators ğ´ and ğµ as defined by7.24 correspond to the real and imaginary parts of ğ‘‡. Thus the informal title of the result below should make sense. 7.23 ğ‘‡ is normal âŸº the real and imaginary parts of ğ‘‡ commute Suppose ğ… = ğ‚ and ğ‘‡ âˆˆ â„’(ğ‘‰). Then ğ‘‡ is normal if and only if there exist commuting self-adjoint operators ğ´ and ğµ such that ğ‘‡ = ğ´ + ğ‘–ğµ. Proof First suppose ğ‘‡ is normal. Let 7.24 ğ´ = ğ‘‡ + ğ‘‡âˆ— 2 and ğµ = ğ‘‡ âˆ’ ğ‘‡âˆ— 2ğ‘– . Then ğ´ and ğµ are self-adjoint and ğ‘‡ = ğ´ + ğ‘–ğµ. A quick computation shows that 7.25 ğ´ğµ âˆ’ ğµğ´ = ğ‘‡âˆ—ğ‘‡ âˆ’ ğ‘‡ğ‘‡âˆ— 2ğ‘– . Because ğ‘‡ is normal, the right side of the equation above equals 0. Thus the operators ğ´ and ğµ commute, as desired. To prove the implication in the other direction, now suppose there exist com- muting self-adjoint operators ğ´ and ğµ such that ğ‘‡ = ğ´ + ğ‘–ğµ. Then ğ‘‡âˆ— = ğ´ âˆ’ ğ‘–ğµ. Adding the last two equations and then dividing by 2produces the equation for ğ´ in 7.24. Subtracting the last two equations and then dividing by 2ğ‘–produces the equation for ğµ in 7.24. Now 7.24 implies 7.25. Because ğµ and ğ´ commute, 7.25 implies that ğ‘‡ is normal, as desired. Section 7A Self-Adjoint and Normal Operators 239 Exercises 7A 1 Suppose ğ‘› is a positive integer. Defineğ‘‡ âˆˆ â„’(ğ…ğ‘›)by ğ‘‡(ğ‘§1, â€¦, ğ‘§ğ‘›) = (0, ğ‘§1, â€¦, ğ‘§ğ‘› âˆ’ 1). Find a formula for ğ‘‡âˆ—(ğ‘§1, â€¦, ğ‘§ğ‘›). 2 Suppose ğ‘‡ âˆˆ â„’(ğ‘‰, ğ‘Š). Prove that ğ‘‡ = 0 âŸº ğ‘‡âˆ— = 0 âŸº ğ‘‡âˆ—ğ‘‡ = 0 âŸº ğ‘‡ğ‘‡âˆ— = 0. 3 Suppose ğ‘‡ âˆˆ â„’(ğ‘‰) and ğœ† âˆˆ ğ…. Prove that ğœ† is an eigenvalue of ğ‘‡ âŸº ğœ† is an eigenvalue of ğ‘‡âˆ—. 4 Suppose ğ‘‡ âˆˆ â„’(ğ‘‰) and ğ‘ˆ is a subspace of ğ‘‰. Prove that ğ‘ˆ is invariant under ğ‘‡ âŸº ğ‘ˆâŸ‚ is invariant under ğ‘‡âˆ—. 5 Suppose ğ‘‡ âˆˆ â„’(ğ‘‰, ğ‘Š). Suppose ğ‘’1, â€¦, ğ‘’ğ‘› is an orthonormal basis of ğ‘‰ and ğ‘“1, â€¦, ğ‘“ğ‘š is an orthonormal basis of ğ‘Š. Prove that â€–ğ‘‡ğ‘’1â€–2 + â‹¯ + â€–ğ‘‡ğ‘’ğ‘›â€–2 = âˆ¥ğ‘‡âˆ—ğ‘“1âˆ¥2 + â‹¯ + âˆ¥ğ‘‡âˆ—ğ‘“ğ‘šâˆ¥ 2 . The numbers â€–ğ‘‡ğ‘’1â€– 2, â€¦, â€–ğ‘‡ğ‘’ğ‘›â€–2 in the equation above depend on the ortho- normal basis ğ‘’1, â€¦, ğ‘’ğ‘›, but the right side of the equation does not depend on ğ‘’1, â€¦, ğ‘’ğ‘›. Thus the equation above shows that the sum on the left side does not depend on which orthonormal basis ğ‘’1, â€¦, ğ‘’ğ‘› is used. 6 Suppose ğ‘‡ âˆˆ â„’(ğ‘‰, ğ‘Š). Prove that (a) ğ‘‡ is injective âŸº ğ‘‡âˆ— is surjective; (b) ğ‘‡ is surjective âŸº ğ‘‡âˆ— is injective. 7 Prove that if ğ‘‡ âˆˆ â„’(ğ‘‰, ğ‘Š), then (a) dim null ğ‘‡âˆ— = dim null ğ‘‡ + dim ğ‘Š âˆ’ dim ğ‘‰; (b) dim range ğ‘‡âˆ— = dim range ğ‘‡. 8 Suppose ğ´ is an ğ‘š-by-ğ‘› matrix with entries in ğ…. Use (b) in Exercise 7 to prove that the row rank of ğ´ equals the column rank of ğ´. This exercise asks for yet another alternative proof of a result that was previously proved in 3.57 and 3.133. 9 Prove that the product of two self-adjoint operators on ğ‘‰ is self-adjoint if and only if the two operators commute. 10 Suppose ğ… = ğ‚ and ğ‘‡ âˆˆ â„’(ğ‘‰). Prove that ğ‘‡ is self-adjoint if and only if âŸ¨ğ‘‡ğ‘£, ğ‘£âŸ© =âŸ¨ğ‘‡âˆ—ğ‘£, ğ‘£âŸ© for all ğ‘£ âˆˆ ğ‘‰. 240 Chapter 7 Operators on Inner Product Spaces 11 Define an operatorğ‘†âˆ¶ ğ…2 â†’ ğ…2 by ğ‘†(ğ‘¤, ğ‘§) = (âˆ’ğ‘§, ğ‘¤). (a) Find a formula for ğ‘†âˆ—. (b) Show that ğ‘† is normal but not self-adjoint. (c) Find all eigenvalues of ğ‘†. If ğ… = ğ‘, then ğ‘† is the operator on ğ‘2 of counterclockwise rotation by 90 âˆ˜. 12 An operator ğµ âˆˆ â„’(ğ‘‰) is called skew if ğµâˆ— = âˆ’ğµ. Suppose that ğ‘‡ âˆˆ â„’(ğ‘‰). Prove that ğ‘‡ is normal if and only if there exist commuting operators ğ´ and ğµ such that ğ´ is self-adjoint, ğµ is a skew operator, and ğ‘‡ = ğ´ + ğµ. 13 Suppose ğ… = ğ‘. Defineğ’œ âˆˆ â„’(â„’(ğ‘‰))by ğ’œğ‘‡ = ğ‘‡âˆ— for all ğ‘‡ âˆˆ â„’(ğ‘‰). (a) Find all eigenvalues of ğ’œ. (b) Find the minimal polynomial of ğ’œ. 14 Define an inner product onğ’«2(ğ‘) by âŸ¨ğ‘, ğ‘âŸ© =âˆ« 1 0 ğ‘ğ‘. Define an operator ğ‘‡ âˆˆ â„’(ğ’«2(ğ‘))by ğ‘‡(ğ‘ğ‘¥2 + ğ‘ğ‘¥ + ğ‘)= ğ‘ğ‘¥. (a) Show that with this inner product, the operator ğ‘‡ is not self-adjoint. (b) The matrix of ğ‘‡ with respect to the basis 1, ğ‘¥, ğ‘¥2 is â›âœâœâœ â 0 0 0 0 1 0 0 0 0 ââŸâŸâŸ â  . This matrix equals its conjugate transpose, even though ğ‘‡ is not self- adjoint. Explain why this is not a contradiction. 15 Suppose ğ‘‡ âˆˆ â„’(ğ‘‰) is invertible. Prove that (a) ğ‘‡ is self-adjoint âŸº ğ‘‡âˆ’1 is self-adjoint; (b) ğ‘‡ is normal âŸº ğ‘‡âˆ’1 is normal. 16 Suppose ğ… = ğ‘. (a) Show that the set of self-adjoint operators on ğ‘‰ is a subspace of â„’(ğ‘‰). (b) What is the dimension of the subspace of â„’(ğ‘‰) in (a) [in terms of dim ğ‘‰]? 17 Suppose ğ… = ğ‚. Show that the set of self-adjoint operators on ğ‘‰ is not a subspace of â„’(ğ‘‰). 18 Suppose dim ğ‘‰ â‰¥ 2. Show that the set of normal operators on ğ‘‰ is not a subspace of â„’(ğ‘‰). Section 7A Self-Adjoint and Normal Operators 241 19 Suppose ğ‘‡ âˆˆ â„’(ğ‘‰) and âˆ¥ğ‘‡âˆ—ğ‘£âˆ¥ â‰¤ â€–ğ‘‡ğ‘£â€–for every ğ‘£ âˆˆ ğ‘‰. Prove that ğ‘‡ is normal. This exercise fails on infinite-dimensional inner product spaces, leading to what are called hyponormal operators, which have a well-developed theory. 20 Suppose ğ‘ƒ âˆˆ â„’(ğ‘‰) is such that ğ‘ƒ2 = ğ‘ƒ. Prove that the following are equivalent. (a) ğ‘ƒ is self-adjoint. (b) ğ‘ƒ is normal. (c) There is a subspace ğ‘ˆ of ğ‘‰ such that ğ‘ƒ = ğ‘ƒğ‘ˆ. 21 Suppose ğ·âˆ¶ ğ’«8(ğ‘) â†’ ğ’«8(ğ‘) is the differentiation operator defined by ğ·ğ‘ = ğ‘â€². Prove that there does not exist an inner product on ğ’«8(ğ‘) that makes ğ· a normal operator. 22 Give an example of an operator ğ‘‡ âˆˆ â„’(ğ‘3)such that ğ‘‡ is normal but not self-adjoint. 23 Suppose ğ‘‡ is a normal operator on ğ‘‰. Suppose also that ğ‘£, ğ‘¤ âˆˆ ğ‘‰ satisfy the equations â€–ğ‘£â€– = â€–ğ‘¤â€– = 2, ğ‘‡ğ‘£ = 3ğ‘£, ğ‘‡ğ‘¤ = 4ğ‘¤. Show that â€–ğ‘‡(ğ‘£ + ğ‘¤)â€– = 10. 24 Suppose ğ‘‡ âˆˆ â„’(ğ‘‰) and ğ‘0 + ğ‘1ğ‘§ + ğ‘2ğ‘§2 + â‹¯ + ğ‘ğ‘š âˆ’ 1ğ‘§ ğ‘š âˆ’ 1 + ğ‘§ ğ‘š is the minimal polynomial of ğ‘‡. Prove that the minimal polynomial of ğ‘‡âˆ— is ğ‘0 + ğ‘1 ğ‘§ + ğ‘2 ğ‘§ 2 + â‹¯ + ğ‘ğ‘š âˆ’ 1 ğ‘§ ğ‘š âˆ’ 1 + ğ‘§ ğ‘š. This exercise shows that the minimal polynomial of ğ‘‡âˆ— equals the minimal polynomial of ğ‘‡ if ğ… = ğ‘. 25 Suppose ğ‘‡ âˆˆ â„’(ğ‘‰). Prove that ğ‘‡ is diagonalizable if and only if ğ‘‡âˆ— is diagonalizable. 26 Fix ğ‘¢, ğ‘¥ âˆˆ ğ‘‰. Defineğ‘‡ âˆˆ â„’(ğ‘‰) by ğ‘‡ğ‘£ = âŸ¨ğ‘£, ğ‘¢âŸ©ğ‘¥for every ğ‘£ âˆˆ ğ‘‰. (a) Prove that if ğ‘‰ is a real vector space, then ğ‘‡ is self-adjoint if and only if the list ğ‘¢, ğ‘¥ is linearly dependent. (b) Prove that ğ‘‡ is normal if and only if the list ğ‘¢, ğ‘¥ is linearly dependent. 27 Suppose ğ‘‡ âˆˆ â„’(ğ‘‰) is normal. Prove that null ğ‘‡ğ‘˜ = null ğ‘‡ and range ğ‘‡ğ‘˜ = range ğ‘‡ for every positive integer ğ‘˜. 28 Suppose ğ‘‡ âˆˆ â„’(ğ‘‰) is normal. Prove that if ğœ† âˆˆ ğ…, then the minimal polynomial of ğ‘‡ is not a polynomial multiple of (ğ‘¥ âˆ’ ğœ†)2. 242 Chapter 7 Operators on Inner Product Spaces 29 Prove or give a counterexample: If ğ‘‡ âˆˆ â„’(ğ‘‰) and there is an orthonormal basis ğ‘’1, â€¦, ğ‘’ğ‘› of ğ‘‰ such that â€–ğ‘‡ğ‘’ğ‘˜â€– = âˆ¥ğ‘‡âˆ—ğ‘’ğ‘˜âˆ¥ for each ğ‘˜ = 1, â€¦, ğ‘›, then ğ‘‡ is normal. 30 Suppose that ğ‘‡ âˆˆ â„’(ğ…3)is normal and ğ‘‡(1, 1, 1) = (2, 2, 2). Suppose (ğ‘§1, ğ‘§2, ğ‘§3) âˆˆ null ğ‘‡. Prove that ğ‘§1 + ğ‘§2 + ğ‘§3 = 0. 31 Fix a positive integer ğ‘›. In the inner product space of continuous real-valued functions on [âˆ’ğœ‹, ğœ‹] with inner product âŸ¨ ğ‘“, ğ‘”âŸ© =âˆ« ğœ‹ âˆ’ğœ‹ ğ‘“ ğ‘”, let ğ‘‰ = span(1, cos ğ‘¥, cos 2ğ‘¥, â€¦, cos ğ‘›ğ‘¥, sin ğ‘¥, sin 2ğ‘¥, â€¦, sin ğ‘›ğ‘¥). (a) Defineğ· âˆˆ â„’(ğ‘‰) by ğ· ğ‘“ = ğ‘“ â€². Show that ğ·âˆ— = âˆ’ğ·. Conclude that ğ· is normal but not self-adjoint. (b) Defineğ‘‡ âˆˆ â„’(ğ‘‰) by ğ‘‡ ğ‘“ = ğ‘“ â€³. Show that ğ‘‡ is self-adjoint. 32 Suppose ğ‘‡âˆ¶ ğ‘‰ â†’ ğ‘Š is a linear map. Show that under the standard identifica- tion of ğ‘‰ with ğ‘‰â€² (see 6.58) and the corresponding identification ofğ‘Š with ğ‘Šâ€², the adjoint map ğ‘‡âˆ— âˆ¶ ğ‘Š â†’ ğ‘‰ corresponds to the dual map ğ‘‡â€² âˆ¶ ğ‘Šâ€² â†’ ğ‘‰â€². More precisely, show that ğ‘‡â€²(ğœ‘ğ‘¤) = ğœ‘ğ‘‡âˆ—ğ‘¤ for all ğ‘¤ âˆˆ ğ‘Š, where ğœ‘ğ‘¤ and ğœ‘ğ‘‡âˆ—ğ‘¤ are defined as in6.58. Open Access This chapter is licensed under the terms of the Creative Commons Attribution-NonCommercial 4.0 International License (https://creativecommons.org/licenses/by-nc/4.0), which permits any noncommercial use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to original author and source, provide a link to the Creative Commons license, and indicate if changes were made. The images or other third party material in this chapter are included in the chapterâ€™s Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the chapterâ€™s Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. Section 7B Spectral Theorem 243 7B Spectral Theorem Recall that a diagonal matrix is a square matrix that is 0everywhere except possibly on the diagonal. Recall that an operator on ğ‘‰ is called diagonalizable if the operator has a diagonal matrix with respect to some basis of ğ‘‰. Recall also that this happens if and only if there is a basis of ğ‘‰ consisting of eigenvectors of the operator (see 5.55). The nicest operators on ğ‘‰ are those for which there is an orthonormal basis of ğ‘‰ with respect to which the operator has a diagonal matrix. These are precisely the operators ğ‘‡ âˆˆ â„’(ğ‘‰) such that there is an orthonormal basis of ğ‘‰ consisting of eigenvectors of ğ‘‡. Our goal in this section is to prove the spectral theorem, which characterizes these operators as the self-adjoint operators when ğ… = ğ‘ and as the normal operators when ğ… = ğ‚. The spectral theorem is probably the most useful tool in the study of operators on inner product spaces. Its extension to certain infinite-dimensional inner product spaces (see, for example, Section 10D of the authorâ€™s book Measure, Integration & Real Analysis) plays a key role in functional analysis. Because the conclusion of the spectral theorem depends on ğ…, we will break the spectral theorem into two pieces, called the real spectral theorem and the complex spectral theorem. Real Spectral Theorem To prove the real spectral theorem, we will need two preliminary results. These preliminary results hold on both real and complex inner product spaces, but they are not needed for the proof of the complex spectral theorem. This completing-the-square technique can be used to derive the quadratic formula. You could guess that the next result is true and even discover its proof by think- ing about quadratic polynomials with real coefficients. Specifically, suppose ğ‘, ğ‘ âˆˆ ğ‘ and ğ‘ 2 < 4ğ‘. Let ğ‘¥ be a real number. Then ğ‘¥2 + ğ‘ğ‘¥ + ğ‘ = (ğ‘¥ + ğ‘ 2 ) 2 + (ğ‘ âˆ’ ğ‘2 4 )> 0. In particular, ğ‘¥2 + ğ‘ğ‘¥ + ğ‘ is an invertible real number (a convoluted way of saying that it is not 0). Replacing the real number ğ‘¥ with a self-adjoint operator (recall the analogy between real numbers and self-adjoint operators) leads to the next result. 7.26 invertible quadratic expressions Suppose ğ‘‡ âˆˆ â„’(ğ‘‰) is self-adjoint and ğ‘, ğ‘ âˆˆ ğ‘ are such that ğ‘ 2 < 4ğ‘. Then ğ‘‡2 + ğ‘ğ‘‡ + ğ‘ğ¼ is an invertible operator. 244 Chapter 7 Operators on Inner Product Spaces Proof Let ğ‘£ be a nonzero vector in ğ‘‰. Then âŸ¨(ğ‘‡2 + ğ‘ğ‘‡ + ğ‘ğ¼)ğ‘£, ğ‘£âŸ©= âŸ¨ğ‘‡2ğ‘£, ğ‘£âŸ©+ ğ‘âŸ¨ğ‘‡ğ‘£, ğ‘£âŸ©+ ğ‘âŸ¨ğ‘£, ğ‘£âŸ© = âŸ¨ğ‘‡ğ‘£, ğ‘‡ğ‘£âŸ©+ ğ‘âŸ¨ğ‘‡ğ‘£, ğ‘£âŸ©+ ğ‘â€–ğ‘£â€– 2 â‰¥ â€–ğ‘‡ğ‘£â€– 2 âˆ’ |ğ‘| â€–ğ‘‡ğ‘£â€– â€–ğ‘£â€– + ğ‘â€–ğ‘£â€– 2 = (â€–ğ‘‡ğ‘£â€– âˆ’ |ğ‘| â€–ğ‘£â€– 2 ) 2 + (ğ‘ âˆ’ ğ‘ 2 4 )â€–ğ‘£â€– 2 > 0, where the third line above holds by the Cauchyâ€“Schwarz inequality (6.14). The last inequality implies that (ğ‘‡2 + ğ‘ğ‘‡ + ğ‘ğ¼)ğ‘£ â‰  0. Thus ğ‘‡2 + ğ‘ğ‘‡ + ğ‘ğ¼ is injective, which implies that it is invertible (see 3.65). The next result will be a key tool in our proof of the real spectral theorem. 7.27 minimal polynomial of self-adjoint operator Suppose ğ‘‡ âˆˆ â„’(ğ‘‰) is self-adjoint. Then the minimal polynomial of ğ‘‡ equals (ğ‘§ âˆ’ ğœ†1)â‹¯(ğ‘§ âˆ’ ğœ†ğ‘š) for some ğœ†1, â€¦, ğœ†ğ‘š âˆˆ ğ‘. Proof First suppose ğ… = ğ‚. The zeros of the minimal polynomial of ğ‘‡ are the eigenvalues of ğ‘‡ [by 5.27(a)]. All eigenvalues of ğ‘‡ are real (by 7.12). Thus the second version of the fundamental theorem of algebra (see 6.69) tells us that the minimal polynomial of ğ‘‡ has the desired form. Now suppose ğ… = ğ‘. By the factorization of a polynomial over ğ‘ (see 4.16) there exist ğœ†1, â€¦, ğœ†ğ‘š âˆˆ ğ‘ and ğ‘1, â€¦, ğ‘ğ‘, ğ‘1, â€¦, ğ‘ğ‘ âˆˆ ğ‘ with ğ‘ğ‘˜ 2 < 4ğ‘ğ‘˜ for each ğ‘˜ such that the minimal polynomial of ğ‘‡ equals 7.28 (ğ‘§ âˆ’ ğœ†1)â‹¯(ğ‘§ âˆ’ ğœ†ğ‘š)(ğ‘§ 2 + ğ‘1ğ‘§ + ğ‘1)â‹¯(ğ‘§ 2 + ğ‘ğ‘ğ‘§ + ğ‘ğ‘); here either ğ‘š or ğ‘ might equal 0, meaning that there are no terms of the corre- sponding form. Now (ğ‘‡ âˆ’ ğœ†1ğ¼)â‹¯(ğ‘‡ âˆ’ ğœ†ğ‘šğ¼)(ğ‘‡2 + ğ‘1ğ‘‡ + ğ‘1ğ¼)â‹¯(ğ‘‡2 + ğ‘ğ‘ğ‘‡ + ğ‘ğ‘ğ¼)= 0. If ğ‘ > 0, then we could multiply both sides of the equation above on the right by the inverse of ğ‘‡2 + ğ‘ğ‘ğ‘‡ + ğ‘ğ‘ğ¼ (which is an invertible operator by 7.26) to obtain a polynomial expression of ğ‘‡ that equals 0. The corresponding polynomial would have degree two less than the degree of 7.28, violating the minimality of the degree of the polynomial with this property. Thus we must have ğ‘ = 0, which means that the minimal polynomial in 7.28 has the form (ğ‘§ âˆ’ ğœ†1)â‹¯(ğ‘§ âˆ’ ğœ†ğ‘š), as desired. The result above along with 5.27(a) implies that every self-adjoint operator has an eigenvalue. In fact, as we will see in the next result, self-adjoint operators have enough eigenvectors to form a basis. Section 7B Spectral Theorem 245 The next result, which gives a complete description of the self-adjoint operators on a real inner product space, is one of the major theorems in linear algebra. 7.29 real spectral theorem Suppose ğ… = ğ‘ and ğ‘‡ âˆˆ â„’(ğ‘‰). Then the following are equivalent. (a) ğ‘‡ is self-adjoint. (b) ğ‘‡ has a diagonal matrix with respect to some orthonormal basis of ğ‘‰. (c) ğ‘‰ has an orthonormal basis consisting of eigenvectors of ğ‘‡. Proof First suppose (a) holds, so ğ‘‡ is self-adjoint. Our results on minimal poly- nomials, specifically6.37 and 7.27, imply that ğ‘‡ has an upper-triangular matrix with respect to some orthonormal basis of ğ‘‰. With respect to this orthonormal basis, the matrix of ğ‘‡âˆ— is the transpose of the matrix of ğ‘‡. However, ğ‘‡âˆ— = ğ‘‡. Thus the transpose of the matrix of ğ‘‡ equals the matrix of ğ‘‡. Because the matrix of ğ‘‡ is upper-triangular, this means that all entries of the matrix above and below the diagonal are 0. Hence the matrix of ğ‘‡ is a diagonal matrix with respect to the orthonormal basis. Thus (a) implies (b). Conversely, now suppose (b) holds, so ğ‘‡ has a diagonal matrix with respect to some orthonormal basis of ğ‘‰. That diagonal matrix equals its transpose. Thus with respect to that basis, the matrix of ğ‘‡âˆ— equals the matrix of ğ‘‡. Hence ğ‘‡âˆ— = ğ‘‡, proving that (b) implies (a). The equivalence of (b) and (c) follows from the definitions[or see the proof that (a) and (b) are equivalent in 5.55]. 7.30 example:an orthonormal basis of eigenvectors for an operator Consider the operator ğ‘‡ on ğ‘3 whose matrix (with respect to the standard basis) is â›âœâœâœ â 14 âˆ’13 8 âˆ’13 14 8 8 8 âˆ’7 ââŸâŸâŸ â  . This matrix with real entries equals its transpose; thus ğ‘‡ is self-adjoint. As you can verify, (1, âˆ’1, 0) âˆš2 , (1, 1, 1) âˆš3 , (1, 1, âˆ’2) âˆš6 is an orthonormal basis of ğ‘3 consisting of eigenvectors of ğ‘‡. With respect to this basis, the matrix of ğ‘‡ is the diagonal matrix â›âœâœâœ â 27 0 0 0 9 0 0 0 âˆ’15 ââŸâŸâŸ â  . See Exercise 17 for a version of the real spectral theorem that applies simulta- neously to more than one operator. 246 Chapter 7 Operators on Inner Product Spaces Complex Spectral Theorem The next result gives a complete description of the normal operators on a complex inner product space. 7.31 complex spectral theorem Suppose ğ… = ğ‚ and ğ‘‡ âˆˆ â„’(ğ‘‰). Then the following are equivalent. (a) ğ‘‡ is normal. (b) ğ‘‡ has a diagonal matrix with respect to some orthonormal basis of ğ‘‰. (c) ğ‘‰ has an orthonormal basis consisting of eigenvectors of ğ‘‡. Proof First suppose (a) holds, so ğ‘‡ is normal. By Schurâ€™s theorem (6.38), there is an orthonormal basis ğ‘’1, â€¦, ğ‘’ğ‘› of ğ‘‰ with respect to which ğ‘‡ has an upper-triangular matrix. Thus we can write 7.32 â„³(ğ‘‡, (ğ‘’1, â€¦, ğ‘’ğ‘›))= â›âœâœâœ â ğ‘1, 1 â‹¯ ğ‘1, ğ‘› â‹± â‹® 0 ğ‘ğ‘›, ğ‘› ââŸâŸâŸ â  . We will show that this matrix is actually a diagonal matrix. We see from the matrix above that â€–ğ‘‡ğ‘’1â€–2 = |ğ‘1, 1| 2, âˆ¥ğ‘‡âˆ—ğ‘’1âˆ¥ 2 = |ğ‘1, 1| 2 + |ğ‘1, 2| 2 + â‹¯ + |ğ‘1, ğ‘›| 2. Because ğ‘‡ is normal, â€–ğ‘‡ğ‘’1â€– = âˆ¥ğ‘‡âˆ—ğ‘’1âˆ¥ (see 7.20). Thus the two equations above imply that all entries in the first row of the matrix in7.32, except possibly the first entry ğ‘1, 1, equal 0. Now 7.32 implies â€–ğ‘‡ğ‘’2â€– 2 = |ğ‘2, 2| 2 (because ğ‘1, 2 = 0, as we showed in the paragraph above) and âˆ¥ğ‘‡âˆ—ğ‘’2âˆ¥ 2 = |ğ‘2, 2| 2 + |ğ‘2, 3| 2 + â‹¯ + |ğ‘2, ğ‘›| 2. Because ğ‘‡ is normal, â€–ğ‘‡ğ‘’2â€– = âˆ¥ğ‘‡âˆ—ğ‘’2âˆ¥. Thus the two equations above imply that all entries in the second row of the matrix in 7.32, except possibly the diagonal entry ğ‘2, 2, equal 0. Continuing in this fashion, we see that all nondiagonal entries in the matrix 7.32 equal 0. Thus (b) holds, completing the proof that (a) implies (b). Now suppose (b) holds, so ğ‘‡ has a diagonal matrix with respect to some orthonormal basis of ğ‘‰. The matrix of ğ‘‡âˆ— (with respect to the same basis) is obtained by taking the conjugate transpose of the matrix of ğ‘‡; hence ğ‘‡âˆ— also has a diagonal matrix. Any two diagonal matrices commute; thus ğ‘‡ commutes with ğ‘‡âˆ—, which means that ğ‘‡ is normal. In other words, (a) holds, completing the proof that (b) implies (a). The equivalence of (b) and (c) follows from the definitions (also see5.55). Section 7B Spectral Theorem 247 See Exercises 13 and 20 for alternative proofs that (a) implies (b) in the previous result. Exercises 14 and 15 interpret the real spectral theorem and the complex spectral theorem by expressing the domain space as an orthogonal direct sum of eigenspaces. See Exercise 16 for a version of the complex spectral theorem that applies simultaneously to more than one operator. The main conclusion of the complex spectral theorem is that every normal operator on a complex finite-dimensional inner product space is diagonalizable by an orthonormal basis, as illustrated by the next example. 7.33 example:an orthonormal basis of eigenvectors for an operator Consider the operator ğ‘‡ âˆˆ â„’(ğ‚ 2)defined byğ‘‡(ğ‘¤, ğ‘§) = (2ğ‘¤ âˆ’ 3ğ‘§, 3ğ‘¤+ 2ğ‘§). The matrix of ğ‘‡ (with respect to the standard basis) is ( 2 âˆ’3 3 2 ). As we saw in Example 7.19, ğ‘‡ is a normal operator. As you can verify, 1 âˆš2 (ğ‘–, 1), 1 âˆš2 (âˆ’ğ‘–, 1) is an orthonormal basis of ğ‚ 2 consisting of eigenvectors of ğ‘‡, and with respect to this basis the matrix of ğ‘‡ is the diagonal matrix ( 2+ 3ğ‘– 0 0 2 âˆ’ 3ğ‘– ). Exercises 7B 1 Prove that a normal operator on a complex inner product space is self-adjoint if and only if all its eigenvalues are real. This exercise strengthens the analogy (for normal operators) between self- adjoint operators and real numbers. 2 Suppose ğ… = ğ‚. Suppose ğ‘‡ âˆˆ â„’(ğ‘‰) is normal and has only one eigenvalue. Prove that ğ‘‡ is a scalar multiple of the identity operator. 3 Suppose ğ… = ğ‚ and ğ‘‡ âˆˆ â„’(ğ‘‰) is normal. Prove that the set of eigenvalues of ğ‘‡ is contained in {0, 1}if and only if there is a subspace ğ‘ˆ of ğ‘‰ such that ğ‘‡ = ğ‘ƒğ‘ˆ. 4 Prove that a normal operator on a complex inner product space is skew (meaning it equals the negative of its adjoint) if and only if all its eigenvalues are purely imaginary (meaning that they have real part equal to 0). 248 Chapter 7 Operators on Inner Product Spaces 5 Prove or give a counterexample: If ğ‘‡ âˆˆ â„’(ğ‚ 3)is a diagonalizable operator, then ğ‘‡ is normal (with respect to the usual inner product). 6 Suppose ğ‘‰ is a complex inner product space and ğ‘‡ âˆˆ â„’(ğ‘‰) is a normal operator such that ğ‘‡9 = ğ‘‡8. Prove that ğ‘‡ is self-adjoint and ğ‘‡2 = ğ‘‡. 7 Give an example of an operator ğ‘‡ on a complex vector space such that ğ‘‡9 = ğ‘‡8 but ğ‘‡2 â‰  ğ‘‡. 8 Suppose ğ… = ğ‚ and ğ‘‡ âˆˆ â„’(ğ‘‰). Prove that ğ‘‡ is normal if and only if every eigenvector of ğ‘‡ is also an eigenvector of ğ‘‡âˆ—. 9 Suppose ğ… = ğ‚ and ğ‘‡ âˆˆ â„’(ğ‘‰). Prove that ğ‘‡ is normal if and only if there exists a polynomial ğ‘ âˆˆ ğ’«(ğ‚) such that ğ‘‡âˆ— = ğ‘(ğ‘‡). 10 Suppose ğ‘‰ is a complex inner product space. Prove that every normal operator on ğ‘‰ has a square root. An operator ğ‘† âˆˆ â„’(ğ‘‰) is called a square root of ğ‘‡ âˆˆ â„’(ğ‘‰) if ğ‘†2 = ğ‘‡. We will discuss more about square roots of operators in Sections 7C and 8C. 11 Prove that every self-adjoint operator on ğ‘‰ has a cube root. An operator ğ‘† âˆˆ â„’(ğ‘‰) is called a cube root of ğ‘‡ âˆˆ â„’(ğ‘‰) if ğ‘†3 = ğ‘‡. 12 Suppose ğ‘‰ is a complex vector space and ğ‘‡ âˆˆ â„’(ğ‘‰) is normal. Prove that if ğ‘† is an operator on ğ‘‰ that commutes with ğ‘‡, then ğ‘† commutes with ğ‘‡âˆ—. The result in this exercise is called Fugledeâ€™s theorem. 13 Without using the complex spectral theorem, use the version of Schurâ€™s theorem that applies to two commuting operators (take â„° = {ğ‘‡, ğ‘‡âˆ—}in Exercise 20 in Section 6B)to give a different proof that if ğ… = ğ‚ and ğ‘‡ âˆˆ â„’(ğ‘‰) is normal, then ğ‘‡ has a diagonal matrix with respect to some orthonormal basis of ğ‘‰. 14 Suppose ğ… = ğ‘ and ğ‘‡ âˆˆ â„’(ğ‘‰). Prove that ğ‘‡ is self-adjoint if and only if all pairs of eigenvectors corresponding to distinct eigenvalues of ğ‘‡ are orthogonal and ğ‘‰ = ğ¸(ğœ†1, ğ‘‡) âŠ• â‹¯ âŠ• ğ¸(ğœ†ğ‘š, ğ‘‡), where ğœ†1, â€¦, ğœ†ğ‘š denote the distinct eigenvalues of ğ‘‡. 15 Suppose ğ… = ğ‚ and ğ‘‡ âˆˆ â„’(ğ‘‰). Prove that ğ‘‡ is normal if and only if all pairs of eigenvectors corresponding to distinct eigenvalues of ğ‘‡ are orthogonal and ğ‘‰ = ğ¸(ğœ†1, ğ‘‡) âŠ• â‹¯ âŠ• ğ¸(ğœ†ğ‘š, ğ‘‡), where ğœ†1, â€¦, ğœ†ğ‘š denote the distinct eigenvalues of ğ‘‡. 16 Suppose ğ… = ğ‚ and â„° âŠ† â„’(ğ‘‰). Prove that there is an orthonormal basis of ğ‘‰ with respect to which every element of â„° has a diagonal matrix if and only if ğ‘† and ğ‘‡ are commuting normal operators for all ğ‘†, ğ‘‡ âˆˆ â„°. This exercise extends the complex spectral theorem to the context of a collection of commuting normal operators. Section 7B Spectral Theorem 249 17 Suppose ğ… = ğ‘ and â„° âŠ† â„’(ğ‘‰). Prove that there is an orthonormal basis of ğ‘‰ with respect to which every element of â„° has a diagonal matrix if and only if ğ‘† and ğ‘‡ are commuting self-adjoint operators for all ğ‘†, ğ‘‡ âˆˆ â„°. This exercise extends the real spectral theorem to the context of a collection of commuting self-adjoint operators. 18 Give an example of a real inner product space ğ‘‰, an operator ğ‘‡ âˆˆ â„’(ğ‘‰), and real numbers ğ‘, ğ‘ with ğ‘ 2 < 4ğ‘such that ğ‘‡2 + ğ‘ğ‘‡ + ğ‘ğ¼ is not invertible. This exercise shows that the hypothesis that ğ‘‡ is self-adjoint cannot be deleted in 7.26, even for real vector spaces. 19 Suppose ğ‘‡ âˆˆ â„’(ğ‘‰) is self-adjoint and ğ‘ˆ is a subspace of ğ‘‰ that is invariant under ğ‘‡. (a) Prove that ğ‘ˆâŸ‚ is invariant under ğ‘‡. (b) Prove that ğ‘‡|ğ‘ˆ âˆˆ â„’(ğ‘ˆ) is self-adjoint. (c) Prove that ğ‘‡|ğ‘ˆâŸ‚ âˆˆ â„’(ğ‘ˆâŸ‚)is self-adjoint. 20 Suppose ğ‘‡ âˆˆ â„’(ğ‘‰) is normal and ğ‘ˆ is a subspace of ğ‘‰ that is invariant under ğ‘‡. (a) Prove that ğ‘ˆâŸ‚ is invariant under ğ‘‡. (b) Prove that ğ‘ˆ is invariant under ğ‘‡âˆ—. (c) Prove that (ğ‘‡|ğ‘ˆ)âˆ— = (ğ‘‡âˆ—)|ğ‘ˆ. (d) Prove that ğ‘‡|ğ‘ˆ âˆˆ â„’(ğ‘ˆ) and ğ‘‡|ğ‘ˆâŸ‚ âˆˆ â„’(ğ‘ˆâŸ‚)are normal operators. This exercise can be used to give yet another proof of the complex spectral theorem (use induction on dim ğ‘‰ and the result that ğ‘‡ has an eigenvector). 21 Suppose that ğ‘‡ is a self-adjoint operator on a finite-dimensional inner product space and that 2and 3are the only eigenvalues of ğ‘‡. Prove that ğ‘‡2 âˆ’ 5ğ‘‡+ 6ğ¼ = 0. 22 Give an example of an operator ğ‘‡ âˆˆ â„’(ğ‚ 3)such that 2and 3are the only eigenvalues of ğ‘‡ and ğ‘‡2 âˆ’ 5ğ‘‡+ 6ğ¼ â‰  0. 23 Suppose ğ‘‡ âˆˆ â„’(ğ‘‰) is self-adjoint, ğœ† âˆˆ ğ…, and ğœ– > 0. Suppose there exists ğ‘£ âˆˆ ğ‘‰ such that â€–ğ‘£â€– = 1and â€–ğ‘‡ğ‘£ âˆ’ ğœ†ğ‘£â€– < ğœ–. Prove that ğ‘‡ has an eigenvalue ğœ†â€² such that âˆ£ğœ† âˆ’ ğœ†â€²âˆ£ < ğœ–. This exercise shows that for a self-adjoint operator, a number that is close to satisfying an equation that would make it an eigenvalue is close to an eigenvalue. 250 Chapter 7 Operators on Inner Product Spaces 24 Suppose ğ‘ˆ is a finite-dimensional vector space andğ‘‡ âˆˆ â„’(ğ‘ˆ). (a) Suppose ğ… = ğ‘. Prove that ğ‘‡ is diagonalizable if and only if there is a basis of ğ‘ˆ such that the matrix of ğ‘‡ with respect to this basis equals its transpose. (b) Suppose ğ… = ğ‚. Prove that ğ‘‡ is diagonalizable if and only if there is a basis of ğ‘ˆ such that the matrix of ğ‘‡ with respect to this basis commutes with its conjugate transpose. This exercise adds another equivalence to the list of conditions equivalent to diagonalizability in 5.55. 25 Suppose that ğ‘‡ âˆˆ â„’(ğ‘‰) and there is an orthonormal basis ğ‘’1, â€¦, ğ‘’ğ‘› of ğ‘‰ consisting of eigenvectors of ğ‘‡, with corresponding eigenvalues ğœ†1, â€¦, ğœ†ğ‘›. Show that if ğ‘˜ âˆˆ {1, â€¦, ğ‘›}, then the pseudoinverse ğ‘‡â€  satisfies the equation ğ‘‡â€ ğ‘’ğ‘˜ = â§ { â¨ { â© 1 ğœ†ğ‘˜ ğ‘’ğ‘˜ if ğœ†ğ‘˜ â‰  0, 0 if ğœ†ğ‘˜ = 0. Section 7C Positive Operators 251 7C Positive Operators 7.34 definition:positive operator An operator ğ‘‡ âˆˆ â„’(ğ‘‰) is called positive if ğ‘‡ is self-adjoint and âŸ¨ğ‘‡ğ‘£, ğ‘£âŸ© â‰¥ 0 for all ğ‘£ âˆˆ ğ‘‰. If ğ‘‰ is a complex vector space, then the requirement that ğ‘‡ be self-adjoint can be dropped from the definition above (by7.14). 7.35 example:positive operators (a) Let ğ‘‡ âˆˆ â„’(ğ…2)be the operator whose matrix (using the standard basis) is (2 âˆ’1 âˆ’1 1 ). Then ğ‘‡ is self-adjoint and âŸ¨ğ‘‡(ğ‘¤, ğ‘§), (ğ‘¤, ğ‘§)âŸ©= 2|ğ‘¤| 2âˆ’2Re(ğ‘¤ğ‘§)+|ğ‘§| 2 = |ğ‘¤ âˆ’ ğ‘§|2 + |ğ‘¤| 2 â‰¥ 0for all (ğ‘¤, ğ‘§) âˆˆ ğ…2. Thus ğ‘‡ is a positive operator. (b) If ğ‘ˆ is a subspace of ğ‘‰, then the orthogonal projection ğ‘ƒğ‘ˆ is a positive operator, as you should verify. (c) If ğ‘‡ âˆˆ â„’(ğ‘‰) is self-adjoint and ğ‘, ğ‘ âˆˆ ğ‘ are such that ğ‘ 2 < 4ğ‘, then ğ‘‡2+ğ‘ğ‘‡+ğ‘ğ¼ is a positive operator, as shown by the proof of 7.26. 7.36 definition:square root An operator ğ‘… is called a square root of an operator ğ‘‡ if ğ‘… 2 = ğ‘‡. 7.37 example:square root of an operator If ğ‘‡ âˆˆ â„’(ğ…3)is defined byğ‘‡(ğ‘§1, ğ‘§2, ğ‘§3) = (ğ‘§3, 0, 0), then the operator ğ‘… âˆˆ â„’(ğ…3)defined byğ‘…(ğ‘§1, ğ‘§2, ğ‘§3) = (ğ‘§2, ğ‘§3, 0)is a square root of ğ‘‡ because ğ‘… 2 = ğ‘‡, as you can verify. Because positive operators correspond to nonnegative numbers, better termi- nology would use the term nonnegative operators. However, operator theorists consistently call these positive opera- tors, so we follow that custom. Some mathematicians use the term positive semidefinite operator, which means the same as positive operator. The characterizations of the positive operators in the next result correspond to characterizations of the nonnegative numbers among ğ‚. Specifically, a num- ber ğ‘§ âˆˆ ğ‚ is nonnegative if and only if it has a nonnegative square root, cor- responding to condition (d). Also, ğ‘§ is nonnegative if and only if it has a real square root, corresponding to condition (e). Finally, ğ‘§ is nonnegative if and only if there exists ğ‘¤ âˆˆ ğ‚ such that ğ‘§ = ğ‘¤ğ‘¤, corresponding to condition (f). See Exercise 20 for another condition that is equivalent to being a positive operator. 252 Chapter 7 Operators on Inner Product Spaces 7.38 characterization of positive operators Let ğ‘‡ âˆˆ â„’(ğ‘‰). Then the following are equivalent. (a) ğ‘‡ is a positive operator. (b) ğ‘‡ is self-adjoint and all eigenvalues of ğ‘‡ are nonnegative. (c) With respect to some orthonormal basis of ğ‘‰, the matrix of ğ‘‡ is a diagonal matrix with only nonnegative numbers on the diagonal. (d) ğ‘‡ has a positive square root. (e) ğ‘‡ has a self-adjoint square root. (f) ğ‘‡ = ğ‘…âˆ—ğ‘… for some ğ‘… âˆˆ â„’(ğ‘‰). Proof We will prove that (a) â‡’ (b) â‡’ (c) â‡’ (d) â‡’ (e) â‡’ (f) â‡’ (a). First suppose (a) holds, so that ğ‘‡ is positive, which implies that ğ‘‡ is self-adjoint (by definition of positive operator). To prove the other condition in (b), suppose ğœ† is an eigenvalue of ğ‘‡. Let ğ‘£ be an eigenvector of ğ‘‡ corresponding to ğœ†. Then 0 â‰¤ âŸ¨ğ‘‡ğ‘£, ğ‘£âŸ© = âŸ¨ğœ†ğ‘£, ğ‘£âŸ© = ğœ†âŸ¨ğ‘£, ğ‘£âŸ©. Thus ğœ† is a nonnegative number. Hence (b) holds, showing that (a) implies (b). Now suppose (b) holds, so that ğ‘‡ is self-adjoint and all eigenvalues of ğ‘‡ are nonnegative. By the spectral theorem (7.29 and 7.31), there is an orthonormal basis ğ‘’1, â€¦, ğ‘’ğ‘› of ğ‘‰ consisting of eigenvectors of ğ‘‡. Let ğœ†1, â€¦, ğœ†ğ‘› be the eigenval- ues of ğ‘‡ corresponding to ğ‘’1, â€¦, ğ‘’ğ‘›; thus each ğœ†ğ‘˜ is a nonnegative number. The matrix of ğ‘‡ with respect to ğ‘’1, â€¦, ğ‘’ğ‘› is the diagonal matrix with ğœ†1, â€¦, ğœ†ğ‘› on the diagonal, which shows that (b) implies (c). Now suppose (c) holds. Suppose ğ‘’1, â€¦, ğ‘’ğ‘› is an orthonormal basis of ğ‘‰ such that the matrix of ğ‘‡ with respect to this basis is a diagonal matrix with nonnegative numbers ğœ†1, â€¦, ğœ†ğ‘› on the diagonal. The linear map lemma (3.4) implies that there exists ğ‘… âˆˆ â„’(ğ‘‰) such that ğ‘…ğ‘’ğ‘˜ = âˆšğœ†ğ‘˜ğ‘’ğ‘˜ for each ğ‘˜ = 1, â€¦, ğ‘›. As you should verify, ğ‘… is a positive operator. Furthermore, ğ‘… 2ğ‘’ğ‘˜ = ğœ†ğ‘˜ğ‘’ğ‘˜ = ğ‘‡ğ‘’ğ‘˜ for each ğ‘˜, which implies that ğ‘…2 = ğ‘‡. Thus ğ‘… is a positive square root of ğ‘‡. Hence (d) holds, which shows that (c) implies (d). Every positive operator is self-adjoint (by definition of positive operator). Thus (d) implies (e). Now suppose (e) holds, meaning that there exists a self-adjoint operator ğ‘… on ğ‘‰ such that ğ‘‡ = ğ‘…2. Then ğ‘‡ = ğ‘…âˆ—ğ‘… (because ğ‘…âˆ— = ğ‘…). Hence (e) implies (f). Finally, suppose (f) holds. Let ğ‘… âˆˆ â„’(ğ‘‰) be such that ğ‘‡ = ğ‘…âˆ—ğ‘…. Then ğ‘‡âˆ— = (ğ‘…âˆ—ğ‘…) âˆ— = ğ‘…âˆ—(ğ‘…âˆ—) âˆ— = ğ‘…âˆ—ğ‘… = ğ‘‡. Hence ğ‘‡ is self-adjoint. To complete the proof that (a) holds, note that âŸ¨ğ‘‡ğ‘£, ğ‘£âŸ© =âŸ¨ğ‘…âˆ—ğ‘…ğ‘£, ğ‘£âŸ©= âŸ¨ğ‘…ğ‘£, ğ‘…ğ‘£âŸ© â‰¥ 0 for every ğ‘£ âˆˆ ğ‘‰. Thus ğ‘‡ is positive, showing that (f) implies (a). Section 7C Positive Operators 253 Every nonnegative number has a unique nonnegative square root. The next result shows that positive operators enjoy a similar property. 7.39 each positive operator has only one positive square root Every positive operator on ğ‘‰ has a unique positive square root. A positive operator can have infinitely many square roots (although only one of them can be positive). For example, the identity operator on ğ‘‰ has infinitely many square roots if dim ğ‘‰ > 1. Proof Suppose ğ‘‡ âˆˆ â„’(ğ‘‰) is positive. Suppose ğ‘£ âˆˆ ğ‘‰ is an eigenvector of ğ‘‡. Hence there exists a real number ğœ† â‰¥ 0 such that ğ‘‡ğ‘£ = ğœ†ğ‘£. Let ğ‘… be a positive square root of ğ‘‡. We will prove that ğ‘…ğ‘£ = âˆšğœ†ğ‘£. This will imply that the behavior of ğ‘… on the eigenvectors of ğ‘‡ is uniquely determined. Because there is a basis of ğ‘‰ consisting of eigenvectors of ğ‘‡ (by the spectral theorem), this will imply that ğ‘… is uniquely determined. To prove that ğ‘…ğ‘£ = âˆšğœ†ğ‘£, note that the spectral theorem asserts that there is an orthonormal basis ğ‘’1, â€¦, ğ‘’ğ‘› of ğ‘‰ consisting of eigenvectors of ğ‘…. Because ğ‘… is a positive operator, all its eigenvalues are nonnegative. Thus there exist nonnegative numbers ğœ†1, â€¦, ğœ†ğ‘› such that ğ‘…ğ‘’ğ‘˜ = âˆšğœ†ğ‘˜ğ‘’ğ‘˜ for each ğ‘˜ = 1, â€¦, ğ‘›. Because ğ‘’1, â€¦, ğ‘’ğ‘› is a basis of ğ‘‰, we can write ğ‘£ = ğ‘1ğ‘’1 + â‹¯ + ğ‘ğ‘›ğ‘’ğ‘› for some numbers ğ‘1, â€¦, ğ‘ğ‘› âˆˆ ğ…. Thus ğ‘…ğ‘£ = ğ‘1âˆšğœ†1ğ‘’1 + â‹¯ + ğ‘ğ‘›âˆšğœ†ğ‘›ğ‘’ğ‘›. Hence ğœ†ğ‘£ = ğ‘‡ğ‘£ = ğ‘…2ğ‘£ = ğ‘1 ğœ†1ğ‘’1 + â‹¯ + ğ‘ğ‘› ğœ†ğ‘›ğ‘’ğ‘›. The equation above implies that ğ‘1 ğœ†ğ‘’1 + â‹¯ + ğ‘ğ‘› ğœ†ğ‘’ğ‘› = ğ‘1 ğœ†1ğ‘’1 + â‹¯ + ğ‘ğ‘› ğœ†ğ‘›ğ‘’ğ‘›. Thus ğ‘ğ‘˜(ğœ† âˆ’ ğœ†ğ‘˜) = 0for each ğ‘˜ = 1, â€¦, ğ‘›. Hence ğ‘£ = âˆ‘ {ğ‘˜ âˆ¶ ğœ†ğ‘˜ = ğœ†} ğ‘ğ‘˜ğ‘’ğ‘˜. Thus ğ‘…ğ‘£ = âˆ‘ {ğ‘˜ âˆ¶ ğœ†ğ‘˜ = ğœ†} ğ‘ğ‘˜âˆšğœ†ğ‘’ğ‘˜ = âˆšğœ†ğ‘£, as desired. The notation defined below makes sense thanks to the result above. 7.40 notation:âˆšğ‘‡ For ğ‘‡ a positive operator, âˆšğ‘‡ denotes the unique positive square root of ğ‘‡. 254 Chapter 7 Operators on Inner Product Spaces 7.41 example:square root of positive operators Define operatorsğ‘†, ğ‘‡ on ğ‘2 (with the usual Euclidean inner product) by ğ‘†(ğ‘¥, ğ‘¦) = (ğ‘¥, 2ğ‘¦) and ğ‘‡(ğ‘¥, ğ‘¦) = (ğ‘¥ + ğ‘¦, ğ‘¥ + ğ‘¦). Then with respect to the standard basis of ğ‘2 we have 7.42 â„³(ğ‘†) = ( 1 0 0 2 ) and â„³(ğ‘‡) = ( 1 1 1 1 ). Each of these matrices equals its transpose; thus ğ‘† and ğ‘‡ are self-adjoint. If (ğ‘¥, ğ‘¦) âˆˆ ğ‘2, then âŸ¨ğ‘†(ğ‘¥, ğ‘¦), (ğ‘¥, ğ‘¦)âŸ©= ğ‘¥2 + 2ğ‘¦ 2 â‰¥ 0 and âŸ¨ğ‘‡(ğ‘¥, ğ‘¦), (ğ‘¥, ğ‘¦)âŸ©= ğ‘¥2 + 2ğ‘¥ğ‘¦+ ğ‘¦2 = (ğ‘¥ + ğ‘¦)2 â‰¥ 0. Thus ğ‘† and ğ‘‡ are positive operators. The standard basis of ğ‘2 is an orthonormal basis consisting of eigenvectors of ğ‘†. Note that ( 1 âˆš2 , 1 âˆš2 ), ( 1 âˆš2 , âˆ’ 1 âˆš2 ) is an orthonormal basis of eigenvectors of ğ‘‡, with eigenvalue 2for the first eigenvector and eigenvalue 0for the second eigenvector. Thus âˆšğ‘‡ has the same eigenvectors, with eigenvalues âˆš2and 0. You can verify that â„³(âˆšğ‘† )= â›âœ â 1 0 0 âˆš2ââŸ â  and â„³(âˆšğ‘‡ )= â›âœâœâœâœ â 1 âˆš2 1 âˆš2 1 âˆš2 1 âˆš2 ââŸâŸâŸâŸ â  with respect to the standard basis by showing that the squares of the matrices above are the matrices in 7.42 and that each matrix above is the matrix of a positive operator. The statement of the next result does not involve a square root, but the clean proof makes nice use of the square root of a positive operator. 7.43 ğ‘‡ positive and âŸ¨ğ‘‡ğ‘£, ğ‘£âŸ© = 0 âŸ¹ ğ‘‡ğ‘£ = 0 Suppose ğ‘‡ is a positive operator on ğ‘‰ and ğ‘£ âˆˆ ğ‘‰ is such that âŸ¨ğ‘‡ğ‘£, ğ‘£âŸ© = 0. Then ğ‘‡ğ‘£ = 0. Proof We have 0 = âŸ¨ğ‘‡ğ‘£, ğ‘£âŸ© =âŸ¨âˆšğ‘‡âˆšğ‘‡ğ‘£, ğ‘£âŸ©= âŸ¨âˆšğ‘‡ğ‘£, âˆšğ‘‡ğ‘£âŸ©= âˆ¥âˆšğ‘‡ğ‘£âˆ¥2 . Hence âˆšğ‘‡ğ‘£ = 0. Thus ğ‘‡ğ‘£ = âˆšğ‘‡(âˆšğ‘‡ğ‘£)= 0, as desired. Section 7C Positive Operators 255 Exercises 7C 1 Suppose ğ‘‡ âˆˆ â„’(ğ‘‰). Prove that if both ğ‘‡ and âˆ’ğ‘‡ are positive operators, then ğ‘‡ = 0. 2 Suppose ğ‘‡ âˆˆ â„’(ğ…4)is the operator whose matrix (with respect to the standard basis) is â›âœâœâœâœâœâœ â 2 âˆ’1 0 0 âˆ’1 2 âˆ’1 0 0 âˆ’1 2 âˆ’1 0 0 âˆ’1 2 ââŸâŸâŸâŸâŸâŸ â  . Show that ğ‘‡ is an invertible positive operator. 3 Suppose ğ‘› is a positive integer and ğ‘‡ âˆˆ â„’(ğ…ğ‘›)is the operator whose matrix (with respect to the standard basis) consists of all 1â€™s. Show that ğ‘‡ is a positive operator. 4 Suppose ğ‘› is an integer with ğ‘› > 1. Show that there exists an ğ‘›-by-ğ‘› matrix ğ´ such that all of the entries of ğ´ are positive numbers and ğ´ = ğ´âˆ—, but the operator on ğ…ğ‘› whose matrix (with respect to the standard basis) equals ğ´ is not a positive operator. 5 Suppose ğ‘‡ âˆˆ â„’(ğ‘‰) is self-adjoint. Prove that ğ‘‡ is a positive operator if and only if for every orthonormal basis ğ‘’1, â€¦, ğ‘’ğ‘› of ğ‘‰, all entries on the diagonal of â„³(ğ‘‡, (ğ‘’1, â€¦, ğ‘’ğ‘›))are nonnegative numbers. 6 Prove that the sum of two positive operators on ğ‘‰ is a positive operator. 7 Suppose ğ‘† âˆˆ â„’(ğ‘‰) is an invertible positive operator and ğ‘‡ âˆˆ â„’(ğ‘‰) is a positive operator. Prove that ğ‘† + ğ‘‡ is invertible. 8 Suppose ğ‘‡ âˆˆ â„’(ğ‘‰). Prove that ğ‘‡ is a positive operator if and only if the pseudoinverse ğ‘‡â€  is a positive operator. 9 Suppose ğ‘‡ âˆˆ â„’(ğ‘‰) is a positive operator and ğ‘† âˆˆ â„’(ğ‘Š, ğ‘‰). Prove that ğ‘†âˆ—ğ‘‡ğ‘† is a positive operator on ğ‘Š. 10 Suppose ğ‘‡ is a positive operator on ğ‘‰. Suppose ğ‘£, ğ‘¤ âˆˆ ğ‘‰ are such that ğ‘‡ğ‘£ = ğ‘¤ and ğ‘‡ğ‘¤ = ğ‘£. Prove that ğ‘£ = ğ‘¤. 11 Suppose ğ‘‡ is a positive operator on ğ‘‰ and ğ‘ˆ is a subspace of ğ‘‰ invariant under ğ‘‡. Prove that ğ‘‡|ğ‘ˆ âˆˆ â„’(ğ‘ˆ) is a positive operator on ğ‘ˆ. 12 Suppose ğ‘‡ âˆˆ â„’(ğ‘‰) is a positive operator. Prove that ğ‘‡ğ‘˜ is a positive operator for every positive integer ğ‘˜. 256 Chapter 7 Operators on Inner Product Spaces 13 Suppose ğ‘‡ âˆˆ â„’(ğ‘‰) is self-adjoint and ğ›¼ âˆˆ ğ‘. (a) Prove that ğ‘‡ âˆ’ ğ›¼ğ¼ is a positive operator if and only if ğ›¼ is less than or equal to every eigenvalue of ğ‘‡. (b) Prove that ğ›¼ğ¼ âˆ’ ğ‘‡ is a positive operator if and only if ğ›¼ is greater than or equal to every eigenvalue of ğ‘‡. 14 Suppose ğ‘‡ is a positive operator on ğ‘‰ and ğ‘£1, â€¦, ğ‘£ğ‘š âˆˆ ğ‘‰. Prove that ğ‘š âˆ‘ ğ‘— = 1 ğ‘š âˆ‘ ğ‘˜ = 1âŸ¨ğ‘‡ğ‘£ğ‘˜, ğ‘£ğ‘—âŸ© â‰¥ 0. 15 Suppose ğ‘‡ âˆˆ â„’(ğ‘‰) is self-adjoint. Prove that there exist positive operators ğ´, ğµ âˆˆ â„’(ğ‘‰) such that ğ‘‡ = ğ´ âˆ’ ğµ and âˆšğ‘‡âˆ—ğ‘‡ = ğ´ + ğµ and ğ´ğµ = ğµğ´ = 0. 16 Suppose ğ‘‡ is a positive operator on ğ‘‰. Prove that null âˆšğ‘‡ = null ğ‘‡ and range âˆšğ‘‡ = range ğ‘‡. 17 Suppose that ğ‘‡ âˆˆ â„’(ğ‘‰) is a positive operator. Prove that there exists a polynomial ğ‘ with real coefficients such that âˆšğ‘‡ = ğ‘(ğ‘‡). 18 Suppose ğ‘† and ğ‘‡ are positive operators on ğ‘‰. Prove that ğ‘†ğ‘‡ is a positive operator if and only if ğ‘† and ğ‘‡ commute. 19 Show that the identity operator on ğ…2 has infinitely many self-adjoint square roots. 20 Suppose ğ‘‡ âˆˆ â„’(ğ‘‰) and ğ‘’1, â€¦, ğ‘’ğ‘› is an orthonormal basis of ğ‘‰. Prove that ğ‘‡ is a positive operator if and only if there exist ğ‘£1, â€¦, ğ‘£ğ‘› âˆˆ ğ‘‰ such that âŸ¨ğ‘‡ğ‘’ğ‘˜, ğ‘’ğ‘—âŸ© = âŸ¨ğ‘£ğ‘˜, ğ‘£ğ‘—âŸ© for all ğ‘—, ğ‘˜ = 1, â€¦, ğ‘›. The numbers {âŸ¨ğ‘‡ğ‘’ğ‘˜, ğ‘’ğ‘—âŸ©} ğ‘—, ğ‘˜ = 1, â€¦, ğ‘› are the entries in the matrix of ğ‘‡ with respect to the orthonormal basis ğ‘’1, â€¦, ğ‘’ğ‘›. 21 Suppose ğ‘› is a positive integer. The ğ‘›-by-ğ‘› Hilbert matrix is the ğ‘›-by-ğ‘› matrix whose entry in row ğ‘—, column ğ‘˜ is 1 ğ‘— + ğ‘˜ âˆ’ 1 . Suppose ğ‘‡ âˆˆ â„’(ğ‘‰) is an operator whose matrix with respect to some orthonormal basis of ğ‘‰ is the ğ‘›-by-ğ‘› Hilbert matrix. Prove that ğ‘‡ is a positive invertible operator. Example: The 4-by-4Hilbert matrix is â›âœâœâœâœâœâœâœâœâœâœâœâœâœ â 1 1 2 1 3 1 4 1 2 1 3 1 4 1 5 1 3 1 4 1 5 1 6 1 4 1 5 1 6 1 7 ââŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸ â  . Section 7C Positive Operators 257 22 Suppose ğ‘‡ âˆˆ â„’(ğ‘‰) is a positive operator and ğ‘¢ âˆˆ ğ‘‰ is such that â€–ğ‘¢â€– = 1 and â€–ğ‘‡ğ‘¢â€– â‰¥ â€–ğ‘‡ğ‘£â€–for all ğ‘£ âˆˆ ğ‘‰ with â€–ğ‘£â€– = 1. Show that ğ‘¢ is an eigenvector of ğ‘‡ corresponding to the largest eigenvalue of ğ‘‡. 23 For ğ‘‡ âˆˆ â„’(ğ‘‰) and ğ‘¢, ğ‘£ âˆˆ ğ‘‰, defineâŸ¨ğ‘¢, ğ‘£âŸ©ğ‘‡ by âŸ¨ğ‘¢, ğ‘£âŸ©ğ‘‡ = âŸ¨ğ‘‡ğ‘¢, ğ‘£âŸ©. (a) Suppose ğ‘‡ âˆˆ â„’(ğ‘‰). Prove that âŸ¨â‹…, â‹…âŸ©ğ‘‡ is an inner product on ğ‘‰ if and only if ğ‘‡ is an invertible positive operator (with respect to the original inner product âŸ¨â‹…, â‹…âŸ©). (b) Prove that every inner product on ğ‘‰ is of the form âŸ¨â‹…, â‹…âŸ©ğ‘‡ for some positive invertible operator ğ‘‡ âˆˆ â„’(ğ‘‰). 24 Suppose ğ‘† and ğ‘‡ are positive operators on ğ‘‰. Prove that null(ğ‘† + ğ‘‡) = null ğ‘† âˆ©null ğ‘‡. 25 Let ğ‘‡ be the second derivative operator in Exercise 31(b) in Section 7A. Show that âˆ’ğ‘‡ is a positive operator. 258 Chapter 7 Operators on Inner Product Spaces 7D Isometries, Unitary Operators, and Matrix Factorization Isometries Linear maps that preserve norms are sufficiently important to deserve a name. 7.44 definition:isometry A linear map ğ‘† âˆˆ â„’(ğ‘‰, ğ‘Š) is called an isometry if â€–ğ‘†ğ‘£â€– = â€–ğ‘£â€– for every ğ‘£ âˆˆ ğ‘‰. In other words, a linear map is an isometry if it preserves norms. The Greek word isos means equal; the Greek word metron means measure. Thus isometry literally means equal measure. If ğ‘† âˆˆ â„’(ğ‘‰, ğ‘Š) is an isometry and ğ‘£ âˆˆ ğ‘‰ is such that ğ‘†ğ‘£ = 0, then â€–ğ‘£â€– = â€–ğ‘†ğ‘£â€– = â€–0â€– = 0, which implies that ğ‘£ = 0. Thus every isometry is injective. 7.45 example:orthonormal basis maps to orthonormal list âŸ¹ isometry Suppose ğ‘’1, â€¦, ğ‘’ğ‘› is an orthonormal basis of ğ‘‰ and ğ‘”1, â€¦, ğ‘”ğ‘› is an orthonormal list in ğ‘Š. Let ğ‘† âˆˆ â„’(ğ‘‰, ğ‘Š) be the linear map such that ğ‘†ğ‘’ğ‘˜ = ğ‘”ğ‘˜ for each ğ‘˜ = 1, â€¦, ğ‘›. To show that ğ‘† is an isometry, suppose ğ‘£ âˆˆ ğ‘‰. Then 7.46 ğ‘£ = âŸ¨ğ‘£, ğ‘’1âŸ©ğ‘’1 + â‹¯ + âŸ¨ğ‘£, ğ‘’ğ‘›âŸ©ğ‘’ğ‘› and 7.47 â€–ğ‘£â€– 2 = âˆ£âŸ¨ğ‘£, ğ‘’1âŸ©âˆ£2 + â‹¯ + âˆ£âŸ¨ğ‘£, ğ‘’ğ‘›âŸ©âˆ£2, where we have used 6.30(b). Applying ğ‘† to both sides of 7.46 gives ğ‘†ğ‘£ = âŸ¨ğ‘£, ğ‘’1âŸ©ğ‘†ğ‘’1 + â‹¯ + âŸ¨ğ‘£, ğ‘’ğ‘›âŸ©ğ‘†ğ‘’ğ‘› = âŸ¨ğ‘£, ğ‘’1âŸ©ğ‘”1 + â‹¯ + âŸ¨ğ‘£, ğ‘’ğ‘›âŸ©ğ‘”ğ‘›. Thus 7.48 â€–ğ‘†ğ‘£â€– 2 = âˆ£âŸ¨ğ‘£, ğ‘’1âŸ©âˆ£2 + â‹¯ + |âŸ¨ğ‘£, ğ‘’ğ‘›âŸ©| 2. Comparing 7.47 and 7.48 shows that â€–ğ‘£â€– = â€–ğ‘†ğ‘£â€–. Thus ğ‘† is an isometry. The next result gives conditions equivalent to being an isometry. The equiv- alence of (a) and (c) shows that a linear map is an isometry if and only if it preserves inner products. The equivalence of (a) and (d) shows that a linear map is an isometry if and only if it maps some orthonormal basis to an orthonormal list. Thus the isometries given by Example 7.45 include all isometries. Furthermore, a linear map is an isometry if and only if it maps every orthonormal basis to an orthonormal list [because whether or not (a) holds does not depend on the basis ğ‘’1, â€¦, ğ‘’ğ‘›]. Section 7D Isometries, Unitary Operators, and Matrix Factorization 259 The equivalence of (a) and (e) in the next result shows that a linear map is an isometry if and only if the columns of its matrix (with respect to any orthonormal bases) form an orthonormal list. Here we are identifying the columns of an ğ‘š-by-ğ‘› matrix with elements of ğ…ğ‘š and then using the Euclidean inner product on ğ…ğ‘š. 7.49 characterization of isometries Suppose ğ‘† âˆˆ â„’(ğ‘‰, ğ‘Š). Suppose ğ‘’1, â€¦, ğ‘’ğ‘› is an orthonormal basis of ğ‘‰ and ğ‘“1, â€¦, ğ‘“ğ‘š is an orthonormal basis of ğ‘Š. Then the following are equivalent. (a) ğ‘† is an isometry. (b) ğ‘†âˆ—ğ‘† = ğ¼. (c) âŸ¨ğ‘†ğ‘¢, ğ‘†ğ‘£âŸ© = âŸ¨ğ‘¢, ğ‘£âŸ©for all ğ‘¢, ğ‘£ âˆˆ ğ‘‰. (d) ğ‘†ğ‘’1, â€¦, ğ‘†ğ‘’ğ‘› is an orthonormal list in ğ‘Š. (e) The columns of â„³(ğ‘†, (ğ‘’1, â€¦, ğ‘’ğ‘›), ( ğ‘“1, â€¦, ğ‘“ğ‘š))form an orthonormal list in ğ…ğ‘š with respect to the Euclidean inner product. Proof First suppose (a) holds, so ğ‘† is an isometry. If ğ‘£ âˆˆ ğ‘‰ then âŸ¨(ğ¼ âˆ’ ğ‘†âˆ—ğ‘†)ğ‘£, ğ‘£âŸ©= âŸ¨ğ‘£, ğ‘£âŸ© âˆ’âŸ¨ğ‘†âˆ—ğ‘†ğ‘£, ğ‘£âŸ©= â€–ğ‘£â€–2 âˆ’ âŸ¨ğ‘†ğ‘£, ğ‘†ğ‘£âŸ© = â€–ğ‘£â€– 2 âˆ’ â€–ğ‘†ğ‘£â€–2 = 0. Hence the self-adjoint operator ğ¼ âˆ’ ğ‘†âˆ—ğ‘† equals 0(by 7.16). Thus ğ‘†âˆ—ğ‘† = ğ¼, proving that (a) implies (b). Now suppose (b) holds, so ğ‘†âˆ—ğ‘† = ğ¼. If ğ‘¢, ğ‘£ âˆˆ ğ‘‰ then âŸ¨ğ‘†ğ‘¢, ğ‘†ğ‘£âŸ© =âŸ¨ğ‘†âˆ—ğ‘†ğ‘¢, ğ‘£âŸ©= âŸ¨ğ¼ğ‘¢, ğ‘£âŸ© = âŸ¨ğ‘¢, ğ‘£âŸ©, proving that (b) implies (c). Now suppose that (c) holds, so âŸ¨ğ‘†ğ‘¢, ğ‘†ğ‘£âŸ© = âŸ¨ğ‘¢, ğ‘£âŸ©for all ğ‘¢, ğ‘£ âˆˆ ğ‘‰. Thus if ğ‘—, ğ‘˜ âˆˆ {1, â€¦, ğ‘›}, then âŸ¨ğ‘†ğ‘’ğ‘—, ğ‘†ğ‘’ğ‘˜âŸ© = âŸ¨ğ‘’ğ‘—, ğ‘’ğ‘˜âŸ©. Hence ğ‘†ğ‘’1, â€¦, ğ‘†ğ‘’ğ‘› is an orthonormal list in ğ‘Š, proving that (c) implies (d). Now suppose that (d) holds, so ğ‘†ğ‘’1, â€¦, ğ‘†ğ‘’ğ‘› is an orthonormal list in ğ‘Š. Let ğ´ = â„³(ğ‘†, (ğ‘’1, â€¦, ğ‘’ğ‘›), ( ğ‘“1, â€¦, ğ‘“ğ‘š)). If ğ‘˜, ğ‘Ÿ âˆˆ {1, â€¦, ğ‘›}, then 7.50 ğ‘š âˆ‘ ğ‘— = 1 ğ´ğ‘—, ğ‘˜ğ´ğ‘—, ğ‘Ÿ = âŸ¨ ğ‘š âˆ‘ ğ‘— = 1 ğ´ğ‘—, ğ‘˜ ğ‘“ğ‘—, ğ‘š âˆ‘ ğ‘— = 1 ğ´ğ‘—, ğ‘Ÿ ğ‘“ğ‘—âŸ©= âŸ¨ğ‘†ğ‘’ğ‘˜, ğ‘†ğ‘’ğ‘ŸâŸ© = â§{ â¨{â© 1 if ğ‘˜ = ğ‘Ÿ, 0 if ğ‘˜ â‰  ğ‘Ÿ. The left side of 7.50 is the inner product in ğ…ğ‘š of columns ğ‘˜ and ğ‘Ÿ of ğ´. Thus the columns of ğ´ form an orthonormal list in ğ…ğ‘š, proving that (d) implies (e). Now suppose (e) holds, so the columns of the matrix ğ´ defined in the paragraph above form an orthonormal list in ğ…ğ‘›. Then 7.50 shows that ğ‘†ğ‘’1, â€¦, ğ‘†ğ‘’ğ‘› is an orthonormal list in ğ‘Š. Thus Example 7.45, with ğ‘†ğ‘’1, â€¦, ğ‘†ğ‘’ğ‘› playing the role of ğ‘”1, â€¦, ğ‘”ğ‘›, shows that ğ‘† is an isometry, proving that (e) implies (a). See Exercises 1 and 11 for additional conditions that are equivalent to being an isometry. 260 Chapter 7 Operators on Inner Product Spaces Unitary Operators In this subsection, we confine our attention to linear maps from a vector space to itself. In other words, we will be working with operators. 7.51 definition:unitary operator An operator ğ‘† âˆˆ â„’(ğ‘‰) is called unitary if ğ‘† is an invertible isometry. Although the words â€œunitaryâ€ and â€œisometryâ€ mean the same thing for operators on finite-dimensional inner product spaces, remember that a uni- tary operator maps a vector space to itself, while an isometry maps a vector space to another (possibly different) vector space. As previously noted, every isometry is injective. Every injective operator on a finite-dimensional vector space is in- vertible (see 3.65). A standing assump- tion for this chapter is that ğ‘‰ is a finite- dimensional inner product space. Thus we could delete the word â€œinvertibleâ€ from the definition above without chang- ing the meaning. The unnecessary word â€œinvertibleâ€ has been retained in the definition above for consistency with the definition readers may encounter when learning about inner product spaces that are not necessarily finite-dimensional. 7.52 example:rotation of ğ‘2 Suppose ğœƒ âˆˆ ğ‘ and ğ‘† is the operator on ğ…2 whose matrix with respect to the standard basis of ğ…2 is ( cos ğœƒ âˆ’ sin ğœƒ sin ğœƒ cos ğœƒ ). The two columns of this matrix form an orthonormal list in ğ…2; hence ğ‘† is an isometry [by the equivalence of (a) and (e) in 7.49]. Thus ğ‘† is a unitary operator. If ğ… = ğ‘, then ğ‘† is the operator of counterclockwise rotation by ğœƒ radians around the origin of ğ‘2. This observation gives us another way to think about why ğ‘† is an isometry, because each rotation around the origin of ğ‘2 preserves norms. The next result (7.53) lists several conditions that are equivalent to being a unitary operator. All the conditions equivalent to being an isometry in 7.49 should be added to this list. The extra conditions in 7.53 arise because of limiting the context to linear maps from a vector space to itself. For example, 7.49 shows that a linear map ğ‘† âˆˆ â„’(ğ‘‰, ğ‘Š) is an isometry if and only if ğ‘†âˆ—ğ‘† = ğ¼, while 7.53 shows that an operator ğ‘† âˆˆ â„’(ğ‘‰) is a unitary operator if and only if ğ‘†âˆ—ğ‘† = ğ‘†ğ‘†âˆ— = ğ¼. Another difference is that 7.49(d) mentions an orthonormal list, while 7.53(d) mentions an orthonormal basis. Also, 7.49(e) mentions the columns of â„³(ğ‘‡), while 7.53(e) mentions the rows of â„³(ğ‘‡). Furthermore, â„³(ğ‘‡) in 7.49(e) is with respect to an orthonormal basis of ğ‘‰ and an orthonormal basis of ğ‘Š, while â„³(ğ‘‡) in 7.53(e) is with respect to a single basis of ğ‘‰ doing double duty. Section 7D Isometries, Unitary Operators, and Matrix Factorization 261 7.53 characterization of unitary operators Suppose ğ‘† âˆˆ â„’(ğ‘‰). Suppose ğ‘’1, â€¦, ğ‘’ğ‘› is an orthonormal basis of ğ‘‰. Then the following are equivalent. (a) ğ‘† is a unitary operator. (b) ğ‘†âˆ—ğ‘† = ğ‘†ğ‘†âˆ— = ğ¼. (c) ğ‘† is invertible and ğ‘† âˆ’1 = ğ‘†âˆ—. (d) ğ‘†ğ‘’1, â€¦, ğ‘†ğ‘’ğ‘› is an orthonormal basis of ğ‘‰. (e) The rows of â„³(ğ‘†, (ğ‘’1, â€¦, ğ‘’ğ‘›))form an orthonormal basis of ğ…ğ‘› with respect to the Euclidean inner product. (f) ğ‘†âˆ— is a unitary operator. Proof First suppose (a) holds, so ğ‘† is a unitary operator. Hence ğ‘†âˆ—ğ‘† = ğ¼ by the equivalence of (a) and (b) in 7.49. Multiply both sides of this equation by ğ‘† âˆ’1 on the right, getting ğ‘†âˆ— = ğ‘†âˆ’1. Thus ğ‘†ğ‘†âˆ— = ğ‘†ğ‘†âˆ’1 = ğ¼, as desired, proving that (a) implies (b). The definitions of invertible and inverse show that (b) implies (c). Now suppose (c) holds, so ğ‘† is invertible and ğ‘†âˆ’1 = ğ‘†âˆ—. Thus ğ‘†âˆ—ğ‘† = ğ¼. Hence ğ‘†ğ‘’1, â€¦, ğ‘†ğ‘’ğ‘› is an orthonormal list in ğ‘‰, by the equivalence of (b) and (d) in 7.49. The length of this list equals dim ğ‘‰. Thus ğ‘†ğ‘’1, â€¦, ğ‘†ğ‘’ğ‘› is an orthonormal basis of ğ‘‰, proving that (c) implies (d). Now suppose (d) holds, so ğ‘†ğ‘’1, â€¦, ğ‘†ğ‘’ğ‘› is an orthonormal basis of ğ‘‰. The equivalence of (a) and (d) in 7.49 shows that ğ‘† is a unitary operator. Thus (ğ‘†âˆ—) âˆ—ğ‘†âˆ— = ğ‘†ğ‘†âˆ— = ğ¼, where the last equation holds because we have already shown that (a) implies (b) in this result. The equation above and the equivalence of (a) and (b) in 7.49 show that ğ‘†âˆ— is an isometry. Thus the columns of â„³(ğ‘†âˆ—, (ğ‘’1, â€¦, ğ‘’ğ‘›))form an orthonormal ba- sis of ğ…ğ‘› [by the equivalence of (a) and (e) of 7.49]. The rows of â„³(ğ‘†, (ğ‘’1, â€¦, ğ‘’ğ‘›)) are the complex conjugates of the columns of â„³(ğ‘†âˆ—, (ğ‘’1, â€¦, ğ‘’ğ‘›)). Thus the rows of â„³(ğ‘†, (ğ‘’1, â€¦, ğ‘’ğ‘›))form an orthonormal basis of ğ…ğ‘›, proving that (d) implies (e). Now suppose (e) holds. Thus the columns of â„³(ğ‘†âˆ—, (ğ‘’1, â€¦, ğ‘’ğ‘›))form an orthonormal basis of ğ…ğ‘›. The equivalence of (a) and (e) in 7.49 shows that ğ‘†âˆ— is an isometry, proving that (e) implies (f). Now suppose (f) holds, so ğ‘†âˆ— is a unitary operator. The chain of implications we have already proved in this result shows that (a) implies (f). Applying this result to ğ‘†âˆ— shows that (ğ‘†âˆ—) âˆ— is a unitary operator, proving that (f) implies (a). We have shown that (a) â‡’ (b) â‡’ (c) â‡’ (d) â‡’ (e) â‡’ (f) â‡’ (a), completing the proof. 262 Chapter 7 Operators on Inner Product Spaces Recall our analogy between ğ‚ and â„’(ğ‘‰). Under this analogy, a complex number ğ‘§ corresponds to an operator ğ‘† âˆˆ â„’(ğ‘‰), and ğ‘§ corresponds to ğ‘†âˆ—. The real numbers (ğ‘§ = ğ‘§)correspond to the self-adjoint operators (ğ‘† = ğ‘†âˆ—), and the nonnegative numbers correspond to the (badly named) positive operators. Another distinguished subset of ğ‚ is the unit circle, which consists of the complex numbers ğ‘§ such that |ğ‘§| = 1. The condition |ğ‘§| = 1is equivalent to the condition ğ‘§ğ‘§ = 1. Under our analogy, this corresponds to the condition ğ‘†âˆ—ğ‘† = ğ¼, which is equivalent to ğ‘† being a unitary operator. Hence the analogy shows that the unit circle in ğ‚ corresponds to the set of unitary operators. In the next two results, this analogy appears in the eigenvalues of unitary operators. Also see Exercise 15 for another example of this analogy. 7.54 eigenvalues of unitary operators have absolute value 1 Suppose ğœ† is an eigenvalue of a unitary operator. Then |ğœ†| = 1. Proof Suppose ğ‘† âˆˆ â„’(ğ‘‰) is a unitary operator and ğœ† is an eigenvalue of ğ‘†. Let ğ‘£ âˆˆ ğ‘‰ be such that ğ‘£ â‰  0and ğ‘†ğ‘£ = ğœ†ğ‘£. Then |ğœ†| â€–ğ‘£â€– = â€–ğœ†ğ‘£â€– = â€–ğ‘†ğ‘£â€– = â€–ğ‘£â€–. Thus |ğœ†| = 1, as desired. The next result characterizes unitary operators on finite-dimensional complex inner product spaces, using the complex spectral theorem as the main tool. 7.55 description of unitary operators on complex inner product spaces Suppose ğ… = ğ‚ and ğ‘† âˆˆ â„’(ğ‘‰). Then the following are equivalent. (a) ğ‘† is a unitary operator. (b) There is an orthonormal basis of ğ‘‰ consisting of eigenvectors of ğ‘† whose corresponding eigenvalues all have absolute value 1. Proof Suppose (a) holds, so ğ‘† is a unitary operator. The equivalence of (a) and (b) in 7.53 shows that ğ‘† is normal. Thus the complex spectral theorem (7.31) shows that there is an orthonormal basis ğ‘’1, â€¦, ğ‘’ğ‘› of ğ‘‰ consisting of eigenvectors of ğ‘†. Every eigenvalue of ğ‘† has absolute value 1(by 7.54), completing the proof that (a) implies (b). Now suppose (b) holds. Let ğ‘’1, â€¦, ğ‘’ğ‘› be an orthonormal basis of ğ‘‰ consisting of eigenvectors of ğ‘† whose corresponding eigenvalues ğœ†1, â€¦, ğœ†ğ‘› all have absolute value 1. Then ğ‘†ğ‘’1, â€¦, ğ‘†ğ‘’ğ‘› is also an orthonormal basis of ğ‘‰ because âŸ¨ğ‘†ğ‘’ğ‘—, ğ‘†ğ‘’ğ‘˜âŸ© = âŸ¨ğœ†ğ‘—ğ‘’ğ‘—, ğœ†ğ‘˜ğ‘’ğ‘˜âŸ© = ğœ†ğ‘— ğœ†ğ‘˜âŸ¨ğ‘’ğ‘—, ğ‘’ğ‘˜âŸ© = â§{ â¨{â© 0 if ğ‘— â‰  ğ‘˜, 1 if ğ‘— = ğ‘˜ for all ğ‘—, ğ‘˜ = 1, â€¦, ğ‘›. Thus the equivalence of (a) and (d) in 7.53 shows that ğ‘† is unitary, proving that (b) implies (a). Section 7D Isometries, Unitary Operators, and Matrix Factorization 263 QR Factorization In this subsection, we shift our attention from operators to matrices. This switch should give you good practice in identifying an operator with a square matrix (after picking a basis of the vector space on which the operator is defined). You should also become more comfortable with translating concepts and results back and forth between the context of operators and the context of square matrices. When starting with ğ‘›-by-ğ‘› matrices instead of operators, unless otherwise specified assume that the associated operators live onğ…ğ‘› (with the Euclidean inner product) and that their matrices are computed with respect to the standard basis of ğ…ğ‘›. We begin by making the following definition, transferring the notion of a unitary operator to a unitary matrix. 7.56 definition:unitary matrix An ğ‘›-by-ğ‘› matrix is called unitary if its columns form an orthonormal list in ğ…ğ‘›. In the definition above, we could have replaced â€œorthonormal list inğ…ğ‘›â€ with â€œorthonormal basis ofğ…ğ‘›â€ because every orthonormal list of lengthğ‘› in an ğ‘›- dimensional inner product space is an orthonormal basis. If ğ‘† âˆˆ â„’(ğ‘‰) and ğ‘’1, â€¦, ğ‘’ğ‘› and ğ‘“1, â€¦, ğ‘“ğ‘› are orthonormal bases of ğ‘‰, then ğ‘† is a unitary operator if and only if â„³(ğ‘†, (ğ‘’1, â€¦, ğ‘’ğ‘›), ( ğ‘“1, â€¦, ğ‘“ğ‘›))is a unitary matrix, as shown by the equivalence of (a) and (e) in 7.49. Also note that we could also have replaced â€œcolumnsâ€ in the definition above with â€œrowsâ€ by using the equivalence between conditions (a) and (e) in 7.53. The next result, whose proof will be left as an exercise for the reader, gives some equivalent conditions for a square matrix to be unitary. In (c), ğ‘„ğ‘£ denotes the matrix product of ğ‘„ and ğ‘£, identifying elements of ğ…ğ‘› with ğ‘›-by-1matrices (sometimes called column vectors). The norm in (c) below is the usual Euclidean norm on ğ…ğ‘› that comes from the Euclidean inner product. In (d), ğ‘„âˆ— denotes the conjugate transpose of the matrix ğ‘„, which corresponds to the adjoint of the associated operator. 7.57 characterizations of unitary matrices Suppose ğ‘„ is an ğ‘›-by-ğ‘› matrix. Then the following are equivalent. (a) ğ‘„ is a unitary matrix. (b) The rows of ğ‘„ form an orthonormal list in ğ…ğ‘›. (c) â€–ğ‘„ğ‘£â€– = â€–ğ‘£â€– for every ğ‘£ âˆˆ ğ…ğ‘›. (d) ğ‘„âˆ—ğ‘„ = ğ‘„ğ‘„âˆ— = ğ¼, the ğ‘›-by-ğ‘› matrix with 1â€™s on the diagonal and 0â€™s elsewhere. 264 Chapter 7 Operators on Inner Product Spaces The QR factorization stated and proved below is the main tool in the widely used QR algorithm (not discussed here) for finding good approximations to eigenvalues and eigenvectors of square matrices. In the result below, if the matrix ğ´ is in ğ…ğ‘›, ğ‘›, then the matrices ğ‘„ and ğ‘… are also in ğ…ğ‘›, ğ‘›. 7.58 QR factorization Suppose ğ´ is a square matrix with linearly independent columns. Then there exist unique matrices ğ‘„ and ğ‘… such that ğ‘„ is unitary, ğ‘… is upper triangular with only positive numbers on its diagonal, and ğ´ = ğ‘„ğ‘…. Proof Let ğ‘£1, â€¦, ğ‘£ğ‘› denote the columns of ğ´, thought of as elements of ğ…ğ‘›. Apply the Gramâ€“Schmidt procedure (6.32) to the list ğ‘£1, â€¦, ğ‘£ğ‘›, getting an orthonormal basis ğ‘’1, â€¦, ğ‘’ğ‘› of ğ…ğ‘› such that 7.59 span(ğ‘£1, â€¦, ğ‘£ğ‘˜) = span(ğ‘’1, â€¦, ğ‘’ğ‘˜) for each ğ‘˜ = 1, â€¦, ğ‘›. Let ğ‘… be the ğ‘›-by-ğ‘› matrix defined by ğ‘…ğ‘—, ğ‘˜ = âŸ¨ğ‘£ğ‘˜, ğ‘’ğ‘—âŸ©, where ğ‘…ğ‘—, ğ‘˜ denotes the entry in row ğ‘—, column ğ‘˜ of ğ‘…. If ğ‘— > ğ‘˜, then ğ‘’ğ‘— is orthogonal to span(ğ‘’1, â€¦, ğ‘’ğ‘˜) and hence ğ‘’ğ‘— is orthogonal to ğ‘£ğ‘˜ (by 7.59). In other words, if ğ‘— > ğ‘˜ then âŸ¨ğ‘£ğ‘˜, ğ‘’ğ‘—âŸ© = 0. Thus ğ‘… is an upper-triangular matrix. Let ğ‘„ be the unitary matrix whose columns are ğ‘’1, â€¦, ğ‘’ğ‘›. If ğ‘˜ âˆˆ {1, â€¦, ğ‘›}, then the ğ‘˜th column of ğ‘„ğ‘… equals a linear combination of the columns of ğ‘„, with the coefficients for the linear combination coming from the ğ‘˜th column of ğ‘…â€”see 3.51(a). Hence the ğ‘˜th column of ğ‘„ğ‘… equals âŸ¨ğ‘£ğ‘˜, ğ‘’1âŸ©ğ‘’1 + â‹¯ + âŸ¨ğ‘£ğ‘˜, ğ‘’ğ‘˜âŸ©ğ‘’ğ‘˜, which equals ğ‘£ğ‘˜ [by 6.30(a)], the ğ‘˜th column of ğ´. Thus ğ´ = ğ‘„ğ‘…, as desired. The equations defining the Gramâ€“Schmidt procedure (see6.32) show that each ğ‘£ğ‘˜ equals a positive multiple of ğ‘’ğ‘˜ plus a linear combination of ğ‘’1, â€¦, ğ‘’ğ‘˜ âˆ’ 1. Thus each âŸ¨ğ‘£ğ‘˜, ğ‘’ğ‘˜âŸ©is a positive number. Hence all entries on the diagonal of ğ‘… are positive numbers, as desired. Finally, to show that ğ‘„ and ğ‘… are unique, suppose we also have ğ´ = Ì‚ğ‘„ Ì‚ğ‘…, where Ì‚ğ‘„ is unitary and Ì‚ğ‘… is upper triangular with only positive numbers on its diagonal. Let ğ‘1, â€¦, ğ‘ğ‘› denote the columns of Ì‚ğ‘„. Thinking of matrix multiplication as above, we see that each ğ‘£ğ‘˜ is a linear combination of ğ‘1, â€¦, ğ‘ğ‘˜, with the coefficients coming from the ğ‘˜th column of Ì‚ğ‘…. This implies that span(ğ‘£1, â€¦, ğ‘£ğ‘˜) = span(ğ‘1, â€¦, ğ‘ğ‘˜) and âŸ¨ğ‘£ğ‘˜, ğ‘ğ‘˜âŸ© > 0. The uniqueness of the orthonormal lists satisfying these conditions (see Exercise 10 in Section 6B) now shows that ğ‘ğ‘˜ = ğ‘’ğ‘˜ for each ğ‘˜ = 1, â€¦, ğ‘›. Hence Ì‚ğ‘„ = ğ‘„, which then implies that Ì‚ğ‘… = ğ‘…, completing the proof of uniqueness. Section 7D Isometries, Unitary Operators, and Matrix Factorization 265 The proof of the QR factorization shows that the columns of the unitary matrix can be computed by applying the Gramâ€“Schmidt procedure to the columns of the matrix to be factored. The next example illustrates the computation of the QR factorization based on the proof that we just completed. 7.60 example:QR factorization of a 3-by-3matrix To find the QR factorization of the matrix ğ´ = â›âœâœâœ â 1 2 1 0 1 âˆ’4 0 3 2 ââŸâŸâŸ â  , follow the proof of 7.58. Thus set ğ‘£1, ğ‘£2, ğ‘£3 equal to the columns of ğ´: ğ‘£1 = (1, 0, 0), ğ‘£2 = (2, 1, 3), ğ‘£3 = (1, âˆ’4, 2). Apply the Gramâ€“Schmidt procedure to ğ‘£1, ğ‘£2, ğ‘£3, producing the orthonormal list ğ‘’1 = (1, 0, 0), ğ‘’2 = (0, 1 âˆš10 , 3 âˆš10 ), ğ‘’3 = (0, âˆ’ 3 âˆš10 , 1 âˆš10 ). Still following the proof of 7.58, let ğ‘„ be the unitary matrix whose columns are ğ‘’1, ğ‘’2, ğ‘’3: ğ‘„ = â›âœâœâœâœâœâœâœâœ â 1 0 0 0 1 âˆš10 âˆ’ 3 âˆš10 0 3 âˆš10 1 âˆš10 ââŸâŸâŸâŸâŸâŸâŸâŸ â  . As in the proof of 7.58, let ğ‘… be the 3-by-3matrix whose entry in row ğ‘—, column ğ‘˜ is âŸ¨ğ‘£ğ‘˜, ğ‘’ğ‘—âŸ©, which gives ğ‘… = â›âœâœâœâœâœâœâœâœ â 1 2 1 0 âˆš10 âˆš10 5 0 0 7âˆš10 5 ââŸâŸâŸâŸâŸâŸâŸâŸ â  . Note that ğ‘… is indeed an upper-triangular matrix with only positive numbers on the diagonal, as required by the QR factorization. Now matrix multiplication can verify that ğ´ = ğ‘„ğ‘… is the desired factorization of ğ´: ğ‘„ğ‘… = â›âœâœâœâœâœâœâœâœ â 1 0 0 0 1 âˆš10 âˆ’ 3 âˆš10 0 3 âˆš10 1 âˆš10 ââŸâŸâŸâŸâŸâŸâŸâŸ â  â›âœâœâœâœâœâœâœâœ â 1 2 1 0 âˆš10 âˆš10 5 0 0 7âˆš10 5 ââŸâŸâŸâŸâŸâŸâŸâŸ â  = â›âœâœâœ â 1 2 1 0 1 âˆ’4 0 3 2 ââŸâŸâŸ â  = ğ´. Thus ğ´ = ğ‘„ğ‘…, as expected. The QR factorization will be the major tool used in the proof of the Cholesky factorization (7.63) in the next subsection. For another nice application of the QR factorization, see the proof of Hadamardâ€™s inequality (9.66). 266 Chapter 7 Operators on Inner Product Spaces If a QR factorization is available, then it can be used to solve a corresponding system of linear equations without using Gaussian elimination. Specifically, suppose ğ´ is an ğ‘›-by-ğ‘› square matrix with linearly independent columns. Suppose that ğ‘ âˆˆ ğ…ğ‘› and we want to solve the equation ğ´ğ‘¥ = ğ‘ for ğ‘¥ = (ğ‘¥1, â€¦, ğ‘¥ğ‘›) âˆˆ ğ…ğ‘› (as usual, we are identifying elements of ğ…ğ‘› with ğ‘›-by-1column vectors). Suppose ğ´ = ğ‘„ğ‘…, where ğ‘„ is unitary and ğ‘… is upper triangular with only positive numbers on its diagonal (ğ‘„ and ğ‘… are computable from ğ´ using just the Gramâ€“Schmidt procedure, as shown in the proof of 7.58). The equation ğ´ğ‘¥ = ğ‘ is equivalent to the equation ğ‘„ğ‘…ğ‘¥ = ğ‘. Multiplying both sides of this last equation by ğ‘„âˆ— on the left and using 7.57(d) gives the equation ğ‘…ğ‘¥ = ğ‘„âˆ—ğ‘. The matrix ğ‘„âˆ— is the conjugate transpose of the matrix ğ‘„. Thus computing ğ‘„âˆ—ğ‘ is straightforward. Because ğ‘… is an upper-triangular matrix with positive numbers on its diagonal, the system of linear equations represented by the equation above can quickly be solved by first solving forğ‘¥ğ‘›, then for ğ‘¥ğ‘› âˆ’ 1, and so on. Cholesky Factorization We begin this subsection with a characterization of positive invertible operators in terms of inner products. 7.61 positive invertible operator A self-adjoint operator ğ‘‡ âˆˆ â„’(ğ‘‰) is a positive invertible operator if and only if âŸ¨ğ‘‡ğ‘£, ğ‘£âŸ© > 0for every nonzero ğ‘£ âˆˆ ğ‘‰. Proof First suppose ğ‘‡ is a positive invertible operator. If ğ‘£ âˆˆ ğ‘‰ and ğ‘£ â‰  0, then because ğ‘‡ is invertible we have ğ‘‡ğ‘£ â‰  0. This implies that âŸ¨ğ‘‡ğ‘£, ğ‘£âŸ© â‰  0(by 7.43). Hence âŸ¨ğ‘‡ğ‘£, ğ‘£âŸ© > 0. To prove the implication in the other direction, suppose now that âŸ¨ğ‘‡ğ‘£, ğ‘£âŸ© > 0 for every nonzero ğ‘£ âˆˆ ğ‘‰. Thus ğ‘‡ğ‘£ â‰  0for every nonzero ğ‘£ âˆˆ ğ‘‰. Hence ğ‘‡ is injective. Thus ğ‘‡ is invertible, as desired. The next definition transfers the result above to the language of matrices. Here we are using the usual Euclidean inner product on ğ…ğ‘› and identifying elements of ğ…ğ‘› with ğ‘›-by-1column vectors. 7.62 definition:positive definite A matrix ğµ âˆˆ ğ…ğ‘›, ğ‘› is called positive definite if ğµâˆ— = ğµ and âŸ¨ğµğ‘¥, ğ‘¥âŸ© > 0 for every nonzero ğ‘¥ âˆˆ ğ…ğ‘›. Section 7D Isometries, Unitary Operators, and Matrix Factorization 267 A matrix is upper triangular if and only if its conjugate transpose is lower triangular (meaning that all entries above the diagonal are 0). The factorization below, which has important consequences in computational linear algebra, writes a positive definite matrix as the product of a lower triangular matrix and its conjugate transpose. Our next result is solely about matrices, although the proof makes use of the identification of results about operators with results about square matrices. In the result below, if the matrix ğµ is in ğ…ğ‘›, ğ‘›, then the matrix ğ‘… is also in ğ…ğ‘›, ğ‘›. 7.63 Cholesky factorization Suppose ğµ is a positive definite matrix. Then there exists a unique upper- triangular matrix ğ‘… with only positive numbers on its diagonal such that ğµ = ğ‘…âˆ—ğ‘…. Proof Because ğµ is positive definite, there exists an invertible square matrixğ´ of the same size as ğµ such that ğµ = ğ´âˆ—ğ´ [by the equivalence of (a) and (f) in 7.38]. Let ğ´ = ğ‘„ğ‘… be the QR factorization of ğ´ (see 7.58), where ğ‘„ is unitary and ğ‘… is upper triangular with only positive numbers on its diagonal. Then ğ´âˆ— = ğ‘…âˆ—ğ‘„âˆ—. AndrÃ©-Louis Cholesky (1875â€“1918) discovered this factorization, which was published posthumously in 1924. Thus ğµ = ğ´âˆ—ğ´ = ğ‘…âˆ—ğ‘„âˆ—ğ‘„ğ‘… = ğ‘…âˆ—ğ‘…, as desired. To prove the uniqueness part of this result, suppose ğ‘† is an upper-triangular matrix with only positive numbers on its diagonal and ğµ = ğ‘†âˆ—ğ‘†. The matrix ğ‘† is invertible because ğµ is invertible (see Exercise 11 in Section 3D). Multiplying both sides of the equation ğµ = ğ‘†âˆ—ğ‘† by ğ‘† âˆ’1 on the left gives the equation ğµğ‘† âˆ’1 = ğ‘†âˆ—. Let ğ´ be the matrix from the first paragraph of this proof. Then (ğ´ğ‘† âˆ’1) âˆ—(ğ´ğ‘† âˆ’1)= (ğ‘†âˆ—) âˆ’1ğ´âˆ—ğ´ğ‘† âˆ’1 = (ğ‘†âˆ—) âˆ’1ğµğ‘† âˆ’1 = (ğ‘†âˆ—) âˆ’1ğ‘†âˆ— = ğ¼. Thus ğ´ğ‘†âˆ’1 is unitary. Hence ğ´ = (ğ´ğ‘†âˆ’1)ğ‘† is a factorization of ğ´ as the product of a unitary matrix and an upper-triangular matrix with only positive numbers on its diagonal. The uniqueness of the QR factorization, as stated in 7.58, now implies that ğ‘† = ğ‘…. In the first paragraph of the proof above, we could have chosenğ´ to be the unique positive definite matrix that is a square root ofğµ (see 7.39). However, the proof was presented with the more general choice of ğ´ because for specific positive definite matricesğµ, it may be easier to find a different choice ofğ´. 268 Chapter 7 Operators on Inner Product Spaces Exercises 7D 1 Suppose dim ğ‘‰ â‰¥ 2and ğ‘† âˆˆ â„’(ğ‘‰, ğ‘Š). Prove that ğ‘† is an isometry if and only if ğ‘†ğ‘’1, ğ‘†ğ‘’2 is an orthonormal list in ğ‘Š for every orthonormal list ğ‘’1, ğ‘’2 of length two in ğ‘‰. 2 Suppose ğ‘‡ âˆˆ â„’(ğ‘‰, ğ‘Š). Prove that ğ‘‡ is a scalar multiple of an isometry if and only if ğ‘‡ preserves orthogonality. The phrase â€œğ‘‡ preserves orthogonalityâ€ means thatâŸ¨ğ‘‡ğ‘¢, ğ‘‡ğ‘£âŸ© = 0for all ğ‘¢, ğ‘£ âˆˆ ğ‘‰ such that âŸ¨ğ‘¢, ğ‘£âŸ© = 0. 3 (a) Show that the product of two unitary operators on ğ‘‰ is a unitary operator. (b) Show that the inverse of a unitary operator on ğ‘‰ is a unitary operator. This exercise shows that the set of unitary operators on ğ‘‰ is a group, where the group operation is the usual product of two operators. 4 Suppose ğ… = ğ‚ and ğ´, ğµ âˆˆ â„’(ğ‘‰) are self-adjoint. Show that ğ´ + ğ‘–ğµ is unitary if and only if ğ´ğµ = ğµğ´ and ğ´ 2 + ğµ 2 = ğ¼. 5 Suppose ğ‘† âˆˆ â„’(ğ‘‰). Prove that the following are equivalent. (a) ğ‘† is a self-adjoint unitary operator. (b) ğ‘† = 2ğ‘ƒ âˆ’ ğ¼for some orthogonal projection ğ‘ƒ on ğ‘‰. (c) There exists a subspace ğ‘ˆ of ğ‘‰ such that ğ‘†ğ‘¢ = ğ‘¢ for every ğ‘¢ âˆˆ ğ‘ˆ and ğ‘†ğ‘¤ = âˆ’ğ‘¤ for every ğ‘¤ âˆˆ ğ‘ˆâŸ‚. 6 Suppose ğ‘‡1, ğ‘‡2 are both normal operators on ğ…3 with 2, 5, 7as eigenvalues. Prove that there exists a unitary operator ğ‘† âˆˆ â„’(ğ…3)such that ğ‘‡1 = ğ‘†âˆ—ğ‘‡2ğ‘†. 7 Give an example of two self-adjoint operators ğ‘‡1, ğ‘‡2 âˆˆ â„’(ğ…4)such that the eigenvalues of both operators are 2, 5, 7but there does not exist a unitary operator ğ‘† âˆˆ â„’(ğ…4)such that ğ‘‡1 = ğ‘†âˆ—ğ‘‡2ğ‘†. Be sure to explain why there is no unitary operator with the required property. 8 Prove or give a counterexample: If ğ‘† âˆˆ â„’(ğ‘‰) and there exists an orthonormal basis ğ‘’1, â€¦, ğ‘’ğ‘› of ğ‘‰ such that â€–ğ‘†ğ‘’ğ‘˜â€– = 1for each ğ‘’ğ‘˜, then ğ‘† is a unitary operator. 9 Suppose ğ… = ğ‚ and ğ‘‡ âˆˆ â„’(ğ‘‰). Suppose every eigenvalue of ğ‘‡ has absolute value 1 and â€–ğ‘‡ğ‘£â€– â‰¤ â€–ğ‘£â€–for every ğ‘£ âˆˆ ğ‘‰. Prove that ğ‘‡ is a unitary operator. 10 Suppose ğ… = ğ‚ and ğ‘‡ âˆˆ â„’(ğ‘‰) is a self-adjoint operator such that â€–ğ‘‡ğ‘£â€– â‰¤ â€–ğ‘£â€– for all ğ‘£ âˆˆ ğ‘‰. (a) Show that ğ¼ âˆ’ ğ‘‡2 is a positive operator. (b) Show that ğ‘‡ + ğ‘–âˆšğ¼ âˆ’ ğ‘‡2 is a unitary operator. 11 Suppose ğ‘† âˆˆ â„’(ğ‘‰). Prove that ğ‘† is a unitary operator if and only if {ğ‘†ğ‘£ âˆ¶ ğ‘£ âˆˆ ğ‘‰ and â€–ğ‘£â€– â‰¤ 1}= {ğ‘£ âˆˆ ğ‘‰ âˆ¶ â€–ğ‘£â€– â‰¤ 1}. 12 Prove or give a counterexample: If ğ‘† âˆˆ â„’(ğ‘‰) is invertible and âˆ¥ğ‘† âˆ’1ğ‘£âˆ¥ = â€–ğ‘†ğ‘£â€– for every ğ‘£ âˆˆ ğ‘‰, then ğ‘† is unitary. Section 7D Isometries, Unitary Operators, and Matrix Factorization 269 13 Explain why the columns of a square matrix of complex numbers form an orthonormal list in ğ‚ ğ‘› if and only if the rows of the matrix form an orthonormal list in ğ‚ ğ‘›. 14 Suppose ğ‘£ âˆˆ ğ‘‰ with â€–ğ‘£â€– = 1and ğ‘ âˆˆ ğ…. Also suppose dim ğ‘‰ â‰¥ 2. Prove that there exists a unitary operator ğ‘† âˆˆ â„’(ğ‘‰) such that âŸ¨ğ‘†ğ‘£, ğ‘£âŸ© = ğ‘if and only if |ğ‘| â‰¤ 1. 15 Suppose ğ‘‡ is a unitary operator on ğ‘‰ such that ğ‘‡ âˆ’ ğ¼ is invertible. (a) Prove that (ğ‘‡ + ğ¼)(ğ‘‡ âˆ’ ğ¼)âˆ’1 is a skew operator (meaning that it equals the negative of its adjoint). (b) Prove that if ğ… = ğ‚, then ğ‘–(ğ‘‡ + ğ¼)(ğ‘‡ âˆ’ ğ¼)âˆ’1 is a self-adjoint operator. The function ğ‘§ â†¦ ğ‘–(ğ‘§ + 1)(ğ‘§ âˆ’ 1) âˆ’1 maps the unit circle in ğ‚ (except for the point 1) to ğ‘. Thus (b) illustrates the analogy between the unitary operators and the unit circle in ğ‚, along with the analogy between the self-adjoint operators and ğ‘. 16 Suppose ğ… = ğ‚ and ğ‘‡ âˆˆ â„’(ğ‘‰) is self-adjoint. Prove that (ğ‘‡ + ğ‘–ğ¼)(ğ‘‡ âˆ’ ğ‘–ğ¼)âˆ’1 is a unitary operator and 1is not an eigenvalue of this operator. 17 Explain why the characterization of unitary matrices given by 7.57 holds. 18 A square matrix ğ´ is called symmetric if it equals its transpose. Prove that if ğ´ is a symmetric matrix with real entries, then there exists a unitary matrix ğ‘„ with real entries such that ğ‘„âˆ—ğ´ğ‘„ is a diagonal matrix. 19 Suppose ğ‘› is a positive integer. For this exercise, we adopt the notation that a typical element ğ‘§ of ğ‚ ğ‘› is denoted by ğ‘§ = (ğ‘§0, ğ‘§1, â€¦, ğ‘§ğ‘› âˆ’ 1). Define linear functionals ğœ”0, ğœ”1, â€¦, ğœ”ğ‘› âˆ’ 1 on ğ‚ ğ‘› by ğœ”ğ‘—(ğ‘§0, ğ‘§1, â€¦, ğ‘§ğ‘› âˆ’ 1) = 1 âˆš ğ‘› ğ‘› âˆ’ 1 âˆ‘ ğ‘š = 0 ğ‘§ğ‘š ğ‘’âˆ’2ğœ‹ğ‘–ğ‘—ğ‘š/ğ‘›. The discrete Fourier transform is the operator â„±âˆ¶ ğ‚ ğ‘› â†’ ğ‚ğ‘› defined by â„±ğ‘§ = (ğœ”0(ğ‘§), ğœ”1(ğ‘§), â€¦, ğœ”ğ‘› âˆ’ 1(ğ‘§)). (a) Show that â„± is a unitary operator on ğ‚ ğ‘›. (b) Show that if (ğ‘§0, â€¦, ğ‘§ğ‘› âˆ’ 1) âˆˆ ğ‚ğ‘› and ğ‘§ğ‘› is defined to equalğ‘§0, then â„±âˆ’1(ğ‘§0, ğ‘§1, â€¦, ğ‘§ğ‘› âˆ’ 1) = â„±(ğ‘§ğ‘›, ğ‘§ğ‘› âˆ’ 1, â€¦, ğ‘§1). (c) Show that â„±4 = ğ¼. The discrete Fourier transform has many important applications in data analysis. The usual Fourier transform involves expressions of the form âˆ« âˆ âˆ’âˆ ğ‘“ (ğ‘¥)ğ‘’âˆ’2ğœ‹ğ‘–ğ‘¡ğ‘¥ğ‘‘ğ‘¥ for complex-valued integrable functions ğ‘“ defined on ğ‘. 20 Suppose ğ´ is a square matrix with linearly independent columns. Prove that there exist unique matrices ğ‘… and ğ‘„ such that ğ‘… is lower triangular with only positive numbers on its diagonal, ğ‘„ is unitary, and ğ´ = ğ‘…ğ‘„. 270 Chapter 7 Operators on Inner Product Spaces 7E Singular Value Decomposition Singular Values We will need the following result in this section. 7.64 properties of ğ‘‡âˆ—ğ‘‡ Suppose ğ‘‡ âˆˆ â„’(ğ‘‰, ğ‘Š). Then (a) ğ‘‡âˆ—ğ‘‡ is a positive operator on ğ‘‰; (b) null ğ‘‡âˆ—ğ‘‡ = null ğ‘‡; (c) range ğ‘‡âˆ—ğ‘‡ = range ğ‘‡âˆ—; (d) dim range ğ‘‡ = dim range ğ‘‡âˆ— = dim range ğ‘‡âˆ—ğ‘‡. Proof (a) We have (ğ‘‡âˆ—ğ‘‡) âˆ— = ğ‘‡âˆ—(ğ‘‡âˆ—) âˆ— = ğ‘‡âˆ—ğ‘‡. Thus ğ‘‡âˆ—ğ‘‡ is self-adjoint. If ğ‘£ âˆˆ ğ‘‰, then âŸ¨(ğ‘‡âˆ—ğ‘‡)ğ‘£, ğ‘£âŸ©= âŸ¨ğ‘‡âˆ—(ğ‘‡ğ‘£), ğ‘£âŸ©= âŸ¨ğ‘‡ğ‘£, ğ‘‡ğ‘£âŸ© = â€–ğ‘‡ğ‘£â€– 2 â‰¥ 0. Thus ğ‘‡âˆ—ğ‘‡ is a positive operator. (b) First suppose ğ‘£ âˆˆ null ğ‘‡âˆ—ğ‘‡. Then â€–ğ‘‡ğ‘£â€– 2 = âŸ¨ğ‘‡ğ‘£, ğ‘‡ğ‘£âŸ© =âŸ¨ğ‘‡âˆ—ğ‘‡ğ‘£, ğ‘£âŸ©= âŸ¨0, ğ‘£âŸ© = 0. Thus ğ‘‡ğ‘£ = 0, proving that null ğ‘‡âˆ—ğ‘‡ âŠ†null ğ‘‡. The inclusion in the other direction is clear, because if ğ‘£ âˆˆ ğ‘‰ and ğ‘‡ğ‘£ = 0, then ğ‘‡âˆ—ğ‘‡ğ‘£ = 0. Thus null ğ‘‡âˆ—ğ‘‡ = null ğ‘‡, completing the proof of (b). (c) We already know from (a) that ğ‘‡âˆ—ğ‘‡ is self-adjoint. Thus range ğ‘‡âˆ—ğ‘‡ = (null ğ‘‡âˆ—ğ‘‡) âŸ‚ = (null ğ‘‡) âŸ‚ = range ğ‘‡âˆ—, where the first and last equalities come from7.6 and the second equality comes from (b). (d) To verify the first equation in (d), note that dim range ğ‘‡ = dim(null ğ‘‡âˆ—) âŸ‚ = dim ğ‘Š âˆ’ dim null ğ‘‡âˆ— = dim range ğ‘‡âˆ—, where the first equality comes from7.6(d), the second equality comes from 6.51, and the last equality comes from the fundamental theorem of linear maps (3.21). The equality dim range ğ‘‡âˆ— = dim range ğ‘‡âˆ—ğ‘‡ follows from (c). Section 7E Singular Value Decomposition 271 The eigenvalues of an operator tell us something about the behavior of the operator. Another collection of numbers, called the singular values, is also useful. Eigenspaces and the notation ğ¸ (used in the examples) were defined in5.52. 7.65 definition:singular values Suppose ğ‘‡ âˆˆ â„’(ğ‘‰, ğ‘Š). The singular values of ğ‘‡ are the nonnegative square roots of the eigenvalues of ğ‘‡âˆ—ğ‘‡, listed in decreasing order, each included as many times as the dimension of the corresponding eigenspace of ğ‘‡âˆ—ğ‘‡. 7.66 example:singular values of an operator on ğ…4 Defineğ‘‡ âˆˆ â„’(ğ…4)by ğ‘‡(ğ‘§1, ğ‘§2, ğ‘§3, ğ‘§4) = (0, 3ğ‘§1, 2ğ‘§2, âˆ’3ğ‘§4). A calculation shows that ğ‘‡âˆ—ğ‘‡(ğ‘§1, ğ‘§2, ğ‘§3, ğ‘§4) = (9ğ‘§1, 4ğ‘§2, 0, 9ğ‘§4), as you should verify. Thus the standard basis of ğ…4 diagonalizes ğ‘‡âˆ—ğ‘‡, and we see that the eigenvalues of ğ‘‡âˆ—ğ‘‡ are 9, 4, and 0. Also, the dimensions of the eigenspaces corresponding to the eigenvalues are dim ğ¸(9, ğ‘‡âˆ—ğ‘‡)= 2 and dim ğ¸(4, ğ‘‡âˆ—ğ‘‡)= 1 and dim ğ¸(0, ğ‘‡âˆ—ğ‘‡)= 1. Taking nonnegative square roots of these eigenvalues of ğ‘‡âˆ—ğ‘‡ and using dimension information from above, we conclude that the singular values of ğ‘‡ are 3, 3, 2, 0. The only eigenvalues of ğ‘‡ are âˆ’3and 0. Thus in this case, the collection of eigenvalues did not pick up the number 2that appears in the definition (and hence the behavior) of ğ‘‡, but the list of singular values does include 2. 7.67 example:singular values of a linear map from ğ…4 to ğ…3 Suppose ğ‘‡ âˆˆ â„’(ğ…4, ğ…3)has matrix (with respect to the standard bases) â›âœâœâœ â 0 0 0 âˆ’5 0 0 0 0 1 1 0 0 ââŸâŸâŸ â  . You can verify that the matrix of ğ‘‡âˆ—ğ‘‡ is â›âœâœâœâœâœâœ â 1 1 0 0 1 1 0 0 0 0 0 0 0 0 0 25 ââŸâŸâŸâŸâŸâŸ â  and that the eigenvalues of the operator ğ‘‡âˆ—ğ‘‡ are 25, 2, 0, with dim ğ¸(25, ğ‘‡âˆ—ğ‘‡)= 1, dim ğ¸(2, ğ‘‡âˆ—ğ‘‡)= 1, and dim ğ¸(0, ğ‘‡âˆ—ğ‘‡)= 2. Thus the singular values of ğ‘‡ are 5, âˆš2, 0, 0. See Exercise 2 for a characterization of the positive singular values. â‰ˆ 100 ğ‘’ Chapter 7 Operators on Inner Product Spaces 7.68 role of positive singular values Suppose that ğ‘‡ âˆˆ â„’(ğ‘‰, ğ‘Š). Then (a) ğ‘‡ is injective âŸº 0is not a singular value of ğ‘‡; (b) the number of positive singular values of ğ‘‡ equals dim range ğ‘‡; (c) ğ‘‡ is surjective âŸº number of positive singular values of ğ‘‡ equals dim ğ‘Š. Proof The linear map ğ‘‡ is injective if and only if null ğ‘‡ = {0}, which happens if and only if null ğ‘‡âˆ—ğ‘‡ = {0}[by 7.64(b)], which happens if and only if 0is not an eigenvalue of ğ‘‡âˆ—ğ‘‡, which happens if and only if 0is not a singular value of ğ‘‡, completing the proof of (a). The spectral theorem applied to ğ‘‡âˆ—ğ‘‡ shows that dim range ğ‘‡âˆ—ğ‘‡ equals the num- ber of positive eigenvalues of ğ‘‡âˆ—ğ‘‡ (counting repetitions). Thus 7.64(c) implies that dim range ğ‘‡ equals the number of positive singular values of ğ‘‡, proving (b). Use (b) and 2.39 to show that (c) holds. The table below compares eigenvalues with singular values. list of eigenvalues list of singular values context: vector spaces context: inner product spaces defined only for linear maps from a vector space to itself defined for linear maps from an inner product space to a possibly different inner product space can be arbitrary real numbers (if ğ… = ğ‘) or complex numbers (if ğ… = ğ‚) are nonnegative numbers can be the empty list if ğ… = ğ‘ length of list equals dimension of domain includes 0 âŸºoperator is not invertible includes 0 âŸºlinear map is not injective no standard order, especially if ğ… = ğ‚ always listed in decreasing order The next result nicely characterizes isometries in terms of singular values. 7.69 isometries characterized by having all singular values equal 1 Suppose that ğ‘† âˆˆ â„’(ğ‘‰, ğ‘Š). Then ğ‘† is an isometry âŸº all singular values of ğ‘† equal 1. Proof We have ğ‘† is an isometry âŸº ğ‘†âˆ—ğ‘† = ğ¼ âŸº all eigenvalues of ğ‘†âˆ—ğ‘† equal 1 âŸº all singular values of ğ‘† equal 1, where the first equivalence comes from7.49 and the second equivalence comes from the spectral theorem (7.29 or 7.31) applied to the self-adjoint operator ğ‘†âˆ—ğ‘†. Section 7E Singular Value Decomposition 273 SVD for Linear Maps and for Matrices The singular value decomposition is useful in computational linear alge- bra because good techniques exist for approximating eigenvalues and eigen- vectors of positive operators such as ğ‘‡âˆ—ğ‘‡, whose eigenvalues and eigenvec- tors lead to the singular value decom- position. The next result shows that every linear map from ğ‘‰ to ğ‘Š has a remarkably clean description in terms of its singular val- ues and orthonormal lists in ğ‘‰ and ğ‘Š. In the next section we will see several important applications of the singular value decomposition (often called the SVD). 7.70 singular value decomposition Suppose ğ‘‡ âˆˆ â„’(ğ‘‰, ğ‘Š) and the positive singular values of ğ‘‡ are ğ‘ 1, â€¦, ğ‘ ğ‘š. Then there exist orthonormal lists ğ‘’1, â€¦, ğ‘’ğ‘š in ğ‘‰ and ğ‘“1, â€¦, ğ‘“ğ‘š in ğ‘Š such that 7.71 ğ‘‡ğ‘£ = ğ‘ 1âŸ¨ğ‘£, ğ‘’1âŸ© ğ‘“1 + â‹¯ + ğ‘ ğ‘šâŸ¨ğ‘£, ğ‘’ğ‘šâŸ© ğ‘“ğ‘š for every ğ‘£ âˆˆ ğ‘‰. Proof Let ğ‘ 1, â€¦, ğ‘ ğ‘› denote the singular values of ğ‘‡ (thus ğ‘› = dim ğ‘‰). Because ğ‘‡âˆ—ğ‘‡ is a positive operator [see 7.64(a)], the spectral theorem implies that there exists an orthonormal basis ğ‘’1, â€¦, ğ‘’ğ‘› of ğ‘‰ with 7.72 ğ‘‡âˆ—ğ‘‡ğ‘’ğ‘˜ = ğ‘ ğ‘˜ 2ğ‘’ğ‘˜ for each ğ‘˜ = 1, â€¦, ğ‘›. For each ğ‘˜ = 1, â€¦, ğ‘š, let 7.73 ğ‘“ğ‘˜ = ğ‘‡ğ‘’ğ‘˜ ğ‘ ğ‘˜ . If ğ‘—, ğ‘˜ âˆˆ {1, â€¦, ğ‘š}, then âŸ¨ ğ‘“ğ‘—, ğ‘“ğ‘˜âŸ© = 1 ğ‘ ğ‘—ğ‘ ğ‘˜ âŸ¨ğ‘‡ğ‘’ğ‘—, ğ‘‡ğ‘’ğ‘˜âŸ© = 1 ğ‘ ğ‘—ğ‘ ğ‘˜ âŸ¨ğ‘’ğ‘—, ğ‘‡âˆ—ğ‘‡ğ‘’ğ‘˜âŸ©= ğ‘ ğ‘˜ ğ‘ ğ‘— âŸ¨ğ‘’ğ‘—, ğ‘’ğ‘˜âŸ© = â§{ â¨{â© 0 if ğ‘— â‰  ğ‘˜, 1 if ğ‘— = ğ‘˜. Thus ğ‘“1, â€¦, ğ‘“ğ‘š is an orthonormal list in ğ‘Š. If ğ‘˜ âˆˆ {1, â€¦, ğ‘›} and ğ‘˜ > ğ‘š, then ğ‘ ğ‘˜ = 0and hence ğ‘‡âˆ—ğ‘‡ğ‘’ğ‘˜ = 0(by 7.72), which implies that ğ‘‡ğ‘’ğ‘˜ = 0[by 7.64(b)]. Suppose ğ‘£ âˆˆ ğ‘‰. Then ğ‘‡ğ‘£ = ğ‘‡(âŸ¨ğ‘£, ğ‘’1âŸ©ğ‘’1 + â‹¯ + âŸ¨ğ‘£, ğ‘’ğ‘›âŸ©ğ‘’ğ‘›) = âŸ¨ğ‘£, ğ‘’1âŸ©ğ‘‡ğ‘’1 + â‹¯ + âŸ¨ğ‘£, ğ‘’ğ‘šâŸ©ğ‘‡ğ‘’ğ‘š = ğ‘ 1âŸ¨ğ‘£, ğ‘’1âŸ© ğ‘“1 + â‹¯ + ğ‘ ğ‘šâŸ¨ğ‘£, ğ‘’ğ‘šâŸ© ğ‘“ğ‘š, where the last index in the first line switched fromğ‘› to ğ‘š in the second line because ğ‘‡ğ‘’ğ‘˜ = 0if ğ‘˜ > ğ‘š (as noted in the paragraph above) and the third line follows from 7.73. The equation above is our desired result. 274 Chapter 7 Operators on Inner Product Spaces Suppose ğ‘‡ âˆˆ â„’(ğ‘‰, ğ‘Š), the positive singular values of ğ‘‡ are ğ‘ 1, â€¦, ğ‘ ğ‘š, and ğ‘’1, â€¦, ğ‘’ğ‘š and ğ‘“1, â€¦, ğ‘“ğ‘š are as in the singular value decomposition 7.70. The orthonormal list ğ‘’1, â€¦, ğ‘’ğ‘š can be extended to an orthonormal basis ğ‘’1, â€¦, ğ‘’dim ğ‘‰ of ğ‘‰ and the orthonormal list ğ‘“1, â€¦, ğ‘“ğ‘š can be extended to an orthonormal basis ğ‘“1, â€¦, ğ‘“dim ğ‘Š of ğ‘Š. The formula 7.71 shows that ğ‘‡ğ‘’ğ‘˜ = â§{ â¨{â© ğ‘ ğ‘˜ ğ‘“ğ‘˜ if 1 â‰¤ ğ‘˜ â‰¤ ğ‘š, 0 if ğ‘š < ğ‘˜ â‰¤dim ğ‘‰. Thus the matrix of ğ‘‡ with respect to the orthonormal bases (ğ‘’1, â€¦, ğ‘’dim ğ‘‰) and ( ğ‘“1, â€¦, ğ‘“dim ğ‘Š) has the simple form â„³(ğ‘‡, (ğ‘’1, â€¦, ğ‘’dim ğ‘‰), ( ğ‘“1, â€¦, ğ‘“dim ğ‘Š)) ğ‘—, ğ‘˜ = â§{ â¨{â© ğ‘ ğ‘˜ if 1 â‰¤ ğ‘— = ğ‘˜ â‰¤ ğ‘š, 0 otherwise. If dim ğ‘‰ = dim ğ‘Š (as happens, for example, if ğ‘Š = ğ‘‰), then the matrix described in the paragraph above is a diagonal matrix. If we extend the definition of diagonal matrix as follows to apply to matrices that are not necessarily square, then we have proved the wonderful result that every linear map from ğ‘‰ to ğ‘Š has a diagonal matrix with respect to appropriate orthonormal bases. 7.74 definition:diagonal matrix An ğ‘€-by-ğ‘ matrix ğ´ is called a diagonal matrix if all entries of the matrix are 0except possibly ğ´ğ‘˜, ğ‘˜ for ğ‘˜ = 1, â€¦, min{ğ‘€, ğ‘}. The table below compares the spectral theorem (7.29 and 7.31) with the singular value decomposition (7.70). spectral theorem singular value decomposition describes only self-adjoint operators (when ğ… = ğ‘) or normal operators (when ğ… = ğ‚) describes arbitrary linear maps from an inner product space to a possibly different inner product space produces a single orthonormal basis produces two orthonormal lists, one for domain space and one for range space, that are not necessarily the same even when range space equals domain space different proofs depending on whether ğ… = ğ‘ or ğ… = ğ‚ same proof works regardless of whether ğ… = ğ‘ or ğ… = ğ‚ The singular value decomposition gives us a new way to understand the adjoint and the inverse of a linear map. Specifically, the next result shows that given a singular value decomposition of a linear map ğ‘‡ âˆˆ â„’(ğ‘‰, ğ‘Š), we can obtain the adjoint of ğ‘‡ simply by interchanging the roles of the ğ‘’â€™s and the ğ‘“ â€™s (see 7.77). Similarly, we can obtain the pseudoinverse ğ‘‡â€  (see 6.68) of ğ‘‡ by interchanging the roles of the ğ‘’â€™s and the ğ‘“ â€™s and replacing each positive singular value ğ‘ ğ‘˜ of ğ‘‡ with 1/ğ‘ ğ‘˜ (see 7.78). Section 7E Singular Value Decomposition 275 Recall that the pseudoinverse ğ‘‡â€  in 7.78 below equals the inverse ğ‘‡âˆ’1 if ğ‘‡ is invertible [see 6.69(a)]. 7.75 singular value decomposition of adjoint and pseudoinverse Suppose ğ‘‡ âˆˆ â„’(ğ‘‰, ğ‘Š) and the positive singular values of ğ‘‡ are ğ‘ 1, â€¦, ğ‘ ğ‘š. Suppose ğ‘’1, â€¦, ğ‘’ğ‘š and ğ‘“1, â€¦, ğ‘“ğ‘š are orthonormal lists in ğ‘‰ and ğ‘Š such that 7.76 ğ‘‡ğ‘£ = ğ‘ 1âŸ¨ğ‘£, ğ‘’1âŸ© ğ‘“1 + â‹¯ + ğ‘ ğ‘šâŸ¨ğ‘£, ğ‘’ğ‘šâŸ© ğ‘“ğ‘š for every ğ‘£ âˆˆ ğ‘‰. Then 7.77 ğ‘‡âˆ—ğ‘¤ = ğ‘ 1âŸ¨ğ‘¤, ğ‘“1âŸ©ğ‘’1 + â‹¯ + ğ‘ ğ‘šâŸ¨ğ‘¤, ğ‘“ğ‘šâŸ©ğ‘’ğ‘š and 7.78 ğ‘‡â€ ğ‘¤ = âŸ¨ğ‘¤, ğ‘“1âŸ© ğ‘ 1 ğ‘’1 + â‹¯ + âŸ¨ğ‘¤, ğ‘“ğ‘šâŸ© ğ‘ ğ‘š ğ‘’ğ‘š for every ğ‘¤ âˆˆ ğ‘Š. Proof If ğ‘£ âˆˆ ğ‘‰ and ğ‘¤ âˆˆ ğ‘Š then âŸ¨ğ‘‡ğ‘£, ğ‘¤âŸ© =âŸ¨ğ‘ 1âŸ¨ğ‘£, ğ‘’1âŸ© ğ‘“1 + â‹¯ + ğ‘ ğ‘šâŸ¨ğ‘£, ğ‘’ğ‘šâŸ© ğ‘“ğ‘š, ğ‘¤âŸ© = ğ‘ 1âŸ¨ğ‘£, ğ‘’1âŸ©âŸ¨ ğ‘“1, ğ‘¤âŸ©+ â‹¯ + ğ‘ ğ‘šâŸ¨ğ‘£, ğ‘’ğ‘šâŸ©âŸ¨ ğ‘“ğ‘š, ğ‘¤âŸ© = âŸ¨ğ‘£, ğ‘ 1âŸ¨ğ‘¤, ğ‘“1âŸ©ğ‘’1 + â‹¯ + ğ‘ ğ‘šâŸ¨ğ‘¤, ğ‘“ğ‘šâŸ©ğ‘’ğ‘šâŸ©. This implies that ğ‘‡âˆ—ğ‘¤ = ğ‘ 1âŸ¨ğ‘¤, ğ‘“1âŸ©ğ‘’1 + â‹¯ + ğ‘ ğ‘šâŸ¨ğ‘¤, ğ‘“ğ‘šâŸ©ğ‘’ğ‘š, proving 7.77. To prove 7.78, suppose ğ‘¤ âˆˆ ğ‘Š. Let ğ‘£ = âŸ¨ğ‘¤, ğ‘“1âŸ© ğ‘ 1 ğ‘’1 + â‹¯ + âŸ¨ğ‘¤, ğ‘“ğ‘šâŸ© ğ‘ ğ‘š ğ‘’ğ‘š. Apply ğ‘‡ to both sides of the equation above, getting ğ‘‡ğ‘£ = âŸ¨ğ‘¤, ğ‘“1âŸ© ğ‘ 1 ğ‘‡ğ‘’1 + â‹¯ + âŸ¨ğ‘¤, ğ‘“ğ‘šâŸ© ğ‘ ğ‘š ğ‘‡ğ‘’ğ‘š = âŸ¨ğ‘¤, ğ‘“1âŸ© ğ‘“1 + â‹¯ + âŸ¨ğ‘¤, ğ‘“ğ‘šâŸ© ğ‘“ğ‘š = ğ‘ƒrange ğ‘‡ ğ‘¤, where the second line holds because 7.76 implies that ğ‘‡ğ‘’ğ‘˜ = ğ‘ ğ‘˜ ğ‘“ğ‘˜ if ğ‘˜ = 1, â€¦, ğ‘š, and the last line above holds because 7.76 implies that ğ‘“1, â€¦, ğ‘“ğ‘š spans range ğ‘‡ and thus is an orthonormal basis of range ğ‘‡ [and hence 6.57(i) applies]. The equation above, the observation that ğ‘£ âˆˆ (null ğ‘‡) âŸ‚ [see Exercise 8(b)], and the definition of ğ‘‡â€ ğ‘¤ (see 6.68) show that ğ‘£ = ğ‘‡â€ ğ‘¤, proving 7.78. 276 Chapter 7 Operators on Inner Product Spaces 7.79 example:finding a singular value decomposition Defineğ‘‡ âˆˆ â„’(ğ…4, ğ…3)by ğ‘‡(ğ‘¥1, ğ‘¥2, ğ‘¥3, ğ‘¥4) = (âˆ’5ğ‘¥4, 0, ğ‘¥1 + ğ‘¥2). We want to find a singular value decomposition ofğ‘‡. The matrix of ğ‘‡ (with respect to the standard bases) is â›âœâœâœ â 0 0 0 âˆ’5 0 0 0 0 1 1 0 0 ââŸâŸâŸ â  . Thus, as discussed in Example 7.67, the matrix of ğ‘‡âˆ—ğ‘‡ is â›âœâœâœâœâœâœ â 1 1 0 0 1 1 0 0 0 0 0 0 0 0 0 25 ââŸâŸâŸâŸâŸâŸ â  , and the positive eigenvalues of ğ‘‡âˆ—ğ‘‡ are 25, 2, with dim ğ¸(25, ğ‘‡âˆ—ğ‘‡)= 1and dim ğ¸(2, ğ‘‡âˆ—ğ‘‡)= 1. Hence the positive singular values of ğ‘‡ are 5, âˆš2. Thus to find a singular value decomposition ofğ‘‡, we must find an orthonormal list ğ‘’1, ğ‘’2 in ğ…4 and an orthonormal list ğ‘“1, ğ‘“2 in ğ…3 such that ğ‘‡ğ‘£ = 5âŸ¨ğ‘£, ğ‘’1âŸ© ğ‘“1 + âˆš2âŸ¨ğ‘£, ğ‘’2âŸ© ğ‘“2 for all ğ‘£ âˆˆ ğ…4. An orthonormal basis of ğ¸(25, ğ‘‡âˆ—ğ‘‡)is the vector (0, 0, 0, 1); an orthonormal basis of ğ¸(2, ğ‘‡âˆ—ğ‘‡)is the vector ( 1 âˆš2 , 1 âˆš2 , 0, 0). Thus, following the proof of 7.70, we take ğ‘’1 = (0, 0, 0, 1) and ğ‘’2 = ( 1 âˆš2 , 1 âˆš2 , 0, 0) and ğ‘“1 = ğ‘‡ğ‘’1 5 = (âˆ’1, 0, 0) and ğ‘“2 = ğ‘‡ğ‘’2 âˆš2 = (0, 0, 1). Then, as expected, we see that ğ‘’1, ğ‘’2 is an orthonormal list in ğ…4 and ğ‘“1, ğ‘“2 is an orthonormal list in ğ…3 and ğ‘‡ğ‘£ = 5âŸ¨ğ‘£, ğ‘’1âŸ© ğ‘“1 + âˆš2âŸ¨ğ‘£, ğ‘’2âŸ© ğ‘“2 for all ğ‘£ âˆˆ ğ…4. Thus we have found a singular value decomposition of ğ‘‡. The next result translates the singular value decomposition from the context of linear maps to the context of matrices. Specifically, the following result gives a factorization of an arbitrary matrix as the product of three nice matrices. The proof gives an explicit construction of these three matrices in terms of the singular value decomposition. In the next result, the phrase â€œorthogonal columnsâ€ should be interpreted to mean that the columns are orthogonal with respect to the standard Euclidean inner product. Section 7E Singular Value Decomposition 277 7.80 matrix version of SVD Suppose ğ´ is an ğ‘€-by-ğ‘› matrix of rank ğ‘š â‰¥ 1. Then there exist an ğ‘€-by-ğ‘š matrix ğµ with orthonormal columns, an ğ‘š-by-ğ‘š diagonal matrix ğ· with positive numbers on the diagonal, and an ğ‘›-by-ğ‘š matrix ğ¶ with orthonormal columns such that ğ´ = ğµğ·ğ¶âˆ—. Proof Let ğ‘‡âˆ¶ ğ…ğ‘› â†’ ğ…ğ‘€ be the linear map whose matrix with respect to the standard bases equals ğ´. Then dim range ğ‘‡ = ğ‘š (by 3.78). Let 7.81 ğ‘‡ğ‘£ = ğ‘ 1âŸ¨ğ‘£, ğ‘’1âŸ© ğ‘“1 + â‹¯ + ğ‘ ğ‘šâŸ¨ğ‘£, ğ‘’ğ‘šâŸ© ğ‘“ğ‘š be a singular value decomposition of ğ‘‡. Let ğµ = the ğ‘€-by-ğ‘š matrix whose columns are ğ‘“1, â€¦, ğ‘“ğ‘š, ğ· = the ğ‘š-by-ğ‘š diagonal matrix whose diagonal entries are ğ‘ 1, â€¦, ğ‘ ğ‘š, ğ¶ = the ğ‘›-by-ğ‘š matrix whose columns are ğ‘’1, â€¦, ğ‘’ğ‘š. Let ğ‘¢1, â€¦, ğ‘¢ğ‘š denote the standard basis of ğ…ğ‘š. If ğ‘˜ âˆˆ {1, â€¦, ğ‘š} then (ğ´ğ¶ âˆ’ ğµğ·)ğ‘¢ğ‘˜ = ğ´ğ‘’ğ‘˜ âˆ’ ğµ(ğ‘ ğ‘˜ğ‘¢ğ‘˜) = ğ‘ ğ‘˜ ğ‘“ğ‘˜ âˆ’ ğ‘ ğ‘˜ ğ‘“ğ‘˜ = 0. Thus ğ´ğ¶ = ğµğ·. Multiply both sides of this last equation by ğ¶âˆ— (the conjugate transpose of ğ¶) on the right to get ğ´ğ¶ğ¶âˆ— = ğµğ·ğ¶âˆ—. Note that the rows of ğ¶âˆ— are the complex conjugates of ğ‘’1, â€¦, ğ‘’ğ‘š. Thus if ğ‘˜ âˆˆ {1, â€¦, ğ‘š}, then the definition of matrix multiplication shows thatğ¶âˆ—ğ‘’ğ‘˜ = ğ‘¢ğ‘˜; hence ğ¶ğ¶âˆ—ğ‘’ğ‘˜ = ğ‘’ğ‘˜. Thus ğ´ğ¶ğ¶âˆ—ğ‘£ = ğ´ğ‘£ for all ğ‘£ âˆˆ span(ğ‘’1, â€¦, ğ‘’ğ‘š). If ğ‘£ âˆˆ (span(ğ‘’1, â€¦, ğ‘’ğ‘š)) âŸ‚, then ğ´ğ‘£ = 0(as follows from 7.81) and ğ¶âˆ—ğ‘£ = 0 (as follows from the definition of matrix multiplication). Henceğ´ğ¶ğ¶âˆ—ğ‘£ = ğ´ğ‘£ for all ğ‘£ âˆˆ (span(ğ‘’1, â€¦, ğ‘’ğ‘š)) âŸ‚. Because ğ´ğ¶ğ¶âˆ— and ğ´ agree on span(ğ‘’1, â€¦, ğ‘’ğ‘š) and on (span(ğ‘’1, â€¦, ğ‘’ğ‘š)) âŸ‚, we conclude that ğ´ğ¶ğ¶âˆ— = ğ´. Thus the displayed equation above becomes ğ´ = ğµğ·ğ¶âˆ—, as desired. Note that the matrix ğ´ in the result above has ğ‘€ğ‘› entries. In comparison, the matrices ğµ, ğ·, and ğ¶ above have a total of ğ‘š(ğ‘€ + ğ‘š + ğ‘›) entries. Thus if ğ‘€ and ğ‘› are large numbers and the rank ğ‘š is considerably less than ğ‘€ and ğ‘›, then the number of entries that must be stored on a computer to represent ğ´ is considerably less than ğ‘€ğ‘›. 278 Chapter 7 Operators on Inner Product Spaces Exercises 7E 1 Suppose ğ‘‡ âˆˆ â„’(ğ‘‰, ğ‘Š). Show that ğ‘‡ = 0if and only if all singular values of ğ‘‡ are 0. 2 Suppose ğ‘‡ âˆˆ â„’(ğ‘‰, ğ‘Š) and ğ‘  > 0. Prove that ğ‘  is a singular value of ğ‘‡ if and only if there exist nonzero vectors ğ‘£ âˆˆ ğ‘‰ and ğ‘¤ âˆˆ ğ‘Š such that ğ‘‡ğ‘£ = ğ‘ ğ‘¤ and ğ‘‡âˆ—ğ‘¤ = ğ‘ ğ‘£. The vectors ğ‘£, ğ‘¤ satisfying both equations above are called a Schmidt pair. Erhard Schmidt introduced the concept of singular values in 1907. 3 Give an example of ğ‘‡ âˆˆ â„’(ğ‚ 2)such that 0is the only eigenvalue of ğ‘‡ and the singular values of ğ‘‡ are 5, 0. 4 Suppose that ğ‘‡ âˆˆ â„’(ğ‘‰, ğ‘Š), ğ‘ 1 is the largest singular value of ğ‘‡, and ğ‘ ğ‘› is the smallest singular value of ğ‘‡. Prove that {â€–ğ‘‡ğ‘£â€– âˆ¶ ğ‘£ âˆˆ ğ‘‰ and â€–ğ‘£â€– = 1}= [ğ‘ ğ‘›, ğ‘ 1]. 5 Suppose ğ‘‡ âˆˆ â„’(ğ‚ 2)is defined byğ‘‡(ğ‘¥, ğ‘¦) = (âˆ’4ğ‘¦, ğ‘¥). Find the singular values of ğ‘‡. 6 Find the singular values of the differentiation operator ğ· âˆˆ â„’(ğ’«2(ğ‘)) defined byğ·ğ‘ = ğ‘â€², where the inner product on ğ’«2(ğ‘) is as in Example 6.34. 7 Suppose that ğ‘‡ âˆˆ â„’(ğ‘‰) is self-adjoint or that ğ… = ğ‚ and ğ‘‡ âˆˆ â„’(ğ‘‰) is normal. Let ğœ†1, â€¦, ğœ†ğ‘› be the eigenvalues of ğ‘‡, each included in this list as many times as the dimension of the corresponding eigenspace. Show that the singular values of ğ‘‡ are |ğœ†1|, â€¦, |ğœ†ğ‘›|, after these numbers have been sorted into decreasing order. 8 Suppose ğ‘‡ âˆˆ â„’(ğ‘‰, ğ‘Š). Suppose ğ‘ 1 â‰¥ ğ‘ 2 â‰¥ â‹¯ â‰¥ ğ‘ ğ‘š > 0and ğ‘’1, â€¦, ğ‘’ğ‘š is an orthonormal list in ğ‘‰ and ğ‘“1, â€¦, ğ‘“ğ‘š is an orthonormal list in ğ‘Š such that ğ‘‡ğ‘£ = ğ‘ 1âŸ¨ğ‘£, ğ‘’1âŸ© ğ‘“1 + â‹¯ + ğ‘ ğ‘šâŸ¨ğ‘£, ğ‘’ğ‘šâŸ© ğ‘“ğ‘š for every ğ‘£ âˆˆ ğ‘‰. (a) Prove that ğ‘“1, â€¦, ğ‘“ğ‘š is an orthonormal basis of range ğ‘‡. (b) Prove that ğ‘’1, â€¦, ğ‘’ğ‘š is an orthonormal basis of (null ğ‘‡) âŸ‚. (c) Prove that ğ‘ 1, â€¦, ğ‘ ğ‘š are the positive singular values of ğ‘‡. (d) Prove that if ğ‘˜ âˆˆ {1, â€¦, ğ‘š}, then ğ‘’ğ‘˜ is an eigenvector of ğ‘‡âˆ—ğ‘‡ with corre- sponding eigenvalue ğ‘ ğ‘˜ 2. (e) Prove that ğ‘‡ğ‘‡âˆ—ğ‘¤ = ğ‘ 1 2âŸ¨ğ‘¤, ğ‘“1âŸ© ğ‘“1 + â‹¯ + ğ‘ ğ‘š 2âŸ¨ğ‘¤, ğ‘“ğ‘šâŸ© ğ‘“ğ‘š for all ğ‘¤ âˆˆ ğ‘Š. Section 7E Singular Value Decomposition 279 9 Suppose ğ‘‡ âˆˆ â„’(ğ‘‰, ğ‘Š). Show that ğ‘‡ and ğ‘‡âˆ— have the same positive singular values. 10 Suppose ğ‘‡ âˆˆ â„’(ğ‘‰, ğ‘Š) has singular values ğ‘ 1, â€¦, ğ‘ ğ‘›. Prove that if ğ‘‡ is an invertible linear map, then ğ‘‡âˆ’1 has singular values 1 ğ‘ ğ‘› , â€¦, 1 ğ‘ 1 . 11 Suppose that ğ‘‡ âˆˆ â„’(ğ‘‰, ğ‘Š) and ğ‘£1, â€¦, ğ‘£ğ‘› is an orthonormal basis of ğ‘‰. Let ğ‘ 1, â€¦, ğ‘ ğ‘› denote the singular values of ğ‘‡. (a) Prove that â€–ğ‘‡ğ‘£1â€–2 + â‹¯ + â€–ğ‘‡ğ‘£ğ‘›â€– 2 = ğ‘ 1 2 + â‹¯ + ğ‘ ğ‘› 2. (b) Prove that if ğ‘Š = ğ‘‰ and ğ‘‡ is a positive operator, then âŸ¨ğ‘‡ğ‘£1, ğ‘£1âŸ©+ â‹¯ + âŸ¨ğ‘‡ğ‘£ğ‘›, ğ‘£ğ‘›âŸ© = ğ‘ 1 + â‹¯ + ğ‘ ğ‘›. See the comment after Exercise 5 in Section 7A. 12 (a) Give an example of a finite-dimensional vector space and an operatorğ‘‡ on it such that the singular values of ğ‘‡2 do not equal the squares of the singular values of ğ‘‡. (b) Suppose ğ‘‡ âˆˆ â„’(ğ‘‰) is normal. Prove that the singular values of ğ‘‡2 equal the squares of the singular values of ğ‘‡. 13 Suppose ğ‘‡1, ğ‘‡2 âˆˆ â„’(ğ‘‰). Prove that ğ‘‡1 and ğ‘‡2 have the same singular values if and only if there exist unitary operators ğ‘†1, ğ‘†2 âˆˆ â„’(ğ‘‰) such that ğ‘‡1 = ğ‘†1ğ‘‡2ğ‘†2. 14 Suppose ğ‘‡ âˆˆ â„’(ğ‘‰, ğ‘Š). Let ğ‘ ğ‘› denote the smallest singular value of ğ‘‡. Prove that ğ‘ ğ‘›â€–ğ‘£â€– â‰¤ â€–ğ‘‡ğ‘£â€–for every ğ‘£ âˆˆ ğ‘‰. 15 Suppose ğ‘‡ âˆˆ â„’(ğ‘‰) and ğ‘ 1 â‰¥ â‹¯ â‰¥ ğ‘ ğ‘› are the singular values of ğ‘‡. Prove that if ğœ† is an eigenvalue of ğ‘‡, then ğ‘ 1 â‰¥ |ğœ†| â‰¥ ğ‘ ğ‘›. 16 Suppose ğ‘‡ âˆˆ â„’(ğ‘‰, ğ‘Š). Prove that (ğ‘‡âˆ—) â€  = (ğ‘‡â€ ) âˆ—. Compare the result in this exercise to the analogous result for invertible linear maps [see 7.5( f )]. 17 Suppose ğ‘‡ âˆˆ â„’(ğ‘‰). Prove that ğ‘‡ is self-adjoint if and only if ğ‘‡â€  is self- adjoint. Matrices unfold Singular values gleam like stars Order in chaos shines â€”written by ChatGPT with input haiku about SVD 280 Chapter 7 Operators on Inner Product Spaces 7F Consequences of Singular Value Decomposition Norms of Linear Maps The singular value decomposition leads to the following upper bound for â€–ğ‘‡ğ‘£â€–. 7.82 upper bound for â€–ğ‘‡ğ‘£â€– Suppose ğ‘‡ âˆˆ â„’(ğ‘‰, ğ‘Š). Let ğ‘ 1 be the largest singular value of ğ‘‡. Then â€–ğ‘‡ğ‘£â€– â‰¤ ğ‘ 1â€–ğ‘£â€– for all ğ‘£ âˆˆ ğ‘‰. For a lower bound on â€–ğ‘‡ğ‘£â€–, look at Exercise 14 in Section 7E. Proof Let ğ‘ 1, â€¦, ğ‘ ğ‘š denote the positive singular values of ğ‘‡, and let ğ‘’1, â€¦, ğ‘’ğ‘š be an orthonormal list in ğ‘‰ and ğ‘“1, â€¦, ğ‘“ğ‘š be an orthonormal list in ğ‘Š that provide a singular value decomposition of ğ‘‡. Thus 7.83 ğ‘‡ğ‘£ = ğ‘ 1âŸ¨ğ‘£, ğ‘’1âŸ© ğ‘“1 + â‹¯ + ğ‘ ğ‘šâŸ¨ğ‘£, ğ‘’ğ‘šâŸ© ğ‘“ğ‘š for all ğ‘£ âˆˆ ğ‘‰. Hence if ğ‘£ âˆˆ ğ‘‰ then â€–ğ‘‡ğ‘£â€– 2 = ğ‘ 1 2 âˆ£âŸ¨ğ‘£, ğ‘’1âŸ©âˆ£2 + â‹¯ + ğ‘ ğ‘š 2 âˆ£âŸ¨ğ‘£, ğ‘’ğ‘šâŸ©âˆ£2 â‰¤ ğ‘ 1 2(âˆ£âŸ¨ğ‘£, ğ‘’1âŸ©âˆ£2 + â‹¯ + âˆ£âŸ¨ğ‘£, ğ‘’ğ‘šâŸ©âˆ£2) â‰¤ ğ‘ 1 2 â€–ğ‘£â€– 2, where the last inequality follows from Besselâ€™s inequality (6.26). Taking square roots of both sides of the inequality above shows that â€–ğ‘‡ğ‘£â€– â‰¤ ğ‘ 1â€–ğ‘£â€–, as desired. Suppose ğ‘‡ âˆˆ â„’(ğ‘‰, ğ‘Š) and ğ‘ 1 is the largest singular value of ğ‘‡. The result above shows that 7.84 â€–ğ‘‡ğ‘£â€– â‰¤ ğ‘ 1 for all ğ‘£ âˆˆ ğ‘‰ with â€–ğ‘£â€– â‰¤ 1. Taking ğ‘£ = ğ‘’1 in 7.83 shows that ğ‘‡ğ‘’1 = ğ‘ 1 ğ‘“1. Because â€– ğ‘“1â€– = 1, this implies that â€–ğ‘‡ğ‘’1â€– = ğ‘ 1. Thus because â€–ğ‘’1â€– = 1, the inequality in 7.84 leads to the equation 7.85 max{â€–ğ‘‡ğ‘£â€– âˆ¶ ğ‘£ âˆˆ ğ‘‰ and â€–ğ‘£â€– â‰¤ 1}= ğ‘ 1. The equation above is the motivation for the following definition, which defines the norm of ğ‘‡ to be the left side of the equation above without needing to refer to singular values or the singular value decomposition. 7.86 definition:norm of a linear map, â€– â‹… â€– Suppose ğ‘‡ âˆˆ â„’(ğ‘‰, ğ‘Š). Then the norm of ğ‘‡, denoted by â€–ğ‘‡â€–, is defined by â€–ğ‘‡â€– = max{â€–ğ‘‡ğ‘£â€– âˆ¶ ğ‘£ âˆˆ ğ‘‰ and â€–ğ‘£â€– â‰¤ 1}. Section 7F Consequences of Singular Value Decomposition 281 In general, the maximum of an infinite set of nonnegative numbers need not exist. However, the discussion before 7.86 shows that the maximum in the definition of the norm of a linear mapğ‘‡ from ğ‘‰ to ğ‘Š does indeed exist (and equals the largest singular value of ğ‘‡). We now have two different uses of the word norm and the notation â€– â‹… â€–. Our first use of this notation was in connection with an inner product onğ‘‰, when we definedâ€–ğ‘£â€– = âˆšâŸ¨ğ‘£, ğ‘£âŸ©for each ğ‘£ âˆˆ ğ‘‰. Our second use of the norm notation and terminology is with the definition we just made ofâ€–ğ‘‡â€– for ğ‘‡ âˆˆ â„’(ğ‘‰, ğ‘Š). The norm â€–ğ‘‡â€– for ğ‘‡ âˆˆ â„’(ğ‘‰, ğ‘Š) does not usually come from taking an inner product of ğ‘‡ with itself (see Exercise 21). You should be able to tell from the context and from the symbols used which meaning of the norm is intended. The properties of the norm on â„’(ğ‘‰, ğ‘Š) listed below look identical to properties of the norm on an inner product space (see 6.9 and 6.17). The inequality in (d) is called the triangle inequality, thus using the same terminology that we used for the norm on ğ‘‰. For the reverse triangle inequality, see Exercise 1. 7.87 basic properties of norms of linear maps Suppose ğ‘‡ âˆˆ â„’(ğ‘‰, ğ‘Š). Then (a) â€–ğ‘‡â€– â‰¥ 0; (b) â€–ğ‘‡â€– = 0 âŸº ğ‘‡ = 0; (c) â€–ğœ†ğ‘‡â€– = |ğœ†| â€–ğ‘‡â€– for all ğœ† âˆˆ ğ…; (d) â€–ğ‘† + ğ‘‡â€– â‰¤ â€–ğ‘†â€–+ â€–ğ‘‡â€– for all ğ‘† âˆˆ â„’(ğ‘‰, ğ‘Š). Proof (a) Because â€–ğ‘‡ğ‘£â€– â‰¥ 0for every ğ‘£ âˆˆ ğ‘‰, the definition ofâ€–ğ‘‡â€– implies that â€–ğ‘‡â€– â‰¥ 0. (b) Suppose â€–ğ‘‡â€– = 0. Thus ğ‘‡ğ‘£ = 0for all ğ‘£ âˆˆ ğ‘‰ with â€–ğ‘£â€– â‰¤ 1. If ğ‘¢ âˆˆ ğ‘‰ with ğ‘¢ â‰  0, then ğ‘‡ğ‘¢ = â€–ğ‘¢â€– ğ‘‡( ğ‘¢ â€–ğ‘¢â€– )= 0, where the last equality holds because ğ‘¢/â€–ğ‘¢â€– has norm 1. Because ğ‘‡ğ‘¢ = 0for all ğ‘¢ âˆˆ ğ‘‰, we have ğ‘‡ = 0. Conversely, if ğ‘‡ = 0then ğ‘‡ğ‘£ = 0for all ğ‘£ âˆˆ ğ‘‰ and hence â€–ğ‘‡â€– = 0. (c) Suppose ğœ† âˆˆ ğ…. Then â€–ğœ†ğ‘‡â€– = max{â€–ğœ†ğ‘‡ğ‘£â€– âˆ¶ ğ‘£ âˆˆ ğ‘‰ and â€–ğ‘£â€– â‰¤ 1} = |ğœ†| max{â€–ğ‘‡ğ‘£â€– âˆ¶ ğ‘£ âˆˆ ğ‘‰ and â€–ğ‘£â€– â‰¤ 1} = |ğœ†| â€–ğ‘‡â€–. (d) Suppose ğ‘† âˆˆ â„’(ğ‘‰, ğ‘Š). The definition ofâ€–ğ‘† + ğ‘‡â€– implies that there exists ğ‘£ âˆˆ ğ‘‰ such that â€–ğ‘£â€– â‰¤ 1and â€–ğ‘† + ğ‘‡â€– = âˆ¥(ğ‘† + ğ‘‡)ğ‘£âˆ¥. Now â€–ğ‘† + ğ‘‡â€– = âˆ¥(ğ‘† + ğ‘‡)ğ‘£âˆ¥ = â€–ğ‘†ğ‘£ + ğ‘‡ğ‘£â€– â‰¤ â€–ğ‘†ğ‘£â€–+ â€–ğ‘‡ğ‘£â€– â‰¤ â€–ğ‘†â€–+ â€–ğ‘‡â€–, completing the proof of (d). 282 Chapter 7 Operators on Inner Product Spaces For ğ‘†, ğ‘‡ âˆˆ â„’(ğ‘‰, ğ‘Š), the quantity â€–ğ‘† âˆ’ ğ‘‡â€– is often called the distance between ğ‘† and ğ‘‡. Informally, think of the condition that â€–ğ‘† âˆ’ ğ‘‡â€– is a small number as meaning that ğ‘† and ğ‘‡ are close together. For example, Exercise 9 asserts that for every ğ‘‡ âˆˆ â„’(ğ‘‰), there is an invertible operator as close to ğ‘‡ as we wish. 7.88 alternative formulas for â€–ğ‘‡â€– Suppose ğ‘‡ âˆˆ â„’(ğ‘‰, ğ‘Š). Then (a) â€–ğ‘‡â€– = the largest singular value of ğ‘‡; (b) â€–ğ‘‡â€– = max{â€–ğ‘‡ğ‘£â€– âˆ¶ ğ‘£ âˆˆ ğ‘‰ and â€–ğ‘£â€– = 1}; (c) â€–ğ‘‡â€– = the smallest number ğ‘ such that â€–ğ‘‡ğ‘£â€– â‰¤ ğ‘â€–ğ‘£â€–for all ğ‘£ âˆˆ ğ‘‰. Proof (a) See 7.85. (b) Let ğ‘£ âˆˆ ğ‘‰ be such that 0 < â€–ğ‘£â€– â‰¤ 1. Let ğ‘¢ = ğ‘£/â€–ğ‘£â€–. Then â€–ğ‘¢â€– = âˆ¥ ğ‘£ â€–ğ‘£â€– âˆ¥ = 1 and â€–ğ‘‡ğ‘¢â€– = âˆ¥ğ‘‡(ğ‘£ â€–ğ‘£â€– )âˆ¥= â€–ğ‘‡ğ‘£â€– â€–ğ‘£â€– â‰¥ â€–ğ‘‡ğ‘£â€–. Thus when finding the maximum ofâ€–ğ‘‡ğ‘£â€– with â€–ğ‘£â€– â‰¤ 1, we can restrict attention to vectors in ğ‘‰ with norm 1, proving (b). (c) Suppose ğ‘£ âˆˆ ğ‘‰ and ğ‘£ â‰  0. Then the definition ofâ€–ğ‘‡â€– implies that âˆ¥ğ‘‡(ğ‘£ â€–ğ‘£â€– )âˆ¥â‰¤ â€–ğ‘‡â€–, which implies that 7.89 â€–ğ‘‡ğ‘£â€– â‰¤ â€–ğ‘‡â€– â€–ğ‘£â€–. Now suppose ğ‘ â‰¥ 0and â€–ğ‘‡ğ‘£â€– â‰¤ ğ‘â€–ğ‘£â€–for all ğ‘£ âˆˆ ğ‘‰. This implies that â€–ğ‘‡ğ‘£â€– â‰¤ ğ‘ for all ğ‘£ âˆˆ ğ‘‰ with â€–ğ‘£â€– â‰¤ 1. Taking the maximum of the left side of the inequality above over all ğ‘£ âˆˆ ğ‘‰ with â€–ğ‘£â€– â‰¤ 1shows that â€–ğ‘‡â€– â‰¤ ğ‘. Thus â€–ğ‘‡â€– is the smallest number ğ‘ such that â€–ğ‘‡ğ‘£â€– â‰¤ ğ‘â€–ğ‘£â€–for all ğ‘£ âˆˆ ğ‘‰. When working with norms of linear maps, you will probably frequently use the inequality 7.89. For computing an approximation of the norm of a linear map ğ‘‡ given the matrix of ğ‘‡ with respect to some orthonormal bases, 7.88(a) is likely to be most useful. The matrix of ğ‘‡âˆ—ğ‘‡ is quickly computable from matrix multiplication. Then a computer can be asked to find an approximation for the largest eigenvalue of ğ‘‡âˆ—ğ‘‡ (excellent numeric algorithms exist for this purpose). Then taking the square root and using 7.88(a) gives an approximation for the norm of ğ‘‡ (which usually cannot be computed exactly). Section 7F Consequences of Singular Value Decomposition 283 You should verify all assertions in the example below. 7.90 example:norms â€¢ If ğ¼ denotes the usual identity operator on ğ‘‰, then â€–ğ¼â€– = 1. â€¢ If ğ‘‡ âˆˆ â„’(ğ…ğ‘›)and the matrix of ğ‘‡ with respect to the standard basis of ğ…ğ‘› consists of all 1â€™s, then â€–ğ‘‡â€– = ğ‘›. â€¢ If ğ‘‡ âˆˆ â„’(ğ‘‰) and ğ‘‰ has an orthonormal basis consisting of eigenvectors of ğ‘‡ with corresponding eigenvalues ğœ†1, â€¦, ğœ†ğ‘›, then â€–ğ‘‡â€– is the maximum of the numbers |ğœ†1|, â€¦, |ğœ†ğ‘›|. â€¢ Suppose ğ‘‡ âˆˆ â„’(ğ‘5)is the operator whose matrix (with respect to the stan- dard basis) is the 5-by-5matrix whose entry in row ğ‘—, column ğ‘˜ is 1/(ğ‘— 2 + ğ‘˜). Standard mathematical software shows that the largest singular value of ğ‘‡ is approximately 0.8and the smallest singular value of ğ‘‡ is approximately 10 âˆ’6. Thus â€–ğ‘‡â€– â‰ˆ 0.8and (using Exercise 10 in Section 7E) âˆ¥ğ‘‡âˆ’1âˆ¥ â‰ˆ 10 6. It is not possible to find exact formulas for these norms. A linear map and its adjoint have the same norm, as shown by the next result. 7.91 norm of the adjoint Suppose ğ‘‡ âˆˆ â„’(ğ‘‰, ğ‘Š). Then âˆ¥ğ‘‡âˆ—âˆ¥ = â€–ğ‘‡â€–. Proof Suppose ğ‘¤ âˆˆ ğ‘Š. Then âˆ¥ğ‘‡âˆ—ğ‘¤âˆ¥ 2 = âŸ¨ğ‘‡âˆ—ğ‘¤, ğ‘‡âˆ—ğ‘¤âŸ©= âŸ¨ğ‘‡ğ‘‡âˆ—ğ‘¤, ğ‘¤âŸ©â‰¤âˆ¥ğ‘‡ğ‘‡âˆ—ğ‘¤âˆ¥ â€–ğ‘¤â€– â‰¤ â€–ğ‘‡â€–âˆ¥ğ‘‡âˆ—ğ‘¤âˆ¥ â€–ğ‘¤â€–. The inequality above implies that âˆ¥ğ‘‡âˆ—ğ‘¤âˆ¥ â‰¤ â€–ğ‘‡â€– â€–ğ‘¤â€–, which along with 7.88(ğ‘) implies that âˆ¥ğ‘‡âˆ—âˆ¥ â‰¤ â€–ğ‘‡â€–. Replacing ğ‘‡ with ğ‘‡âˆ— in the inequality âˆ¥ğ‘‡âˆ—âˆ¥ â‰¤ â€–ğ‘‡â€–and then using the equation (ğ‘‡âˆ—) âˆ— = ğ‘‡ shows that â€–ğ‘‡â€– â‰¤âˆ¥ğ‘‡âˆ—âˆ¥. Thus âˆ¥ğ‘‡âˆ—âˆ¥ = â€–ğ‘‡â€–, as desired. You may want to construct an alternative proof of the result above using Exercise 9 in Section 7E, which asserts that a linear map and its adjoint have the same positive singular values. Approximation by Linear Maps with Lower-Dimensional Range The next result is a spectacular application of the singular value decomposition. It says that to best approximate a linear map by a linear map whose range has dimension at most ğ‘˜, chop off the singular value decomposition after the first ğ‘˜ terms. Specifically, the linear mapğ‘‡ğ‘˜ in the next result has the property that dim range ğ‘‡ğ‘˜ = ğ‘˜ and ğ‘‡ğ‘˜ minimizes the distance to ğ‘‡ among all linear maps with range of dimension at most ğ‘˜. This result leads to algorithms for compressing huge matrices while preserving their most important information. 284 Chapter 7 Operators on Inner Product Spaces 7.92 best approximation by linear map whose range has dimension â‰¤ ğ‘˜ Suppose ğ‘‡ âˆˆ â„’(ğ‘‰, ğ‘Š) and ğ‘ 1 â‰¥ â‹¯ â‰¥ ğ‘ ğ‘š are the positive singular values of ğ‘‡. Suppose 1 â‰¤ ğ‘˜ < ğ‘š. Then min{â€–ğ‘‡ âˆ’ ğ‘†â€– âˆ¶ ğ‘† âˆˆ â„’(ğ‘‰, ğ‘Š) and dim range ğ‘† â‰¤ ğ‘˜}= ğ‘ ğ‘˜ + 1. Furthermore, if ğ‘‡ğ‘£ = ğ‘ 1âŸ¨ğ‘£, ğ‘’1âŸ© ğ‘“1 + â‹¯ + ğ‘ ğ‘šâŸ¨ğ‘£, ğ‘’ğ‘šâŸ© ğ‘“ğ‘š is a singular value decomposition of ğ‘‡ and ğ‘‡ğ‘˜ âˆˆ â„’(ğ‘‰, ğ‘Š) is defined by ğ‘‡ğ‘˜ğ‘£ = ğ‘ 1âŸ¨ğ‘£, ğ‘’1âŸ© ğ‘“1 + â‹¯ + ğ‘ ğ‘˜âŸ¨ğ‘£, ğ‘’ğ‘˜âŸ© ğ‘“ğ‘˜ for each ğ‘£ âˆˆ ğ‘‰, then dim range ğ‘‡ğ‘˜ = ğ‘˜ and â€–ğ‘‡ âˆ’ ğ‘‡ğ‘˜â€– = ğ‘ ğ‘˜ + 1. Proof If ğ‘£ âˆˆ ğ‘‰ then âˆ¥(ğ‘‡ âˆ’ ğ‘‡ğ‘˜)ğ‘£âˆ¥2 = âˆ¥ğ‘ ğ‘˜ + 1âŸ¨ğ‘£, ğ‘’ğ‘˜ + 1âŸ© ğ‘“ğ‘˜ + 1 + â‹¯ + ğ‘ ğ‘šâŸ¨ğ‘£, ğ‘’ğ‘šâŸ© ğ‘“ğ‘šâˆ¥ 2 = ğ‘ ğ‘˜ + 1 2 âˆ£âŸ¨ğ‘£, ğ‘’ğ‘˜ + 1âŸ©âˆ£2 + â‹¯ + ğ‘ ğ‘š 2 âˆ£âŸ¨ğ‘£, ğ‘’ğ‘šâŸ©âˆ£2 â‰¤ ğ‘ ğ‘˜ + 1 2(âˆ£âŸ¨ğ‘£, ğ‘’ğ‘˜ + 1âŸ©âˆ£2 + â‹¯ + âˆ£âŸ¨ğ‘£, ğ‘’ğ‘šâŸ©âˆ£2) â‰¤ ğ‘ ğ‘˜ + 1 2 â€–ğ‘£â€– 2. Thus â€–ğ‘‡ âˆ’ ğ‘‡ğ‘˜â€– â‰¤ ğ‘ ğ‘˜ + 1. The equation (ğ‘‡ âˆ’ ğ‘‡ğ‘˜)ğ‘’ğ‘˜ + 1 = ğ‘ ğ‘˜ + 1 ğ‘“ğ‘˜ + 1 now shows that â€–ğ‘‡ âˆ’ ğ‘‡ğ‘˜â€– = ğ‘ ğ‘˜ + 1. Suppose ğ‘† âˆˆ â„’(ğ‘‰, ğ‘Š) and dim range ğ‘† â‰¤ ğ‘˜. Thus ğ‘†ğ‘’1, â€¦, ğ‘†ğ‘’ğ‘˜ + 1, which is a list of length ğ‘˜ + 1, is linearly dependent. Hence there exist ğ‘1, â€¦, ğ‘ğ‘˜ + 1 âˆˆ ğ…, not all 0, such that ğ‘1ğ‘†ğ‘’1 + â‹¯ + ğ‘ğ‘˜ + 1ğ‘†ğ‘’ğ‘˜ + 1 = 0. Now ğ‘1ğ‘’1 + â‹¯ + ğ‘ğ‘˜ + 1ğ‘’ğ‘˜ + 1 â‰  0because ğ‘1, â€¦, ğ‘ğ‘˜ + 1 are not all 0. We have âˆ¥(ğ‘‡ âˆ’ ğ‘†)(ğ‘1ğ‘’1 + â‹¯ + ğ‘ğ‘˜ + 1ğ‘’ğ‘˜ + 1)âˆ¥ 2 = âˆ¥ğ‘‡(ğ‘1ğ‘’1 + â‹¯ + ğ‘ğ‘˜ + 1ğ‘’ğ‘˜ + 1)âˆ¥ 2 = â€–ğ‘ 1ğ‘1 ğ‘“1 + â‹¯ + ğ‘ ğ‘˜ + 1ğ‘ğ‘˜ + 1 ğ‘“ğ‘˜ + 1â€– 2 = ğ‘ 1 2 |ğ‘1| 2 + â‹¯ + ğ‘ ğ‘˜ + 1 2 |ğ‘ğ‘˜ + 1| 2 â‰¥ ğ‘ ğ‘˜ + 1 2(|ğ‘1| 2 + â‹¯ + |ğ‘ğ‘˜ + 1| 2) = ğ‘ ğ‘˜ + 1 2 â€–ğ‘1ğ‘’1 + â‹¯ + ğ‘ğ‘˜ + 1ğ‘’ğ‘˜ + 1â€– 2. Because ğ‘1ğ‘’1 + â‹¯ + ğ‘ğ‘˜ + 1ğ‘’ğ‘˜ + 1 â‰  0, the inequality above implies that â€–ğ‘‡ âˆ’ ğ‘†â€– â‰¥ ğ‘ ğ‘˜ + 1. Thus ğ‘† = ğ‘‡ğ‘˜ minimizes â€–ğ‘‡ âˆ’ ğ‘†â€– among ğ‘† âˆˆ â„’(ğ‘‰, ğ‘Š) with dim range ğ‘† â‰¤ ğ‘˜. For other examples of the use of the singular value decomposition in best approximation, see Exercise 22, which finds a subspace of given dimension on which the restriction of a linear map is as small as possible, and Exercise 27, which finds a unitary operator that is as close as possible to a given operator. Section 7F Consequences of Singular Value Decomposition 285 Polar Decomposition Recall our discussion before 7.54 of the analogy between complex numbers ğ‘§ with |ğ‘§| = 1and unitary operators. Continuing with this analogy, note that every complex number ğ‘§ except 0can be written in the form ğ‘§ = ( ğ‘§ |ğ‘§| )|ğ‘§| = ( ğ‘§ |ğ‘§| )âˆšğ‘§ğ‘§, where the first factor, namely,ğ‘§/|ğ‘§|, has absolute value 1. Our analogy leads us to guess that every operator ğ‘‡ âˆˆ â„’(ğ‘‰) can be written as a unitary operator times âˆšğ‘‡âˆ—ğ‘‡. That guess is indeed correct. The corresponding result is called the polar decomposition, which gives a beautiful description of an arbitrary operator on ğ‘‰. Note that if ğ‘‡ âˆˆ â„’(ğ‘‰), then ğ‘‡âˆ—ğ‘‡ is a positive operator [as was shown in 7.64(a)]. Thus the operator âˆšğ‘‡âˆ—ğ‘‡ makes sense and is well defined as a positive operator on ğ‘‰. The polar decomposition that we are about to state and prove says that every operator on ğ‘‰ is the product of a unitary operator and a positive operator. Thus we can write an arbitrary operator on ğ‘‰ as the product of two nice operators, each of which comes from a class that we can completely describe and that we understand reasonably well. The unitary operators are described by 7.55 if ğ… = ğ‚; the positive operators are described by the real and complex spectral theorems (7.29 and 7.31). Specifically, consider the caseğ… = ğ‚, and suppose ğ‘‡ = ğ‘†âˆšğ‘‡âˆ—ğ‘‡ is a polar decomposition of an operator ğ‘‡ âˆˆ â„’(ğ‘‰), where ğ‘† is a unitary operator. Then there is an orthonormal basis of ğ‘‰ with respect to which ğ‘† has a diagonal matrix, and there is an orthonormal basis of ğ‘‰ with respect to which âˆšğ‘‡âˆ—ğ‘‡ has a diagonal matrix. Warning: There may not exist an orthonormal basis that simultaneously puts the matrices of both ğ‘† and âˆšğ‘‡âˆ—ğ‘‡ into these nice diagonal formsâ€”ğ‘† may require one orthonormal basis and âˆšğ‘‡âˆ—ğ‘‡ may require a different orthonormal basis. However (still assuming that ğ… = ğ‚), if ğ‘‡ is normal, then an orthonormal basis of ğ‘‰ can be chosen such that both ğ‘† and âˆšğ‘‡âˆ—ğ‘‡ have diagonal matrices with respect to this basisâ€”see Exercise 31. The converse is also true: If ğ‘‡ âˆˆ â„’(ğ‘‰) and ğ‘‡ = ğ‘†âˆšğ‘‡âˆ—ğ‘‡ for some unitary operator ğ‘† âˆˆ â„’(ğ‘‰) such that ğ‘† and âˆšğ‘‡âˆ—ğ‘‡ both have diagonal matrices with respect to the same orthonormal basis of ğ‘‰, then ğ‘‡ is normal. This holds because ğ‘‡ then has a diagonal matrix with respect to this same orthonormal basis, which implies that ğ‘‡ is normal [by the equivalence of (c) and (a) in 7.31]. 286 Chapter 7 Operators on Inner Product Spaces The polar decomposition below is valid on both real and complex inner product spaces and for all operators on those spaces. 7.93 polar decomposition Suppose ğ‘‡ âˆˆ â„’(ğ‘‰). Then there exists a unitary operator ğ‘† âˆˆ â„’(ğ‘‰) such that ğ‘‡ = ğ‘†âˆšğ‘‡âˆ—ğ‘‡. Proof Let ğ‘ 1, â€¦, ğ‘ ğ‘š be the positive singular values of ğ‘‡, and let ğ‘’1, â€¦, ğ‘’ğ‘š and ğ‘“1, â€¦, ğ‘“ğ‘š be orthonormal lists in ğ‘‰ such that 7.94 ğ‘‡ğ‘£ = ğ‘ 1âŸ¨ğ‘£, ğ‘’1âŸ© ğ‘“1 + â‹¯ + ğ‘ ğ‘šâŸ¨ğ‘£, ğ‘’ğ‘šâŸ© ğ‘“ğ‘š for every ğ‘£ âˆˆ ğ‘‰. Extend ğ‘’1, â€¦, ğ‘’ğ‘š and ğ‘“1, â€¦, ğ‘“ğ‘š to orthonormal bases ğ‘’1, â€¦, ğ‘’ğ‘› and ğ‘“1, â€¦, ğ‘“ğ‘› of ğ‘‰. Defineğ‘† âˆˆ â„’(ğ‘‰) by ğ‘†ğ‘£ = âŸ¨ğ‘£, ğ‘’1âŸ© ğ‘“1 + â‹¯ + âŸ¨ğ‘£, ğ‘’ğ‘›âŸ© ğ‘“ğ‘› for each ğ‘£ âˆˆ ğ‘‰. Then â€–ğ‘†ğ‘£â€– 2 = âˆ¥âŸ¨ğ‘£, ğ‘’1âŸ© ğ‘“1 + â‹¯ + âŸ¨ğ‘£, ğ‘’ğ‘›âŸ© ğ‘“ğ‘›âˆ¥ 2 = âˆ£âŸ¨ğ‘£, ğ‘’1âŸ©âˆ£2 + â‹¯ + âˆ£âŸ¨ğ‘£, ğ‘’ğ‘›âŸ©âˆ£2 = â€–ğ‘£â€–2. Thus ğ‘† is a unitary operator. Applying ğ‘‡âˆ— to both sides of 7.94 and then using the formula for ğ‘‡âˆ— given by 7.77 shows that ğ‘‡âˆ—ğ‘‡ğ‘£ = ğ‘ 1 2âŸ¨ğ‘£, ğ‘’1âŸ©ğ‘’1 + â‹¯ + ğ‘ ğ‘š 2âŸ¨ğ‘£, ğ‘’ğ‘šâŸ©ğ‘’ğ‘š for every ğ‘£ âˆˆ ğ‘‰. Thus if ğ‘£ âˆˆ ğ‘‰, then âˆšğ‘‡âˆ—ğ‘‡ğ‘£ = ğ‘ 1âŸ¨ğ‘£, ğ‘’1âŸ©ğ‘’1 + â‹¯ + ğ‘ ğ‘šâŸ¨ğ‘£, ğ‘’ğ‘šâŸ©ğ‘’ğ‘š because the operator that sends ğ‘£ to the right side of the equation above is a positive operator whose square equals ğ‘‡âˆ—ğ‘‡. Now ğ‘†âˆšğ‘‡âˆ—ğ‘‡ğ‘£ = ğ‘†(ğ‘ 1âŸ¨ğ‘£, ğ‘’1âŸ©ğ‘’1 + â‹¯ + ğ‘ ğ‘šâŸ¨ğ‘£, ğ‘’ğ‘šâŸ©ğ‘’ğ‘š) = ğ‘ 1âŸ¨ğ‘£, ğ‘’1âŸ© ğ‘“1 + â‹¯ + ğ‘ ğ‘šâŸ¨ğ‘£, ğ‘’ğ‘šâŸ© ğ‘“ğ‘š = ğ‘‡ğ‘£, where the last equation follows from 7.94. Exercise 27 shows that the unitary operator ğ‘† produced in the proof above is as close as a unitary operator can be to ğ‘‡. Alternative proofs of the polar decomposition directly use the spectral theorem, avoiding the singular value decomposition. However, the proof above seems cleaner than those alternative proofs. Section 7F Consequences of Singular Value Decomposition 287 Operators Applied to Ellipsoids and Parallelepipeds 7.95 definition:ball, ğµ The ball in ğ‘‰ of radius 1centered at 0, denoted by ğµ, is defined by ğµ = {ğ‘£ âˆˆ ğ‘‰ âˆ¶ â€–ğ‘£â€– < 1}. The ball ğµ in ğ‘2. If dim ğ‘‰ = 2, the word disk is sometimes used instead of ball. However, using ball in all dimensions is less confusing. Similarly, if dim ğ‘‰ = 2, then the word ellipse is sometimes used instead of the word ellipsoid that we are about to define. Again, using ellipsoid in all dimensions is less confusing. You can think of the ellipsoid defined below as obtained by starting with the ball ğµ and then stretching by a factor of ğ‘ ğ‘˜ along each ğ‘“ğ‘˜ axis. 7.96 definition:ellipsoid, ğ¸(ğ‘ 1 ğ‘“1, â€¦, ğ‘ ğ‘› ğ‘“ğ‘›), principal axes Suppose that ğ‘“1, â€¦, ğ‘“ğ‘› is an orthonormal basis of ğ‘‰ and ğ‘ 1, â€¦, ğ‘ ğ‘› are positive numbers. The ellipsoid ğ¸(ğ‘ 1 ğ‘“1, â€¦, ğ‘ ğ‘› ğ‘“ğ‘›) with principal axes ğ‘ 1 ğ‘“1, â€¦, ğ‘ ğ‘› ğ‘“ğ‘› is defined by ğ¸(ğ‘ 1 ğ‘“1, â€¦, ğ‘ ğ‘› ğ‘“ğ‘›) = {ğ‘£ âˆˆ ğ‘‰ âˆ¶ |âŸ¨ğ‘£, ğ‘“1âŸ©| 2 ğ‘ 1 2 + â‹¯ + |âŸ¨ğ‘£, ğ‘“ğ‘›âŸ©| 2 ğ‘ ğ‘› 2 < 1}. The ellipsoid notation ğ¸(ğ‘ 1 ğ‘“1, â€¦, ğ‘ ğ‘› ğ‘“ğ‘›) does not explicitly include the inner product space ğ‘‰, even though the definition above depends onğ‘‰. However, the in- ner product space ğ‘‰ should be clear from the context and also from the requirement that ğ‘“1, â€¦, ğ‘“ğ‘› be an orthonormal basis of ğ‘‰. 7.97 example:ellipsoids The ellipsoid ğ¸(2 ğ‘“1, ğ‘“2) in ğ‘2, where ğ‘“1, ğ‘“2 is the standard basis of ğ‘2. The ellipsoid ğ¸(2 ğ‘“1, ğ‘“2) in ğ‘2, where ğ‘“1 = ( 1 âˆš2 , 1 âˆš2 )and ğ‘“2 = (âˆ’ 1 âˆš2 , 1 âˆš2 ). 288 Chapter 7 Operators on Inner Product Spaces The ellipsoid ğ¸(4 ğ‘“1, 3 ğ‘“2, 2 ğ‘“3) in ğ‘3, where ğ‘“1, ğ‘“2, ğ‘“3 is the standard basis of ğ‘3. The ellipsoid ğ¸( ğ‘“1, â€¦, ğ‘“ğ‘›) equals the ball ğµ in ğ‘‰ for every orthonormal basis ğ‘“1, â€¦, ğ‘“ğ‘› of ğ‘‰ [by Parsevalâ€™s identity 6.30(b)]. 7.98 notation:ğ‘‡(Î©) For ğ‘‡ a function defined onğ‘‰ and Î© âŠ† ğ‘‰, defineğ‘‡(Î©) by ğ‘‡(Î©) = {ğ‘‡ğ‘£ âˆ¶ ğ‘£ âˆˆ Î©}. Thus if ğ‘‡ is a function defined onğ‘‰, then ğ‘‡(ğ‘‰) = range ğ‘‡. The next result states that every invertible operator ğ‘‡ âˆˆ â„’(ğ‘‰) maps the ball ğµ in ğ‘‰ onto an ellipsoid in ğ‘‰. The proof shows that the principal axes of this ellipsoid come from the singular value decomposition of ğ‘‡. 7.99 invertible operator takes ball to ellipsoid Suppose ğ‘‡ âˆˆ â„’(ğ‘‰) is invertible. Then ğ‘‡ maps the ball ğµ in ğ‘‰ onto an ellipsoid in ğ‘‰. Proof Suppose ğ‘‡ has singular value decomposition 7.100 ğ‘‡ğ‘£ = ğ‘ 1âŸ¨ğ‘£, ğ‘’1âŸ© ğ‘“1 + â‹¯ + ğ‘ ğ‘›âŸ¨ğ‘£, ğ‘’ğ‘›âŸ© ğ‘“ğ‘› for all ğ‘£ âˆˆ ğ‘‰, where ğ‘ 1, â€¦, ğ‘ ğ‘› are the singular values of ğ‘‡ and ğ‘’1, â€¦, ğ‘’ğ‘› and ğ‘“1, â€¦, ğ‘“ğ‘› are both orthonormal bases of ğ‘‰. We will show that ğ‘‡(ğµ) = ğ¸(ğ‘ 1 ğ‘“1, â€¦, ğ‘ ğ‘› ğ‘“ğ‘›). First suppose ğ‘£ âˆˆ ğµ. Because ğ‘‡ is invertible, none of the singular values ğ‘ 1, â€¦, ğ‘ ğ‘› equals 0(see 7.68). Thus 7.100 implies that âˆ£âŸ¨ğ‘‡ğ‘£, ğ‘“1âŸ©âˆ£2 ğ‘ 1 2 + â‹¯ + âˆ£âŸ¨ğ‘‡ğ‘£, ğ‘“ğ‘›âŸ©âˆ£2 ğ‘ ğ‘› 2 = |âŸ¨ğ‘£, ğ‘’1âŸ©| 2 + â‹¯ + |âŸ¨ğ‘£, ğ‘’ğ‘›âŸ©| 2 < 1. Thus ğ‘‡ğ‘£ âˆˆ ğ¸(ğ‘ 1 ğ‘“1, â€¦, ğ‘ ğ‘› ğ‘“ğ‘›). Hence ğ‘‡(ğµ) âŠ† ğ¸(ğ‘ 1 ğ‘“1, â€¦, ğ‘ ğ‘› ğ‘“ğ‘›). To prove inclusion in the other direction, now suppose ğ‘¤ âˆˆ ğ¸(ğ‘ 1 ğ‘“1, â€¦, ğ‘ ğ‘› ğ‘“ğ‘›). Let ğ‘£ = âŸ¨ğ‘¤, ğ‘“1âŸ© ğ‘ 1 ğ‘’1 + â‹¯ + âŸ¨ğ‘¤, ğ‘“ğ‘›âŸ© ğ‘ ğ‘› ğ‘’ğ‘›. Then â€–ğ‘£â€– < 1and 7.100 implies that ğ‘‡ğ‘£ = âŸ¨ğ‘¤, ğ‘“1âŸ© ğ‘“1 + â‹¯ + âŸ¨ğ‘¤, ğ‘“ğ‘›âŸ© ğ‘“ğ‘› = ğ‘¤. Thus ğ‘‡(ğµ) âŠ‡ ğ¸(ğ‘ 1 ğ‘“1, â€¦, ğ‘ ğ‘› ğ‘“ğ‘›). Section 7F Consequences of Singular Value Decomposition 289 We now use the previous result to show that invertible operators take all ellipsoids, not just the ball of radius 1, to ellipsoids. 7.101 invertible operator takes ellipsoids to ellipsoids Suppose ğ‘‡ âˆˆ â„’(ğ‘‰) is invertible and ğ¸ is an ellipsoid in ğ‘‰. Then ğ‘‡(ğ¸) is an ellipsoid in ğ‘‰. Proof There exist orthonormal basis ğ‘“1, â€¦, ğ‘“ğ‘› of ğ‘‰ and positive numbers ğ‘ 1, â€¦, ğ‘ ğ‘› such that ğ¸ = ğ¸(ğ‘ 1 ğ‘“1, â€¦, ğ‘ ğ‘› ğ‘“ğ‘›). Defineğ‘† âˆˆ â„’(ğ‘‰) by ğ‘†(ğ‘1 ğ‘“1 + â‹¯ + ğ‘ğ‘› ğ‘“ğ‘›) = ğ‘1ğ‘ 1 ğ‘“1 + â‹¯ + ğ‘ğ‘›ğ‘ ğ‘› ğ‘“ğ‘›. Then ğ‘† maps the ball ğµ of ğ‘‰ onto ğ¸, as you can verify. Thus ğ‘‡(ğ¸) = ğ‘‡(ğ‘†(ğµ))= (ğ‘‡ğ‘†)(ğµ). The equation above and 7.99, applied to ğ‘‡ğ‘†, show that ğ‘‡(ğ¸) is an ellipsoid in ğ‘‰. Recall (see 3.95) that if ğ‘¢ âˆˆ ğ‘‰ and Î© âŠ† ğ‘‰then ğ‘¢ + Î© is defined by ğ‘¢ + Î© = {ğ‘¢ + ğ‘¤ âˆ¶ ğ‘¤ âˆˆ Î©}. Geometrically, the sets Î© and ğ‘¢ + Î© look the same, but they are in different locations. In the following definition, ifdim ğ‘‰ = 2then the word parallelogram is often used instead of parallelepiped. 7.102 definition:ğ‘ƒ(ğ‘£1, â€¦, ğ‘£ğ‘›), parallelepiped Suppose ğ‘£1, â€¦, ğ‘£ğ‘› is a basis of ğ‘‰. Let ğ‘ƒ(ğ‘£1, â€¦, ğ‘£ğ‘›) = {ğ‘1ğ‘£1 + â‹¯ + ğ‘ğ‘›ğ‘£ğ‘› âˆ¶ ğ‘1, â€¦, ğ‘ğ‘› âˆˆ (0, 1)}. A parallelepiped is a set of the form ğ‘¢ + ğ‘ƒ(ğ‘£1, â€¦, ğ‘£ğ‘›) for some ğ‘¢ âˆˆ ğ‘‰. The vectors ğ‘£1, â€¦, ğ‘£ğ‘› are called the edges of this parallelepiped. 7.103 example:parallelepipeds The parallelepiped (0.3, 0.5)+ ğ‘ƒ((1, 0), (1, 1))in ğ‘2. A parallelepiped in ğ‘3. 290 Chapter 7 Operators on Inner Product Spaces 7.104 invertible operator takes parallelepipeds to parallelepipeds Suppose ğ‘¢ âˆˆ ğ‘‰ and ğ‘£1, â€¦, ğ‘£ğ‘› is a basis of ğ‘‰. Suppose ğ‘‡ âˆˆ â„’(ğ‘‰) is invertible. Then ğ‘‡(ğ‘¢ + ğ‘ƒ(ğ‘£1, â€¦, ğ‘£ğ‘›))= ğ‘‡ğ‘¢ + ğ‘ƒ(ğ‘‡ğ‘£1, â€¦, ğ‘‡ğ‘£ğ‘›). Proof Because ğ‘‡ is invertible, the list ğ‘‡ğ‘£1, â€¦, ğ‘‡ğ‘£ğ‘› is a basis of ğ‘‰. The linearity of ğ‘‡ implies that ğ‘‡(ğ‘¢ + ğ‘1ğ‘£1 + â‹¯ + ğ‘ğ‘›ğ‘£ğ‘›) = ğ‘‡ğ‘¢ + ğ‘1ğ‘‡ğ‘£1 + â‹¯ + ğ‘ğ‘›ğ‘‡ğ‘£ğ‘› for all ğ‘1, â€¦, ğ‘ğ‘› âˆˆ (0, 1). Thus ğ‘‡(ğ‘¢ + ğ‘ƒ(ğ‘£1, â€¦, ğ‘£ğ‘›))= ğ‘‡ğ‘¢ + ğ‘ƒ(ğ‘‡ğ‘£1, â€¦, ğ‘‡ğ‘£ğ‘›). Just as the rectangles are distinguished among the parallelograms in ğ‘2, we give a special name to the parallelepipeds in ğ‘‰ whose defining edges are orthogonal to each other. 7.105 definition:box A box in ğ‘‰ is a set of the form ğ‘¢ + ğ‘ƒ(ğ‘Ÿ1ğ‘’1, â€¦, ğ‘Ÿğ‘›ğ‘’ğ‘›), where ğ‘¢ âˆˆ ğ‘‰ and ğ‘Ÿ1, â€¦, ğ‘Ÿğ‘› are positive numbers and ğ‘’1, â€¦, ğ‘’ğ‘› is an orthonormal basis of ğ‘‰. Note that in the special case of ğ‘2 each box is a rectangle, but the terminology box can be used in all dimensions. 7.106 example:boxes The box (1, 0)+ ğ‘ƒ(âˆš2 ğ‘’1, âˆš2 ğ‘’2), where ğ‘’1 = ( 1 âˆš2 , 1 âˆš2 )and ğ‘’2 = (âˆ’ 1 âˆš2 , 1 âˆš2 ). The box ğ‘ƒ(ğ‘’1, 2ğ‘’2, ğ‘’3), where ğ‘’1, ğ‘’2, ğ‘’3 is the standard basis of ğ‘3. Suppose ğ‘‡ âˆˆ â„’(ğ‘‰) is invertible. Then ğ‘‡ maps every parallelepiped in ğ‘‰ to a parallelepiped in ğ‘‰ (by 7.104). In particular, ğ‘‡ maps every box in ğ‘‰ to a parallelepiped in ğ‘‰. This raises the question of whether ğ‘‡ maps some boxes in ğ‘‰ to boxes in ğ‘‰. The following result answers this question, with the help of the singular value decomposition. Section 7F Consequences of Singular Value Decomposition 291 7.107 every invertible operator takes some boxes to boxes Suppose ğ‘‡ âˆˆ â„’(ğ‘‰) is invertible. Suppose ğ‘‡ has singular value decomposition ğ‘‡ğ‘£ = ğ‘ 1âŸ¨ğ‘£, ğ‘’1âŸ© ğ‘“1 + â‹¯ + ğ‘ ğ‘›âŸ¨ğ‘£, ğ‘’ğ‘›âŸ© ğ‘“ğ‘›, where ğ‘ 1, â€¦, ğ‘ ğ‘› are the singular values of ğ‘‡ and ğ‘’1, â€¦, ğ‘’ğ‘› and ğ‘“1, â€¦, ğ‘“ğ‘› are orthonormal bases of ğ‘‰ and the equation above holds for all ğ‘£ âˆˆ ğ‘‰. Then ğ‘‡ maps the box ğ‘¢ + ğ‘ƒ(ğ‘Ÿ1ğ‘’1, â€¦, ğ‘Ÿğ‘›ğ‘’ğ‘›) onto the box ğ‘‡ğ‘¢ + ğ‘ƒ(ğ‘Ÿ1ğ‘ 1 ğ‘“1, â€¦, ğ‘Ÿğ‘›ğ‘ ğ‘› ğ‘“ğ‘›) for all positive numbers ğ‘Ÿ1, â€¦, ğ‘Ÿğ‘› and all ğ‘¢ âˆˆ ğ‘‰. Proof If ğ‘1, â€¦, ğ‘ğ‘› âˆˆ (0, 1)and ğ‘Ÿ1, â€¦, ğ‘Ÿğ‘› are positive numbers and ğ‘¢ âˆˆ ğ‘‰, then ğ‘‡(ğ‘¢ + ğ‘1ğ‘Ÿ1ğ‘’1 + â‹¯ + ğ‘ğ‘›ğ‘Ÿğ‘›ğ‘’ğ‘›) = ğ‘‡ğ‘¢ + ğ‘1ğ‘Ÿ1ğ‘ 1 ğ‘“1 + â‹¯ + ğ‘ğ‘›ğ‘Ÿğ‘›ğ‘ ğ‘› ğ‘“ğ‘›. Thus ğ‘‡(ğ‘¢ + ğ‘ƒ(ğ‘Ÿ1ğ‘’1, â€¦, ğ‘Ÿğ‘›ğ‘’ğ‘›))= ğ‘‡ğ‘¢ + ğ‘ƒ(ğ‘Ÿ1ğ‘ 1 ğ‘“1, â€¦, ğ‘Ÿğ‘›ğ‘ ğ‘› ğ‘“ğ‘›). Volume via Singular Values Our goal in this subsection is to understand how an operator changes the volume of subsets of its domain. Because notions of volume belong to analysis rather than to linear algebra, we will work only with an intuitive notion of volume. Our intuitive approach to volume can be converted into appropriate correct definitions, correct statements, and correct proofs using the machinery of analysis. Our intuition about volume works best in real inner product spaces. Thus the assumption that ğ… = ğ‘ will appear frequently in the rest of this subsection. If dim ğ‘‰ = ğ‘›, then by volume we will mean ğ‘›-dimensional volume. You should be familiar with this concept in ğ‘3. When ğ‘› = 2, this is usually called area instead of volume, but for consistency we use the word volume in all dimensions. The most fundamental intuition about volume is that the volume of a box (whose defining edges are by definition orthogonal to each other) is the product of the lengths of the defining edges. Thus we make the following definition. 7.108 definition:volume of a box Suppose ğ… = ğ‘. If ğ‘¢ âˆˆ ğ‘‰ and ğ‘Ÿ1, â€¦, ğ‘Ÿğ‘› are positive numbers and ğ‘’1, â€¦, ğ‘’ğ‘› is an orthonormal basis of ğ‘‰, then volume(ğ‘¢ + ğ‘ƒ(ğ‘Ÿ1ğ‘’1, â€¦, ğ‘Ÿğ‘›ğ‘’ğ‘›))= ğ‘Ÿ1 Ã— â‹¯ Ã— ğ‘Ÿğ‘›. The definition above agrees with the familiar formulas for the area (which we are calling the volume) of a rectangle in ğ‘2 and for the volume of a box in ğ‘3. For example, the first box in Example7.106 has two-dimensional volume (or area) 2 because the defining edges of that box have lengthâˆš2and âˆš2. The second box in Example 7.106 has three-dimensional volume 2because the defining edges of that box have length 1, 2, and 1. 292 Chapter 7 Operators on Inner Product Spaces Volume of this ball â‰ˆ sum of the volumes of the five boxes. To define the volume of a subset ofğ‘‰, approximate the subset by a finite collection of disjoint boxes, and then add up the volumes of the approximating collection of boxes. As we approximate a subset of ğ‘‰ more accurately by disjoint unions of more boxes, we get a better approximation to the volume. These ideas should remind you of how the Riemann integral is defined by approximating the area under a curve by a disjoint collection of rectangles. This discussion leads to the following nonrigorous but intuitive definition. 7.109 definition:volume Suppose ğ… = ğ‘ and Î© âŠ† ğ‘‰. Then the volume of Î©, denoted by volume Î©, is approximately the sum of the volumes of a collection of disjoint boxes that approximate Î©. We are ignoring many reasonable questions by taking an intuitive approach to volume. For example, if we approximate Î© by boxes with respect to one basis, do we get the same volume if we approximate Î© by boxes with respect to a different basis? If Î©1 and Î©2 are disjoint subsets of ğ‘‰, is volume(Î©1 âˆª Î©1) = volume Î©1 + volume Î©2? Provided that we consider only reasonably nice subsets of ğ‘‰, techniques of analysis show that both these questions have affirmative answers that agree with our intuition about volume. 7.110 example:volume change by a linear map Each box here has twice the width and the same height as the boxes in the previous figure. Suppose that ğ‘‡ âˆˆ â„’(ğ‘2)is defined by ğ‘‡ğ‘£ = 2âŸ¨ğ‘£, ğ‘’1âŸ©ğ‘’1 + âŸ¨ğ‘£, ğ‘’2âŸ©ğ‘’2, where ğ‘’1, ğ‘’2 is the standard basis of ğ‘2. This linear map stretches by a factor of 2along the ğ‘’1 axis. The ball approximated by five boxes above gets mapped by ğ‘‡ to the ellipsoid shown here. Each of the five boxes in the original figure gets mapped to a box of twice the width and the same height as in the original figure. Hence each box gets mapped to a box of twice the volume (area) as in the original figure. The sum of the volumes of the five new boxes approximates the volume of the ellipsoid. Thus ğ‘‡ changes the volume of the ball by a factor of 2. In the example above, ğ‘‡ maps boxes with respect to the basis ğ‘’1, ğ‘’2 to boxes with respect to the same basis; thus we can see how ğ‘‡ changes volume. In general, an operator maps boxes to parallelepipeds that are not boxes. However, if we choose the right basis (coming from the singular value decomposition!), then boxes with respect to that basis get mapped to boxes with respect to a possibly different basis, as shown in 7.107. This observation leads to a natural proof of the following result. Section 7F Consequences of Singular Value Decomposition 293 7.111 volume changes by a factor of the product of the singular values Suppose ğ… = ğ‘, ğ‘‡ âˆˆ â„’(ğ‘‰) is invertible, and Î© âŠ† ğ‘‰. Then volume ğ‘‡(Î©) = (product of singular values of ğ‘‡)(volume Î©). Proof Suppose ğ‘‡ has singular value decomposition ğ‘‡ğ‘£ = ğ‘ 1âŸ¨ğ‘£, ğ‘’1âŸ© ğ‘“1 + â‹¯ + ğ‘ ğ‘›âŸ¨ğ‘£, ğ‘’ğ‘›âŸ© ğ‘“ğ‘› for all ğ‘£ âˆˆ ğ‘‰, where ğ‘’1, â€¦, ğ‘’ğ‘› and ğ‘“1, â€¦, ğ‘“ğ‘› are orthonormal bases of ğ‘‰. Approximate Î© by boxes of the form ğ‘¢ + ğ‘ƒ(ğ‘Ÿ1ğ‘’1, â€¦, ğ‘Ÿğ‘›ğ‘’ğ‘›), which have volume ğ‘Ÿ1 Ã— â‹¯ Ã— ğ‘Ÿğ‘›. The operator ğ‘‡ maps each box ğ‘¢ + ğ‘ƒ(ğ‘Ÿ1ğ‘’1, â€¦, ğ‘Ÿğ‘›ğ‘’ğ‘›) onto the box ğ‘‡ğ‘¢ + ğ’«(ğ‘Ÿ1ğ‘ 1 ğ‘“1, â€¦, ğ‘Ÿğ‘›ğ‘ ğ‘› ğ‘“ğ‘›), which has volume (ğ‘ 1 Ã— â‹¯ Ã— ğ‘ ğ‘›)(ğ‘Ÿ1 Ã— â‹¯ Ã— ğ‘Ÿğ‘›). The operator ğ‘‡ maps a collection of boxes that approximate Î© onto a collection of boxes that approximate ğ‘‡(Î©). Because ğ‘‡ changes the volume of each box in a collection that approximates Î© by a factor of ğ‘ 1 Ã— â‹¯ Ã— ğ‘ ğ‘›, the linear map ğ‘‡ changes the volume of Î© by the same factor. Suppose ğ‘‡ âˆˆ â„’(ğ‘‰). As we will see when we get to determinants, the product of the singular values of ğ‘‡ equals |det ğ‘‡|; see 9.60 and 9.61. Properties of an Operator as Determined by Its Eigenvalues We conclude this chapter by presenting the table below. The context of this table is a finite-dimensional complex inner product space. The first column of the table shows a property that a normal operator on such a space might have. The second column of the table shows a subset of ğ‚ such that the operator has the corresponding property if and only if all eigenvalues of the operator lie in the specified subset. For example, the first row of the table states that a normal operator is invertible if and only if all its eigenvalues are nonzero (this first row is the only one in the table that does not need the hypothesis that the operator is normal). Make sure you can explain why all results in the table hold. For example, the last row of the table holds because the norm of an operator equals its largest singular value (by 7.85) and the singular values of a normal operator, assuming ğ… = ğ‚, equal the absolute values of the eigenvalues (by Exercise 7 in Section 7E). properties of a normal operator eigenvalues are contained in invertible ğ‚\\{0} self-adjoint ğ‘ skew {ğœ† âˆˆ ğ‚ âˆ¶ Re ğœ† = 0} orthogonal projection {0, 1} positive [0, âˆ) unitary {ğœ† âˆˆ ğ‚ âˆ¶ |ğœ†| = 1} norm is less than 1 {ğœ† âˆˆ ğ‚ âˆ¶ |ğœ†| < 1} 294 Chapter 7 Operators on Inner Product Spaces Exercises 7F 1 Prove that if ğ‘†, ğ‘‡ âˆˆ â„’(ğ‘‰, ğ‘Š), then âˆ£ â€–ğ‘†â€– âˆ’ â€–ğ‘‡â€– âˆ£ â‰¤ â€–ğ‘† âˆ’ ğ‘‡â€–. The inequality above is called the reverse triangle inequality. 2 Suppose that ğ‘‡ âˆˆ â„’(ğ‘‰) is self-adjoint or that ğ… = ğ‚ and ğ‘‡ âˆˆ â„’(ğ‘‰) is normal. Prove that â€–ğ‘‡â€– = max{|ğœ†| âˆ¶ ğœ† is an eigenvalue of ğ‘‡}. 3 Suppose ğ‘‡ âˆˆ â„’(ğ‘‰, ğ‘Š) and ğ‘£ âˆˆ ğ‘‰. Prove that â€–ğ‘‡ğ‘£â€– = â€–ğ‘‡â€– â€–ğ‘£â€– âŸº ğ‘‡âˆ—ğ‘‡ğ‘£ = â€–ğ‘‡â€–2ğ‘£. 4 Suppose ğ‘‡ âˆˆ â„’(ğ‘‰, ğ‘Š), ğ‘£ âˆˆ ğ‘‰, and â€–ğ‘‡ğ‘£â€– = â€–ğ‘‡â€– â€–ğ‘£â€–. Prove that if ğ‘¢ âˆˆ ğ‘‰ and âŸ¨ğ‘¢, ğ‘£âŸ© = 0, then âŸ¨ğ‘‡ğ‘¢, ğ‘‡ğ‘£âŸ© = 0. 5 Suppose ğ‘ˆ is a finite-dimensional inner product space,ğ‘‡ âˆˆ â„’(ğ‘‰, ğ‘ˆ), and ğ‘† âˆˆ â„’(ğ‘ˆ, ğ‘Š). Prove that â€–ğ‘†ğ‘‡â€– â‰¤ â€–ğ‘†â€– â€–ğ‘‡â€–. 6 Prove or give a counterexample: If ğ‘†, ğ‘‡ âˆˆ â„’(ğ‘‰), then â€–ğ‘†ğ‘‡â€– = â€–ğ‘‡ğ‘†â€–. 7 Show that definingğ‘‘(ğ‘†, ğ‘‡) = â€–ğ‘† âˆ’ ğ‘‡â€– for ğ‘†, ğ‘‡ âˆˆ â„’(ğ‘‰, ğ‘Š) makes ğ‘‘ a metric on â„’(ğ‘‰, ğ‘Š). This exercise is intended for readers who are familiar with metric spaces. 8 (a) Prove that if ğ‘‡ âˆˆ â„’(ğ‘‰) and â€–ğ¼ âˆ’ ğ‘‡â€– < 1, then ğ‘‡ is invertible. (b) Suppose that ğ‘† âˆˆ â„’(ğ‘‰) is invertible. Prove that if ğ‘‡ âˆˆ â„’(ğ‘‰) and â€–ğ‘† âˆ’ ğ‘‡â€– < 1/âˆ¥ğ‘† âˆ’1âˆ¥, then ğ‘‡ is invertible. This exercise shows that the set of invertible operators in â„’(ğ‘‰) is an open subset of â„’(ğ‘‰), using the metric defined in Exercise 7. 9 Suppose ğ‘‡ âˆˆ â„’(ğ‘‰). Prove that for every ğœ– > 0, there exists an invertible operator ğ‘† âˆˆ â„’(ğ‘‰) such that 0 < â€–ğ‘‡ âˆ’ ğ‘†â€– < ğœ–. 10 Suppose dim ğ‘‰ > 1and ğ‘‡ âˆˆ â„’(ğ‘‰) is not invertible. Prove that for every ğœ– > 0, there exists ğ‘† âˆˆ â„’(ğ‘‰) such that 0 < â€–ğ‘‡ âˆ’ ğ‘†â€– < ğœ–and ğ‘† is not invertible. 11 Suppose ğ… = ğ‚ and ğ‘‡ âˆˆ â„’(ğ‘‰). Prove that for every ğœ– > 0there exists a diagonalizable operator ğ‘† âˆˆ â„’(ğ‘‰) such that 0 < â€–ğ‘‡ âˆ’ ğ‘†â€– < ğœ–. 12 Suppose ğ‘‡ âˆˆ â„’(ğ‘‰) is a positive operator. Show that âˆ¥âˆšğ‘‡ âˆ¥ = âˆšâ€–ğ‘‡â€–. 13 Suppose ğ‘†, ğ‘‡ âˆˆ â„’(ğ‘‰) are positive operators. Show that â€–ğ‘† âˆ’ ğ‘‡â€– â‰¤max{â€–ğ‘†â€–, â€–ğ‘‡â€–}â‰¤ â€–ğ‘†+ ğ‘‡â€–. 14 Suppose ğ‘ˆ and ğ‘Š are subspaces of ğ‘‰ such that â€–ğ‘ƒğ‘ˆ âˆ’ ğ‘ƒğ‘Šâ€– < 1. Prove that dim ğ‘ˆ = dim ğ‘Š. Section 7F Consequences of Singular Value Decomposition 295 15 Defineğ‘‡ âˆˆ â„’(ğ…3)by ğ‘‡(ğ‘§1, ğ‘§2, ğ‘§3) = (ğ‘§3, 2ğ‘§1, 3ğ‘§2). Find (explicitly) a unitary operator ğ‘† âˆˆ â„’(ğ…3)such that ğ‘‡ = ğ‘†âˆšğ‘‡âˆ—ğ‘‡. 16 Suppose ğ‘† âˆˆ â„’(ğ‘‰) is a positive invertible operator. Prove that there exists ğ›¿ > 0such that ğ‘‡ is a positive operator for every self-adjoint operator ğ‘‡ âˆˆ â„’(ğ‘‰) with â€–ğ‘† âˆ’ ğ‘‡â€– < ğ›¿. 17 Prove that if ğ‘¢ âˆˆ ğ‘‰ and ğœ‘ğ‘¢ is the linear functional on ğ‘‰ defined by the equation ğœ‘ğ‘¢(ğ‘£) = âŸ¨ğ‘£, ğ‘¢âŸ©, then â€–ğœ‘ğ‘¢â€– = â€–ğ‘¢â€–. Here we are thinking of the scalar field ğ… as an inner product space with âŸ¨ğ›¼, ğ›½âŸ© = ğ›¼ğ›½for all ğ›¼, ğ›½ âˆˆ ğ…. Thus â€–ğœ‘ğ‘¢â€– means the norm of ğœ‘ğ‘¢ as a linear map from ğ‘‰ to ğ…. 18 Suppose ğ‘’1, â€¦, ğ‘’ğ‘› is an orthonormal basis of ğ‘‰ and ğ‘‡ âˆˆ â„’(ğ‘‰, ğ‘Š). (a) Prove that max{â€–ğ‘‡ğ‘’1â€–, â€¦, â€–ğ‘‡ğ‘’ğ‘›â€–} â‰¤ â€–ğ‘‡â€– â‰¤(â€–ğ‘‡ğ‘’1â€–2 + â‹¯ + â€–ğ‘‡ğ‘’ğ‘›â€–2) 1/2. (b) Prove that â€–ğ‘‡â€– = (â€–ğ‘‡ğ‘’1â€–2 +â‹¯+â€–ğ‘‡ğ‘’ğ‘›â€–2) 1/2 if and only if dim range ğ‘‡ â‰¤ 1. Here ğ‘’1, â€¦, ğ‘’ğ‘› is an arbitrary orthonormal basis of ğ‘‰, not necessarily con- nected with a singular value decomposition of ğ‘‡. If ğ‘ 1, â€¦, ğ‘ ğ‘› is the list of singular values of ğ‘‡, then the right side of the inequality above equals (ğ‘ 1 2 + â‹¯ + ğ‘ ğ‘› 2) 1/2, as was shown in Exercise 11(a) in Section 7E. 19 Prove that if ğ‘‡ âˆˆ â„’(ğ‘‰, ğ‘Š), then âˆ¥ğ‘‡âˆ—ğ‘‡âˆ¥ = â€–ğ‘‡â€–2. This formula for âˆ¥ğ‘‡âˆ—ğ‘‡âˆ¥ leads to the important subject of ğ¶âˆ—-algebras. 20 Suppose ğ‘‡ âˆˆ â„’(ğ‘‰) is normal. Prove that âˆ¥ğ‘‡ğ‘˜âˆ¥ = â€–ğ‘‡â€– ğ‘˜ for every positive integer ğ‘˜. 21 Suppose dim ğ‘‰ > 1and dim ğ‘Š > 1. Prove that the norm on â„’(ğ‘‰, ğ‘Š) does not come from an inner product. In other words, prove that there does not exist an inner product on â„’(ğ‘‰, ğ‘Š) such that max{â€–ğ‘‡ğ‘£â€– âˆ¶ ğ‘£ âˆˆ ğ‘‰ and â€–ğ‘£â€– â‰¤ 1}= âˆšâŸ¨ğ‘‡, ğ‘‡âŸ© for all ğ‘‡ âˆˆ â„’(ğ‘‰, ğ‘Š). 22 Suppose ğ‘‡ âˆˆ â„’(ğ‘‰, ğ‘Š). Let ğ‘› = dim ğ‘‰ and let ğ‘ 1 â‰¥ â‹¯ â‰¥ ğ‘ ğ‘› denote the singular values of ğ‘‡. Prove that if 1 â‰¤ ğ‘˜ â‰¤ ğ‘›, then min{â€–ğ‘‡|ğ‘ˆâ€– âˆ¶ ğ‘ˆ is a subspace of ğ‘‰ with dim ğ‘ˆ = ğ‘˜}= ğ‘ ğ‘› âˆ’ ğ‘˜ + 1. 23 Suppose ğ‘‡ âˆˆ â„’(ğ‘‰, ğ‘Š). Show that ğ‘‡ is uniformly continuous with respect to the metrics on ğ‘‰ and ğ‘Š that arise from the norms on those spaces (see Exercise 23 in Section 6B). 24 Suppose ğ‘‡ âˆˆ â„’(ğ‘‰) is invertible. Prove that âˆ¥ğ‘‡âˆ’1âˆ¥ = â€–ğ‘‡â€–âˆ’1 âŸº ğ‘‡ â€–ğ‘‡â€– is a unitary operator. 296 Chapter 7 Operators on Inner Product Spaces 25 Fix ğ‘¢, ğ‘¥ âˆˆ ğ‘‰ with ğ‘¢ â‰  0. Defineğ‘‡ âˆˆ â„’(ğ‘‰) by ğ‘‡ğ‘£ = âŸ¨ğ‘£, ğ‘¢âŸ©ğ‘¥for every ğ‘£ âˆˆ ğ‘‰. Prove that âˆšğ‘‡âˆ—ğ‘‡ğ‘£ = â€–ğ‘¥â€– â€–ğ‘¢â€– âŸ¨ğ‘£, ğ‘¢âŸ©ğ‘¢ for every ğ‘£ âˆˆ ğ‘‰. 26 Suppose ğ‘‡ âˆˆ â„’(ğ‘‰). Prove that ğ‘‡ is invertible if and only if there exists a unique unitary operator ğ‘† âˆˆ â„’(ğ‘‰) such that ğ‘‡ = ğ‘†âˆšğ‘‡âˆ—ğ‘‡. 27 Suppose ğ‘‡ âˆˆ â„’(ğ‘‰) and ğ‘ 1, â€¦, ğ‘ ğ‘› are the singular values of ğ‘‡. Let ğ‘’1, â€¦, ğ‘’ğ‘› and ğ‘“1, â€¦, ğ‘“ğ‘› be orthonormal bases of ğ‘‰ such that ğ‘‡ğ‘£ = ğ‘ 1âŸ¨ğ‘£, ğ‘’1âŸ© ğ‘“1 + â‹¯ + ğ‘ ğ‘›âŸ¨ğ‘£, ğ‘’ğ‘›âŸ© ğ‘“ğ‘› for all ğ‘£ âˆˆ ğ‘‰. Defineğ‘† âˆˆ â„’(ğ‘‰) by ğ‘†ğ‘£ = âŸ¨ğ‘£, ğ‘’1âŸ© ğ‘“1 + â‹¯ + âŸ¨ğ‘£, ğ‘’ğ‘›âŸ© ğ‘“ğ‘›. (a) Show that ğ‘† is unitary and â€–ğ‘‡ âˆ’ ğ‘†â€– = max{|ğ‘ 1 âˆ’ 1|, â€¦, |ğ‘ ğ‘› âˆ’ 1|}. (b) Show that if ğ¸ âˆˆ â„’(ğ‘‰) is unitary, then â€–ğ‘‡ âˆ’ ğ¸â€– â‰¥ â€–ğ‘‡ âˆ’ ğ‘†â€–. This exercise finds a unitary operator ğ‘† that is as close as possible (among the unitary operators) to a given operator ğ‘‡. 28 Suppose ğ‘‡ âˆˆ â„’(ğ‘‰). Prove that there exists a unitary operator ğ‘† âˆˆ â„’(ğ‘‰) such that ğ‘‡ = âˆšğ‘‡ğ‘‡âˆ— ğ‘†. 29 Suppose ğ‘‡ âˆˆ â„’(ğ‘‰). (a) Use the polar decomposition to show that there exists a unitary operator ğ‘† âˆˆ â„’(ğ‘‰) such that ğ‘‡ğ‘‡âˆ— = ğ‘†ğ‘‡âˆ—ğ‘‡ğ‘†âˆ—. (b) Show how (a) implies that ğ‘‡ and ğ‘‡âˆ— have the same singular values. 30 Suppose ğ‘‡ âˆˆ â„’(ğ‘‰), ğ‘† âˆˆ â„’(ğ‘‰) is a unitary operator, and ğ‘… âˆˆ â„’(ğ‘‰) is a positive operator such that ğ‘‡ = ğ‘†ğ‘…. Prove that ğ‘… = âˆšğ‘‡âˆ—ğ‘‡. This exercise shows that if we write ğ‘‡ as the product of a unitary operator and a positive operator (as in the polar decomposition 7.93), then the positive operator equals âˆšğ‘‡âˆ—ğ‘‡. 31 Suppose ğ… = ğ‚ and ğ‘‡ âˆˆ â„’(ğ‘‰) is normal. Prove that there exists a unitary operator ğ‘† âˆˆ â„’(ğ‘‰) such that ğ‘‡ = ğ‘†âˆšğ‘‡âˆ—ğ‘‡ and such that ğ‘† and âˆšğ‘‡âˆ—ğ‘‡ both have diagonal matrices with respect to the same orthonormal basis of ğ‘‰. 32 Suppose that ğ‘‡ âˆˆ â„’(ğ‘‰, ğ‘Š) and ğ‘‡ â‰  0. Let ğ‘ 1, â€¦, ğ‘ ğ‘š denote the positive singular values of ğ‘‡. Show that there exists an orthonormal basis ğ‘’1, â€¦, ğ‘’ğ‘š of (null ğ‘‡) âŸ‚ such that ğ‘‡(ğ¸( ğ‘’1 ğ‘ 1 , â€¦, ğ‘’ğ‘š ğ‘ ğ‘š )) equals the ball in range ğ‘‡ of radius 1centered at 0. Chapter 8 Operators on Complex Vector Spaces In this chapter we delve deeper into the structure of operators, with most of the attention on complex vector spaces. Some of the results in this chapter apply to both real and complex vector spaces; thus we do not make a standing assumption that ğ… = ğ‚. Also, an inner product does not help with this material, so we return to the general setting of a finite-dimensional vector space. Even on a finite-dimensional complex vector space, an operator may not have enough eigenvectors to form a basis of the vector space. Thus we will consider the closely related objects called generalized eigenvectors. We will see that for each operator on a finite-dimensional complex vector space, there is a basis of the vector space consisting of generalized eigenvectors of the operator. The generalized eigenspace decomposition then provides a good description of arbitrary operators on a finite-dimensional complex vector space. Nilpotent operators, which are operators that when raised to some power equal 0, have an important role in these investigations. Nilpotent operators provide a key tool in our proof that every invertible operator on a finite-dimensional complex vector space has a square root and in our approach to Jordan form. This chapter concludes by defining the trace and proving its key properties. standing assumptions for this chapter â€¢ ğ… denotes ğ‘ or ğ‚. â€¢ ğ‘‰ denotes a finite-dimensional nonzero vector space overğ….DavidIliffCCBY-SA The Long Room of the Old Library at the University of Dublin, where William Hamilton (1805â€“1865) was a student and then a faculty member. Hamilton proved a special case of what we now call the Cayleyâ€“Hamilton theorem in 1853. 297 S. Axler, Linear Algebra Done Right, Undergraduate Texts in Mathematics, https://doi.org/10.1007/978-3-031-41026-0_8 Â© Sheldon Axler 2024 298 Chapter 8 Operators on Complex Vector Spaces 8A Generalized Eigenvectors and Nilpotent Operators Null Spaces of Powers of an Operator We begin this chapter with a study of null spaces of powers of an operator. 8.1 sequence of increasing null spaces Suppose ğ‘‡ âˆˆ â„’(ğ‘‰). Then {0} =null ğ‘‡0 âŠ†null ğ‘‡1 âŠ† â‹¯ âŠ†null ğ‘‡ğ‘˜ âŠ†null ğ‘‡ğ‘˜ + 1 âŠ† â‹¯ . Proof Suppose ğ‘˜ is a nonnegative integer and ğ‘£ âˆˆ null ğ‘‡ğ‘˜. Then ğ‘‡ğ‘˜ğ‘£ = 0, which implies that ğ‘‡ğ‘˜ + 1ğ‘£ = ğ‘‡(ğ‘‡ğ‘˜ğ‘£)= ğ‘‡(0) = 0. Thus ğ‘£ âˆˆ null ğ‘‡ğ‘˜ + 1. Hence null ğ‘‡ğ‘˜ âŠ†null ğ‘‡ğ‘˜ + 1, as desired. For similar results about decreasing sequences of ranges, see Exercises 6, 7, and 8. The following result states that if two consecutive terms in the sequence of sub- spaces above are equal, then all later terms in the sequence are equal. 8.2 equality in the sequence of null spaces Suppose ğ‘‡ âˆˆ â„’(ğ‘‰) and ğ‘š is a nonnegative integer such that null ğ‘‡ğ‘š = null ğ‘‡ğ‘š + 1. Then null ğ‘‡ğ‘š = null ğ‘‡ğ‘š + 1 = null ğ‘‡ğ‘š + 2 = null ğ‘‡ğ‘š + 3 = â‹¯ . Proof Let ğ‘˜ be a positive integer. We want to prove that null ğ‘‡ğ‘š + ğ‘˜ = null ğ‘‡ğ‘š + ğ‘˜ + 1. We already know from 8.1 that null ğ‘‡ğ‘š + ğ‘˜ âŠ†null ğ‘‡ğ‘š + ğ‘˜ + 1. To prove the inclusion in the other direction, suppose ğ‘£ âˆˆ null ğ‘‡ğ‘š + ğ‘˜ + 1. Then ğ‘‡ğ‘š + 1(ğ‘‡ğ‘˜ğ‘£)= ğ‘‡ğ‘š + ğ‘˜ + 1ğ‘£ = 0. Hence ğ‘‡ğ‘˜ğ‘£ âˆˆ null ğ‘‡ğ‘š + 1 = null ğ‘‡ğ‘š. Thus ğ‘‡ğ‘š + ğ‘˜ğ‘£ = ğ‘‡ğ‘š(ğ‘‡ğ‘˜ğ‘£)= 0, which means that ğ‘£ âˆˆ null ğ‘‡ğ‘š + ğ‘˜. This implies that null ğ‘‡ğ‘š + ğ‘˜ + 1 âŠ†null ğ‘‡ğ‘š + ğ‘˜, completing the proof. The result above raises the question of whether there exists a nonnegative integer ğ‘š such that null ğ‘‡ğ‘š = null ğ‘‡ğ‘š + 1. The next result shows that this equality holds at least when ğ‘š equals the dimension of the vector space on which ğ‘‡ operates. Section 8A Generalized Eigenvectors and Nilpotent Operators 299 8.3 null spaces stop growing Suppose ğ‘‡ âˆˆ â„’(ğ‘‰). Then null ğ‘‡dim ğ‘‰ = null ğ‘‡dim ğ‘‰ + 1 = null ğ‘‡dim ğ‘‰ + 2 = â‹¯ . Proof We only need to prove that null ğ‘‡dim ğ‘‰ = null ğ‘‡dim ğ‘‰ + 1 (by 8.2). Suppose this is not true. Then, by 8.1 and 8.2, we have {0} =null ğ‘‡0 âŠŠ null ğ‘‡1 âŠŠ â‹¯ âŠŠ null ğ‘‡dim ğ‘‰ âŠŠ null ğ‘‡dim ğ‘‰ + 1, where the symbol âŠŠ means â€œcontained in but not equal toâ€. At each of the strict inclusions in the chain above, the dimension increases by at least 1. Thus dim null ğ‘‡dim ğ‘‰ + 1 â‰¥dim ğ‘‰ + 1, a contradiction because a subspace of ğ‘‰ cannot have a larger dimension than dim ğ‘‰. It is not true that ğ‘‰ = null ğ‘‡ âŠ• range ğ‘‡ for every ğ‘‡ âˆˆ â„’(ğ‘‰). However, the next result can be a useful substitute. 8.4 ğ‘‰ is the direct sum of null ğ‘‡dim ğ‘‰ and range ğ‘‡dim ğ‘‰ Suppose ğ‘‡ âˆˆ â„’(ğ‘‰). Then ğ‘‰ = null ğ‘‡dim ğ‘‰ âŠ• range ğ‘‡dim ğ‘‰. Proof Let ğ‘› = dim ğ‘‰. First we show that 8.5 (null ğ‘‡ğ‘›)âˆ©(range ğ‘‡ğ‘›)= {0}. Suppose ğ‘£ âˆˆ (null ğ‘‡ğ‘›)âˆ©(range ğ‘‡ğ‘›). Then ğ‘‡ğ‘›ğ‘£ = 0, and there exists ğ‘¢ âˆˆ ğ‘‰ such that ğ‘£ = ğ‘‡ğ‘›ğ‘¢. Applying ğ‘‡ğ‘› to both sides of the last equation shows that ğ‘‡ğ‘›ğ‘£ = ğ‘‡2ğ‘›ğ‘¢. Hence ğ‘‡2ğ‘›ğ‘¢ = 0, which implies that ğ‘‡ğ‘›ğ‘¢ = 0(by 8.3). Thus ğ‘£ = ğ‘‡ğ‘›ğ‘¢ = 0, completing the proof of 8.5. Now 8.5 implies that null ğ‘‡ğ‘› + range ğ‘‡ğ‘› is a direct sum (by 1.46). Also, dim(null ğ‘‡ğ‘› âŠ• range ğ‘‡ğ‘›)= dim null ğ‘‡ğ‘› + dim range ğ‘‡ğ‘› = dim ğ‘‰, where the first equality above comes from3.94 and the second equality comes from the fundamental theorem of linear maps (3.21). The equation above implies that null ğ‘‡ğ‘› âŠ• range ğ‘‡ğ‘› = ğ‘‰ (see 2.39), as desired. For an improvement of the result above, see Exercise 19. 8.6 example:ğ…3 = null ğ‘‡3 âŠ• range ğ‘‡3 for ğ‘‡ âˆˆ â„’(ğ…3) Suppose ğ‘‡ âˆˆ â„’(ğ…3)is defined by ğ‘‡(ğ‘§1, ğ‘§2, ğ‘§3) = (4ğ‘§2, 0, 5ğ‘§3). 300 Chapter 8 Operators on Complex Vector Spaces Then null ğ‘‡ = {(ğ‘§1, 0, 0)âˆ¶ ğ‘§1 âˆˆ ğ…}and range ğ‘‡ = {(ğ‘§1, 0, ğ‘§3) âˆ¶ ğ‘§1, ğ‘§3 âˆˆ ğ…}. Thus null ğ‘‡ âˆ©range ğ‘‡ â‰  {0}. Hence null ğ‘‡ + range ğ‘‡ is not a direct sum. Also note that null ğ‘‡ + range ğ‘‡ â‰  ğ…3. However, we have ğ‘‡3(ğ‘§1, ğ‘§2, ğ‘§3) = (0, 0, 125ğ‘§3). Thus we see that null ğ‘‡3 = {(ğ‘§1, ğ‘§2, 0)âˆ¶ ğ‘§1, ğ‘§2 âˆˆ ğ…} and range ğ‘‡3 = {(0, 0, ğ‘§3) âˆ¶ ğ‘§3 âˆˆ ğ…}. Hence ğ…3 = null ğ‘‡3 âŠ• range ğ‘‡3, as expected by 8.4. Generalized Eigenvectors Some operators do not have enough eigenvectors to lead to good descriptions of their behavior. Thus in this subsection we introduce the concept of generalized eigenvectors, which will play a major role in our description of the structure of an operator. To understand why we need more than eigenvectors, letâ€™s examine the question of describing an operator by decomposing its domain into invariant subspaces. Fix ğ‘‡ âˆˆ â„’(ğ‘‰). We seek to describe ğ‘‡ by finding a â€œniceâ€ direct sum decomposition ğ‘‰ = ğ‘‰1 âŠ• â‹¯ âŠ• ğ‘‰ğ‘š, where each ğ‘‰ğ‘˜ is a subspace of ğ‘‰ invariant under ğ‘‡. The simplest possible nonzero invariant subspaces are one-dimensional. A decomposition as above in which each ğ‘‰ğ‘˜ is a one-dimensional subspace of ğ‘‰ invariant under ğ‘‡ is possible if and only if ğ‘‰ has a basis consisting of eigenvectors of ğ‘‡ (see 5.55). This happens if and only if ğ‘‰ has an eigenspace decomposition 8.7 ğ‘‰ = ğ¸(ğœ†1, ğ‘‡) âŠ• â‹¯ âŠ• ğ¸(ğœ†ğ‘š, ğ‘‡), where ğœ†1, â€¦, ğœ†ğ‘š are the distinct eigenvalues of ğ‘‡ (see 5.55). The spectral theorem in the previous chapter shows that if ğ‘‰ is an inner product space, then a decomposition of the form 8.7 holds for every self-adjoint operator if ğ… = ğ‘ and for every normal operator if ğ… = ğ‚ because operators of those types have enough eigenvectors to form a basis of ğ‘‰ (see 7.29 and 7.31). However, a decomposition of the form 8.7 may not hold for more general operators, even on a complex vector space. An example was given by the operator in 5.57, which does not have enough eigenvectors for 8.7 to hold. Generalized eigenvectors and generalized eigenspaces, which we now introduce, will remedy this situation. 8.8 definition: generalized eigenvector Suppose ğ‘‡ âˆˆ â„’(ğ‘‰) and ğœ† is an eigenvalue of ğ‘‡. A vector ğ‘£ âˆˆ ğ‘‰ is called a generalized eigenvector of ğ‘‡ corresponding to ğœ† if ğ‘£ â‰  0and (ğ‘‡ âˆ’ ğœ†ğ¼)ğ‘˜ğ‘£ = 0 for some positive integer ğ‘˜. Section 8A Generalized Eigenvectors and Nilpotent Operators 301 Generalized eigenvalues are not de- fined because doing so would not lead to anything new. Reason: if (ğ‘‡ âˆ’ ğœ†ğ¼)ğ‘˜ is not injective for some positive inte- ger ğ‘˜, then ğ‘‡ âˆ’ ğœ†ğ¼ is not injective, and hence ğœ† is an eigenvalue of ğ‘‡. A nonzero vector ğ‘£ âˆˆ ğ‘‰ is a general- ized eigenvector of ğ‘‡ corresponding to ğœ† if and only if (ğ‘‡ âˆ’ ğœ†ğ¼)dim ğ‘‰ğ‘£ = 0, as follows from applying 8.1 and 8.3 to the operator ğ‘‡ âˆ’ ğœ†ğ¼. As we know, an operator on a complex vector space may not have enough eigenvectors to form a basis of the domain. The next result shows that on a complex vector space there are enough generalized eigenvectors to do this. 8.9 a basis of generalized eigenvectors Suppose ğ… = ğ‚ and ğ‘‡ âˆˆ â„’(ğ‘‰). Then there is a basis of ğ‘‰ consisting of generalized eigenvectors of ğ‘‡. Proof Let ğ‘› = dim ğ‘‰. We will use induction on ğ‘›. To get started, note that the desired result holds if ğ‘› = 1because then every nonzero vector in ğ‘‰ is an eigenvector of ğ‘‡. This step is where we use the hypothesis that ğ… = ğ‚, because if ğ… = ğ‘ then ğ‘‡ may not have any eigenvalues. Now suppose ğ‘› > 1and the de- sired result holds for all smaller values of dim ğ‘‰. Let ğœ† be an eigenvalue of ğ‘‡. Applying 8.4 to ğ‘‡ âˆ’ ğœ†ğ¼ shows that ğ‘‰ = null(ğ‘‡ âˆ’ ğœ†ğ¼)ğ‘› âŠ• range(ğ‘‡ âˆ’ ğœ†ğ¼)ğ‘›. If null(ğ‘‡ âˆ’ ğœ†ğ¼)ğ‘› = ğ‘‰, then every nonzero vector in ğ‘‰ is a generalized eigen- vector of ğ‘‡, and thus in this case there is a basis of ğ‘‰ consisting of generalized eigenvectors of ğ‘‡. Hence we can assume that null(ğ‘‡ âˆ’ ğœ†ğ¼)ğ‘› â‰  ğ‘‰, which implies that range(ğ‘‡ âˆ’ ğœ†ğ¼)ğ‘› â‰  {0}. Also, null(ğ‘‡ âˆ’ ğœ†ğ¼)ğ‘› â‰  {0}, because ğœ† is an eigenvalue of ğ‘‡. Thus we have 0 <dim range(ğ‘‡ âˆ’ ğœ†ğ¼) ğ‘› < ğ‘›. Furthermore, range(ğ‘‡ âˆ’ ğœ†ğ¼)ğ‘› is invariant under ğ‘‡ [by 5.18 with ğ‘(ğ‘§) = (ğ‘§ âˆ’ ğœ†)ğ‘›]. Let ğ‘† âˆˆ â„’(range(ğ‘‡ âˆ’ ğœ†ğ¼)ğ‘›)equal ğ‘‡ restricted to range(ğ‘‡ âˆ’ ğœ†ğ¼)ğ‘›. Our induction hypothesis applied to the operator ğ‘† implies that there is a basis of range(ğ‘‡ âˆ’ ğœ†ğ¼)ğ‘› consisting of generalized eigenvectors of ğ‘†, which of course are generalized eigenvectors of ğ‘‡. Adjoining that basis of range(ğ‘‡âˆ’ğœ†ğ¼)ğ‘› to a basis of null(ğ‘‡âˆ’ğœ†ğ¼)ğ‘› gives a basis of ğ‘‰ consisting of generalized eigenvectors of ğ‘‡. If ğ… = ğ‘ and dim ğ‘‰ > 1, then some operators on ğ‘‰ have the property that there exists a basis of ğ‘‰ consisting of generalized eigenvectors of the operator, and (unlike what happens when ğ… = ğ‚) other operators do not have this property. See Exercise 11 for a necessary and sufficient condition that determines whether an operator has this property. 302 Chapter 8 Operators on Complex Vector Spaces 8.10 example: generalized eigenvectors of an operator on ğ‚ 3 Defineğ‘‡ âˆˆ â„’(ğ‚ 3)by ğ‘‡(ğ‘§1, ğ‘§2, ğ‘§3) = (4ğ‘§2, 0, 5ğ‘§3) for each (ğ‘§1, ğ‘§2, ğ‘§3) âˆˆ ğ‚3. A routine use of the definition of eigenvalue shows that the eigenvalues of ğ‘‡ are 0and 5. Furthermore, the eigenvectors corresponding to the eigenvalue 0are the nonzero vectors of the form (ğ‘§1, 0, 0), and the eigenvectors corresponding to the eigenvalue 5are the nonzero vectors of the form (0, 0, ğ‘§3). Hence this operator does not have enough eigenvectors to span its domain ğ‚ 3. We compute that ğ‘‡3(ğ‘§1, ğ‘§2, ğ‘§3) = (0, 0, 125ğ‘§3). Thus 8.1 and 8.3 imply that the generalized eigenvectors of ğ‘‡ corresponding to the eigenvalue 0are the nonzero vectors of the form (ğ‘§1, ğ‘§2, 0). We also have (ğ‘‡ âˆ’ 5ğ¼) 3(ğ‘§1, ğ‘§2, ğ‘§3) = (âˆ’125ğ‘§1 + 300ğ‘§2, âˆ’125ğ‘§2, 0). Thus the generalized eigenvectors of ğ‘‡ corresponding to the eigenvalue 5are the nonzero vectors of the form (0, 0, ğ‘§3). The paragraphs above show that each of the standard basis vectors of ğ‚ 3 is a generalized eigenvector of ğ‘‡. Thus ğ‚ 3 indeed has a basis consisting of generalized eigenvectors of ğ‘‡, as promised by 8.9. If ğ‘£ is an eigenvector of ğ‘‡ âˆˆ â„’(ğ‘‰), then the corresponding eigenvalue ğœ† is uniquely determined by the equation ğ‘‡ğ‘£ = ğœ†ğ‘£, which can be satisfied by only one ğœ† âˆˆ ğ… (because ğ‘£ â‰  0). However, if ğ‘£ is a generalized eigenvector of ğ‘‡, then it is not obvious that the equation (ğ‘‡ âˆ’ ğœ†ğ¼) dim ğ‘‰ğ‘£ = 0can be satisfied by only one ğœ† âˆˆ ğ…. Fortunately, the next result tells us that all is well on this issue. 8.11 generalized eigenvector corresponds to a unique eigenvalue Suppose ğ‘‡ âˆˆ â„’(ğ‘‰). Then each generalized eigenvector of ğ‘‡ corresponds to only one eigenvalue of ğ‘‡. Proof Suppose ğ‘£ âˆˆ ğ‘‰ is a generalized eigenvector of ğ‘‡ corresponding to eigen- values ğ›¼ and ğœ† of ğ‘‡. Let ğ‘š be the smallest positive integer such that (ğ‘‡âˆ’ğ›¼ğ¼)ğ‘šğ‘£ = 0. Let ğ‘› = dim ğ‘‰. Then 0 = (ğ‘‡ âˆ’ ğœ†ğ¼) ğ‘›ğ‘£ = ((ğ‘‡ âˆ’ ğ›¼ğ¼) + (ğ›¼ âˆ’ ğœ†)ğ¼) ğ‘›ğ‘£ = ğ‘› âˆ‘ ğ‘˜ = 0 ğ‘ğ‘˜(ğ›¼ âˆ’ ğœ†)ğ‘› âˆ’ ğ‘˜(ğ‘‡ âˆ’ ğ›¼ğ¼)ğ‘˜ğ‘£, where ğ‘0 = 1and the values of the other binomial coefficients ğ‘ğ‘˜ do not matter. Apply the operator (ğ‘‡ âˆ’ ğ›¼ğ¼)ğ‘š âˆ’ 1 to both sides of the equation above, getting 0 = (ğ›¼ âˆ’ ğœ†) ğ‘›(ğ‘‡ âˆ’ ğ›¼ğ¼)ğ‘š âˆ’ 1ğ‘£. Because (ğ‘‡ âˆ’ ğ›¼ğ¼)ğ‘š âˆ’ 1ğ‘£ â‰  0, the equation above implies that ğ›¼ = ğœ†, as desired. Section 8A Generalized Eigenvectors and Nilpotent Operators 303 We saw earlier (5.11) that eigenvectors corresponding to distinct eigenvalues are linearly independent. Now we prove a similar result for generalized eigen- vectors, with a proof that roughly follows the pattern of the proof of that earlier result. 8.12 linearly independent generalized eigenvectors Suppose that ğ‘‡ âˆˆ â„’(ğ‘‰). Then every list of generalized eigenvectors of ğ‘‡ corresponding to distinct eigenvalues of ğ‘‡ is linearly independent. Proof Suppose the desired result is false. Then there exists a smallest positive integer ğ‘š such that there exists a linearly dependent list ğ‘£1, â€¦, ğ‘£ğ‘š of generalized eigenvectors of ğ‘‡ corresponding to distinct eigenvalues ğœ†1, â€¦, ğœ†ğ‘š of ğ‘‡ (note that ğ‘š â‰¥ 2because a generalized eigenvector is, by definition, nonzero). Thus there exist ğ‘1, â€¦, ğ‘ğ‘š âˆˆ ğ…, none of which are 0(because of the minimality of ğ‘š), such that ğ‘1ğ‘£1 + â‹¯ + ğ‘ğ‘šğ‘£ğ‘š = 0. Let ğ‘› = dim ğ‘‰. Apply (ğ‘‡ âˆ’ ğœ†ğ‘šğ¼) ğ‘› to both sides of the equation above, getting 8.13 ğ‘1(ğ‘‡ âˆ’ ğœ†ğ‘šğ¼) ğ‘›ğ‘£1 + â‹¯ + ğ‘ğ‘š âˆ’ 1(ğ‘‡ âˆ’ ğœ†ğ‘šğ¼) ğ‘›ğ‘£ğ‘š âˆ’ 1 = 0. Suppose ğ‘˜ âˆˆ {1, â€¦, ğ‘š âˆ’ 1}. Then (ğ‘‡ âˆ’ ğœ†ğ‘šğ¼) ğ‘›ğ‘£ğ‘˜ â‰  0 because otherwise ğ‘£ğ‘˜ would be a generalized eigenvector of ğ‘‡ corresponding to the distinct eigenvalues ğœ†ğ‘˜ and ğœ†ğ‘š, which would contradict 8.11. However, (ğ‘‡ âˆ’ ğœ†ğ‘˜ğ¼) ğ‘›((ğ‘‡ âˆ’ ğœ†ğ‘šğ¼) ğ‘›ğ‘£ğ‘˜)= (ğ‘‡ âˆ’ ğœ†ğ‘šğ¼) ğ‘›((ğ‘‡ âˆ’ ğœ†ğ‘˜ğ¼) ğ‘›ğ‘£ğ‘˜)= 0. Thus the last two displayed equations show that (ğ‘‡ âˆ’ ğœ†ğ‘šğ¼) ğ‘›ğ‘£ğ‘˜ is a generalized eigenvector of ğ‘‡ corresponding to the eigenvalue ğœ†ğ‘˜. Hence (ğ‘‡ âˆ’ ğœ†ğ‘šğ¼) ğ‘›ğ‘£1, â€¦, (ğ‘‡ âˆ’ ğœ†ğ‘šğ¼) ğ‘›ğ‘£ğ‘š âˆ’ 1 is a linearly dependent list (by 8.13) of ğ‘š âˆ’ 1generalized eigenvectors correspond- ing to distinct eigenvalues, contradicting the minimality of ğ‘š. This contradiction completes the proof. Nilpotent Operators 8.14 definition: nilpotent An operator is called nilpotent if some power of it equals 0. Thus an operator on ğ‘‰ is nilpotent if every nonzero vector in ğ‘‰ is a generalized eigenvector of ğ‘‡ corresponding to the eigenvalue 0. 304 Chapter 8 Operators on Complex Vector Spaces 8.15 example: nilpotent operators (a) The operator ğ‘‡ âˆˆ â„’(ğ…4)defined by ğ‘‡(ğ‘§1, ğ‘§2, ğ‘§3, ğ‘§4) = (0, 0, ğ‘§1, ğ‘§2) is nilpotent because ğ‘‡2 = 0. (b) The operator on ğ…3 whose matrix (with respect to the standard basis) is â›âœâœâœ â âˆ’3 9 0 âˆ’7 9 6 4 0 âˆ’6 ââŸâŸâŸ â  is nilpotent, as can be shown by cubing the matrix above to get the zero matrix. (c) The operator of differentiation on ğ’«ğ‘š(ğ‘) is nilpotent because the (ğ‘š + 1) th derivative of every polynomial of degree at most ğ‘š equals 0. Note that on this space of dimension ğ‘š + 1, we need to raise the nilpotent operator to the power ğ‘š + 1to get the 0operator. The Latin word nil means nothing or zero; the Latin word potens means having power. Thus nilpotent literally means having a power that is zero. The next result shows that when rais- ing a nilpotent operator to a power, we never need to use a power higher than the dimension of the space. For a slightly stronger result, see Exercise 18. 8.16 nilpotent operator raised to dimension of domain is 0 Suppose ğ‘‡ âˆˆ â„’(ğ‘‰) is nilpotent. Then ğ‘‡dim ğ‘‰ = 0. Proof Because ğ‘‡ is nilpotent, there exists a positive integer ğ‘˜ such that ğ‘‡ğ‘˜ = 0. Thus null ğ‘‡ğ‘˜ = ğ‘‰. Now 8.1 and 8.3 imply that null ğ‘‡dim ğ‘‰ = ğ‘‰. Thus ğ‘‡dim ğ‘‰ = 0. 8.17 eigenvalues of nilpotent operator Suppose ğ‘‡ âˆˆ â„’(ğ‘‰). (a) If ğ‘‡ is nilpotent, then 0is an eigenvalue of ğ‘‡ and ğ‘‡ has no other eigenvalues. (b) If ğ… = ğ‚ and 0is the only eigenvalue of ğ‘‡, then ğ‘‡ is nilpotent. Proof (a) To prove (a), suppose ğ‘‡ is nilpotent. Hence there is a positive integer ğ‘š such that ğ‘‡ğ‘š = 0. This implies that ğ‘‡ is not injective. Thus 0is an eigenvalue of ğ‘‡. Section 8A Generalized Eigenvectors and Nilpotent Operators 305 To show that ğ‘‡ has no other eigenvalues, suppose ğœ† is an eigenvalue of ğ‘‡. Then there exists a nonzero vector ğ‘£ âˆˆ ğ‘‰ such that ğœ†ğ‘£ = ğ‘‡ğ‘£. Repeatedly applying ğ‘‡ to both sides of this equation shows that ğœ† ğ‘šğ‘£ = ğ‘‡ğ‘šğ‘£ = 0. Thus ğœ† = 0, as desired. (b) Suppose ğ… = ğ‚ and 0is the only eigenvalue of ğ‘‡. By 5.27(b), the minimal polynomial of ğ‘‡ equals ğ‘§ğ‘š for some positive integer ğ‘š. Thus ğ‘‡ğ‘š = 0. Hence ğ‘‡ is nilpotent. Exercise 23 shows that the hypothesis that ğ… = ğ‚ cannot be deleted in (b) of the result above. Given an operator on ğ‘‰, we want to find a basis ofğ‘‰ such that the matrix of the operator with respect to this basis is as simple as possible, meaning that the matrix contains many 0â€™s. The next result shows that if ğ‘‡ is nilpotent, then we can choose a basis of ğ‘‰ such that the matrix of ğ‘‡ with respect to this basis has more than half of its entries equal to 0. Later in this chapter we will do even better. 8.18 minimal polynomial and upper-triangular matrix of nilpotent operator Suppose ğ‘‡ âˆˆ â„’(ğ‘‰). Then the following are equivalent. (a) ğ‘‡ is nilpotent. (b) The minimal polynomial of ğ‘‡ is ğ‘§ğ‘š for some positive integer ğ‘š. (c) There is a basis of ğ‘‰ with respect to which the matrix of ğ‘‡ has the form â›âœâœâœ â 0 âˆ— â‹± 0 0 ââŸâŸâŸ â  , where all entries on and below the diagonal equal 0. Proof Suppose (a) holds, so ğ‘‡ is nilpotent. Thus there exists a positive integer ğ‘› such that ğ‘‡ğ‘› = 0. Now 5.29 implies that ğ‘§ğ‘› is a polynomial multiple of the minimal polynomial of ğ‘‡. Thus the minimal polynomial of ğ‘‡ is ğ‘§ğ‘š for some positive integer ğ‘š, proving that (a) implies (b). Now suppose (b) holds, so the minimal polynomial of ğ‘‡ is ğ‘§ ğ‘š for some positive integer ğ‘š. This implies, by 5.27(a), that 0(which is the only zero of ğ‘§ ğ‘š)is the only eigenvalue of ğ‘‡. This further implies, by 5.44, that there is a basis of ğ‘‰ with respect to which the matrix of ğ‘‡ is upper triangular. This also implies, by 5.41, that all entries on the diagonal of this matrix are 0, proving that (b) implies (c). Now suppose (c) holds. Then 5.40 implies that ğ‘‡dim ğ‘‰ = 0. Thus ğ‘‡ is nilpotent, proving that (c) implies (a). 306 Chapter 8 Operators on Complex Vector Spaces Exercises 8A 1 Suppose ğ‘‡ âˆˆ â„’(ğ‘‰). Prove that if dim null ğ‘‡4 = 8and dim null ğ‘‡6 = 9, then dim null ğ‘‡ğ‘š = 9for all integers ğ‘š â‰¥ 5. 2 Suppose ğ‘‡ âˆˆ â„’(ğ‘‰), ğ‘š is a positive integer, ğ‘£ âˆˆ ğ‘‰, and ğ‘‡ğ‘š âˆ’ 1ğ‘£ â‰  0but ğ‘‡ğ‘šğ‘£ = 0. Prove that ğ‘£, ğ‘‡ğ‘£, ğ‘‡2ğ‘£, â€¦, ğ‘‡ğ‘š âˆ’ 1ğ‘£ is linearly independent. The result in this exercise is used in the proof of 8.45. 3 Suppose ğ‘‡ âˆˆ â„’(ğ‘‰). Prove that ğ‘‰ = null ğ‘‡ âŠ• range ğ‘‡ âŸº null ğ‘‡2 = null ğ‘‡. 4 Suppose ğ‘‡ âˆˆ â„’(ğ‘‰), ğœ† âˆˆ ğ…, and ğ‘š is a positive integer such that the minimal polynomial of ğ‘‡ is a polynomial multiple of (ğ‘§ âˆ’ ğœ†)ğ‘š. Prove that dim null(ğ‘‡ âˆ’ ğœ†ğ¼)ğ‘š â‰¥ ğ‘š. 5 Suppose ğ‘‡ âˆˆ â„’(ğ‘‰) and ğ‘š is a positive integer. Prove that dim null ğ‘‡ğ‘š â‰¤ ğ‘šdim null ğ‘‡. Hint: Exercise 21 in Section 3B may be useful. 6 Suppose ğ‘‡ âˆˆ â„’(ğ‘‰). Show that ğ‘‰ = range ğ‘‡0 âŠ‡range ğ‘‡1 âŠ‡ â‹¯ âŠ‡range ğ‘‡ğ‘˜ âŠ‡range ğ‘‡ğ‘˜ + 1 âŠ‡ â‹¯ . 7 Suppose ğ‘‡ âˆˆ â„’(ğ‘‰) and ğ‘š is a nonnegative integer such that range ğ‘‡ğ‘š = range ğ‘‡ğ‘š + 1. Prove that range ğ‘‡ğ‘˜ = range ğ‘‡ğ‘š for all ğ‘˜ > ğ‘š. 8 Suppose ğ‘‡ âˆˆ â„’(ğ‘‰). Prove that range ğ‘‡dim ğ‘‰ = range ğ‘‡dim ğ‘‰ + 1 = range ğ‘‡dim ğ‘‰ + 2 = â‹¯ . 9 Suppose ğ‘‡ âˆˆ â„’(ğ‘‰) and ğ‘š is a nonnegative integer. Prove that null ğ‘‡ğ‘š = null ğ‘‡ğ‘š + 1 âŸº range ğ‘‡ğ‘š = range ğ‘‡ğ‘š + 1. 10 Defineğ‘‡ âˆˆ â„’(ğ‚ 2)by ğ‘‡(ğ‘¤, ğ‘§) = (ğ‘§, 0). Find all generalized eigenvectors of ğ‘‡. 11 Suppose that ğ‘‡ âˆˆ â„’(ğ‘‰). Prove that there is a basis of ğ‘‰ consisting of generalized eigenvectors of ğ‘‡ if and only if the minimal polynomial of ğ‘‡ equals (ğ‘§ âˆ’ ğœ†1)â‹¯(ğ‘§ âˆ’ ğœ†ğ‘š) for some ğœ†1, â€¦, ğœ†ğ‘š âˆˆ ğ…. Assume ğ… = ğ‘ because the case ğ… = ğ‚ follows from 5.27(b) and 8.9. This exercise states that the condition for there to be a basis of ğ‘‰ consisting of generalized eigenvectors of ğ‘‡ is the same as the condition for there to be a basis with respect to which ğ‘‡ has an upper-triangular matrix (see 5.44). Caution: If ğ‘‡ has an upper-triangular matrix with respect to a basis ğ‘£1, â€¦, ğ‘£ğ‘› of ğ‘‰, then ğ‘£1 is an eigenvector of ğ‘‡ but it is not necessarily true that ğ‘£2, â€¦, ğ‘£ğ‘› are generalized eigenvectors of ğ‘‡. Section 8A Generalized Eigenvectors and Nilpotent Operators 307 12 Suppose ğ‘‡ âˆˆ â„’(ğ‘‰) is such that every vector in ğ‘‰ is a generalized eigenvector of ğ‘‡. Prove that there exists ğœ† âˆˆ ğ… such that ğ‘‡ âˆ’ ğœ†ğ¼ is nilpotent. 13 Suppose ğ‘†, ğ‘‡ âˆˆ â„’(ğ‘‰) and ğ‘†ğ‘‡ is nilpotent. Prove that ğ‘‡ğ‘† is nilpotent. 14 Suppose ğ‘‡ âˆˆ â„’(ğ‘‰) is nilpotent and ğ‘‡ â‰  0. Prove ğ‘‡ is not diagonalizable. 15 Suppose ğ… = ğ‚ and ğ‘‡ âˆˆ â„’(ğ‘‰). Prove that ğ‘‡ is diagonalizable if and only if every generalized eigenvector of ğ‘‡ is an eigenvector of ğ‘‡. For ğ… = ğ‚, this exercise adds another equivalence to the list of conditions for diagonalizability in 5.55. 16 (a) Give an example of nilpotent operators ğ‘†, ğ‘‡ on the same vector space such that neither ğ‘† + ğ‘‡ nor ğ‘†ğ‘‡ is nilpotent. (b) Suppose ğ‘†, ğ‘‡ âˆˆ â„’(ğ‘‰) are nilpotent and ğ‘†ğ‘‡ = ğ‘‡ğ‘†. Prove that ğ‘† + ğ‘‡ and ğ‘†ğ‘‡ are nilpotent. 17 Suppose ğ‘‡ âˆˆ â„’(ğ‘‰) is nilpotent and ğ‘š is a positive integer such that ğ‘‡ğ‘š = 0. (a) Prove that ğ¼ âˆ’ ğ‘‡ is invertible and that (ğ¼ âˆ’ ğ‘‡)âˆ’1 = ğ¼ + ğ‘‡ + â‹¯ + ğ‘‡ğ‘š âˆ’ 1. (b) Explain how you would guess the formula above. 18 Suppose ğ‘‡ âˆˆ â„’(ğ‘‰) is nilpotent. Prove that ğ‘‡1 + dim range ğ‘‡ = 0. If dim range ğ‘‡ < dim ğ‘‰ âˆ’ 1, then this exercise improves 8.16. 19 Suppose ğ‘‡ âˆˆ â„’(ğ‘‰) is not nilpotent. Show that ğ‘‰ = null ğ‘‡dim ğ‘‰ âˆ’ 1 âŠ• range ğ‘‡dim ğ‘‰ âˆ’ 1. For operators that are not nilpotent, this exercise improves 8.4. 20 Suppose ğ‘‰ is an inner product space and ğ‘‡ âˆˆ â„’(ğ‘‰) is normal and nilpotent. Prove that ğ‘‡ = 0. 21 Suppose ğ‘‡ âˆˆ â„’(ğ‘‰) is such that null ğ‘‡dim ğ‘‰ âˆ’ 1 â‰  null ğ‘‡dim ğ‘‰. Prove that ğ‘‡ is nilpotent and that dim null ğ‘‡ğ‘˜ = ğ‘˜ for every integer ğ‘˜ with 0 â‰¤ ğ‘˜ â‰¤dim ğ‘‰. 22 Suppose ğ‘‡ âˆˆ â„’(ğ‚ 5)is such that range ğ‘‡4 â‰  range ğ‘‡5. Prove that ğ‘‡ is nilpotent. 23 Give an example of an operator ğ‘‡ on a finite-dimensional real vector space such that 0is the only eigenvalue of ğ‘‡ but ğ‘‡ is not nilpotent. This exercise shows that the implication (b) âŸ¹ (a) in 8.17 does not hold without the hypothesis that ğ… = ğ‚. 24 For each item in Example 8.15, find a basis of the domain vector space such that the matrix of the nilpotent operator with respect to that basis has the upper-triangular form promised by 8.18(c). 25 Suppose that ğ‘‰ is an inner product space and ğ‘‡ âˆˆ â„’(ğ‘‰) is nilpotent. Show that there is an orthonormal basis of ğ‘‰ with respect to which the matrix of ğ‘‡ has the upper-triangular form promised by 8.18(c). 308 Chapter 8 Operators on Complex Vector Spaces 8B Generalized Eigenspace Decomposition Generalized Eigenspaces 8.19 definition: generalized eigenspace, ğº(ğœ†, ğ‘‡) Suppose ğ‘‡ âˆˆ â„’(ğ‘‰) and ğœ† âˆˆ ğ…. The generalized eigenspace of ğ‘‡ correspond- ing to ğœ†, denoted by ğº(ğœ†, ğ‘‡), is defined by ğº(ğœ†, ğ‘‡) = {ğ‘£ âˆˆ ğ‘‰ âˆ¶ (ğ‘‡ âˆ’ ğœ†ğ¼)ğ‘˜ğ‘£ = 0for some positive integer ğ‘˜}. Thus ğº(ğœ†, ğ‘‡) is the set of generalized eigenvectors of ğ‘‡ corresponding to ğœ†, along with the 0vector. Because every eigenvector of ğ‘‡ is a generalized eigenvector of ğ‘‡ (take ğ‘˜ = 1 in the definition of generalized eigenvector), each eigenspace is contained in the corresponding generalized eigenspace. In other words, if ğ‘‡ âˆˆ â„’(ğ‘‰) and ğœ† âˆˆ ğ…, then ğ¸(ğœ†, ğ‘‡) âŠ† ğº(ğœ†, ğ‘‡). The next result implies that if ğ‘‡ âˆˆ â„’(ğ‘‰) and ğœ† âˆˆ ğ…, then the generalized eigenspace ğº(ğœ†, ğ‘‡) is a subspace of ğ‘‰ (because the null space of each linear map on ğ‘‰ is a subspace of ğ‘‰). 8.20 description of generalized eigenspaces Suppose ğ‘‡ âˆˆ â„’(ğ‘‰) and ğœ† âˆˆ ğ…. Then ğº(ğœ†, ğ‘‡) = null(ğ‘‡ âˆ’ ğœ†ğ¼)dim ğ‘‰. Proof Suppose ğ‘£ âˆˆ null(ğ‘‡ âˆ’ ğœ†ğ¼)dim ğ‘‰. The definitions implyğ‘£ âˆˆ ğº(ğœ†, ğ‘‡). Thus ğº(ğœ†, ğ‘‡) âŠ‡null(ğ‘‡ âˆ’ ğœ†ğ¼)dim ğ‘‰. Conversely, suppose ğ‘£ âˆˆ ğº(ğœ†, ğ‘‡). Thus there is a positive integer ğ‘˜ such that ğ‘£ âˆˆ null(ğ‘‡ âˆ’ ğœ†ğ¼)ğ‘˜. From 8.1 and 8.3 (with ğ‘‡ âˆ’ ğœ†ğ¼ replacing ğ‘‡), we get ğ‘£ âˆˆ null(ğ‘‡ âˆ’ ğœ†ğ¼)dim ğ‘‰. Thus ğº(ğœ†, ğ‘‡) âŠ†null(ğ‘‡ âˆ’ ğœ†ğ¼)dim ğ‘‰, completing the proof. 8.21 example: generalized eigenspaces of an operator on ğ‚ 3 Defineğ‘‡ âˆˆ â„’(ğ‚ 3)by ğ‘‡(ğ‘§1, ğ‘§2, ğ‘§3) = (4ğ‘§2, 0, 5ğ‘§3). In Example 8.10, we saw that the eigenvalues of ğ‘‡ are 0and 5, and we found the corresponding sets of generalized eigenvectors. Taking the union of those sets with {0}, we have ğº(0, ğ‘‡) = {(ğ‘§1, ğ‘§2, 0)âˆ¶ ğ‘§1, ğ‘§2 âˆˆ ğ‚} and ğº(5, ğ‘‡) = {(0, 0, ğ‘§3) âˆ¶ ğ‘§3 âˆˆ ğ‚}. Note that ğ‚ 3 = ğº(0, ğ‘‡) âŠ• ğº(5, ğ‘‡). Section 8B Generalized Eigenspace Decomposition 309 In Example 8.21, the domain space ğ‚ 3 is the direct sum of the generalized eigenspaces of the operator ğ‘‡ in that example. Our next result shows that this behavior holds in general. Specifically, the following major result shows that if ğ… = ğ‚ and ğ‘‡ âˆˆ â„’(ğ‘‰), then ğ‘‰ is the direct sum of the generalized eigenspaces of ğ‘‡, each of which is invariant under ğ‘‡ and on which ğ‘‡ is a nilpotent operator plus a scalar multiple of the identity. Thus the next result achieves our goal of decomposing ğ‘‰ into invariant subspaces on which ğ‘‡ has a known behavior. As we will see, the proof follows from putting together what we have learned about generalized eigenspaces and then using our result that for each operator ğ‘‡ âˆˆ â„’(ğ‘‰), there exists a basis of ğ‘‰ consisting of generalized eigenvectors of ğ‘‡. 8.22 generalized eigenspace decomposition Suppose ğ… = ğ‚ and ğ‘‡ âˆˆ â„’(ğ‘‰). Let ğœ†1, â€¦, ğœ†ğ‘š be the distinct eigenvalues of ğ‘‡. Then (a) ğº(ğœ†ğ‘˜, ğ‘‡) is invariant under ğ‘‡ for each ğ‘˜ = 1, â€¦, ğ‘š; (b) (ğ‘‡ âˆ’ ğœ†ğ‘˜ğ¼)|ğº( ğœ†ğ‘˜, ğ‘‡)is nilpotent for each ğ‘˜ = 1, â€¦, ğ‘š; (c) ğ‘‰ = ğº(ğœ†1, ğ‘‡) âŠ• â‹¯ âŠ• ğº(ğœ†ğ‘š, ğ‘‡). Proof (a) Suppose ğ‘˜ âˆˆ {1, â€¦, ğ‘š}. Then 8.20 shows that ğº(ğœ†ğ‘˜, ğ‘‡) = null(ğ‘‡ âˆ’ ğœ†ğ‘˜ğ¼) dim ğ‘‰. Thus 5.18, with ğ‘(ğ‘§) = (ğ‘§âˆ’ ğœ†ğ‘˜)dim ğ‘‰, implies that ğº(ğœ†ğ‘˜, ğ‘‡) is invariant under ğ‘‡, proving (a). (b) Suppose ğ‘˜ âˆˆ {1, â€¦, ğ‘š}. If ğ‘£ âˆˆ ğº(ğœ†ğ‘˜, ğ‘‡), then (ğ‘‡ âˆ’ ğœ†ğ‘˜ğ¼) dim ğ‘‰ğ‘£ = 0(by 8.20). Thus ((ğ‘‡ âˆ’ ğœ†ğ‘˜ğ¼)|ğº( ğœ†ğ‘˜, ğ‘‡)) dim ğ‘‰ = 0. Hence (ğ‘‡ âˆ’ ğœ†ğ‘˜ğ¼)|ğº( ğœ†ğ‘˜, ğ‘‡)is nilpotent, proving (b). (c) To show that ğº(ğœ†1, ğ‘‡) + â‹¯ + ğº(ğœ†ğ‘š, ğ‘‡) is a direct sum, suppose ğ‘£1 + â‹¯ + ğ‘£ğ‘š = 0, where each ğ‘£ğ‘˜ is in ğº(ğœ†ğ‘˜, ğ‘‡). Because generalized eigenvectors of ğ‘‡ cor- responding to distinct eigenvalues are linearly independent (by 8.12), this implies that each ğ‘£ğ‘˜ equals 0. Thus ğº(ğœ†1, ğ‘‡) + â‹¯ + ğº(ğœ†ğ‘š, ğ‘‡) is a direct sum (by 1.45). Finally, each vector in ğ‘‰ can be written as a finite sum of generalized eigen- vectors of ğ‘‡ (by 8.9). Thus ğ‘‰ = ğº(ğœ†1, ğ‘‡) âŠ• â‹¯ âŠ• ğº(ğœ†ğ‘š, ğ‘‡), proving (c). For the analogous result when ğ… = ğ‘, see Exercise 8. 310 Chapter 8 Operators on Complex Vector Spaces Multiplicity of an Eigenvalue If ğ‘‰ is a complex vector space and ğ‘‡ âˆˆ â„’(ğ‘‰), then the decomposition of ğ‘‰ pro- vided by the generalized eigenspace decomposition (8.22) can be a powerful tool. The dimensions of the subspaces involved in this decomposition are sufficiently important to get a name, which is given in the next definition. 8.23 definition: multiplicity â€¢ Suppose ğ‘‡ âˆˆ â„’(ğ‘‰). The multiplicity of an eigenvalue ğœ† of ğ‘‡ is defined to be the dimension of the corresponding generalized eigenspace ğº(ğœ†, ğ‘‡). â€¢ In other words, the multiplicity of an eigenvalue ğœ† of ğ‘‡ equals dim null(ğ‘‡ âˆ’ ğœ†ğ¼)dim ğ‘‰. The second bullet point above holds because ğº(ğœ†, ğ‘‡) = null(ğ‘‡ âˆ’ ğœ†ğ¼) dim ğ‘‰ (see 8.20). 8.24 example: multiplicity of each eigenvalue of an operator Suppose ğ‘‡ âˆˆ â„’(ğ‚ 3)is defined by ğ‘‡(ğ‘§1, ğ‘§2, ğ‘§3) = (6ğ‘§1 + 3ğ‘§2 + 4ğ‘§3, 6ğ‘§2 + 2ğ‘§3, 7ğ‘§3). The matrix of ğ‘‡ (with respect to the standard basis) is â›âœâœâœ â 6 3 4 0 6 2 0 0 7 ââŸâŸâŸ â  . The eigenvalues of ğ‘‡ are the diagonal entries 6and 7, as follows from 5.41. You can verify that the generalized eigenspaces of ğ‘‡ are as follows: ğº(6, ğ‘‡) = span((1, 0, 0), (0, 1, 0)) and ğº(7, ğ‘‡) = span((10, 2, 1)). In this example, the multiplicity of each eigenvalue equals the number of times that eigenvalue appears on the diago- nal of an upper-triangular matrix rep- resenting the operator. This behavior always happens, as we will see in 8.31. Thus the eigenvalue 6has multiplicity 2 and the eigenvalue 7has multiplicity 1. The direct sum ğ‚ 3 = ğº(6, ğ‘‡) âŠ• ğº(7, ğ‘‡) is the generalized eigenspace decom- position promised by 8.22. A basis of ğ‚ 3 consisting of generalized eigen- vectors of ğ‘‡, as promised by 8.9, is (1, 0, 0), (0, 1, 0), (10, 2, 1). There does not exist a basis of ğ‚ 3 consisting of eigen- vectors of this operator. In the example above, the sum of the multiplicities of the eigenvalues of ğ‘‡ equals 3, which is the dimension of the domain of ğ‘‡. The next result shows that this holds for all operators on finite-dimensional complex vector spaces. Section 8B Generalized Eigenspace Decomposition 311 8.25 sum of the multiplicities equals dim ğ‘‰ Suppose ğ… = ğ‚ and ğ‘‡ âˆˆ â„’(ğ‘‰). Then the sum of the multiplicities of all eigenvalues of ğ‘‡ equals dim ğ‘‰. Proof The desired result follows from the generalized eigenspace decomposition (8.22) and the formula for the dimension of a direct sum (see 3.94). The terms algebraic multiplicity and geometric multiplicity are used in some books. In case you encounter this terminology, be aware that the algebraic multi- plicity is the same as the multiplicity defined here and the geometric multiplicity is the dimension of the corresponding eigenspace. In other words, if ğ‘‡ âˆˆ â„’(ğ‘‰) and ğœ† is an eigenvalue of ğ‘‡, then algebraic multiplicity of ğœ† = dim null(ğ‘‡ âˆ’ ğœ†ğ¼)dim ğ‘‰ = dim ğº(ğœ†, ğ‘‡), geometric multiplicity of ğœ† = dim null(ğ‘‡ âˆ’ ğœ†ğ¼) = dim ğ¸(ğœ†, ğ‘‡). Note that as defined above, the algebraic multiplicity also has a geometric meaning as the dimension of a certain null space. The definition of multiplicity given here is cleaner than the traditional definition that involves determinants;9.62 implies that these definitions are equivalent. If ğ‘‰ is an inner product space, ğ‘‡ âˆˆ â„’(ğ‘‰) is normal, and ğœ† is an eigenvalue of ğ‘‡, then the algebraic multiplicity of ğœ† equals the geometric multiplicity of ğœ†, as can be seen from applying Exercise 27 in Section 7A to the normal operator ğ‘‡ âˆ’ ğœ†ğ¼. As a special case, the singular values of ğ‘† âˆˆ â„’(ğ‘‰, ğ‘Š) (here ğ‘‰ and ğ‘Š are both finite-dimensional inner product spaces) depend on the multiplicities (either algebraic or geometric) of the eigenvalues of the self-adjoint operator ğ‘†âˆ—ğ‘†. The next definition associates a monic polynomial with each operator on a finite-dimensional complex vector space. 8.26 definition:characteristic polynomial Suppose ğ… = ğ‚ and ğ‘‡ âˆˆ â„’(ğ‘‰). Let ğœ†1, â€¦, ğœ†ğ‘š denote the distinct eigenvalues of ğ‘‡, with multiplicities ğ‘‘1, â€¦, ğ‘‘ğ‘š. The polynomial (ğ‘§ âˆ’ ğœ†1) ğ‘‘1â‹¯(ğ‘§ âˆ’ ğœ†ğ‘š) ğ‘‘ğ‘š is called the characteristic polynomial of ğ‘‡. 8.27 example:the characteristic polynomial of an operator Suppose ğ‘‡ âˆˆ â„’(ğ‚ 3)is defined as in Example8.24. Because the eigenvalues of ğ‘‡ are 6, with multiplicity 2, and 7, with multiplicity 1, we see that the characteristic polynomial of ğ‘‡ is (ğ‘§ âˆ’ 6) 2(ğ‘§ âˆ’ 7). 312 Chapter 8 Operators on Complex Vector Spaces 8.28 degree and zeros of characteristic polynomial Suppose ğ… = ğ‚ and ğ‘‡ âˆˆ â„’(ğ‘‰). Then (a) the characteristic polynomial of ğ‘‡ has degree dim ğ‘‰; (b) the zeros of the characteristic polynomial of ğ‘‡ are the eigenvalues of ğ‘‡. Proof Our result about the sum of the multiplicities (8.25) implies (a). The definition of the characteristic polynomial implies (b). Most texts define the characteristic polynomial using determinants (the two definitions are equivalent by9.62). The approach taken here, which is considerably simpler, leads to the following nice proof of the Cayleyâ€“Hamilton theorem. 8.29 Cayleyâ€“Hamilton theorem Suppose ğ… = ğ‚, ğ‘‡ âˆˆ â„’(ğ‘‰), and ğ‘ is the characteristic polynomial of ğ‘‡. Then ğ‘(ğ‘‡) = 0. Proof Let ğœ†1, â€¦, ğœ†ğ‘š be the distinct eigenvalues of ğ‘‡, and let ğ‘‘ğ‘˜ = dim ğº(ğœ†ğ‘˜, ğ‘‡). For each ğ‘˜ âˆˆ {1, â€¦, ğ‘š}, we know that (ğ‘‡ âˆ’ ğœ†ğ‘˜ğ¼)|ğº( ğœ†ğ‘˜, ğ‘‡)is nilpotent. Thus we have Arthur Cayley (1821â€“1895) published three mathematics papers before com- pleting his undergraduate degree. (ğ‘‡ âˆ’ ğœ†ğ‘˜ğ¼) ğ‘‘ğ‘˜|ğº( ğœ†ğ‘˜, ğ‘‡)= 0 (by 8.16) for each ğ‘˜ âˆˆ {1, â€¦, ğ‘š}. The generalized eigenspace decom- position (8.22) states that every vector in ğ‘‰ is a sum of vectors in ğº(ğœ†1, ğ‘‡), â€¦, ğº(ğœ†ğ‘š, ğ‘‡). Thus to prove that ğ‘(ğ‘‡) = 0, we only need to show that ğ‘(ğ‘‡)|ğº( ğœ†ğ‘˜, ğ‘‡)= 0for each ğ‘˜. Fix ğ‘˜ âˆˆ {1, â€¦, ğ‘š}. We have ğ‘(ğ‘‡) = (ğ‘‡ âˆ’ ğœ†1ğ¼) ğ‘‘1â‹¯(ğ‘‡ âˆ’ ğœ†ğ‘šğ¼) ğ‘‘ğ‘š. The operators on the right side of the equation above all commute, so we can move the factor (ğ‘‡ âˆ’ ğœ†ğ‘˜ğ¼) ğ‘‘ğ‘˜ to be the last term in the expression on the right. Because (ğ‘‡ âˆ’ ğœ†ğ‘˜ğ¼) ğ‘‘ğ‘˜|ğº( ğœ†ğ‘˜, ğ‘‡)= 0, we have ğ‘(ğ‘‡)|ğº( ğœ†ğ‘˜, ğ‘‡)= 0, as desired. The next result implies that if the minimal polynomial of an operator ğ‘‡ âˆˆ â„’(ğ‘‰) has degree dim ğ‘‰ (as happens almost alwaysâ€”see the paragraphs following 5.24), then the characteristic polynomial of ğ‘‡ equals the minimal polynomial of ğ‘‡. 8.30 characteristic polynomial is a multiple of minimal polynomial Suppose ğ… = ğ‚ and ğ‘‡ âˆˆ â„’(ğ‘‰). Then the characteristic polynomial of ğ‘‡ is a polynomial multiple of the minimal polynomial of ğ‘‡. Proof The desired result follows immediately from the Cayleyâ€“Hamilton theo- rem (8.29) and 5.29. Section 8B Generalized Eigenspace Decomposition 313 Now we can prove that the result suggested by Example 8.24 holds for all operators on finite-dimensional complex vector spaces. 8.31 multiplicity of an eigenvalue equals number of times on diagonal Suppose ğ… = ğ‚ and ğ‘‡ âˆˆ â„’(ğ‘‰). Suppose ğ‘£1, â€¦, ğ‘£ğ‘› is a basis of ğ‘‰ such that â„³(ğ‘‡, (ğ‘£1, â€¦, ğ‘£ğ‘›))is upper triangular. Then the number of times that each eigenvalue ğœ† of ğ‘‡ appears on the diagonal of â„³(ğ‘‡, (ğ‘£1, â€¦, ğ‘£ğ‘›))equals the multiplicity of ğœ† as an eigenvalue of ğ‘‡. Proof Let ğ´ = â„³(ğ‘‡, (ğ‘£1, â€¦, ğ‘£ğ‘›)). Thus ğ´ is an upper-triangular matrix. Let ğœ†1, â€¦, ğœ†ğ‘› denote the entries on the diagonal of ğ´. Thus for each ğ‘˜ âˆˆ {1, â€¦, ğ‘›}, we have ğ‘‡ğ‘£ğ‘˜ = ğ‘¢ğ‘˜ + ğœ†ğ‘˜ğ‘£ğ‘˜ for some ğ‘¢ğ‘˜ âˆˆ span(ğ‘£1, â€¦, ğ‘£ğ‘˜ âˆ’ 1). Hence if ğ‘˜ âˆˆ {1, â€¦, ğ‘›} and ğœ†ğ‘˜ â‰  0, then ğ‘‡ğ‘£ğ‘˜ is not a linear combination of ğ‘‡ğ‘£1, â€¦, ğ‘‡ğ‘£ğ‘˜ âˆ’ 1. The linear dependence lemma (2.19) now implies that the list of those ğ‘‡ğ‘£ğ‘˜ such that ğœ†ğ‘˜ â‰  0is linearly independent. Let ğ‘‘ denote the number of indices ğ‘˜ âˆˆ {1, â€¦, ğ‘›} such that ğœ†ğ‘˜ = 0. The conclusion of the previous paragraph implies that dim range ğ‘‡ â‰¥ ğ‘› âˆ’ ğ‘‘. Because ğ‘› = dim ğ‘‰ = dim null ğ‘‡ + dim range ğ‘‡, the inequality above implies that 8.32 dim null ğ‘‡ â‰¤ ğ‘‘. The matrix of the operator ğ‘‡ğ‘› with respect to the basis ğ‘£1, â€¦, ğ‘£ğ‘› is the upper- triangular matrix ğ´ ğ‘›, which has diagonal entries ğœ†1 ğ‘›, â€¦, ğœ†ğ‘› ğ‘› [see Exercise 2(b) in Section 5C]. Because ğœ†ğ‘˜ ğ‘› = 0if and only if ğœ†ğ‘˜ = 0, the number of times that 0 appears on the diagonal of ğ´ ğ‘› equals ğ‘‘. Thus applying 8.32 with ğ‘‡ replaced with ğ‘‡ğ‘›, we have 8.33 dim null ğ‘‡ğ‘› â‰¤ ğ‘‘. For ğœ† an eigenvalue of ğ‘‡, let ğ‘š ğœ† denote the multiplicity of ğœ† as an eigenvalue of ğ‘‡ and let ğ‘‘ ğœ† denote the number of times that ğœ† appears on the diagonal of ğ´. Replacing ğ‘‡ in 8.33 with ğ‘‡ âˆ’ ğœ†ğ¼, we see that 8.34 ğ‘š ğœ† â‰¤ ğ‘‘ğœ† for each eigenvalue ğœ† of ğ‘‡. The sum of the multiplicities ğ‘š ğœ† over all eigenvalues ğœ† of ğ‘‡ equals ğ‘›, the dimension of ğ‘‰ (by 8.25). The sum of the numbers ğ‘‘ ğœ† over all eigenvalues ğœ† of ğ‘‡ also equals ğ‘›, because the diagonal of ğ´ has length ğ‘›. Thus summing both sides of 8.34 over all eigenvalues ğœ† of ğ‘‡ produces an equality. Hence 8.34 must actually be an equality for each eigenvalue ğœ† of ğ‘‡. Thus the multiplicity of ğœ† as an eigenvalue of ğ‘‡ equals the number of times that ğœ† appears on the diagonal of ğ´, as desired. â‰ˆ 100ğœ‹ Chapter 8 Operators on Complex Vector Spaces Block Diagonal Matrices Often we can understand a matrix better by thinking of it as composed of smaller matrices. To interpret our results in matrix form, we make the following definition, gener- alizing the notion of a diagonal matrix. If each matrix ğ´ğ‘˜ in the definition below is a 1-by-1matrix, then we actually have a diagonal matrix. 8.35 definition: block diagonal matrix A block diagonal matrix is a square matrix of the form â›âœâœâœ â ğ´1 0 â‹± 0 ğ´ğ‘š ââŸâŸâŸ â  , where ğ´1, â€¦, ğ´ğ‘š are square matrices lying along the diagonal and all other entries of the matrix equal 0. 8.36 example:a block diagonal matrix The 5-by-5matrix ğ´ = â›âœâœâœâœâœâœâœâœâœâœâœâœâœâœâœ â (4) 0 0 0 0 0 0 â›âœ â 2 âˆ’3 0 2 ââŸ â  0 0 0 0 0 0 0 0 0 0 â›âœ â 1 7 0 1 ââŸ â  ââŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸ â  is a block diagonal matrix with ğ´ = â›âœâœâœâœâœ â ğ´1 0 ğ´2 0 ğ´3 ââŸâŸâŸâŸâŸ â  , where ğ´1 = (4), ğ´2 = â›âœ â 2 âˆ’3 0 2 ââŸ â  , ğ´3 = â›âœ â 1 7 0 1 ââŸ â  . Here the inner matrices in the 5-by-5matrix above are blocked off to show how we can think of it as a block diagonal matrix. Note that in the example above, each of ğ´1, ğ´2, ğ´3 is an upper-triangular matrix whose diagonal entries are all equal. The next result shows that with respect to an appropriate basis, every operator on a finite-dimensional complex vector space has a matrix of this form. Note that this result gives us many more zeros in the matrix than are needed to make it upper triangular. Section 8B Generalized Eigenspace Decomposition 315 8.37 block diagonal matrix with upper-triangular blocks Suppose ğ… = ğ‚ and ğ‘‡ âˆˆ â„’(ğ‘‰). Let ğœ†1, â€¦, ğœ†ğ‘š be the distinct eigenvalues of ğ‘‡, with multiplicities ğ‘‘1, â€¦, ğ‘‘ğ‘š. Then there is a basis of ğ‘‰ with respect to which ğ‘‡ has a block diagonal matrix of the form â›âœâœâœ â ğ´1 0 â‹± 0 ğ´ğ‘š ââŸâŸâŸ â  , where each ğ´ğ‘˜ is a ğ‘‘ğ‘˜-by-ğ‘‘ğ‘˜ upper-triangular matrix of the form ğ´ğ‘˜ = â›âœâœâœ â ğœ†ğ‘˜ âˆ— â‹± 0 ğœ†ğ‘˜ ââŸâŸâŸ â  . Proof Each (ğ‘‡ âˆ’ ğœ†ğ‘˜ğ¼)|ğº( ğœ†ğ‘˜, ğ‘‡)is nilpotent (see 8.22). For each ğ‘˜, choose a basis of ğº(ğœ†ğ‘˜, ğ‘‡), which is a vector space of dimension ğ‘‘ğ‘˜, such that the matrix of (ğ‘‡ âˆ’ ğœ†ğ‘˜ğ¼)|ğº( ğœ†ğ‘˜, ğ‘‡)with respect to this basis is as in 8.18(c). Thus with respect to this basis, the matrix of ğ‘‡|ğº( ğœ†ğ‘˜, ğ‘‡), which equals (ğ‘‡ âˆ’ ğœ†ğ‘˜ğ¼)|ğº( ğœ†ğ‘˜, ğ‘‡)+ ğœ†ğ‘˜ğ¼|ğº( ğœ†ğ‘˜, ğ‘‡), looks like the desired form shown above for ğ´ğ‘˜. The generalized eigenspace decomposition (8.22) shows that putting together the bases of the ğº(ğœ†ğ‘˜, ğ‘‡)â€™s chosen above gives a basis of ğ‘‰. The matrix of ğ‘‡ with respect to this basis has the desired form. 8.38 example: block diagonal matrix via generalized eigenvectors Let ğ‘‡ âˆˆ â„’(ğ‚ 3)be defined byğ‘‡(ğ‘§1, ğ‘§2, ğ‘§3) = (6ğ‘§1 + 3ğ‘§2 + 4ğ‘§3, 6ğ‘§2 + 2ğ‘§3, 7ğ‘§3). The matrix of ğ‘‡ (with respect to the standard basis) is â›âœâœâœ â 6 3 4 0 6 2 0 0 7 ââŸâŸâŸ â  , which is an upper-triangular matrix but is not of the form promised by 8.37. As we saw in Example 8.24, the eigenvalues of ğ‘‡ are 6and 7, and ğº(6, ğ‘‡) = span((1, 0, 0), (0, 1, 0)) and ğº(7, ğ‘‡) = span((10, 2, 1)). We also saw that a basis of ğ‚ 3 consisting of generalized eigenvectors of ğ‘‡ is (1, 0, 0), (0, 1, 0), (10, 2, 1). The matrix of ğ‘‡ with respect to this basis is â›âœâœâœ â ( 6 3 0 6 ) 0 0 0 0 (7) ââŸâŸâŸ â  , which is a matrix of the block diagonal form promised by 8.37. 316 Chapter 8 Operators on Complex Vector Spaces Exercises 8B 1 Defineğ‘‡ âˆˆ â„’(ğ‚ 2)by ğ‘‡(ğ‘¤, ğ‘§) = (âˆ’ğ‘§, ğ‘¤). Find the generalized eigenspaces corresponding to the distinct eigenvalues of ğ‘‡. 2 Suppose ğ‘‡ âˆˆ â„’(ğ‘‰) is invertible. Prove that ğº(ğœ†, ğ‘‡) = ğº( 1 ğœ† , ğ‘‡âˆ’1)for every ğœ† âˆˆ ğ… with ğœ† â‰  0. 3 Suppose ğ‘‡ âˆˆ â„’(ğ‘‰). Suppose ğ‘† âˆˆ â„’(ğ‘‰) is invertible. Prove that ğ‘‡ and ğ‘† âˆ’1ğ‘‡ğ‘† have the same eigenvalues with the same multiplicities. 4 Suppose dim ğ‘‰ â‰¥ 2and ğ‘‡ âˆˆ â„’(ğ‘‰) is such that null ğ‘‡dim ğ‘‰ âˆ’ 2 â‰  null ğ‘‡dim ğ‘‰ âˆ’ 1. Prove that ğ‘‡ has at most two distinct eigenvalues. 5 Suppose ğ‘‡ âˆˆ â„’(ğ‘‰) and 3and 8are eigenvalues of ğ‘‡. Let ğ‘› = dim ğ‘‰. Prove that ğ‘‰ = (null ğ‘‡ğ‘› âˆ’ 2)âŠ• (range ğ‘‡ğ‘› âˆ’ 2). 6 Suppose ğ‘‡ âˆˆ â„’(ğ‘‰) and ğœ† is an eigenvalue of ğ‘‡. Explain why the exponent of ğ‘§ âˆ’ ğœ† in the factorization of the minimal polynomial of ğ‘‡ is the smallest positive integer ğ‘š such that (ğ‘‡ âˆ’ ğœ†ğ¼)ğ‘š|ğº( ğœ†, ğ‘‡)= 0. 7 Suppose ğ‘‡ âˆˆ â„’(ğ‘‰) and ğœ† is an eigenvalue of ğ‘‡ with multiplicity ğ‘‘. Prove that ğº(ğœ†, ğ‘‡) = null(ğ‘‡ âˆ’ ğœ†ğ¼)ğ‘‘. If ğ‘‘ < dim ğ‘‰, then this exercise improves 8.20. 8 Suppose ğ‘‡ âˆˆ â„’(ğ‘‰) and ğœ†1, â€¦, ğœ†ğ‘š are the distinct eigenvalues of ğ‘‡. Prove that ğ‘‰ = ğº(ğœ†1, ğ‘‡) âŠ• â‹¯ âŠ• ğº(ğœ†ğ‘š, ğ‘‡) if and only if the minimal polynomial of ğ‘‡ equals (ğ‘§ âˆ’ ğœ†1)ğ‘˜1â‹¯(ğ‘§ âˆ’ ğœ†ğ‘š) ğ‘˜ğ‘š for some positive integers ğ‘˜1, â€¦, ğ‘˜ğ‘š. The case ğ… = ğ‚ follows immediately from 5.27(b) and the generalized eigenspace decomposition (8.22); thus this exercise is interesting only when ğ… = ğ‘. 9 Suppose ğ… = ğ‚ and ğ‘‡ âˆˆ â„’(ğ‘‰). Prove that there exist ğ·, ğ‘ âˆˆ â„’(ğ‘‰) such that ğ‘‡ = ğ· + ğ‘, the operator ğ· is diagonalizable, ğ‘ is nilpotent, and ğ·ğ‘ = ğ‘ğ·. 10 Suppose ğ‘‰ is a complex inner product space, ğ‘’1, â€¦, ğ‘’ğ‘› is an orthonormal basis of ğ‘‡, and ğ‘‡ âˆˆ â„’(ğ‘‰). Let ğœ†1, â€¦, ğœ†ğ‘› be the eigenvalues of ğ‘‡, each included as many times as its multiplicity. Prove that |ğœ†1| 2 + â‹¯ + |ğœ†ğ‘›| 2 â‰¤ â€–ğ‘‡ğ‘’1â€– 2 + â‹¯ + â€–ğ‘‡ğ‘’ğ‘›â€–2. See the comment after Exercise 5 in Section 7A. 11 Give an example of an operator on ğ‚ 4 whose characteristic polynomial equals (ğ‘§ âˆ’ 7) 2(ğ‘§ âˆ’ 8) 2. Section 8B Generalized Eigenspace Decomposition 317 12 Give an example of an operator on ğ‚ 4 whose characteristic polynomial equals (ğ‘§ âˆ’ 1)(ğ‘§ âˆ’ 5) 3 and whose minimal polynomial equals (ğ‘§ âˆ’ 1)(ğ‘§ âˆ’ 5) 2. 13 Give an example of an operator on ğ‚ 4 whose characteristic and minimal polynomials both equal ğ‘§(ğ‘§ âˆ’ 1) 2(ğ‘§ âˆ’ 3). 14 Give an example of an operator on ğ‚ 4 whose characteristic polynomial equals ğ‘§(ğ‘§ âˆ’ 1) 2(ğ‘§ âˆ’ 3)and whose minimal polynomial equals ğ‘§(ğ‘§ âˆ’ 1)(ğ‘§ âˆ’ 3). 15 Let ğ‘‡ be the operator on ğ‚ 4 defined byğ‘‡(ğ‘§1, ğ‘§2, ğ‘§3, ğ‘§4) = (0, ğ‘§1, ğ‘§2, ğ‘§3). Find the characteristic polynomial and the minimal polynomial of ğ‘‡. 16 Let ğ‘‡ be the operator on ğ‚ 6 defined by ğ‘‡(ğ‘§1, ğ‘§2, ğ‘§3, ğ‘§4, ğ‘§5, ğ‘§6) = (0, ğ‘§1, ğ‘§2, 0, ğ‘§4, 0). Find the characteristic polynomial and the minimal polynomial of ğ‘‡. 17 Suppose ğ… = ğ‚ and ğ‘ƒ âˆˆ â„’(ğ‘‰) is such that ğ‘ƒ2 = ğ‘ƒ. Prove that the characteris- tic polynomial of ğ‘ƒ is ğ‘§ğ‘š(ğ‘§âˆ’1) ğ‘›, where ğ‘š = dim null ğ‘ƒ and ğ‘› = dim range ğ‘ƒ. 18 Suppose ğ‘‡ âˆˆ â„’(ğ‘‰) and ğœ† is an eigenvalue of ğ‘‡. Explain why the following four numbers equal each other. (a) The exponent of ğ‘§ âˆ’ ğœ† in the factorization of the minimal polynomial of ğ‘‡. (b) The smallest positive integer ğ‘š such that (ğ‘‡ âˆ’ ğœ†ğ¼)ğ‘š|ğº( ğœ†, ğ‘‡)= 0. (c) The smallest positive integer ğ‘š such that null(ğ‘‡ âˆ’ ğœ†ğ¼)ğ‘š = null(ğ‘‡ âˆ’ ğœ†ğ¼)ğ‘š + 1. (d) The smallest positive integer ğ‘š such that range(ğ‘‡ âˆ’ ğœ†ğ¼)ğ‘š = range(ğ‘‡ âˆ’ ğœ†ğ¼)ğ‘š + 1. 19 Suppose ğ… = ğ‚ and ğ‘† âˆˆ â„’(ğ‘‰) is a unitary operator. Prove that the constant term in the characteristic polynomial of ğ‘† has absolute value 1. 20 Suppose that ğ… = ğ‚ and ğ‘‰1, â€¦, ğ‘‰ğ‘š are nonzero subspaces of ğ‘‰ such that ğ‘‰ = ğ‘‰1 âŠ• â‹¯ âŠ• ğ‘‰ğ‘š. Suppose ğ‘‡ âˆˆ â„’(ğ‘‰) and each ğ‘‰ğ‘˜ is invariant under ğ‘‡. For each ğ‘˜, let ğ‘ğ‘˜ denote the characteristic polynomial of ğ‘‡|ğ‘‰ğ‘˜. Prove that the characteristic polynomial of ğ‘‡ equals ğ‘1â‹¯ğ‘ğ‘š. 21 Suppose ğ‘, ğ‘ âˆˆ ğ’«(ğ‚) are monic polynomials with the same zeros and ğ‘ is a polynomial multiple of ğ‘. Prove that there exists ğ‘‡ âˆˆ â„’(ğ‚ deg ğ‘)such that the characteristic polynomial of ğ‘‡ is ğ‘ and the minimal polynomial of ğ‘‡ is ğ‘. This exercise implies that every monic polynomial is the characteristic polynomial of some operator. 318 Chapter 8 Operators on Complex Vector Spaces 22 Suppose ğ´ and ğµ are block diagonal matrices of the form ğ´ = â›âœâœâœ â ğ´1 0 â‹± 0 ğ´ğ‘š ââŸâŸâŸ â  , ğµ = â›âœâœâœ â ğµ1 0 â‹± 0 ğµğ‘š ââŸâŸâŸ â  , where ğ´ğ‘˜ and ğµğ‘˜ are square matrices of the same size for each ğ‘˜ = 1, â€¦, ğ‘š. Show that ğ´ğµ is a block diagonal matrix of the form ğ´ğµ = â›âœâœâœ â ğ´1ğµ1 0 â‹± 0 ğ´ğ‘šğµğ‘š ââŸâŸâŸ â  . 23 Suppose ğ… = ğ‘, ğ‘‡ âˆˆ â„’(ğ‘‰), and ğœ† âˆˆ ğ‚. (a) Show that ğ‘¢ + ğ‘–ğ‘£ âˆˆ ğº(ğœ†, ğ‘‡ğ‚) if and only if ğ‘¢ âˆ’ ğ‘–ğ‘£ âˆˆ ğº(ğœ†, ğ‘‡ğ‚). (b) Show that the multiplicity of ğœ† as an eigenvalue of ğ‘‡ğ‚ equals the multiplicity of ğœ† as an eigenvalue of ğ‘‡ğ‚. (c) Use (b) and the result about the sum of the multiplicities (8.25) to show that if dim ğ‘‰ is an odd number, then ğ‘‡ğ‚ has a real eigenvalue. (d) Use (c) and the result about real eigenvalues of ğ‘‡ğ‚ (Exercise 17 in Section 5A) to show that if dim ğ‘‰ is an odd number, then ğ‘‡ has an eigenvalue (thus giving an alternative proof of 5.34). See Exercise 33 in Section 3B for the definition of the complexification ğ‘‡ğ‚. Open Access This chapter is licensed under the terms of the Creative Commons Attribution-NonCommercial 4.0 International License (https://creativecommons.org/licenses/by-nc/4.0), which permits any noncommercial use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to original author and source, provide a link to the Creative Commons license, and indicate if changes were made. The images or other third party material in this chapter are included in the chapterâ€™s Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the chapterâ€™s Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. Section 8C Consequences of Generalized Eigenspace Decomposition 319 8C Consequences of Generalized Eigenspace Decomposition Square Roots of Operators Recall that a square root of an operator ğ‘‡ âˆˆ â„’(ğ‘‰) is an operator ğ‘… âˆˆ â„’(ğ‘‰) such that ğ‘…2 = ğ‘‡ (see 7.36). Every complex number has a square root, but not every operator on a complex vector space has a square root. For example, the operator on ğ‚ 3 defined byğ‘‡(ğ‘§1, ğ‘§2, ğ‘§3) = (ğ‘§2, ğ‘§3, 0)does not have a square root, as you are asked to show in Exercise 1. The noninvertibility of that operator is no accident, as we will soon see. We begin by showing that the identity plus any nilpotent operator has a square root. 8.39 identity plus nilpotent has a square root Suppose ğ‘‡ âˆˆ â„’(ğ‘‰) is nilpotent. Then ğ¼ + ğ‘‡ has a square root. Proof Consider the Taylor series for the function âˆš1+ ğ‘¥: 8.40 âˆš1+ ğ‘¥ = 1+ ğ‘1ğ‘¥ + ğ‘2ğ‘¥2 + â‹¯ . Because ğ‘1 = 1 2 , the formula above implies that 1+ ğ‘¥ 2 is a good estimate for âˆš1+ ğ‘¥ when ğ‘¥ is small. We do not find an explicit formula for the coefficients or worry about whether the infinite sum converges because we use this equation only as motivation. Because ğ‘‡ is nilpotent, ğ‘‡ğ‘š = 0for some positive integer ğ‘š. In 8.40, suppose we replace ğ‘¥ with ğ‘‡ and 1with ğ¼. Then the infinite sum on the right side becomes a finite sum (becauseğ‘‡ğ‘˜ = 0for all ğ‘˜ â‰¥ ğ‘š). Thus we guess that there is a square root of ğ¼ + ğ‘‡ of the form ğ¼ + ğ‘1ğ‘‡ + ğ‘2ğ‘‡2 + â‹¯ + ğ‘ğ‘š âˆ’ 1ğ‘‡ğ‘š âˆ’ 1. Having made this guess, we can try to choose ğ‘1, ğ‘2, â€¦, ğ‘ğ‘š âˆ’ 1 such that the operator above has its square equal to ğ¼ + ğ‘‡. Now (ğ¼+ğ‘1ğ‘‡ + ğ‘2ğ‘‡2 + ğ‘3ğ‘‡3 + â‹¯ + ğ‘ğ‘š âˆ’ 1ğ‘‡ğ‘š âˆ’ 1) 2 = ğ¼ + 2ğ‘1ğ‘‡ + (2ğ‘2 + ğ‘1 2)ğ‘‡2 + (2ğ‘3 + 2ğ‘1ğ‘2)ğ‘‡3 + â‹¯ + (2ğ‘ğ‘š âˆ’ 1 + terms involving ğ‘1, â€¦, ğ‘ğ‘š âˆ’ 2)ğ‘‡ğ‘š âˆ’ 1. We want the right side of the equation above to equal ğ¼ + ğ‘‡. Hence choose ğ‘1 such that 2ğ‘1 = 1(thus ğ‘1 = 1/2). Next, choose ğ‘2 such that 2ğ‘2 + ğ‘1 2 = 0(thus ğ‘2 = âˆ’1/8). Then choose ğ‘3 such that the coefficient of ğ‘‡3 on the right side of the equation above equals 0(thus ğ‘3 = 1/16). Continue in this fashion for each ğ‘˜ = 4, â€¦, ğ‘š âˆ’ 1, at each step solving for ğ‘ğ‘˜ so that the coefficient of ğ‘‡ğ‘˜ on the right side of the equation above equals 0. Actually we do not care about the explicit formula for the ğ‘ğ‘˜â€™s. We only need to know that some choice of the ğ‘ğ‘˜â€™s gives a square root of ğ¼ + ğ‘‡. 320 Chapter 8 Operators on Complex Vector Spaces The previous lemma is valid on real and complex vector spaces. However, the result below holds only on complex vector spaces. For example, the operator of multiplication by âˆ’1on the one-dimensional real vector space ğ‘ has no square root. Representation of a complex number with polar coordinates. For the proof below, we need to know that every ğ‘§ âˆˆ ğ‚ has a square root in ğ‚. To show this, write ğ‘§ = ğ‘Ÿ(cos ğœƒ + ğ‘– sin ğœƒ), where ğ‘Ÿ is the length of the line segment in the complex plane from the origin to ğ‘§ and ğœƒ is the angle of that line segment with the positive horizontal axis. Then âˆš ğ‘Ÿ(cos ğœƒ 2 + ğ‘– sin ğœƒ 2 ) is a square root of ğ‘§, as you can verify by showing that the square of the complex number above equals ğ‘§. 8.41 over ğ‚, invertible operators have square roots Suppose ğ‘‰ is a complex vector space and ğ‘‡ âˆˆ â„’(ğ‘‰) is invertible. Then ğ‘‡ has a square root. Proof Let ğœ†1, â€¦, ğœ†ğ‘š be the distinct eigenvalues of ğ‘‡. For each ğ‘˜, there exists a nilpotent operator ğ‘‡ğ‘˜ âˆˆ â„’(ğº(ğœ†ğ‘˜, ğ‘‡))such that ğ‘‡|ğº( ğœ†ğ‘˜, ğ‘‡)= ğœ†ğ‘˜ğ¼ + ğ‘‡ğ‘˜ [see 8.22(c)]. Because ğ‘‡ is invertible, none of the ğœ†ğ‘˜â€™s equals 0, so we can write ğ‘‡|ğº( ğœ†ğ‘˜, ğ‘‡)= ğœ†ğ‘˜(ğ¼ + ğ‘‡ğ‘˜ ğœ†ğ‘˜ ) for each ğ‘˜. Because ğ‘‡ğ‘˜/ğœ†ğ‘˜ is nilpotent, ğ¼ + ğ‘‡ğ‘˜/ğœ†ğ‘˜ has a square root (by 8.39). Multiplying a square root of the complex number ğœ†ğ‘˜ by a square root of ğ¼ + ğ‘‡ğ‘˜/ğœ†ğ‘˜, we obtain a square root ğ‘…ğ‘˜ of ğ‘‡|ğº( ğœ†ğ‘˜, ğ‘‡). By the generalized eigenspace decomposition (8.22), a typical vector ğ‘£ âˆˆ ğ‘‰ can be written uniquely in the form ğ‘£ = ğ‘¢1 + â‹¯ + ğ‘¢ğ‘š, where each ğ‘¢ğ‘˜ is in ğº(ğœ†ğ‘˜, ğ‘‡). Using this decomposition, define an operator ğ‘… âˆˆ â„’(ğ‘‰) by ğ‘…ğ‘£ = ğ‘…1ğ‘¢1 + â‹¯ + ğ‘…ğ‘šğ‘¢ğ‘š. You should verify that this operator ğ‘… is a square root of ğ‘‡, completing the proof. By imitating the techniques in this subsection, you should be able to prove that if ğ‘‰ is a complex vector space and ğ‘‡ âˆˆ â„’(ğ‘‰) is invertible, then ğ‘‡ has a ğ‘˜th root for every positive integer ğ‘˜. Section 8C Consequences of Generalized Eigenspace Decomposition 321 Jordan Form We know that if ğ‘‰ is a complex vector space, then for every ğ‘‡ âˆˆ â„’(ğ‘‰) there is a basis of ğ‘‰ with respect to which ğ‘‡ has a nice upper-triangular matrix (see 8.37). In this subsection we will see that we can do even betterâ€”there is a basis of ğ‘‰ with respect to which the matrix of ğ‘‡ contains 0â€™s everywhere except possibly on the diagonal and the line directly above the diagonal. We begin by looking at two examples of nilpotent operators. 8.42 example: nilpotent operator with nice matrix Let ğ‘‡ be the operator on ğ‚ 4 defined by ğ‘‡(ğ‘§1, ğ‘§2, ğ‘§3, ğ‘§4) = (0, ğ‘§1, ğ‘§2, ğ‘§3). Then ğ‘‡4 = 0; thus ğ‘‡ is nilpotent. If ğ‘£ = (1, 0, 0, 0), then ğ‘‡3ğ‘£, ğ‘‡2ğ‘£, ğ‘‡ğ‘£, ğ‘£ is a basis of ğ‚ 4. The matrix of ğ‘‡ with respect to this basis is â›âœâœâœâœâœâœ â 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 ââŸâŸâŸâŸâŸâŸ â  . The next example of a nilpotent operator has more complicated behavior than the example above. 8.43 example: nilpotent operator with slightly more complicated matrix Let ğ‘‡ be the operator on ğ‚ 6 defined by ğ‘‡(ğ‘§1, ğ‘§2, ğ‘§3, ğ‘§4, ğ‘§5, ğ‘§6) = (0, ğ‘§1, ğ‘§2, 0, ğ‘§4, 0). Then ğ‘‡3 = 0; thus ğ‘‡ is nilpotent. In contraast to the nice behavior of the nilpotent operator of the previous example, for this nilpotent operator there does not exist a vector ğ‘£ âˆˆ ğ‚6 such that ğ‘‡5ğ‘£, ğ‘‡4ğ‘£, ğ‘‡3ğ‘£, ğ‘‡2ğ‘£, ğ‘‡ğ‘£, ğ‘£ is a basis of ğ‚ 6. However, if we take ğ‘£1 = (1, 0, 0, 0, 0, 0), ğ‘£2 = (0, 0, 0, 1, 0, 0), and ğ‘£3 = (0, 0, 0, 0, 0, 1), then ğ‘‡2ğ‘£1, ğ‘‡ğ‘£1, ğ‘£1, ğ‘‡ğ‘£2, ğ‘£2, ğ‘£3 is a basis of ğ‚ 6. The matrix of ğ‘‡ with respect to this basis is â›âœâœâœâœâœâœâœâœâœâœâœâœâœ â â›âœâœâœ â 0 1 0 0 0 1 0 0 0 ââŸâŸâŸ â  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ( 0 1 0 0 ) 0 0 0 0 0 0 0(0) ââŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸ â  . Here the inner matrices are blocked off to show that we can think of the 6-by-6 matrix above as a block diagonal matrix consisting of a 3-by-3block with 1â€™s on the line above the diagonal and 0â€™s elsewhere, a 2-by-2block with 1above the diagonal and 0â€™s elsewhere, and a 1-by-1block containing 0. 322 Chapter 8 Operators on Complex Vector Spaces Our next goal is to show that every nilpotent operator ğ‘‡ âˆˆ â„’(ğ‘‰) behaves similarly to the operator in the previous example. Specifically, there is a finite collection of vectors ğ‘£1, â€¦, ğ‘£ğ‘› âˆˆ ğ‘‰ such that there is a basis of ğ‘‰ consisting of the vectors of the form ğ‘‡ğ‘—ğ‘£ğ‘˜, as ğ‘˜ varies from 1to ğ‘› and ğ‘— varies (in reverse order) from 0to the largest nonnegative integer ğ‘šğ‘˜ such that ğ‘‡ğ‘šğ‘˜ğ‘£ğ‘˜ â‰  0. With respect to this basis, the matrix of ğ‘‡ looks like the matrix in the previous example. More specifically,ğ‘‡ has a block diagonal matrix with respect to this basis, with each block a square matrix that is 0everywhere except on the line above the diagonal. In the next definition, the diagonal of eachğ´ğ‘˜ is filled with some eigenvalue ğœ†ğ‘˜ of ğ‘‡, the line directly above the diagonal of ğ´ğ‘˜ is filled with1â€™s, and all other entries in ğ´ğ‘˜ are 0(to understand why each ğœ†ğ‘˜ is an eigenvalue of ğ‘‡, see 5.41). The ğœ†ğ‘˜â€™s need not be distinct. Also, ğ´ğ‘˜ may be a 1-by-1matrix (ğœ†ğ‘˜) containing just an eigenvalue of ğ‘‡. If each ğœ†ğ‘˜ is 0, then the next definition captures the behavior described in the paragraph above (recall that if ğ‘‡ is nilpotent, then 0is the only eigenvalue of ğ‘‡). 8.44 definition: Jordan basis Suppose ğ‘‡ âˆˆ â„’(ğ‘‰). A basis of ğ‘‰ is called a Jordan basis for ğ‘‡ if with respect to this basis ğ‘‡ has a block diagonal matrix â›âœâœâœ â ğ´1 0 â‹± 0 ğ´ğ‘ ââŸâŸâŸ â  in which each ğ´ğ‘˜ is an upper-triangular matrix of the form ğ´ğ‘˜ = â›âœâœâœâœâœâœ â ğœ†ğ‘˜ 1 0 â‹± â‹± â‹± 1 0 ğœ†ğ‘˜ ââŸâŸâŸâŸâŸâŸ â  . Most of the work in proving that every operator on a finite-dimensional com- plex vector space has a Jordan basis occurs in proving the special case below of nilpotent operators. This special case holds on real vector spaces as well as complex vector spaces. 8.45 every nilpotent operator has a Jordan basis Suppose ğ‘‡ âˆˆ â„’(ğ‘‰) is nilpotent. Then there is a basis of ğ‘‰ that is a Jordan basis for ğ‘‡. Proof We will prove this result by induction on dim ğ‘‰. To get started, note that the desired result holds if dim ğ‘‰ = 1(because in that case, the only nilpotent operator is the 0operator). Now assume that dim ğ‘‰ > 1and that the desired result holds on all vector spaces of smaller dimension. Section 8C Consequences of Generalized Eigenspace Decomposition 323 Let ğ‘š be the smallest positive integer such that ğ‘‡ğ‘š = 0. Thus there exists ğ‘¢ âˆˆ ğ‘‰ such that ğ‘‡ğ‘š âˆ’ 1ğ‘¢ â‰  0. Let ğ‘ˆ = span(ğ‘¢, ğ‘‡ğ‘¢, â€¦, ğ‘‡ğ‘š âˆ’ 1ğ‘¢). The list ğ‘¢, ğ‘‡ğ‘¢, â€¦, ğ‘‡ğ‘š âˆ’ 1ğ‘¢ is linearly independent (see Exercise 2 in Section 8A). If ğ‘ˆ = ğ‘‰, then writing this list in reverse order gives a Jordan basis for ğ‘‡ and we are done. Thus we can assume that ğ‘ˆ â‰  ğ‘‰. Note that ğ‘ˆ is invariant under ğ‘‡. By our induction hypothesis, there is a basis of ğ‘ˆ that is a Jordan basis for ğ‘‡|ğ‘ˆ. The strategy of our proof is that we will find a subspace ğ‘Š of ğ‘‰ such that ğ‘Š is also invariant under ğ‘‡ and ğ‘‰ = ğ‘ˆ âŠ• ğ‘Š. Again by our induction hypothesis, there will be a basis of ğ‘Š that is a Jordan basis for ğ‘‡|ğ‘Š. Putting together the Jordan bases for ğ‘‡|ğ‘ˆ and ğ‘‡|ğ‘Š, we will have a Jordan basis for ğ‘‡. Let ğœ‘ âˆˆ ğ‘‰â€² be such that ğœ‘(ğ‘‡ğ‘š âˆ’ 1ğ‘¢)â‰  0. Let ğ‘Š = {ğ‘£ âˆˆ ğ‘‰ âˆ¶ ğœ‘(ğ‘‡ğ‘˜ğ‘£)= 0for each ğ‘˜ = 0, â€¦, ğ‘š âˆ’ 1}. Then ğ‘Š is a subspace of ğ‘‰ that is invariant under ğ‘‡ (the invariance holds because if ğ‘£ âˆˆ ğ‘Š then ğœ‘(ğ‘‡ğ‘˜(ğ‘‡ğ‘£))= 0for ğ‘˜ = 0, â€¦, ğ‘š âˆ’ 1, where the case ğ‘˜ = ğ‘š âˆ’ 1 holds because ğ‘‡ğ‘š = 0). We will show that ğ‘‰ = ğ‘ˆ âŠ• ğ‘Š, which by the previous paragraph will complete the proof. To show that ğ‘ˆ + ğ‘Š is a direct sum, suppose ğ‘£ âˆˆ ğ‘ˆ âˆ© ğ‘Šwith ğ‘£ â‰  0. Because ğ‘£ âˆˆ ğ‘ˆ, there exist ğ‘0, â€¦, ğ‘ğ‘š âˆ’ 1 âˆˆ ğ… such that ğ‘£ = ğ‘0ğ‘¢ + ğ‘1ğ‘‡ğ‘¢ + â‹¯ + ğ‘ğ‘š âˆ’ 1ğ‘‡ğ‘š âˆ’ 1ğ‘¢. Let ğ‘— be the smallest index such that ğ‘ğ‘— â‰  0. Apply ğ‘‡ğ‘š âˆ’ ğ‘— âˆ’ 1 to both sides of the equation above, getting ğ‘‡ğ‘š âˆ’ ğ‘— âˆ’ 1ğ‘£ = ğ‘ğ‘—ğ‘‡ğ‘š âˆ’ 1ğ‘¢, where we have used the equation ğ‘‡ğ‘š = 0. Now apply ğœ‘ to both sides of the equation above, getting ğœ‘(ğ‘‡ğ‘š âˆ’ ğ‘— âˆ’ 1ğ‘£)= ğ‘ğ‘—ğœ‘(ğ‘‡ğ‘š âˆ’ 1ğ‘¢)â‰  0. The equation above shows that ğ‘£ âˆ‰ ğ‘Š. Hence we have proved that ğ‘ˆ âˆ© ğ‘Š = {0}, which implies that ğ‘ˆ + ğ‘Š is a direct sum (see 1.46). To show that ğ‘ˆ âŠ• ğ‘Š = ğ‘‰, defineğ‘†âˆ¶ ğ‘‰ â†’ ğ…ğ‘š by ğ‘†ğ‘£ = (ğœ‘(ğ‘£), ğœ‘(ğ‘‡ğ‘£), â€¦, ğœ‘(ğ‘‡ğ‘š âˆ’ 1ğ‘£)). Thus null ğ‘† = ğ‘Š. Hence dim ğ‘Š = dim null ğ‘† = dim ğ‘‰ âˆ’ dim range ğ‘† â‰¥dim ğ‘‰ âˆ’ ğ‘š, where the second equality comes from the fundamental theorem of linear maps (3.21). Using the inequality above, we have dim(ğ‘ˆ âŠ• ğ‘Š) = dim ğ‘ˆ + dim ğ‘Š â‰¥ ğ‘š+ (dim ğ‘‰ âˆ’ ğ‘š) = dim ğ‘‰. Thus ğ‘ˆ âŠ• ğ‘Š = ğ‘‰ (by 2.39), completing the proof. 324 Chapter 8 Operators on Complex Vector Spaces Camille Jordan (1838â€“1922) pub- lished a proof of 8.46 in 1870. Now the generalized eigenspace de- composition allows us to extend the pre- vious result to operators that may not be nilpotent. Doing this requires that we deal with complex vector spaces. 8.46 Jordan form Suppose ğ… = ğ‚ and ğ‘‡ âˆˆ â„’(ğ‘‰). Then there is a basis of ğ‘‰ that is a Jordan basis for ğ‘‡. Proof Let ğœ†1, â€¦, ğœ†ğ‘š be the distinct eigenvalues of ğ‘‡. The generalized eigenspace decomposition states that ğ‘‰ = ğº(ğœ†1, ğ‘‡) âŠ• â‹¯ âŠ• ğº(ğœ†ğ‘š, ğ‘‡), where each (ğ‘‡ âˆ’ ğœ†ğ‘˜ğ¼)|ğº( ğœ†ğ‘˜, ğ‘‡)is nilpotent (see 8.22). Thus 8.45 implies that some basis of each ğº(ğœ†ğ‘˜, ğ‘‡) is a Jordan basis for (ğ‘‡ âˆ’ ğœ†ğ‘˜ğ¼)|ğº( ğœ†ğ‘˜, ğ‘‡). Put these bases together to get a basis of ğ‘‰ that is a Jordan basis for ğ‘‡. Exercises 8C 1 Suppose ğ‘‡ âˆˆ â„’(ğ‚ 3)is the operator defined byğ‘‡(ğ‘§1, ğ‘§2, ğ‘§3) = (ğ‘§2, ğ‘§3, 0). Prove that ğ‘‡ does not have a square root. 2 Defineğ‘‡ âˆˆ â„’(ğ…5)by ğ‘‡(ğ‘¥1, ğ‘¥2, ğ‘¥3, ğ‘¥4, ğ‘¥5) = (2ğ‘¥2, 3ğ‘¥3, âˆ’ğ‘¥4, 4ğ‘¥5, 0). (a) Show that ğ‘‡ is nilpotent. (b) Find a square root of ğ¼ + ğ‘‡. 3 Suppose ğ‘‰ is a complex vector space. Prove that every invertible operator on ğ‘‰ has a cube root. 4 Suppose ğ‘‰ is a real vector space. Prove that the operator âˆ’ğ¼ on ğ‘‰ has a square root if and only if dim ğ‘‰ is an even number. 5 Suppose ğ‘‡ âˆˆ â„’(ğ‚ 2)is the operator defined byğ‘‡(ğ‘¤, ğ‘§) = (âˆ’ğ‘¤ âˆ’ ğ‘§, 9ğ‘¤+ 5ğ‘§). Find a Jordan basis for ğ‘‡. 6 Find a basis of ğ’«4(ğ‘) that is a Jordan basis for the differentiation operator ğ· on ğ’«4(ğ‘) defined byğ·ğ‘ = ğ‘â€². 7 Suppose ğ‘‡ âˆˆ â„’(ğ‘‰) is nilpotent and ğ‘£1, â€¦, ğ‘£ğ‘› is a Jordan basis for ğ‘‡. Prove that the minimal polynomial of ğ‘‡ is ğ‘§ ğ‘š + 1, where ğ‘š is the length of the longest consecutive string of 1â€™s that appears on the line directly above the diagonal in the matrix of ğ‘‡ with respect to ğ‘£1, â€¦, ğ‘£ğ‘›. 8 Suppose ğ‘‡ âˆˆ â„’(ğ‘‰) and ğ‘£1, â€¦, ğ‘£ğ‘› is a basis of ğ‘‰ that is a Jordan basis for ğ‘‡. Describe the matrix of ğ‘‡2 with respect to this basis. Section 8C Consequences of Generalized Eigenspace Decomposition 325 9 Suppose ğ‘‡ âˆˆ â„’(ğ‘‰) is nilpotent. Explain why there exist ğ‘£1, â€¦, ğ‘£ğ‘› âˆˆ ğ‘‰ and nonnegative integers ğ‘š1, â€¦, ğ‘šğ‘› such that (a) and (b) below both hold. (a) ğ‘‡ğ‘š1ğ‘£1, â€¦, ğ‘‡ğ‘£1, ğ‘£1, â€¦, ğ‘‡ğ‘šğ‘›ğ‘£ğ‘›, â€¦, ğ‘‡ğ‘£ğ‘›, ğ‘£ğ‘› is a basis of ğ‘‰. (b) ğ‘‡ğ‘š1 + 1ğ‘£1 = â‹¯ = ğ‘‡ğ‘šğ‘› + 1ğ‘£ğ‘› = 0. 10 Suppose ğ‘‡ âˆˆ â„’(ğ‘‰) and ğ‘£1, â€¦, ğ‘£ğ‘› is a basis of ğ‘‰ that is a Jordan basis for ğ‘‡. Describe the matrix of ğ‘‡ with respect to the basis ğ‘£ğ‘›, â€¦, ğ‘£1 obtained by reversing the order of the ğ‘£â€™s. 11 Suppose ğ‘‡ âˆˆ â„’(ğ‘‰). Explain why every vector in each Jordan basis for ğ‘‡ is a generalized eigenvector of ğ‘‡. 12 Suppose ğ‘‡ âˆˆ â„’(ğ‘‰) is diagonalizable. Show that â„³(ğ‘‡) is a diagonal matrix with respect to every Jordan basis for ğ‘‡. 13 Suppose ğ‘‡ âˆˆ â„’(ğ‘‰) is nilpotent. Prove that if ğ‘£1, â€¦, ğ‘£ğ‘› are vectors in ğ‘‰ and ğ‘š1, â€¦, ğ‘šğ‘› are nonnegative integers such that ğ‘‡ğ‘š1ğ‘£1, â€¦, ğ‘‡ğ‘£1, ğ‘£1, â€¦, ğ‘‡ğ‘šğ‘›ğ‘£ğ‘›, â€¦, ğ‘‡ğ‘£ğ‘›, ğ‘£ğ‘› is a basis of ğ‘‰ and ğ‘‡ğ‘š1 + 1ğ‘£1 = â‹¯ = ğ‘‡ğ‘šğ‘› + 1ğ‘£ğ‘› = 0, then ğ‘‡ğ‘š1ğ‘£1, â€¦, ğ‘‡ğ‘šğ‘›ğ‘£ğ‘› is a basis of null ğ‘‡. This exercise shows that ğ‘› = dim null ğ‘‡. Thus the positive integer ğ‘› that appears above depends only on ğ‘‡ and not on the specific Jordan basis chosen for ğ‘‡. 14 Suppose ğ… = ğ‚ and ğ‘‡ âˆˆ â„’(ğ‘‰). Prove that there does not exist a direct sum decomposition of ğ‘‰ into two nonzero subspaces invariant under ğ‘‡ if and only if the minimal polynomial of ğ‘‡ is of the form (ğ‘§ âˆ’ ğœ†) dim ğ‘‰ for some ğœ† âˆˆ ğ‚. 326 Chapter 8 Operators on Complex Vector Spaces 8D Trace: A Connection Between Matrices and Operators We begin this section by defining the trace of a square matrix. After developing some properties of the trace of a square matrix, we will use this concept to define the trace of an operator. 8.47 definition:trace of a matrix Suppose ğ´ is a square matrix with entries in ğ…. The trace of ğ´, denoted by tr ğ´, is defined to be the sum of the diagonal entries ofğ´. 8.48 example: trace of a 3-by-3matrix Suppose ğ´ = â›âœâœâœ â 3 âˆ’1 âˆ’2 3 2 âˆ’3 1 2 0 ââŸâŸâŸ â  . The diagonal entries of ğ´, which are shown in red above, are 3, 2, and 0. Thus tr ğ´ = 3+ 2+ 0 = 5. Matrix multiplication is not commutative, but the next result shows that the order of matrix multiplication does not matter to the trace. 8.49 trace of ğ´ğµ equals trace of ğµğ´ Suppose ğ´ is an ğ‘š-by-ğ‘› matrix and ğµ is an ğ‘›-by-ğ‘š matrix. Then tr(ğ´ğµ) = tr(ğµğ´). Proof Suppose ğ´ = â›âœâœâœâœ â ğ´1, 1 â‹¯ ğ´1, ğ‘› â‹® â‹® ğ´ğ‘š, 1 â‹¯ ğ´ğ‘š, ğ‘› ââŸâŸâŸâŸ â  , ğµ = â›âœâœâœâœ â ğµ1, 1 â‹¯ ğµ1, ğ‘š â‹® â‹® ğµğ‘›, 1 â‹¯ ğµğ‘›, ğ‘š ââŸâŸâŸâŸ â  . The ğ‘—th term on the diagonal of the ğ‘š-by-ğ‘š matrix ğ´ğµ equals âˆ‘ ğ‘› ğ‘˜ = 1 ğ´ğ‘—, ğ‘˜ğµğ‘˜, ğ‘—. Thus tr(ğ´ğµ) = ğ‘š âˆ‘ ğ‘— = 1 ğ‘› âˆ‘ ğ‘˜ = 1 ğ´ğ‘—, ğ‘˜ğµğ‘˜, ğ‘— = ğ‘› âˆ‘ ğ‘˜ = 1 ğ‘š âˆ‘ ğ‘— = 1 ğµğ‘˜, ğ‘—ğ´ğ‘—, ğ‘˜ = ğ‘› âˆ‘ ğ‘˜ = 1(ğ‘˜th term on diagonal of the ğ‘›-by-ğ‘› matrix ğµğ´) = tr(ğµğ´), as desired. Section 8D Trace: A Connection Between Matrices and Operators 327 We want to define the trace of an operatorğ‘‡ âˆˆ â„’(ğ‘‰) to be the trace of the matrix of ğ‘‡ with respect to some basis of ğ‘‰. However, this definition should not depend on the choice of basis. The following result will make this possible. 8.50 trace of matrix of operator does not depend on basis Suppose ğ‘‡ âˆˆ â„’(ğ‘‰). Suppose ğ‘¢1, â€¦, ğ‘¢ğ‘› and ğ‘£1, â€¦, ğ‘£ğ‘› are bases of ğ‘‰. Then tr â„³(ğ‘‡, (ğ‘¢1, â€¦, ğ‘¢ğ‘›))= tr â„³(ğ‘‡, (ğ‘£1, â€¦, ğ‘£ğ‘›)). Proof Let ğ´ = â„³(ğ‘‡, (ğ‘¢1, â€¦, ğ‘¢ğ‘›))and ğµ = â„³(ğ‘‡, (ğ‘£1, â€¦, ğ‘£ğ‘›)). The change-of- basis formula tells us that there exists an invertible ğ‘›-by-ğ‘› matrix ğ¶ such that ğ´ = ğ¶âˆ’1ğµğ¶ (see 3.84). Thus tr ğ´ = tr((ğ¶âˆ’1ğµ)ğ¶) = tr(ğ¶(ğ¶âˆ’1ğµ)) = tr((ğ¶ğ¶âˆ’1)ğµ) = tr ğµ, where the second line comes from 8.49. Because of 8.50, the following definition now makes sense. 8.51 definition: trace of an operator Suppose ğ‘‡ âˆˆ â„’(ğ‘‰). The trace of ğ‘‡, denote tr ğ‘‡, is defined by tr ğ‘‡ = tr â„³(ğ‘‡, (ğ‘£1, â€¦, ğ‘£ğ‘›)), where ğ‘£1, â€¦, ğ‘£ğ‘› is any basis of ğ‘‰. Suppose ğ‘‡ âˆˆ â„’(ğ‘‰) and ğœ† is an eigenvalue of ğ‘‡. Recall that we defined the multiplicity of ğœ† to be the dimension of the generalized eigenspace ğº(ğœ†, ğ‘‡) (see 8.23); we proved that this multiplicity equals dim null(ğ‘‡ âˆ’ ğœ†ğ¼)dim ğ‘‰ (see 8.20). Recall also that if ğ‘‰ is a complex vector space, then the sum of the multiplicities of all eigenvalues of ğ‘‡ equals dim ğ‘‰ (see 8.25). In the definition below, the sum of the eigenvalues â€œwith each eigenvalue included as many times as its multiplicityâ€ means that ifğœ†1, â€¦, ğœ†ğ‘š are the distinct eigenvalues of ğ‘‡ with multiplicities ğ‘‘1, â€¦, ğ‘‘ğ‘š, then the sum is ğ‘‘1 ğœ†1 + â‹¯ + ğ‘‘ğ‘š ğœ†ğ‘š. Or if you prefer to work with a list of not-necessarily-distinct eigenvalues, with each eigenvalue included as many times as its multiplicity, then the eigenvalues could be denoted by ğœ†1, â€¦, ğœ†ğ‘› (where ğ‘› equals dim ğ‘‰) and the sum is ğœ†1 + â‹¯ + ğœ†ğ‘›. 328 Chapter 8 Operators on Complex Vector Spaces 8.52 on complex vector spaces, trace equals sum of eigenvalues Suppose ğ… = ğ‚ and ğ‘‡ âˆˆ â„’(ğ‘‰). Then tr ğ‘‡ equals the sum of the eigenvalues of ğ‘‡, with each eigenvalue included as many times as its multiplicity. Proof There is a basis of ğ‘‰ with respect to which ğ‘‡ has an upper-triangular matrix with the diagonal entries of the matrix consisting of the eigenvalues of ğ‘‡, with each eigenvalue included as many times as its multiplicityâ€”see 8.37. Thus the definition of the trace of an operator along with8.50, which allows us to use a basis of our choice, implies that tr ğ‘‡ equals the sum of the eigenvalues of ğ‘‡, with each eigenvalue included as many times as its multiplicity. 8.53 example: trace of an operator on ğ‚ 3 Suppose ğ‘‡ âˆˆ â„’(ğ‚ 3)is defined by ğ‘‡(ğ‘§1, ğ‘§2, ğ‘§3) = (3ğ‘§1 âˆ’ ğ‘§2 âˆ’ 2ğ‘§3, 3ğ‘§1 + 2ğ‘§2 âˆ’ 3ğ‘§3, ğ‘§1 + 2ğ‘§2). Then the matrix of ğ‘‡ with respect to the standard basis of ğ‚ 3 is â›âœâœâœ â 3 âˆ’1 âˆ’2 3 2 âˆ’3 1 2 0 ââŸâŸâŸ â  . Adding up the diagonal entries of this matrix, we see that tr ğ‘‡ = 5. The eigenvalues of ğ‘‡ are 1, 2+ 3ğ‘–, and 2 âˆ’ 3ğ‘–, each with multiplicity 1, as you can verify. The sum of these eigenvalues, each included as many times as its multiplicity, is 1+ (2+ 3ğ‘–)+ (2 âˆ’ 3ğ‘–), which equals 5, as expected by 8.52. The trace has a close connection with the characteristic polynomial. Suppose ğ… = ğ‚, ğ‘‡ âˆˆ â„’(ğ‘‰), and ğœ†1, â€¦, ğœ†ğ‘› are the eigenvalues of ğ‘‡, with each eigenvalue included as many times as its multiplicity. Then by definition (see8.26), the characteristic polynomial of ğ‘‡ equals (ğ‘§ âˆ’ ğœ†1)â‹¯(ğ‘§ âˆ’ ğœ†ğ‘›). Expanding the polynomial above, we can write the characteristic polynomial of ğ‘‡ in the form ğ‘§ğ‘› âˆ’ (ğœ†1 + â‹¯ + ğœ†ğ‘›)ğ‘§ ğ‘› âˆ’ 1 + â‹¯ + (âˆ’1) ğ‘›(ğœ†1â‹¯ğœ†ğ‘›). The expression above immediately leads to the next result. Also see 9.65, which does not require the hypothesis that ğ… = ğ‚. 8.54 trace and characteristic polynomial Suppose ğ… = ğ‚ and ğ‘‡ âˆˆ â„’(ğ‘‰). Let ğ‘› = dim ğ‘‰. Then tr ğ‘‡ equals the negative of the coefficient of ğ‘§ ğ‘› âˆ’ 1 in the characteristic polynomial of ğ‘‡. Section 8D Trace: A Connection Between Matrices and Operators 329 The next result gives a nice formula for the trace of an operator on an inner product space. 8.55 trace on an inner product space Suppose ğ‘‰ is an inner product space, ğ‘‡ âˆˆ â„’(ğ‘‰), and ğ‘’1, â€¦, ğ‘’ğ‘› is an orthonor- mal basis of ğ‘‰. Then tr ğ‘‡ = âŸ¨ğ‘‡ğ‘’1, ğ‘’1âŸ©+ â‹¯ + âŸ¨ğ‘‡ğ‘’ğ‘›, ğ‘’ğ‘›âŸ©. Proof The desired formula follows from the observation that the entry in row ğ‘˜, column ğ‘˜ of â„³(ğ‘‡, (ğ‘’1, â€¦, ğ‘’ğ‘›))equals âŸ¨ğ‘‡ğ‘’ğ‘˜, ğ‘’ğ‘˜âŸ©[use6.30(a) with ğ‘£ = ğ‘‡ğ‘’ğ‘˜]. The algebraic properties of the trace as defined on square matrices translate to algebraic properties of the trace as defined on operators, as shown in the next result. 8.56 trace is linear The function trâˆ¶ â„’(ğ‘‰) â†’ ğ… is a linear functional on â„’(ğ‘‰) such that tr(ğ‘†ğ‘‡) = tr(ğ‘‡ğ‘†) for all ğ‘†, ğ‘‡ âˆˆ â„’(ğ‘‰). Proof Choose a basis of ğ‘‰. All matrices of operators in this proof will be with respect to that basis. Suppose ğ‘†, ğ‘‡ âˆˆ â„’(ğ‘‰). If ğœ† âˆˆ ğ…, then tr(ğœ†ğ‘‡) = tr â„³(ğœ†ğ‘‡) = tr(ğœ†â„³(ğ‘‡))= ğœ† tr â„³(ğ‘‡) = ğœ† tr ğ‘‡, where the first and last equalities come from the definition of the trace of an operator, the second equality comes from 3.38, and the third equality follows from the definition of the trace of a square matrix. Also, tr(ğ‘† + ğ‘‡) = tr â„³(ğ‘† + ğ‘‡) = tr(â„³(ğ‘†) + â„³(ğ‘‡))= tr â„³(ğ‘†) + tr â„³(ğ‘‡) = tr ğ‘† + tr ğ‘‡, where the first and last equalities come from the definition of the trace of an operator, the second equality comes from 3.35, and the third equality follows from the definition of the trace of a square matrix. The two paragraphs above show that trâˆ¶ â„’(ğ‘‰) â†’ ğ… is a linear functional on â„’(ğ‘‰). Furthermore, tr(ğ‘†ğ‘‡) = tr â„³(ğ‘†ğ‘‡) = tr(â„³(ğ‘†)â„³(ğ‘‡))= tr(â„³(ğ‘‡)â„³(ğ‘†))= tr â„³(ğ‘‡ğ‘†) = tr(ğ‘‡ğ‘†), where the second and fourth equalities come from 3.43 and the crucial third equality comes from 8.49. The equations tr(ğ‘†ğ‘‡) = tr(ğ‘‡ğ‘†) and tr ğ¼ = dim ğ‘‰ uniquely characterize the trace among the linear functionals on â„’(ğ‘‰)â€”see Exercise 10. 330 Chapter 8 Operators on Complex Vector Spaces The statement of the next result does not involve traces, but the short proof uses traces. When something like this happens in mathematics, then usually a good definition lurks in the back- ground. The equation tr(ğ‘†ğ‘‡) = tr(ğ‘‡ğ‘†) leads to our next result, which does not hold on infinite-dimensional vector spaces (see Exercise 13). However, additional hy- potheses on ğ‘†, ğ‘‡, and ğ‘‰ lead to an infinite- dimensional generalization of the result below, with important applications to quantum theory. 8.57 identity operator is not the difference of ğ‘†ğ‘‡ and ğ‘‡ğ‘† There do not exist operators ğ‘†, ğ‘‡ âˆˆ â„’(ğ‘‰) such that ğ‘†ğ‘‡ âˆ’ ğ‘‡ğ‘† = ğ¼. Proof Suppose ğ‘†, ğ‘‡ âˆˆ â„’(ğ‘‰). Then tr(ğ‘†ğ‘‡ âˆ’ ğ‘‡ğ‘†) = tr(ğ‘†ğ‘‡) âˆ’ tr(ğ‘‡ğ‘†) = 0, where both equalities come from 8.56. The trace of ğ¼ equals dim ğ‘‰, which is not 0. Because ğ‘†ğ‘‡ âˆ’ ğ‘‡ğ‘† and ğ¼ have different traces, they cannot be equal. Exercises 8D 1 Suppose ğ‘‰ is an inner product space and ğ‘£, ğ‘¤ âˆˆ ğ‘‰. Define an operator ğ‘‡ âˆˆ â„’(ğ‘‰) by ğ‘‡ğ‘¢ = âŸ¨ğ‘¢, ğ‘£âŸ©ğ‘¤. Find a formula for tr ğ‘‡. 2 Suppose ğ‘ƒ âˆˆ â„’(ğ‘‰) satisfiesğ‘ƒ2 = ğ‘ƒ. Prove that tr ğ‘ƒ = dim range ğ‘ƒ. 3 Suppose ğ‘‡ âˆˆ â„’(ğ‘‰) and ğ‘‡5 = ğ‘‡. Prove that the real and imaginary parts of tr ğ‘‡ are both integers. 4 Suppose ğ‘‰ is an inner product space and ğ‘‡ âˆˆ â„’(ğ‘‰). Prove that tr ğ‘‡âˆ— = tr ğ‘‡. 5 Suppose ğ‘‰ is an inner product space. Suppose ğ‘‡ âˆˆ â„’(ğ‘‰) is a positive operator and tr ğ‘‡ = 0. Prove that ğ‘‡ = 0. 6 Suppose ğ‘‰ is an inner product space and ğ‘ƒ, ğ‘„ âˆˆ â„’(ğ‘‰) are orthogonal projections. Prove that tr(ğ‘ƒğ‘„) â‰¥ 0. 7 Suppose ğ‘‡ âˆˆ â„’(ğ‚ 3)is the operator whose matrix is â›âœâœâœ â 51 âˆ’12 âˆ’21 60 âˆ’40 âˆ’28 57 âˆ’68 1 ââŸâŸâŸ â  . Someone tells you (accurately) that âˆ’48and 24are eigenvalues of ğ‘‡. Without using a computer or writing anything down, find the third eigenvalue ofğ‘‡. Section 8D Trace: A Connection Between Matrices and Operators 331 8 Prove or give a counterexample: If ğ‘†, ğ‘‡ âˆˆ â„’(ğ‘‰), then tr(ğ‘†ğ‘‡) = (tr ğ‘†)(tr ğ‘‡). 9 Suppose ğ‘‡ âˆˆ â„’(ğ‘‰) is such that tr(ğ‘†ğ‘‡) = 0for all ğ‘† âˆˆ â„’(ğ‘‰). Prove that ğ‘‡ = 0. 10 Prove that the trace is the only linear functional ğœâˆ¶ â„’(ğ‘‰) â†’ ğ… such that ğœ(ğ‘†ğ‘‡) = ğœ(ğ‘‡ğ‘†) for all ğ‘†, ğ‘‡ âˆˆ â„’(ğ‘‰) and ğœ(ğ¼) = dim ğ‘‰. Hint: Suppose that ğ‘£1, â€¦, ğ‘£ğ‘› is a basis of ğ‘‰. For ğ‘—, ğ‘˜ âˆˆ {1, â€¦, ğ‘›}, define ğ‘ƒğ‘—, ğ‘˜ âˆˆ â„’(ğ‘‰) by ğ‘ƒğ‘—, ğ‘˜(ğ‘1ğ‘£1 + â‹¯ + ğ‘ğ‘›ğ‘£ğ‘›) = ğ‘ğ‘˜ğ‘£ğ‘—. Prove that ğœ(ğ‘ƒğ‘—, ğ‘˜) = â§{ â¨{â© 1 if ğ‘— = ğ‘˜, 0 if ğ‘— â‰  ğ‘˜. Then for ğ‘‡ âˆˆ â„’(ğ‘‰), use the equation ğ‘‡ = âˆ‘ ğ‘› ğ‘˜ = 1 âˆ‘ ğ‘› ğ‘— = 1 â„³(ğ‘‡)ğ‘—, ğ‘˜ğ‘ƒğ‘—, ğ‘˜ to show that ğœ(ğ‘‡) = tr ğ‘‡. 11 Suppose ğ‘‰ and ğ‘Š are inner product spaces and ğ‘‡ âˆˆ â„’(ğ‘‰, ğ‘Š). Prove that if ğ‘’1, â€¦, ğ‘’ğ‘› is an orthonormal basis of ğ‘‰ and ğ‘“1, â€¦, ğ‘“ğ‘š is an orthonormal basis of ğ‘Š, then tr(ğ‘‡âˆ—ğ‘‡)= ğ‘› âˆ‘ ğ‘˜ = 1 ğ‘š âˆ‘ ğ‘— = 1|âŸ¨ğ‘‡ğ‘’ğ‘˜, ğ‘“ğ‘—âŸ©| 2. The numbers âŸ¨ğ‘‡ğ‘’ğ‘˜, ğ‘“ğ‘—âŸ©are the entries of the matrix of ğ‘‡ with respect to the orthonormal bases ğ‘’1, â€¦, ğ‘’ğ‘› and ğ‘“1, â€¦, ğ‘“ğ‘š. These numbers depend on the bases, but tr(ğ‘‡âˆ—ğ‘‡) does not depend on a choice of bases. Thus this exercise shows that the sum of the squares of the absolute values of the matrix entries does not depend on which orthonormal bases are used. 12 Suppose ğ‘‰ and ğ‘Š are finite-dimensional inner product spaces. (a) Prove that âŸ¨ğ‘†, ğ‘‡âŸ© =tr(ğ‘‡âˆ—ğ‘†)defines an inner product onâ„’(ğ‘‰, ğ‘Š). (b) Suppose ğ‘’1, â€¦, ğ‘’ğ‘› is an orthonormal basis of ğ‘‰ and ğ‘“1, â€¦, ğ‘“ğ‘š is an or- thonormal basis of ğ‘Š. Show that the inner product on â„’(ğ‘‰, ğ‘Š) from (a) is the same as the standard inner product on ğ…ğ‘šğ‘›, where we identify each element of â„’(ğ‘‰, ğ‘Š) with its matrix (with respect to the bases just mentioned) and then with an element of ğ…ğ‘šğ‘›. Caution: The norm of a linear map ğ‘‡ âˆˆ â„’(ğ‘‰, ğ‘Š) as defined by 7.86 is not the same as the norm that comes from the inner product in (a) above. Unless explicitly stated otherwise, always assume that â€–ğ‘‡â€– refers to the norm as defined by 7.86. The norm that comes from the inner product in (a) is called the Frobenius norm or the Hilbertâ€“Schmidt norm. 13 Find ğ‘†, ğ‘‡ âˆˆ â„’(ğ’«(ğ…))such that ğ‘†ğ‘‡ âˆ’ ğ‘‡ğ‘† = ğ¼. Hint: Make an appropriate modification of the operators in Example 3.9. This exercise shows that additional hypotheses are needed on ğ‘† and ğ‘‡ to extend 8.57 to the setting of infinite-dimensional vector spaces. Chapter 9 Multilinear Algebra and Determinants We begin this chapter by investigating bilinear forms and quadratic forms on a vector space. Then we will move on to multilinear forms. We will show that the vector space of alternating ğ‘›-linear forms has dimension one on a vector space of dimension ğ‘›. This result will allow us to give a clean basis-free definition of the determinant of an operator. This approach to the determinant via alternating multilinear forms leads to straightforward proofs of key properties of determinants. For example, we will see that the determinant is multiplicative, meaning that det(ğ‘†ğ‘‡) = (det ğ‘†)(det ğ‘‡) for all operators ğ‘† and ğ‘‡ on the same vector space. We will also see that ğ‘‡ is invertible if and only if det ğ‘‡ â‰  0. Another important result states that the determinant of an operator on a complex vector space equals the product of the eigenvalues of the operator, with each eigenvalue included as many times as its multiplicity. The chapter concludes with an introduction to tensor products. standing assumptions for this chapter â€¢ ğ… denotes ğ‘ or ğ‚. â€¢ ğ‘‰ and ğ‘Š denote finite-dimensional nonzero vector spaces overğ….MatthewPetroffCCBY-SA The Mathematical Institute at the University of GÃ¶ttingen. This building opened in 1930, when Emmy Noether (1882â€“1935) had already been a research mathematician and faculty member at the university for 15 years (the first eight years without salary). Noether was fired by the Nazi government in 1933. By then Noether and her collaborators had created many of the foundations of modern algebra, including an abstract algebra viewpoint that contributed to the development of linear algebra. 332 S. Axler, Linear Algebra Done Right, Undergraduate Texts in Mathematics, https://doi.org/10.1007/978-3-031-41026-0_9 Â© Sheldon Axler 2024 Section 9A Bilinear Forms and Quadratic Forms 333 9A Bilinear Forms and Quadratic Forms Bilinear Forms A bilinear form on ğ‘‰ is a function from ğ‘‰ Ã— ğ‘‰ to ğ… that is linear in each slot separately, meaning that if we hold either slot fixed then we have a linear function in the other slot. Here is the formal definition. 9.1 definition: bilinear form A bilinear form on ğ‘‰ is a function ğ›½âˆ¶ ğ‘‰ Ã— ğ‘‰ â†’ ğ… such that ğ‘£ â†¦ ğ›½(ğ‘£, ğ‘¢) and ğ‘£ â†¦ ğ›½(ğ‘¢, ğ‘£) are both linear functionals on ğ‘‰ for every ğ‘¢ âˆˆ ğ‘‰. Recall that the term linear functional, used in the definition above, means a linear function that maps into the scalar field ğ…. Thus the term bilinear functional would be more consistent terminology than bilinear form, which unfortunately has become standard. For example, if ğ‘‰ is a real inner prod- uct space, then the function that takes an ordered pair (ğ‘¢, ğ‘£) âˆˆ ğ‘‰ Ã— ğ‘‰ to âŸ¨ğ‘¢, ğ‘£âŸ©is a bilinear form on ğ‘‰. If ğ‘‰ is a nonzero complex inner product space, then this function is not a bilinear form because the inner product is not linear in the sec- ond slot (complex scalars come out of the second slot as their complex conjugates). If ğ… = ğ‘, then a bilinear form differs from an inner product in that an inner product requires symmetry [meaning that ğ›½(ğ‘£, ğ‘¤) = ğ›½(ğ‘¤, ğ‘£) for all ğ‘£, ğ‘¤ âˆˆ ğ‘‰] and positive definiteness[meaning that ğ›½(ğ‘£, ğ‘£) > 0for all ğ‘£ âˆˆ ğ‘‰\\{0}], but these properties are not required for a bilinear form. 9.2 example: bilinear forms â€¢ The function ğ›½âˆ¶ ğ…3 Ã— ğ…3 â†’ ğ… defined by ğ›½((ğ‘¥1, ğ‘¥2, ğ‘¥3), (ğ‘¦1, ğ‘¦2, ğ‘¦3))= ğ‘¥1ğ‘¦2 âˆ’ 5ğ‘¥2ğ‘¦3 + 2ğ‘¥3ğ‘¦1 is a bilinear form on ğ…3. â€¢ Suppose ğ´ is an ğ‘›-by-ğ‘› matrix with ğ´ğ‘—, ğ‘˜ âˆˆ ğ… in row ğ‘—, column ğ‘˜. Define a bilinear form ğ›½ğ´ on ğ…ğ‘› by ğ›½ğ´((ğ‘¥1, â€¦, ğ‘¥ğ‘›), (ğ‘¦1, â€¦, ğ‘¦ğ‘›))= ğ‘› âˆ‘ ğ‘˜ = 1 ğ‘› âˆ‘ ğ‘— = 1 ğ´ğ‘—, ğ‘˜ğ‘¥ğ‘—ğ‘¦ğ‘˜. The first bullet point is a special case of this bullet point withğ‘› = 3and ğ´ = â›âœâœâœ â 0 1 0 0 0 âˆ’5 2 0 0 ââŸâŸâŸ â  . 334 Chapter 9 Multilinear Algebra and Determinants â€¢ Suppose ğ‘‰ is a real inner product space and ğ‘‡ âˆˆ â„’(ğ‘‰). Then the function ğ›½âˆ¶ ğ‘‰ Ã— ğ‘‰ â†’ ğ‘ defined by ğ›½(ğ‘¢, ğ‘£) = âŸ¨ğ‘¢, ğ‘‡ğ‘£âŸ© is a bilinear form on ğ‘‰. â€¢ If ğ‘› is a positive integer, then the function ğ›½âˆ¶ ğ’«ğ‘›(ğ‘) Ã— ğ’«ğ‘›(ğ‘) â†’ ğ‘ defined by ğ›½(ğ‘, ğ‘) = ğ‘(2) â‹… ğ‘ â€²(3) is a bilinear form on ğ’«ğ‘›(ğ‘). â€¢ Suppose ğœ‘, ğœ âˆˆ ğ‘‰â€². Then the function ğ›½âˆ¶ ğ‘‰ Ã— ğ‘‰ â†’ ğ… defined by ğ›½(ğ‘¢, ğ‘£) = ğœ‘(ğ‘¢) â‹… ğœ(ğ‘£) is a bilinear form on ğ‘‰. â€¢ More generally, suppose that ğœ‘1, â€¦, ğœ‘ğ‘›, ğœ1, â€¦, ğœğ‘› âˆˆ ğ‘‰â€². Then the function ğ›½âˆ¶ ğ‘‰ Ã— ğ‘‰ â†’ ğ… defined by ğ›½(ğ‘¢, ğ‘£) = ğœ‘1(ğ‘¢) â‹… ğœ1(ğ‘£) + â‹¯ + ğœ‘ğ‘›(ğ‘¢) â‹… ğœğ‘›(ğ‘£) is a bilinear form on ğ‘‰. A bilinear form on ğ‘‰ is a function from ğ‘‰ Ã— ğ‘‰ to ğ…. Because ğ‘‰ Ã— ğ‘‰ is a vector space, this raises the question of whether a bilinear form can also be a linear map from ğ‘‰Ã— ğ‘‰ to ğ…. Note that none of the bilinear forms in 9.2 are linear maps except in some special cases in which the bilinear form is the zero map. Exercise 3 shows that a bilinear form ğ›½ on ğ‘‰ is a linear map on ğ‘‰ Ã— ğ‘‰ only if ğ›½ = 0. 9.3 definition: ğ‘‰(2) The set of bilinear forms on ğ‘‰ is denoted by ğ‘‰(2). With the usual operations of addition and scalar multiplication of functions, ğ‘‰(2)is a vector space. For ğ‘‡ an operator on an ğ‘›-dimensional vector space ğ‘‰ and a basis ğ‘’1, â€¦, ğ‘’ğ‘› of ğ‘‰, we used an ğ‘›-by-ğ‘› matrix to provide information about ğ‘‡. We now do the same thing for bilinear forms on ğ‘‰. 9.4 definition: matrix of a bilinear form, â„³(ğ›½) Suppose ğ›½ is a bilinear form on ğ‘‰ and ğ‘’1, â€¦, ğ‘’ğ‘› is a basis of ğ‘‰. The matrix of ğ›½ with respect to this basis is the ğ‘›-by-ğ‘› matrix â„³(ğ›½) whose entry â„³(ğ›½)ğ‘—, ğ‘˜ in row ğ‘—, column ğ‘˜ is given by â„³(ğ›½)ğ‘—, ğ‘˜ = ğ›½(ğ‘’ğ‘—, ğ‘’ğ‘˜). If the basis ğ‘’1, â€¦, ğ‘’ğ‘› is not clear from the context, then the notation â„³(ğ›½, (ğ‘’1, â€¦, ğ‘’ğ‘›))is used. Section 9A Bilinear Forms and Quadratic Forms 335 Recall that ğ…ğ‘›, ğ‘› denotes the vector space of ğ‘›-by-ğ‘› matrices with entries in ğ… and that dim ğ…ğ‘›, ğ‘› = ğ‘›2 (see 3.39 and 3.40). 9.5 dim ğ‘‰(2)= (dim ğ‘‰) 2 Suppose ğ‘’1, â€¦, ğ‘’ğ‘› is a basis of ğ‘‰. Then the map ğ›½ â†¦ â„³(ğ›½) is an isomorphism of ğ‘‰(2)onto ğ…ğ‘›, ğ‘›. Furthermore, dim ğ‘‰(2)= (dim ğ‘‰) 2. Proof The map ğ›½ â†¦ â„³(ğ›½) is clearly a linear map of ğ‘‰(2)into ğ…ğ‘›, ğ‘›. For ğ´ âˆˆ ğ…ğ‘›, ğ‘›, define a bilinear formğ›½ğ´ on ğ‘‰ by ğ›½ğ´(ğ‘¥1ğ‘’1 + â‹¯ + ğ‘¥ğ‘›ğ‘’ğ‘›, ğ‘¦1ğ‘’1 + â‹¯ + ğ‘¦ğ‘›ğ‘’ğ‘›) = ğ‘› âˆ‘ ğ‘˜ = 1 ğ‘› âˆ‘ ğ‘— = 1 ğ´ğ‘—, ğ‘˜ğ‘¥ğ‘—ğ‘¦ğ‘˜ for ğ‘¥1, â€¦, ğ‘¥ğ‘›, ğ‘¦1, â€¦, ğ‘¦ğ‘› âˆˆ ğ… (if ğ‘‰ = ğ…ğ‘› and ğ‘’1, â€¦, ğ‘’ğ‘› is the standard basis of ğ…ğ‘›, this ğ›½ğ´ is the same as the bilinear form ğ›½ğ´ in the second bullet point of Example 9.2). The linear map ğ›½ â†¦ â„³(ğ›½) from ğ‘‰(2)to ğ…ğ‘›, ğ‘› and the linear map ğ´ â†¦ ğ›½ğ´ from ğ…ğ‘›, ğ‘› to ğ‘‰(2)are inverses of each other because ğ›½â„³(ğ›½)= ğ›½ for all ğ›½ âˆˆ ğ‘‰(2)and â„³(ğ›½ğ´) = ğ´ for all ğ´ âˆˆ ğ…ğ‘›, ğ‘›, as you should verify. Thus both maps are isomorphisms and the two spaces that they connect have the same dimension. Hence dim ğ‘‰(2)= dim ğ…ğ‘›, ğ‘› = ğ‘›2 = (dim ğ‘‰) 2. Recall that ğ¶ t denotes the transpose of a matrix ğ¶. The matrix ğ¶ t is obtained by interchanging the rows and the columns of ğ¶. 9.6 composition of a bilinear form and an operator Suppose ğ›½ is a bilinear form on ğ‘‰ and ğ‘‡ âˆˆ â„’(ğ‘‰). Define bilinear formsğ›¼ and ğœŒ on ğ‘‰ by ğ›¼(ğ‘¢, ğ‘£) = ğ›½(ğ‘¢, ğ‘‡ğ‘£) and ğœŒ(ğ‘¢, ğ‘£) = ğ›½(ğ‘‡ğ‘¢, ğ‘£). Let ğ‘’1, â€¦, ğ‘’ğ‘› be a basis of ğ‘‰. Then â„³(ğ›¼) = â„³(ğ›½)â„³(ğ‘‡) and â„³(ğœŒ) = â„³(ğ‘‡) tâ„³(ğ›½). Proof If ğ‘—, ğ‘˜ âˆˆ {1, â€¦, ğ‘›}, then â„³(ğ›¼)ğ‘—, ğ‘˜ = ğ›¼(ğ‘’ğ‘—, ğ‘’ğ‘˜) = ğ›½(ğ‘’ğ‘—, ğ‘‡ğ‘’ğ‘˜) = ğ›½(ğ‘’ğ‘—, ğ‘› âˆ‘ ğ‘š = 1 â„³(ğ‘‡)ğ‘š, ğ‘˜ ğ‘’ğ‘š) = ğ‘› âˆ‘ ğ‘š = 1 ğ›½(ğ‘’ğ‘—, ğ‘’ğ‘š)â„³(ğ‘‡)ğ‘š, ğ‘˜ = (â„³(ğ›½)â„³(ğ‘‡)) ğ‘—, ğ‘˜. Thus â„³(ğ›¼) = â„³(ğ›½)â„³(ğ‘‡). The proof that â„³(ğœŒ) = â„³(ğ‘‡) tâ„³(ğ›½) is similar. 336 Chapter 9 Multilinear Algebra and Determinants The result below shows how the matrix of a bilinear form changes if we change the basis. The formula in the result below should be compared to the change- of-basis formula for the matrix of an operator (see 3.84). The two formulas are similar, except that the transpose ğ¶ t appears in the formula below and the inverse ğ¶âˆ’1 appears in the change-of-basis formula for the matrix of an operator. 9.7 change-of-basis formula Suppose ğ›½ âˆˆ ğ‘‰(2). Suppose ğ‘’1, â€¦, ğ‘’ğ‘› and ğ‘“1, â€¦, ğ‘“ğ‘› are bases of ğ‘‰. Let ğ´ = â„³(ğ›½, (ğ‘’1, â€¦, ğ‘’ğ‘›)) and ğµ = â„³(ğ›½, ( ğ‘“1, â€¦, ğ‘“ğ‘›)) and ğ¶ = â„³(ğ¼, (ğ‘’1, â€¦, ğ‘’ğ‘›), ( ğ‘“1, â€¦, ğ‘“ğ‘›)). Then ğ´ = ğ¶ tğµğ¶. Proof The linear map lemma (3.4) tells us that there exists an operator ğ‘‡ âˆˆ â„’(ğ‘‰) such that ğ‘‡ ğ‘“ğ‘˜ = ğ‘’ğ‘˜ for each ğ‘˜ = 1, â€¦, ğ‘›. The definition of the matrix of an operator with respect to a basis implies that â„³(ğ‘‡, ( ğ‘“1, â€¦, ğ‘“ğ‘›))= ğ¶. Define bilinear formsğ›¼, ğœŒ on ğ‘‰ by ğ›¼(ğ‘¢, ğ‘£) = ğ›½(ğ‘¢, ğ‘‡ğ‘£) and ğœŒ(ğ‘¢, ğ‘£) = ğ›¼(ğ‘‡ğ‘¢, ğ‘£) = ğ›½(ğ‘‡ğ‘¢, ğ‘‡ğ‘£). Then ğ›½(ğ‘’ğ‘—, ğ‘’ğ‘˜) = ğ›½(ğ‘‡ ğ‘“ğ‘—, ğ‘‡ ğ‘“ğ‘˜) = ğœŒ( ğ‘“ğ‘—, ğ‘“ğ‘˜) for all ğ‘—, ğ‘˜ âˆˆ {1, â€¦, ğ‘›}. Thus ğ´ = â„³(ğœŒ, ( ğ‘“1, â€¦, ğ‘“ğ‘›)) = ğ¶ tâ„³(ğ›¼, ğ‘“1, â€¦, ğ‘“ğ‘›)) = ğ¶ tğµğ¶, where the second and third lines each follow from 9.6. 9.8 example: the matrix of a bilinear form on ğ’«2(ğ‘) Define a bilinear formğ›½ on ğ’«2(ğ‘) by ğ›½(ğ‘, ğ‘) = ğ‘(2) â‹… ğ‘ â€²(3). Let ğ´ = â„³(ğ›½, (1, ğ‘¥ âˆ’ 2, (ğ‘¥ âˆ’ 3) 2)) and ğµ = â„³(ğ›½, (1, ğ‘¥, ğ‘¥2)) and ğ¶ = â„³(ğ¼, (1, ğ‘¥ âˆ’ 2, (ğ‘¥ âˆ’ 3) 2), (1, ğ‘¥, ğ‘¥2)). Then ğ´ = â›âœâœâœ â 0 1 0 0 0 0 0 1 0 ââŸâŸâŸ â  and ğµ = â›âœâœâœ â 0 1 6 0 2 12 0 4 24 ââŸâŸâŸ â  and ğ¶ = â›âœâœâœ â 1 âˆ’2 9 0 1 âˆ’6 0 0 1 ââŸâŸâŸ â  . Now the change-of-basis formula 9.7 asserts that ğ´ = ğ¶ tğµğ¶, which you can verify with matrix multiplication using the matrices above. Section 9A Bilinear Forms and Quadratic Forms 337 Symmetric Bilinear Forms 9.9 definition: symmetric bilinear form, ğ‘‰(2) sym A bilinear form ğœŒ âˆˆ ğ‘‰(2)is called symmetric if ğœŒ(ğ‘¢, ğ‘¤) = ğœŒ(ğ‘¤, ğ‘¢) for all ğ‘¢, ğ‘¤ âˆˆ ğ‘‰. The set of symmetric bilinear forms on ğ‘‰ is denoted by ğ‘‰(2) sym. 9.10 example: symmetric bilinear forms â€¢ If ğ‘‰ is a real inner product space and ğœŒ âˆˆ ğ‘‰(2)is defined by ğœŒ(ğ‘¢, ğ‘¤) = âŸ¨ğ‘¢, ğ‘¤âŸ©, then ğœŒ is a symmetric bilinear form on ğ‘‰. â€¢ Suppose ğ‘‰ is a real inner product space and ğ‘‡ âˆˆ â„’(ğ‘‰). DefineğœŒ âˆˆ ğ‘‰(2)by ğœŒ(ğ‘¢, ğ‘¤) = âŸ¨ğ‘¢, ğ‘‡ğ‘¤âŸ©. Then ğœŒ is a symmetric bilinear form on ğ‘‰ if and only if ğ‘‡ is a self-adjoint operator (the previous bullet point is the special case ğ‘‡ = ğ¼). â€¢ Suppose ğœŒâˆ¶ â„’(ğ‘‰) Ã— â„’(ğ‘‰) â†’ ğ… is defined by ğœŒ(ğ‘†, ğ‘‡) = tr(ğ‘†ğ‘‡). Then ğœŒ is a symmetric bilinear form on â„’(ğ‘‰) because trace is a linear functional on â„’(ğ‘‰) and tr(ğ‘†ğ‘‡) = tr(ğ‘‡ğ‘†) for all ğ‘†, ğ‘‡ âˆˆ â„’(ğ‘‰); see 8.56. 9.11 definition: symmetric matrix A square matrix ğ´ is called symmetric if it equals its transpose. An operator on ğ‘‰ may have a symmetric matrix with respect to some but not all bases of ğ‘‰. In contrast, the next result shows that a bilinear form on ğ‘‰ has a sym- metric matrix with respect to either all bases of ğ‘‰ or with respect to no bases of ğ‘‰. 9.12 symmetric bilinear forms are diagonalizable Suppose ğœŒ âˆˆ ğ‘‰(2). Then the following are equivalent. (a) ğœŒ is a symmetric bilinear form on ğ‘‰. (b) â„³(ğœŒ, (ğ‘’1, â€¦, ğ‘’ğ‘›))is a symmetric matrix for every basis ğ‘’1, â€¦, ğ‘’ğ‘› of ğ‘‰. (c) â„³(ğœŒ, (ğ‘’1, â€¦, ğ‘’ğ‘›))is a symmetric matrix for some basis ğ‘’1, â€¦, ğ‘’ğ‘› of ğ‘‰. (d) â„³(ğœŒ, (ğ‘’1, â€¦, ğ‘’ğ‘›))is a diagonal matrix for some basis ğ‘’1, â€¦, ğ‘’ğ‘› of ğ‘‰. 338 Chapter 9 Multilinear Algebra and Determinants Proof First suppose (a) holds, so ğœŒ is a symmetric bilinear form. Suppose ğ‘’1, â€¦, ğ‘’ğ‘› is a basis of ğ‘‰ and ğ‘—, ğ‘˜ âˆˆ {1, â€¦, ğ‘›}. Then ğœŒ(ğ‘’ğ‘—, ğ‘’ğ‘˜) = ğœŒ(ğ‘’ğ‘˜, ğ‘’ğ‘—) because ğœŒ is symmetric. Thus â„³(ğœŒ, (ğ‘’1, â€¦, ğ‘’ğ‘›))is a symmetric matrix, showing that (a) implies (b). Clearly (b) implies (c). Now suppose (c) holds and ğ‘’1, â€¦, ğ‘’ğ‘› is a basis of ğ‘‰ such that â„³(ğœŒ, (ğ‘’1, â€¦, ğ‘’ğ‘›)) is a symmetric matrix. Suppose ğ‘¢, ğ‘¤ âˆˆ ğ‘‰. There exist ğ‘1, â€¦, ğ‘ğ‘›, ğ‘1, â€¦, ğ‘ğ‘› âˆˆ ğ… such that ğ‘¢ = ğ‘1ğ‘’1 + â‹¯ + ğ‘ğ‘›ğ‘’ğ‘› and ğ‘¤ = ğ‘1ğ‘’1 + â‹¯ + ğ‘ğ‘›ğ‘’ğ‘›. Now ğœŒ(ğ‘¢, ğ‘¤) = ğœŒ(ğ‘› âˆ‘ ğ‘— = 1 ğ‘ğ‘—ğ‘’ğ‘—, ğ‘› âˆ‘ ğ‘˜ = 1 ğ‘ğ‘˜ğ‘’ğ‘˜) = ğ‘› âˆ‘ ğ‘— = 1 ğ‘› âˆ‘ ğ‘˜ = 1 ğ‘ğ‘—ğ‘ğ‘˜ğœŒ(ğ‘’ğ‘—, ğ‘’ğ‘˜) = ğ‘› âˆ‘ ğ‘— = 1 ğ‘› âˆ‘ ğ‘˜ = 1 ğ‘ğ‘—ğ‘ğ‘˜ğœŒ(ğ‘’ğ‘˜, ğ‘’ğ‘—) = ğœŒ( ğ‘› âˆ‘ ğ‘˜ = 1 ğ‘ğ‘˜ğ‘’ğ‘˜, ğ‘› âˆ‘ ğ‘— = 1 ğ‘ğ‘—ğ‘’ğ‘—) = ğœŒ(ğ‘¤, ğ‘¢), where the third line holds because â„³(ğœŒ) is a symmetric matrix. The equation above shows that ğœŒ is a symmetric bilinear form, proving that (c) implies (a). At this point, we have proved that (a), (b), (c) are equivalent. Because every diagonal matrix is symmetric, (d) implies (c). To complete the proof, we will show that (a) implies (d) by induction on ğ‘› = dim ğ‘‰. If ğ‘› = 1, then (a) implies (d) because every 1-by-1matrix is diagonal. Now suppose ğ‘› > 1and the implication (a) âŸ¹ (d) holds for one less dimension. Suppose (a) holds, so ğœŒ is a symmetric bilinear form. If ğœŒ = 0, then the matrix of ğœŒ with respect to every basis of ğ‘‰ is the zero matrix, which is a diagonal matrix. Hence we can assume that ğœŒ â‰  0, which means there exist ğ‘¢, ğ‘¤ âˆˆ ğ‘‰ such that ğœŒ(ğ‘¢, ğ‘¤) â‰  0. Now 2ğœŒ(ğ‘¢, ğ‘¤) = ğœŒ(ğ‘¢ + ğ‘¤, ğ‘¢ + ğ‘¤) âˆ’ ğœŒ(ğ‘¢, ğ‘¢) âˆ’ ğœŒ(ğ‘¤, ğ‘¤). Because the left side of the equation above is nonzero, the three terms on the right cannot all equal 0. Hence there exists ğ‘£ âˆˆ ğ‘‰ such that ğœŒ(ğ‘£, ğ‘£) â‰  0. Let ğ‘ˆ = {ğ‘¢ âˆˆ ğ‘‰ âˆ¶ ğœŒ(ğ‘¢, ğ‘£) = 0}. Thus ğ‘ˆ is the null space of the linear functional ğ‘¢ â†¦ ğœŒ(ğ‘¢, ğ‘£) on ğ‘‰. This linear functional is not the zero linear functional because ğ‘£ âˆ‰ ğ‘ˆ. Thus dim ğ‘ˆ = ğ‘› âˆ’ 1. By our induction hypothesis, there is a basis ğ‘’1, â€¦, ğ‘’ğ‘› âˆ’ 1 of ğ‘ˆ such that the symmetric bilinear form ğœŒ|ğ‘ˆ Ã— ğ‘ˆ has a diagonal matrix with respect to this basis. Because ğ‘£ âˆ‰ ğ‘ˆ, the list ğ‘’1, â€¦, ğ‘’ğ‘› âˆ’ 1, ğ‘£ is a basis of ğ‘‰. Suppose ğ‘˜ âˆˆ {1, â€¦, ğ‘›âˆ’1}. Then ğœŒ(ğ‘’ğ‘˜, ğ‘£) = 0by the construction of ğ‘ˆ. Because ğœŒ is symmetric, we also have ğœŒ(ğ‘£, ğ‘’ğ‘˜) = 0. Thus the matrix of ğœŒ with respect to ğ‘’1, â€¦, ğ‘’ğ‘› âˆ’ 1, ğ‘£ is a diagonal matrix, completing the proof that (a) implies (d). Section 9A Bilinear Forms and Quadratic Forms 339 The previous result states that every symmetric bilinear form has a diagonal matrix with respect to some basis. If our vector space happens to be a real inner product space, then the next result shows that every symmetric bilinear form has a diagonal matrix with respect to some orthonormal basis. Note that the inner product here is unrelated to the bilinear form. 9.13 diagonalization of a symmetric bilinear form by an orthonormal basis Suppose ğ‘‰ is a real inner product space and ğœŒ is a symmetric bilinear form on ğ‘‰. Then ğœŒ has a diagonal matrix with respect to some orthonormal basis of ğ‘‰. Proof Let ğ‘“1, â€¦, ğ‘“ğ‘› be an orthonormal basis of ğ‘‰. Let ğµ = â„³(ğœŒ, ( ğ‘“1, â€¦, ğ‘“ğ‘›)). Then ğµ is a symmetric matrix (by 9.12). Let ğ‘‡ âˆˆ â„’(ğ‘‰) be the operator such that â„³(ğ‘‡, ( ğ‘“1, â€¦, ğ‘“ğ‘›))= ğµ. Thus ğ‘‡ is self-adjoint. The real spectral theorem (7.29) states that ğ‘‡ has a diagonal matrix with respect to some orthonormal basis ğ‘’1, â€¦, ğ‘’ğ‘› of ğ‘‰. Let ğ¶ = â„³(ğ¼, (ğ‘’1, â€¦, ğ‘’ğ‘›), ( ğ‘“1, â€¦, ğ‘“ğ‘›)). Thus ğ¶âˆ’1ğ‘‡ğ¶ is the matrix of ğ‘‡ with respect to the basis ğ‘’1, â€¦, ğ‘’ğ‘› (by 3.84). Hence ğ¶âˆ’1ğ‘‡ğ¶ is a diagonal matrix. Now ğ‘€(ğœŒ, (ğ‘’1, â€¦, ğ‘’ğ‘›))= ğ¶ tğ‘‡ğ¶ = ğ¶âˆ’1ğ‘‡ğ¶, where the first equality holds by9.7 and the second equality holds because ğ¶ is a unitary matrix with real entries (which implies that ğ¶âˆ’1 = ğ¶ t; see 7.57). Now we turn our attention to alternating bilinear forms. Alternating multilinear forms will play a major role in our approach to determinants later in this chapter. 9.14 definition: alternating bilinear form, ğ‘‰(2) alt A bilinear form ğ›¼ âˆˆ ğ‘‰(2)is called alternating if ğ›¼(ğ‘£, ğ‘£) = 0 for all ğ‘£ âˆˆ ğ‘‰. The set of alternating bilinear forms on ğ‘‰ is denoted by ğ‘‰(2) alt . 9.15 example: alternating bilinear forms â€¢ Suppose ğ‘› â‰¥ 3and ğ›¼âˆ¶ ğ…ğ‘› Ã— ğ…ğ‘› â†’ ğ… is defined by ğ›¼((ğ‘¥1, â€¦, ğ‘¥ğ‘›), (ğ‘¦1, â€¦, ğ‘¦ğ‘›))= ğ‘¥1ğ‘¦2 âˆ’ ğ‘¥2ğ‘¦1 + ğ‘¥1ğ‘¦3 âˆ’ ğ‘¥3ğ‘¦1. Then ğ›¼ is an alternating bilinear form on ğ…ğ‘›. â€¢ Suppose ğœ‘, ğœ âˆˆ ğ‘‰â€². Then the bilinear form ğ›¼ on ğ‘‰ defined by ğ›¼(ğ‘¢, ğ‘¤) = ğœ‘(ğ‘¢)ğœ(ğ‘¤) âˆ’ ğœ‘(ğ‘¤)ğœ(ğ‘¢) is alternating. 340 Chapter 9 Multilinear Algebra and Determinants The next result shows that a bilinear form is alternating if and only if switching the order of the two inputs multiplies the output by âˆ’1. 9.16 characterization of alternating bilinear forms A bilinear form ğ›¼ on ğ‘‰ is alternating if and only if ğ›¼(ğ‘¢, ğ‘¤) = âˆ’ğ›¼(ğ‘¤, ğ‘¢) for all ğ‘¢, ğ‘¤ âˆˆ ğ‘‰. Proof First suppose that ğ›¼ is alternating. If ğ‘¢, ğ‘¤ âˆˆ ğ‘‰, then 0 = ğ›¼(ğ‘¢+ ğ‘¤, ğ‘¢ + ğ‘¤) = ğ›¼(ğ‘¢, ğ‘¢) + ğ›¼(ğ‘¢, ğ‘¤) + ğ›¼(ğ‘¤, ğ‘¢) + ğ›¼(ğ‘¤, ğ‘¤) = ğ›¼(ğ‘¢, ğ‘¤) + ğ›¼(ğ‘¤, ğ‘¢). Thus ğ›¼(ğ‘¢, ğ‘¤) = âˆ’ğ›¼(ğ‘¤, ğ‘¢), as desired. To prove the implication in the other direction, suppose ğ›¼(ğ‘¢, ğ‘¤) = âˆ’ğ›¼(ğ‘¤, ğ‘¢) for all ğ‘¢, ğ‘¤ âˆˆ ğ‘‰. Then ğ›¼(ğ‘£, ğ‘£) = âˆ’ğ›¼(ğ‘£, ğ‘£) for all ğ‘£ âˆˆ ğ‘‰, which implies that ğ›¼(ğ‘£, ğ‘£) = 0for all ğ‘£ âˆˆ ğ‘‰. Thus ğ›¼ is alternating. Now we show that the vector space of bilinear forms on ğ‘‰ is the direct sum of the symmetric bilinear forms on ğ‘‰ and the alternating bilinear forms on ğ‘‰. 9.17 ğ‘‰(2)= ğ‘‰(2) sym âŠ• ğ‘‰(2) alt The sets ğ‘‰(2) sym and ğ‘‰(2) alt are subspaces of ğ‘‰(2). Furthermore, ğ‘‰(2)= ğ‘‰(2) sym âŠ• ğ‘‰(2) alt . Proof The definition of symmetric bilinear form implies that the sum of any two symmetric bilinear forms on ğ‘‰ is a bilinear form on ğ‘‰, and any scalar multiple of any bilinear form on ğ‘‰ is a bilinear form on ğ‘‰. Thus ğ‘‰(2) sym is a subspace of ğ‘‰(2). Similarly, the verification thatğ‘‰(2) alt is a subspace of ğ‘‰(2)is straightforward. Next, we want to show that ğ‘‰(2)= ğ‘‰(2) sym + ğ‘‰(2) alt . To do this, suppose ğ›½ âˆˆ ğ‘‰(2). DefineğœŒ, ğ›¼ âˆˆ ğ‘‰(2)by ğœŒ(ğ‘¢, ğ‘¤) = ğ›½(ğ‘¢, ğ‘¤) + ğ›½(ğ‘¤, ğ‘¢) 2 and ğ›¼(ğ‘¢, ğ‘¤) = ğ›½(ğ‘¢, ğ‘¤) âˆ’ ğ›½(ğ‘¤, ğ‘¢) 2 . Then ğœŒ âˆˆ ğ‘‰(2) sym and ğ›¼ âˆˆ ğ‘‰(2) alt , and ğ›½ = ğœŒ + ğ›¼. Thus ğ‘‰(2)= ğ‘‰(2) sym + ğ‘‰(2) alt . Finally, to show that the intersection of the two subspaces under consideration equals {0}, suppose ğ›½ âˆˆ ğ‘‰(2) sym âˆ© ğ‘‰ (2) alt . Then 9.16 implies that ğ›½(ğ‘¢, ğ‘¤) = âˆ’ğ›½(ğ‘¤, ğ‘¢) = âˆ’ğ›½(ğ‘¢, ğ‘¤) for all ğ‘¢, ğ‘¤ âˆˆ ğ‘‰, which implies that ğ›½ = 0. Thus ğ‘‰(2)= ğ‘‰(2) sym âŠ• ğ‘‰(2) alt , as implied by 1.46. Section 9A Bilinear Forms and Quadratic Forms 341 Quadratic Forms 9.18 definition: quadratic form associated with a bilinear form, ğ‘ğ›½ For ğ›½ a bilinear form on ğ‘‰, define a functionğ‘ğ›½ âˆ¶ ğ‘‰ â†’ ğ… by ğ‘ğ›½(ğ‘£) = ğ›½(ğ‘£, ğ‘£). A function ğ‘âˆ¶ ğ‘‰ â†’ ğ… is called a quadratic form on ğ‘‰ if there exists a bilinear form ğ›½ on ğ‘‰ such that ğ‘ = ğ‘ğ›½. Note that if ğ›½ is a bilinear form, then ğ‘ğ›½ = 0if and only if ğ›½ is alternating. 9.19 example: quadratic form Suppose ğ›½ is the bilinear form on ğ‘3 defined by ğ›½((ğ‘¥1, ğ‘¥2, ğ‘¥3), (ğ‘¦1, ğ‘¦2, ğ‘¦3))= ğ‘¥1ğ‘¦1 âˆ’ 4ğ‘¥1ğ‘¦2 + 8ğ‘¥1ğ‘¦3 âˆ’ 3ğ‘¥3ğ‘¦3. Then ğ‘ğ›½ is the quadratic form on ğ‘3 given by the formula ğ‘ğ›½(ğ‘¥1, ğ‘¥2, ğ‘¥3) = ğ‘¥1 2 âˆ’ 4ğ‘¥1ğ‘¥2 + 8ğ‘¥1ğ‘¥3 âˆ’ 3ğ‘¥3 2. The quadratic form in the example above is typical of quadratic forms on ğ…ğ‘›, as shown in the next result. 9.20 quadratic forms on ğ…ğ‘› Suppose ğ‘› is a positive integer and ğ‘ is a function from ğ…ğ‘› to ğ…. Then ğ‘ is a quadratic form on ğ…ğ‘› if and only if there exist numbers ğ´ğ‘—, ğ‘˜ âˆˆ ğ… for ğ‘—, ğ‘˜ âˆˆ {1, â€¦, ğ‘›} such that ğ‘(ğ‘¥1, â€¦, ğ‘¥ğ‘›) = ğ‘› âˆ‘ ğ‘˜ = 1 ğ‘› âˆ‘ ğ‘— = 1 ğ´ğ‘—, ğ‘˜ğ‘¥ğ‘—ğ‘¥ğ‘˜ for all (ğ‘¥1, â€¦, ğ‘¥ğ‘›) âˆˆ ğ…ğ‘›. Proof First suppose ğ‘ is a quadratic form on ğ…ğ‘›. Thus there exists a bilinear form ğ›½ on ğ…ğ‘› such that ğ‘ = ğ‘ğ›½. Let ğ´ be the matrix of ğ›½ with respect to the standard basis of ğ…ğ‘›. Then for all (ğ‘¥1, â€¦, ğ‘¥ğ‘›) âˆˆ ğ…ğ‘›, we have the desired equation ğ‘(ğ‘¥1, â€¦, ğ‘¥ğ‘›) = ğ›½((ğ‘¥1, â€¦, ğ‘¥ğ‘›), (ğ‘¥1, â€¦, ğ‘¥ğ‘›))= ğ‘› âˆ‘ ğ‘˜ = 1 ğ‘› âˆ‘ ğ‘— = 1 ğ´ğ‘—, ğ‘˜ğ‘¥ğ‘—ğ‘¥ğ‘˜. Conversely, suppose there exist numbers ğ´ğ‘—, ğ‘˜ âˆˆ ğ… for ğ‘—, ğ‘˜ âˆˆ {1, â€¦, ğ‘›} such that ğ‘(ğ‘¥1, â€¦, ğ‘¥ğ‘›) = ğ‘› âˆ‘ ğ‘˜ = 1 ğ‘˜ âˆ‘ ğ‘— = 1 ğ´ğ‘—, ğ‘˜ğ‘¥ğ‘—ğ‘¥ğ‘˜ for all (ğ‘¥1, â€¦, ğ‘¥ğ‘›) âˆˆ ğ…ğ‘›. Define a bilinear formğ›½ on ğ…ğ‘› by ğ›½((ğ‘¥1, â€¦, ğ‘¥ğ‘›), (ğ‘¦1, â€¦, ğ‘¦ğ‘›))= ğ‘› âˆ‘ ğ‘˜ = 1 ğ‘˜ âˆ‘ ğ‘— = 1 ğ´ğ‘—, ğ‘˜ğ‘¥ğ‘—ğ‘¦ğ‘˜. Then ğ‘ = ğ‘ğ›½, as desired. 342 Chapter 9 Multilinear Algebra and Determinants Although quadratic forms are defined in terms of an arbitrary bilinear form, the equivalence of (a) and (b) in the result below shows that a symmetric bilinear form can always be used. 9.21 characterization of quadratic forms Suppose ğ‘âˆ¶ ğ‘‰ â†’ ğ… is a function. The following are equivalent. (a) ğ‘ is a quadratic form. (b) There exists a unique symmetric bilinear form ğœŒ on ğ‘‰ such that ğ‘ = ğ‘ğœŒ. (c) ğ‘(ğœ†ğ‘£) = ğœ†2ğ‘(ğ‘£) for all ğœ† âˆˆ ğ… and all ğ‘£ âˆˆ ğ‘‰, and the function (ğ‘¢, ğ‘¤) â†¦ ğ‘(ğ‘¢ + ğ‘¤) âˆ’ ğ‘(ğ‘¢) âˆ’ ğ‘(ğ‘¤) is a symmetric bilinear form on ğ‘‰. (d) ğ‘(2ğ‘£) = 4ğ‘(ğ‘£)for all ğ‘£ âˆˆ ğ‘‰, and the function (ğ‘¢, ğ‘¤) â†¦ ğ‘(ğ‘¢ + ğ‘¤) âˆ’ ğ‘(ğ‘¢) âˆ’ ğ‘(ğ‘¤) is a symmetric bilinear form on ğ‘‰. Proof First suppose (a) holds, so ğ‘ is a quadratic form. Hence there exists a bilinear form ğ›½ such that ğ‘ = ğ‘ğ›½. By 9.17, there exist a symmetric bilinear form ğœŒ on ğ‘‰ and an alternating bilinear form ğ›¼ on ğ‘‰ such that ğ›½ = ğœŒ + ğ›¼. Now ğ‘ = ğ‘ğ›½ = ğ‘ğœŒ + ğ‘ğ›¼ = ğ‘ğœŒ. If ğœŒ â€² âˆˆ ğ‘‰(2) sym also satisfiesğ‘ğœŒâ€² = ğ‘, then ğ‘ğœŒâ€² âˆ’ ğœŒ = 0; thus ğœŒ â€² âˆ’ ğœŒ âˆˆ ğ‘‰(2) sym âˆ© ğ‘‰ (2) alt , which implies that ğœŒ â€² = ğœŒ (by 9.17). This completes the proof that (a) implies (b). Now suppose (b) holds, so there exists a symmetric bilinear form ğœŒ on ğ‘‰ such that ğ‘ = ğ‘ğœŒ. If ğœ† âˆˆ ğ… and ğ‘£ âˆˆ ğ‘‰ then ğ‘(ğœ†ğ‘£) = ğœŒ(ğœ†ğ‘£, ğœ†ğ‘£) = ğœ†ğœŒ(ğ‘£, ğœ†ğ‘£) = ğœ†2ğœŒ(ğ‘£, ğ‘£) = ğœ†2ğ‘(ğ‘£), showing that the first part of (c) holds. If ğ‘¢, ğ‘¤ âˆˆ ğ‘‰, then ğ‘(ğ‘¢ + ğ‘¤) âˆ’ ğ‘(ğ‘¢) âˆ’ ğ‘(ğ‘¤) = ğœŒ(ğ‘¢ + ğ‘¤, ğ‘¢ + ğ‘¤) âˆ’ ğœŒ(ğ‘¢, ğ‘¢) âˆ’ ğœŒ(ğ‘¤, ğ‘¤) = 2ğœŒ(ğ‘¢, ğ‘¤). Thus the function (ğ‘¢, ğ‘¤) â†¦ ğ‘(ğ‘¢+ğ‘¤)âˆ’ğ‘(ğ‘¢)âˆ’ğ‘(ğ‘¤) equals 2ğœŒ, which is a symmetric bilinear form on ğ‘‰, completing the proof that (b) implies (c). Clearly (c) implies (d). Now suppose (d) holds. Let ğœŒ be the symmetric bilinear form on ğ‘‰ defined by ğœŒ(ğ‘¢, ğ‘¤) = ğ‘(ğ‘¢ + ğ‘¤) âˆ’ ğ‘(ğ‘¢) âˆ’ ğ‘(ğ‘¤) 2 . If ğ‘£ âˆˆ ğ‘‰, then ğœŒ(ğ‘£, ğ‘£) = ğ‘(2ğ‘£) âˆ’ ğ‘(ğ‘£) âˆ’ ğ‘(ğ‘£) 2 = 4ğ‘(ğ‘£) âˆ’ 2ğ‘(ğ‘£) 2 = ğ‘(ğ‘£). Thus ğ‘ = ğ‘ğœŒ, completing the proof that (d) implies (a). Section 9A Bilinear Forms and Quadratic Forms 343 9.22 example: symmetric bilinear form associated with a quadratic form Suppose ğ‘ is the quadratic form on ğ‘3 given by the formula ğ‘(ğ‘¥1, ğ‘¥2, ğ‘¥3) = ğ‘¥1 2 âˆ’ 4ğ‘¥1ğ‘¥2 + 8ğ‘¥1ğ‘¥3 âˆ’ 3ğ‘¥3 2. A bilinear form ğ›½ on ğ‘3 such that ğ‘ = ğ‘ğ›½ is given by Example 9.19, but this bilinear form is not symmetric, as promised by 9.21(b). However, the bilinear form ğœŒ on ğ‘3 defined by ğœŒ((ğ‘¥1, ğ‘¥2, ğ‘¥3), (ğ‘¦1, ğ‘¦2, ğ‘¦3))= ğ‘¥1ğ‘¦1 âˆ’ 2ğ‘¥1ğ‘¦2 âˆ’ 2ğ‘¥2ğ‘¦1 + 4ğ‘¥1ğ‘¦3 + 4ğ‘¥3ğ‘¦1 âˆ’ 3ğ‘¥3ğ‘¦3 is symmetric and satisfiesğ‘ = ğ‘ğœŒ. The next result states that for each quadratic form we can choose a basis such that the quadratic form looks like a weighted sum of squares of the coordinates, meaning that there are no cross terms of the form ğ‘¥ğ‘—ğ‘¥ğ‘˜ with ğ‘— â‰  ğ‘˜. 9.23 diagonalization of quadratic form Suppose ğ‘ is a quadratic form on ğ‘‰. (a) There exist a basis ğ‘’1, â€¦, ğ‘’ğ‘› of ğ‘‰ and ğœ†1, â€¦, ğœ†ğ‘› âˆˆ ğ… such that ğ‘(ğ‘¥1ğ‘’1 + â‹¯ + ğ‘¥ğ‘›ğ‘’ğ‘›) = ğœ†1ğ‘¥1 2 + â‹¯ + ğœ†ğ‘›ğ‘¥ğ‘› 2 for all ğ‘¥1, â€¦, ğ‘¥ğ‘› âˆˆ ğ…. (b) If ğ… = ğ‘ and ğ‘‰ is an inner product space, then the basis in (a) can be chosen to be an orthonormal basis of ğ‘‰. Proof (a) There exists a symmetric bilinear form ğœŒ on ğ‘‰ such that ğ‘ = ğ‘ğœŒ (by 9.21). Now there exists a basis ğ‘’1, â€¦, ğ‘’ğ‘› of ğ‘‰ such that â„³(ğœŒ, (ğ‘’1, â€¦, ğ‘’ğ‘›))is a diagonal matrix (by 9.12). Let ğœ†1, â€¦, ğœ†ğ‘› denote the entries on the diagonal of this matrix. Thus ğœŒ(ğ‘’ğ‘—, ğ‘’ğ‘˜) = â§{ â¨{â© ğœ†ğ‘— if ğ‘— = ğ‘˜, 0 if ğ‘— â‰  ğ‘˜ for all ğ‘—, ğ‘˜ âˆˆ {1, â€¦, ğ‘›}. If ğ‘¥1, â€¦, ğ‘¥ğ‘› âˆˆ ğ…, then ğ‘(ğ‘¥1ğ‘’1 + â‹¯ + ğ‘¥ğ‘›ğ‘’ğ‘›) = ğœŒ(ğ‘¥1ğ‘’1 + â‹¯ + ğ‘¥ğ‘›ğ‘’ğ‘›, ğ‘¥1ğ‘’1 + â‹¯ + ğ‘¥ğ‘›ğ‘’ğ‘›) = ğ‘› âˆ‘ ğ‘˜ = 1 ğ‘› âˆ‘ ğ‘— = 1 ğ‘¥ğ‘—ğ‘¥ğ‘˜ğœŒ(ğ‘’ğ‘—, ğ‘’ğ‘˜) = ğœ†1ğ‘¥1 2 + â‹¯ + ğœ†ğ‘›ğ‘¥ğ‘› 2, as desired. (b) Suppose ğ… = ğ‘ and ğ‘‰ is an inner product space. Then 9.13 tells us that the basis in (a) can be chosen to be an orthonormal basis of ğ‘‰. 344 Chapter 9 Multilinear Algebra and Determinants Exercises 9A 1 Prove that if ğ›½ is a bilinear form on ğ…, then there exists ğ‘ âˆˆ ğ… such that ğ›½(ğ‘¥, ğ‘¦) = ğ‘ğ‘¥ğ‘¦ for all ğ‘¥, ğ‘¦ âˆˆ ğ…. 2 Let ğ‘› = dim ğ‘‰. Suppose ğ›½ is a bilinear form on ğ‘‰. Prove that there exist ğœ‘1, â€¦, ğœ‘ğ‘›, ğœ1, â€¦, ğœğ‘› âˆˆ ğ‘‰â€² such that ğ›½(ğ‘¢, ğ‘£) = ğœ‘1(ğ‘¢) â‹… ğœ1(ğ‘£) + â‹¯ + ğœ‘ğ‘›(ğ‘¢) â‹… ğœğ‘›(ğ‘£) for all ğ‘¢, ğ‘£ âˆˆ ğ‘‰. This exercise shows that if ğ‘› = dim ğ‘‰, then every bilinear form on ğ‘‰ is of the form given by the last bullet point of Example 9.2. 3 Suppose ğ›½âˆ¶ ğ‘‰Ã— ğ‘‰ â†’ ğ… is a bilinear form on ğ‘‰ and also is a linear functional on ğ‘‰ Ã— ğ‘‰. Prove that ğ›½ = 0. 4 Suppose ğ‘‰ is a real inner product space and ğ›½ is a bilinear form on ğ‘‰. Show that there exists a unique operator ğ‘‡ âˆˆ â„’(ğ‘‰) such that ğ›½(ğ‘¢, ğ‘£) = âŸ¨ğ‘¢, ğ‘‡ğ‘£âŸ© for all ğ‘¢, ğ‘£ âˆˆ ğ‘‰. This exercise states that if ğ‘‰ is a real inner product space, then every bilinear form on ğ‘‰ is of the form given by the third bullet point in 9.2. 5 Suppose ğ›½ is a bilinear form on a real inner product space ğ‘‰ and ğ‘‡ is the unique operator on ğ‘‰ such that ğ›½(ğ‘¢, ğ‘£) = âŸ¨ğ‘¢, ğ‘‡ğ‘£âŸ©for all ğ‘¢, ğ‘£ âˆˆ ğ‘‰ (see Exercise 4). Show that ğ›½ is an inner product on ğ‘‰ if and only if ğ‘‡ is an invertible positive operator on ğ‘‰. 6 Prove or give a counterexample: If ğœŒ is a symmetric bilinear form on ğ‘‰, then {ğ‘£ âˆˆ ğ‘‰ âˆ¶ ğœŒ(ğ‘£, ğ‘£) = 0} is a subspace of ğ‘‰. 7 Explain why the proof of 9.13 (diagonalization of a symmetric bilinear form by an orthonormal basis on a real inner product space) fails if the hypothesis that ğ… = ğ‘ is dropped. 8 Find formulas for dim ğ‘‰(2) sym and dim ğ‘‰(2) alt in terms of dim ğ‘‰. 9 Suppose that ğ‘› is a positive integer and ğ‘‰ = {ğ‘ âˆˆ ğ’«ğ‘›(ğ‘) âˆ¶ ğ‘(0) = ğ‘(1)}. Defineğ›¼âˆ¶ ğ‘‰ Ã— ğ‘‰ â†’ ğ‘ by ğ›¼(ğ‘, ğ‘) = âˆ« 1 0 ğ‘ğ‘ â€². Show that ğ›¼ is an alternating bilinear form on ğ‘‰. Section 9A Bilinear Forms and Quadratic Forms 345 10 Suppose that ğ‘› is a positive integer and ğ‘‰ = {ğ‘ âˆˆ ğ’«ğ‘›(ğ‘) âˆ¶ ğ‘(0) = ğ‘(1)and ğ‘ â€²(0) = ğ‘ â€²(1)}. DefineğœŒâˆ¶ ğ‘‰ Ã— ğ‘‰ â†’ ğ‘ by ğœŒ(ğ‘, ğ‘) = âˆ«1 0 ğ‘ğ‘ â€³. Show that ğœŒ is a symmetric bilinear form on ğ‘‰. Open Access This chapter is licensed under the terms of the Creative Commons Attribution-NonCommercial 4.0 International License (https://creativecommons.org/licenses/by-nc/4.0), which permits any noncommercial use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to original author and source, provide a link to the Creative Commons license, and indicate if changes were made. The images or other third party material in this chapter are included in the chapterâ€™s Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the chapterâ€™s Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. 346 Chapter 9 Multilinear Algebra and Determinants 9B Alternating Multilinear Forms Multilinear Forms 9.24 definition: ğ‘‰ğ‘š For ğ‘š a positive integer, defineğ‘‰ğ‘š by ğ‘‰ğ‘š = ğ‘‰ Ã— â‹¯ Ã— ğ‘‰âŸ ğ‘š times . Now we can defineğ‘š-linear forms as a generalization of the bilinear forms that we discussed in the previous section. 9.25 definition: ğ‘š-linear form, ğ‘‰(ğ‘š), multilinear form â€¢ For ğ‘š a positive integer, an ğ‘š-linear form on ğ‘‰ is a function ğ›½âˆ¶ ğ‘‰ğ‘š â†’ ğ… that is linear in each slot when the other slots are held fixed. This means that for each ğ‘˜ âˆˆ {1, â€¦, ğ‘š} and all ğ‘¢1, â€¦, ğ‘¢ğ‘š âˆˆ ğ‘‰, the function ğ‘£ â†¦ ğ›½(ğ‘¢1, â€¦, ğ‘¢ğ‘˜ âˆ’ 1, ğ‘£, ğ‘¢ğ‘˜ + 1, â€¦, ğ‘¢ğ‘š) is a linear map from ğ‘‰ to ğ…. â€¢ The set of ğ‘š-linear forms on ğ‘‰ is denoted by ğ‘‰(ğ‘š). â€¢ A function ğ›½ is called a multilinear form on ğ‘‰ if it is an ğ‘š-linear form on ğ‘‰ for some positive integer ğ‘š. In the definition above, the expressionğ›½(ğ‘¢1, â€¦, ğ‘¢ğ‘˜ âˆ’ 1, ğ‘£, ğ‘¢ğ‘˜ + 1, â€¦, ğ‘¢ğ‘š) means ğ›½(ğ‘£, ğ‘¢2, â€¦, ğ‘¢ğ‘š) if ğ‘˜ = 1and means ğ›½(ğ‘¢1, â€¦, ğ‘¢ğ‘š âˆ’ 1, ğ‘£) if ğ‘˜ = ğ‘š. A 1-linear form on ğ‘‰ is a linear functional on ğ‘‰. A 2-linear form on ğ‘‰ is a bilinear form on ğ‘‰. You can verify that with the usual addition and scalar multiplication of functions, ğ‘‰(ğ‘š)is a vector space. 9.26 example:ğ‘š-linear forms â€¢ Suppose ğ›¼, ğœŒ âˆˆ ğ‘‰(2). Define a functionğ›½âˆ¶ ğ‘‰4 â†’ ğ… by ğ›½(ğ‘£1, ğ‘£2, ğ‘£3, ğ‘£4) = ğ›¼(ğ‘£1, ğ‘£2) ğœŒ(ğ‘£3, ğ‘£4). Then ğ›½ âˆˆ ğ‘‰(4). â€¢ Defineğ›½âˆ¶ (â„’(ğ‘‰)) ğ‘š â†’ ğ… by ğ›½(ğ‘‡1, â€¦, ğ‘‡ğ‘š) = tr(ğ‘‡1â‹¯ğ‘‡ğ‘š). Then ğ›½ is an ğ‘š-linear form on â„’(ğ‘‰). Section 9B Alternating Multilinear Forms 347 Alternating multilinear forms, which we now define, play an important role as we head toward defining determinants. 9.27 definition:alternating forms, ğ‘‰(ğ‘š) alt Suppose ğ‘š is a positive integer. â€¢ An ğ‘š-linear form ğ›¼ on ğ‘‰ is called alternating if ğ›¼(ğ‘£1, â€¦, ğ‘£ğ‘š) = 0whenever ğ‘£1, â€¦, ğ‘£ğ‘š is a list of vectors in ğ‘‰ with ğ‘£ğ‘— = ğ‘£ğ‘˜ for some two distinct values of ğ‘— and ğ‘˜ in {1, â€¦, ğ‘š}. â€¢ ğ‘‰(ğ‘š) alt = {ğ›¼ âˆˆ ğ‘‰(ğ‘š)âˆ¶ ğ›¼ is an alternating ğ‘š-linear form on ğ‘‰}. You should verify that ğ‘‰(ğ‘š) alt is a subspace of ğ‘‰(ğ‘š). See Example 9.15 for examples of alternating 2-linear forms. See Exercise 2 for an example of an alternating 3-linear form. The next result tells us that if a linearly dependent list is input to an alternating multilinear form, then the output equals 0. 9.28 alternating multilinear forms and linear dependence Suppose ğ‘š is a positive integer and ğ›¼ is an alternating ğ‘š-linear form on ğ‘‰. If ğ‘£1, â€¦, ğ‘£ğ‘š is a linearly dependent list in ğ‘‰, then ğ›¼(ğ‘£1, â€¦, ğ‘£ğ‘š) = 0. Proof Suppose ğ‘£1, â€¦, ğ‘£ğ‘š is a linearly dependent list in ğ‘‰. By the linear depen- dence lemma (2.19), some ğ‘£ğ‘˜ is a linear combination of ğ‘£1, â€¦, ğ‘£ğ‘˜ âˆ’ 1. Thus there exist ğ‘1, â€¦, ğ‘ğ‘˜ âˆ’ 1 such that ğ‘£ğ‘˜ = ğ‘1ğ‘£1 + â‹¯ + ğ‘ğ‘˜ âˆ’ 1ğ‘£ğ‘˜ âˆ’ 1. Now ğ›¼(ğ‘£1, â€¦, ğ‘£ğ‘š) = ğ›¼(ğ‘£1, â€¦, ğ‘£ğ‘˜ âˆ’ 1, ğ‘˜ âˆ’ 1 âˆ‘ ğ‘— = 1 ğ‘ğ‘—ğ‘£ğ‘—, ğ‘£ğ‘˜ + 1, â€¦, ğ‘£ğ‘š) = ğ‘˜ âˆ’ 1 âˆ‘ ğ‘— = 1 ğ‘ğ‘— ğ›¼(ğ‘£1, â€¦, ğ‘£ğ‘˜ âˆ’ 1, ğ‘£ğ‘—, ğ‘£ğ‘˜ + 1, â€¦, ğ‘£ğ‘š) = 0. The next result states that if ğ‘š > dim ğ‘‰, then there are no alternating ğ‘š-linear forms on ğ‘‰ other than the function on ğ‘‰ğ‘š that is identically 0. 9.29 no nonzero alternating ğ‘š-linear forms for ğ‘š > dim ğ‘‰ Suppose ğ‘š > dim ğ‘‰. Then 0is the only alternating ğ‘š-linear form on ğ‘‰. Proof Suppose that ğ›¼ is an alternating ğ‘š-linear form on ğ‘‰ and ğ‘£1, â€¦, ğ‘£ğ‘š âˆˆ ğ‘‰. Because ğ‘š > dim ğ‘‰, this list is not linearly independent (by 2.22). Thus 9.28 implies that ğ›¼(ğ‘£1, â€¦, ğ‘£ğ‘š) = 0. Hence ğ›¼ is the zero function from ğ‘‰ğ‘š to ğ…. 348 Chapter 9 Multilinear Algebra and Determinants Alternating Multilinear Forms and Permutations 9.30 swapping input vectors in an alternating multilinear form Suppose ğ‘š is a positive integer, ğ›¼ is an alternating ğ‘š-linear form on ğ‘‰, and ğ‘£1, â€¦, ğ‘£ğ‘š is a list of vectors in ğ‘‰. Then swapping the vectors in any two slots of ğ›¼(ğ‘£1, â€¦, ğ‘£ğ‘š) changes the value of ğ›¼ by a factor of âˆ’1. Proof Put ğ‘£1 + ğ‘£2 in both the first two slots, getting 0 = ğ›¼(ğ‘£1 + ğ‘£2, ğ‘£1 + ğ‘£2, ğ‘£3, â€¦, ğ‘£ğ‘š). Use the multilinear properties of ğ›¼ to expand the right side of the equation above (as in the proof of 9.16) to get ğ›¼(ğ‘£2, ğ‘£1, ğ‘£3, â€¦, ğ‘£ğ‘š) = âˆ’ğ›¼(ğ‘£1, ğ‘£2, ğ‘£3, â€¦, ğ‘£ğ‘š). Similarly, swapping the vectors in any two slots of ğ›¼(ğ‘£1, â€¦, ğ‘£ğ‘š) changes the value of ğ›¼ by a factor of âˆ’1. To see what can happen with multiple swaps, suppose ğ›¼ is an alternating 3-linear form on ğ‘‰ and ğ‘£1, ğ‘£2, ğ‘£3 âˆˆ ğ‘‰. To evaluate ğ›¼(ğ‘£3, ğ‘£1, ğ‘£2) in terms of ğ›¼(ğ‘£1, ğ‘£2, ğ‘£3), start with ğ›¼(ğ‘£3, ğ‘£1, ğ‘£2) and swap the entries in the first and third slots, getting ğ›¼(ğ‘£3, ğ‘£1, ğ‘£2) = âˆ’ğ›¼(ğ‘£2, ğ‘£1, ğ‘£3). Now in the last expression, swap the entries in the first and second slots, getting ğ›¼(ğ‘£3, ğ‘£1, ğ‘£2) = âˆ’ğ›¼(ğ‘£2, ğ‘£1, ğ‘£3) = ğ›¼(ğ‘£1, ğ‘£2, ğ‘£3). More generally, we see that if we do an odd number of swaps, then the value of ğ›¼ changes by a factor of âˆ’1, and if we do an even number of swaps, then the value of ğ›¼ does not change. To deal with arbitrary multiple swaps, we need a bit of information about permutations. 9.31 definition: permutation, perm ğ‘š Suppose ğ‘š is a positive integer. â€¢ A permutation of (1, â€¦, ğ‘š) is a list (ğ‘—1, â€¦, ğ‘—ğ‘š) that contains each of the numbers 1, â€¦, ğ‘š exactly once. â€¢ The set of all permutations of (1, â€¦, ğ‘š) is denoted by perm ğ‘š. For example, (2, 3, 4, 5, 1) âˆˆperm 5. You should think of an element of perm ğ‘š as a rearrangement of the firstğ‘š positive integers. The number of swaps used to change a permutation (ğ‘—1, â€¦, ğ‘—ğ‘š) to the stan- dard order (1, â€¦, ğ‘š) can depend on the specific swaps selected. The following definition has the advantage of assigning a well-defined sign to every permutation. Section 9B Alternating Multilinear Forms 349 9.32 definition: sign of a permutation The sign of a permutation (ğ‘—1, â€¦, ğ‘—ğ‘š) is defined by sign(ğ‘—1, â€¦, ğ‘—ğ‘š) = (âˆ’1) ğ‘, where ğ‘ is the number of pairs of integers (ğ‘˜, â„“) with 1 â‰¤ ğ‘˜ < â„“ â‰¤ ğ‘šsuch that ğ‘˜ appears after â„“ in the list (ğ‘—1, â€¦, ğ‘—ğ‘š). Hence the sign of a permutation equals 1if the natural order has been changed an even number of times and equals âˆ’1if the natural order has been changed an odd number of times. 9.33 example: signs â€¢ The permutation (1, â€¦, ğ‘š) [no changes in the natural order] has sign1. â€¢ The only pair of integers (ğ‘˜, â„“) with ğ‘˜ < â„“ such that ğ‘˜ appears after â„“ in the list (2, 1, 3, 4)is (1, 2). Thus the permutation (2, 1, 3, 4)has sign âˆ’1. â€¢ In the permutation (2, 3, â€¦, ğ‘š, 1), the only pairs (ğ‘˜, â„“) with ğ‘˜ < â„“ that appear with changed order are (1, 2), (1, 3), â€¦, (1, ğ‘š). Because we have ğ‘š âˆ’ 1such pairs, the sign of this permutation equals (âˆ’1) ğ‘š âˆ’ 1. 9.34 swapping two entries in a permutation Swapping two entries in a permutation multiplies the sign of the permutation by âˆ’1. Proof Suppose we have two permutations, where the second permutation is obtained from the first by swapping two entries. The two swapped entries were in their natural order in the first permutation if and only if they are not in their natural order in the second permutation. Thus we have a net change (so far) of 1 or âˆ’1(both odd numbers) in the number of pairs not in their natural order. Consider each entry between the two swapped entries. If an intermediate entry was originally in the natural order with respect to both swapped entries, then it is now in the natural order with respect to neither swapped entry. Similarly, if an intermediate entry was originally in the natural order with respect to neither of the swapped entries, then it is now in the natural order with respect to both swapped entries. If an intermediate entry was originally in the natural order with respect to exactly one of the swapped entries, then that is still true. Thus the net change (for each pair containing an entry between the two swapped entries) in the number of pairs not in their natural order is 2, âˆ’2, or 0(all even numbers). For all other pairs of entries, there is no change in whether or not they are in their natural order. Thus the total net change in the number of pairs not in their natural order is an odd number. Hence the sign of the second permutation equals âˆ’1times the sign of the first permutation. 350 Chapter 9 Multilinear Algebra and Determinants 9.35 permutations and alternating multilinear forms Suppose ğ‘š is a positive integer and ğ›¼ âˆˆ ğ‘‰(ğ‘š) alt . Then ğ›¼(ğ‘£ğ‘—1, â€¦, ğ‘£ğ‘—ğ‘š) = (sign(ğ‘—1, â€¦, ğ‘—ğ‘š))ğ›¼(ğ‘£1, â€¦, ğ‘£ğ‘š) for every list ğ‘£1, â€¦, ğ‘£ğ‘š of vectors in ğ‘‰ and all (ğ‘—1, â€¦, ğ‘—ğ‘š) âˆˆ perm ğ‘š. Proof Suppose ğ‘£1, â€¦, ğ‘£ğ‘š âˆˆ ğ‘‰ and (ğ‘—1, â€¦, ğ‘—ğ‘š) âˆˆ perm ğ‘š. We can get from (ğ‘—1, â€¦, ğ‘—ğ‘š) to (1, â€¦, ğ‘š) by a series of swaps of entries in different slots. Each such swap changes the value of ğ›¼ by a factor of âˆ’1(by 9.30) and also changes the sign of the remaining permutation by a factor of âˆ’1(by 9.34). After an appropriate number of swaps, we reach the permutation 1, â€¦, ğ‘š, which has sign 1. Thus the value of ğ›¼ changed signs an even number of times if sign(ğ‘—1, â€¦, ğ‘—ğ‘š) = 1and an odd number of times if sign(ğ‘—1, â€¦, ğ‘—ğ‘š) = âˆ’1, which gives the desired result. Our use of permutations now leads in a natural way to the following beautiful formula for alternating ğ‘›-linear forms on an ğ‘›-dimensional vector space. 9.36 formula for (dim ğ‘‰)-linear alternating forms on ğ‘‰ Let ğ‘› = dim ğ‘‰. Suppose ğ‘’1, â€¦, ğ‘’ğ‘› is a basis of ğ‘‰ and ğ‘£1, â€¦, ğ‘£ğ‘› âˆˆ ğ‘‰. For each ğ‘˜ âˆˆ {1, â€¦, ğ‘›}, let ğ‘1, ğ‘˜, â€¦, ğ‘ğ‘›, ğ‘˜ âˆˆ ğ… be such that ğ‘£ğ‘˜ = ğ‘› âˆ‘ ğ‘— = 1 ğ‘ğ‘—, ğ‘˜ğ‘’ğ‘—. Then ğ›¼(ğ‘£1, â€¦, ğ‘£ğ‘›) = ğ›¼(ğ‘’1, â€¦, ğ‘’ğ‘›) âˆ‘ (ğ‘—1, â€¦, ğ‘—ğ‘›) âˆˆperm ğ‘› (sign(ğ‘—1, â€¦, ğ‘—ğ‘›))ğ‘ğ‘—1, 1â‹¯ğ‘ğ‘—ğ‘›, ğ‘› for every alternating ğ‘›-linear form ğ›¼ on ğ‘‰. Proof Suppose ğ›¼ is an alternating ğ‘›-linear form ğ›¼ on ğ‘‰. Then ğ›¼(ğ‘£1, â€¦, ğ‘£ğ‘›) = ğ›¼( ğ‘› âˆ‘ ğ‘—1 = 1 ğ‘ğ‘—1, 1ğ‘’ğ‘—1, â€¦, ğ‘› âˆ‘ ğ‘—ğ‘› = 1 ğ‘ğ‘—ğ‘›, ğ‘›ğ‘’ğ‘—ğ‘›) = ğ‘› âˆ‘ ğ‘—1 = 1 â‹¯ ğ‘› âˆ‘ ğ‘—ğ‘› = 1 ğ‘ğ‘—1, 1â‹¯ğ‘ğ‘—ğ‘›, ğ‘› ğ›¼(ğ‘’ğ‘—1, â€¦, ğ‘’ğ‘—ğ‘›) = âˆ‘ (ğ‘—1, â€¦, ğ‘—ğ‘›) âˆˆperm ğ‘› ğ‘ğ‘—1, 1â‹¯ğ‘ğ‘—ğ‘›, ğ‘› ğ›¼(ğ‘’ğ‘—1, â€¦, ğ‘’ğ‘—ğ‘›) = ğ›¼(ğ‘’1, â€¦, ğ‘’ğ‘›) âˆ‘ (ğ‘—1, â€¦, ğ‘—ğ‘›) âˆˆperm ğ‘› (sign(ğ‘—1, â€¦, ğ‘—ğ‘›))ğ‘ğ‘—1, 1â‹¯ğ‘ğ‘—ğ‘›, ğ‘›, where the third line holds because ğ›¼(ğ‘’ğ‘—1, â€¦, ğ‘’ğ‘—ğ‘›) = 0if ğ‘—1, â€¦, ğ‘—ğ‘› are not distinct integers, and the last line holds by 9.35. Section 9B Alternating Multilinear Forms 351 The following result will be the key to our definition of the determinant in the next section. 9.37 dim ğ‘‰(dim ğ‘‰) alt = 1 The vector space ğ‘‰(dim ğ‘‰) alt has dimension one. Proof Let ğ‘› = dim ğ‘‰. Suppose ğ›¼ and ğ›¼ â€² are alternating ğ‘›-linear forms on ğ‘‰ with ğ›¼ â‰  0. Let ğ‘’1, â€¦, ğ‘’ğ‘› be such that ğ›¼(ğ‘’1, â€¦, ğ‘’ğ‘›) â‰  0. There exists ğ‘ âˆˆ ğ… such that ğ›¼ â€²(ğ‘’1, â€¦, ğ‘’ğ‘›) = ğ‘ğ›¼(ğ‘’1, â€¦, ğ‘’ğ‘›). Furthermore, 9.28 implies that ğ‘’1, â€¦, ğ‘’ğ‘› is linearly independent and thus is a basis of ğ‘‰. Suppose ğ‘£1, â€¦, ğ‘£ğ‘› âˆˆ ğ‘‰. Let ğ‘ğ‘—, ğ‘˜ be as in 9.36 for ğ‘—, ğ‘˜ = 1, â€¦, ğ‘›. Then ğ›¼â€²(ğ‘£1, â€¦, ğ‘£ğ‘›) = ğ›¼â€²(ğ‘’1, â€¦, ğ‘’ğ‘›) âˆ‘ (ğ‘—1, â€¦, ğ‘—ğ‘›) âˆˆperm ğ‘›(sign(ğ‘—1, â€¦, ğ‘—ğ‘›))ğ‘ğ‘—1, 1â‹¯ğ‘ğ‘—ğ‘›, ğ‘› = ğ‘ğ›¼(ğ‘’1, â€¦, ğ‘’ğ‘›) âˆ‘ (ğ‘—1, â€¦, ğ‘—ğ‘›) âˆˆperm ğ‘›(sign(ğ‘—1, â€¦, ğ‘—ğ‘›))ğ‘ğ‘—1, 1â‹¯ğ‘ğ‘—ğ‘›, ğ‘› = ğ‘ğ›¼(ğ‘£1, â€¦, ğ‘£ğ‘›), where the first and last lines above come from9.36. The equation above implies that ğ›¼â€² = ğ‘ğ›¼. Thus ğ›¼â€², ğ›¼ is not a linearly independent list, which implies that dim ğ‘‰(ğ‘›) alt â‰¤ 1. To complete the proof, we only need to show that there exists a nonzero alternating ğ‘›-linear form ğ›¼ on ğ‘‰ (thus eliminating the possibility that dim ğ‘‰(ğ‘›) alt equals 0). To do this, let ğ‘’1, â€¦, ğ‘’ğ‘› be any basis of ğ‘‰, and let ğœ‘1, â€¦, ğœ‘ğ‘› âˆˆ ğ‘‰â€² be the linear functionals on ğ‘‰ that allow us to express each element of ğ‘‰ as a linear combination of ğ‘’1, â€¦, ğ‘’ğ‘›. In other words, ğ‘£ = ğ‘› âˆ‘ ğ‘— = 1 ğœ‘ğ‘—(ğ‘£)ğ‘’ğ‘— for every ğ‘£ âˆˆ ğ‘‰ (see 3.114). Now for ğ‘£1, â€¦, ğ‘£ğ‘› âˆˆ ğ‘‰, define 9.38 ğ›¼(ğ‘£1, â€¦, ğ‘£ğ‘›) = âˆ‘ (ğ‘—1, â€¦, ğ‘—ğ‘›) âˆˆperm ğ‘›(sign(ğ‘—1, â€¦, ğ‘—ğ‘›))ğœ‘ğ‘—1(ğ‘£1)â‹¯ğœ‘ğ‘—ğ‘›(ğ‘£ğ‘›). The verification thatğ›¼ is an ğ‘›-linear form on ğ‘‰ is straightforward. To see that ğ›¼ is alternating, suppose ğ‘£1, â€¦, ğ‘£ğ‘› âˆˆ ğ‘‰ with ğ‘£1 = ğ‘£2. For each (ğ‘—1, â€¦, ğ‘—ğ‘›) âˆˆ perm ğ‘›, the permutation (ğ‘—2, ğ‘—1, ğ‘—3, â€¦, ğ‘—ğ‘›) has the opposite sign. Be- cause ğ‘£1 = ğ‘£2, the contributions from these two permutations to the sum in 9.38 cancel either other. Hence ğ›¼(ğ‘£1, ğ‘£1, ğ‘£3, â€¦, ğ‘£ğ‘›) = 0. Similarly, ğ›¼(ğ‘£1, â€¦, ğ‘£ğ‘›) = 0if any two vectors in the list ğ‘£1, â€¦, ğ‘£ğ‘› are equal. Thus ğ›¼ is alternating. Finally, consider 9.38 with each ğ‘£ğ‘˜ = ğ‘’ğ‘˜. Because ğœ‘ğ‘—(ğ‘’ğ‘˜) equals 0if ğ‘— â‰  ğ‘˜ and equals 1if ğ‘— = ğ‘˜, only the permutation (1, â€¦, ğ‘›) makes a nonzero contribution to the right side of 9.38 in this case, giving the equation ğ›¼(ğ‘’1, â€¦, ğ‘’ğ‘›) = 1. Thus we have produced a nonzero alternating ğ‘›-linear form ğ›¼ on ğ‘‰, as desired. 352 Chapter 9 Multilinear Algebra and Determinants The formula 9.38 used in the last proof to construct a nonzero alternating ğ‘›- linear form came from the formula in 9.36, and that formula arose naturally from the properties of an alternating multilinear form. Earlier we showed that the value of an alternating multilinear form applied to a linearly dependent list is 0; see 9.28. The next result provides a converse of 9.28 for ğ‘›-linear multilinear forms when ğ‘› = dim ğ‘‰. In the following result, the statement that ğ›¼ is nonzero means (as usual for a function) that ğ›¼ is not the function on ğ‘‰ğ‘› that is identically 0. 9.39 alternating (dim ğ‘‰)-linear forms and linear independence Let ğ‘› = dim ğ‘‰. Suppose ğ›¼ is a nonzero alternating ğ‘›-linear form on ğ‘‰ and ğ‘’1, â€¦, ğ‘’ğ‘› is a list of vectors in ğ‘‰. Then ğ›¼(ğ‘’1, â€¦, ğ‘’ğ‘›) â‰  0 if and only if ğ‘’1, â€¦, ğ‘’ğ‘› is linearly independent. Proof First suppose ğ›¼(ğ‘’1, â€¦, ğ‘’ğ‘›) â‰  0. Then 9.28 implies that ğ‘’1, â€¦, ğ‘’ğ‘› is linearly independent. To prove the implication in the other direction, now suppose ğ‘’1, â€¦, ğ‘’ğ‘› is linearly independent. Because ğ‘› = dim ğ‘‰, this implies that ğ‘’1, â€¦, ğ‘’ğ‘› is a basis of ğ‘‰ (see 2.38). Because ğ›¼ is not the zero ğ‘›-linear form, there exist ğ‘£1, â€¦, ğ‘£ğ‘› âˆˆ ğ‘‰ such that ğ›¼(ğ‘£1, â€¦, ğ‘£ğ‘›) â‰  0. Now 9.36 implies that ğ›¼(ğ‘’1, â€¦, ğ‘’ğ‘›) â‰  0. Exercises 9B 1 Suppose ğ‘š is a positive integer. Show that dim ğ‘‰(ğ‘š)= (dim ğ‘‰) ğ‘š. 2 Suppose ğ‘› â‰¥ 3and ğ›¼âˆ¶ ğ…ğ‘› Ã— ğ…ğ‘› Ã— ğ…ğ‘› â†’ ğ… is defined by ğ›¼((ğ‘¥1, â€¦, ğ‘¥ğ‘›), (ğ‘¦1, â€¦, ğ‘¦ğ‘›), (ğ‘§1, â€¦, ğ‘§ğ‘›)) = ğ‘¥1ğ‘¦2ğ‘§3 âˆ’ ğ‘¥2ğ‘¦1ğ‘§3 âˆ’ ğ‘¥3ğ‘¦2ğ‘§1 âˆ’ ğ‘¥1ğ‘¦3ğ‘§2 + ğ‘¥3ğ‘¦1ğ‘§2 + ğ‘¥2ğ‘¦3ğ‘§1. Show that ğ›¼ is an alternating 3-linear form on ğ…ğ‘›. 3 Suppose ğ‘š is a positive integer and ğ›¼ is an ğ‘š-linear form on ğ‘‰ such that ğ›¼(ğ‘£1, â€¦, ğ‘£ğ‘š) = 0whenever ğ‘£1, â€¦, ğ‘£ğ‘š is a list of vectors in ğ‘‰ with ğ‘£ğ‘— = ğ‘£ğ‘— + 1 for some ğ‘— âˆˆ {1, â€¦, ğ‘š âˆ’ 1}. Prove that ğ›¼ is an alternating ğ‘š-linear form on ğ‘‰. 4 Prove or give a counterexample: If ğ›¼ âˆˆ ğ‘‰(4) alt , then {(ğ‘£1, ğ‘£2, ğ‘£3, ğ‘£4) âˆˆ ğ‘‰4 âˆ¶ ğ›¼(ğ‘£1, ğ‘£2, ğ‘£3, ğ‘£4) = 0} is a subspace of ğ‘‰4. Section 9B Alternating Multilinear Forms 353 5 Suppose ğ‘š is a positive integer and ğ›½ is an ğ‘š-linear form on ğ‘‰. Define an ğ‘š-linear form ğ›¼ on ğ‘‰ by ğ›¼(ğ‘£1, â€¦, ğ‘£ğ‘š) = âˆ‘ (ğ‘—1, â€¦, ğ‘—ğ‘š) âˆˆperm ğ‘š(sign(ğ‘—1, â€¦, ğ‘—ğ‘š))ğ›½(ğ‘£ğ‘—1, â€¦, ğ‘£ğ‘—ğ‘š) for ğ‘£1, â€¦, ğ‘£ğ‘š âˆˆ ğ‘‰. Explain why ğ›¼ âˆˆ ğ‘‰(ğ‘š) alt . 6 Suppose ğ‘š is a positive integer and ğ›½ is an ğ‘š-linear form on ğ‘‰. Define an ğ‘š-linear form ğ›¼ on ğ‘‰ by ğ›¼(ğ‘£1, â€¦, ğ‘£ğ‘š) = âˆ‘ (ğ‘—1, â€¦, ğ‘—ğ‘š) âˆˆperm ğ‘š ğ›½(ğ‘£ğ‘—1, â€¦, ğ‘£ğ‘—ğ‘š) for ğ‘£1, â€¦, ğ‘£ğ‘š âˆˆ ğ‘‰. Explain why ğ›¼(ğ‘£ğ‘˜1, â€¦, ğ‘£ğ‘˜ğ‘š) = ğ›¼(ğ‘£1, â€¦, ğ‘£ğ‘š) for all ğ‘£1, â€¦, ğ‘£ğ‘š âˆˆ ğ‘‰ and all (ğ‘˜1, â€¦, ğ‘˜ğ‘š) âˆˆ perm ğ‘š. 7 Give an example of a nonzero alternating 2-linear form ğ›¼ on ğ‘3 and a linearly independent list ğ‘£1, ğ‘£2 in ğ‘3 such that ğ›¼(ğ‘£1, ğ‘£2) = 0. This exercise shows that 9.39 can fail if the hypothesis that ğ‘› = dim ğ‘‰ is deleted. 354 Chapter 9 Multilinear Algebra and Determinants 9C Determinants Defining the Determinant The next definition will lead us to a clean, beautiful, basis-free definition of the determinant of an operator. 9.40 definition: ğ›¼ğ‘‡ Suppose that ğ‘š is a positive integer and ğ‘‡ âˆˆ â„’(ğ‘‰). For ğ›¼ âˆˆ ğ‘‰(ğ‘š) alt , define ğ›¼ğ‘‡ âˆˆ ğ‘‰(ğ‘š) alt by ğ›¼ğ‘‡(ğ‘£1, â€¦, ğ‘£ğ‘š) = ğ›¼(ğ‘‡ğ‘£1, â€¦, ğ‘‡ğ‘£ğ‘š) for each list ğ‘£1, â€¦, ğ‘£ğ‘š of vectors in ğ‘‰. Suppose ğ‘‡ âˆˆ â„’(ğ‘‰). If ğ›¼ âˆˆ ğ‘‰(ğ‘š) alt and ğ‘£1, â€¦, ğ‘£ğ‘š is a list of vectors in ğ‘‰ with ğ‘£ğ‘— = ğ‘£ğ‘˜ for some ğ‘— â‰  ğ‘˜, then ğ‘‡ğ‘£ğ‘— = ğ‘‡ğ‘£ğ‘˜, which implies that ğ›¼ğ‘‡(ğ‘£1, â€¦, ğ‘£ğ‘š) = ğ›¼(ğ‘‡ğ‘£1, â€¦, ğ‘‡ğ‘£ğ‘š) = 0. Thus the function ğ›¼ â†¦ ğ›¼ğ‘‡ is a linear map of ğ‘‰(ğ‘š) alt to itself. We know that dim ğ‘‰(dim ğ‘‰) alt = 1(see 9.37). Every linear map from a one- dimensional vector space to itself is multiplication by some unique scalar. For the linear map ğ›¼ â†¦ ğ›¼ğ‘‡, we now definedet ğ‘‡ to be that scalar. 9.41 definition: determinant of an operator, det ğ‘‡ Suppose ğ‘‡ âˆˆ â„’(ğ‘‰). The determinant of ğ‘‡, denoted by det ğ‘‡, is defined to be the unique number in ğ… such that ğ›¼ğ‘‡ = (det ğ‘‡) ğ›¼ for all ğ›¼ âˆˆ ğ‘‰(dim ğ‘‰) alt . 9.42 example: determinants of operators Let ğ‘› = dim ğ‘‰. â€¢ If ğ¼ is the identity operator on ğ‘‰, then ğ›¼ğ¼ = ğ›¼ for all ğ›¼ âˆˆ ğ‘‰(ğ‘›) alt . Thus det ğ¼ = 1. â€¢ More generally, if ğœ† âˆˆ ğ…, then ğ›¼ ğœ†ğ¼ = ğœ†ğ‘›ğ›¼ for all ğ›¼ âˆˆ ğ‘‰(ğ‘›) alt . Thus det(ğœ†ğ¼) = ğœ†ğ‘›. â€¢ Still more generally, if ğ‘‡ âˆˆ â„’(ğ‘‰) and ğœ† âˆˆ ğ…, then ğ›¼ ğœ†ğ‘‡ = ğœ†ğ‘›ğ›¼ğ‘‡ = ğœ† ğ‘›(det ğ‘‡)ğ›¼ for all ğ›¼ âˆˆ ğ‘‰(ğ‘›) alt . Thus det(ğœ†ğ‘‡) = ğœ†ğ‘› det ğ‘‡. â€¢ Suppose ğ‘‡ âˆˆ â„’(ğ‘‰) and there is a basis ğ‘’1, â€¦, ğ‘’ğ‘› of ğ‘‰ consisting of eigenvectors of ğ‘‡, with corresponding eigenvalues ğœ†1, â€¦, ğœ†ğ‘›. If ğ›¼ âˆˆ ğ‘‰(ğ‘›) alt , then ğ›¼ğ‘‡(ğ‘’1, â€¦, ğ‘’ğ‘›) = ğ›¼(ğœ†1ğ‘’1, â€¦, ğœ†ğ‘›ğ‘’ğ‘›) = (ğœ†1â‹¯ğœ†ğ‘›)ğ›¼(ğ‘’1, â€¦, ğ‘’ğ‘›). If ğ›¼ â‰  0, then 9.39 implies ğ›¼(ğ‘’1, â€¦, ğ‘’ğ‘›) â‰  0. Thus the equation above implies det ğ‘‡ = ğœ†1â‹¯ğœ†ğ‘›. Section 9C Determinants 355 Our next task is to define and give a formula for the determinant of a square matrix. To do this, we associate with each square matrix an operator and then define the determinant of the matrix to be the determinant of the associated operator. 9.43 definition: determinant of a matrix, det ğ´ Suppose that ğ‘› is a positive integer and ğ´ is an ğ‘›-by-ğ‘› square matrix with entries in ğ…. Let ğ‘‡ âˆˆ â„’(ğ…ğ‘›)be the operator whose matrix with respect to the standard basis of ğ…ğ‘› equals ğ´. The determinant of ğ´, denoted by det ğ´, is defined bydet ğ´ = det ğ‘‡. 9.44 example: determinants of matrices â€¢ If ğ¼ is the ğ‘›-by-ğ‘› identity matrix, then the corresponding operator on ğ…ğ‘› is the identity operator ğ¼ on ğ…ğ‘›. Thus the first bullet point of9.42 implies that the determinant of the identity matrix is 1. â€¢ Suppose ğ´ is a diagonal matrix with ğœ†1, â€¦, ğœ†ğ‘› on the diagonal. Then the corresponding operator on ğ…ğ‘› has the standard basis of ğ…ğ‘› as eigenvectors, with eigenvalues ğœ†1, â€¦, ğœ†ğ‘›. Thus the last bullet point of 9.42 implies that det ğ´ = ğœ†1â‹¯ğœ†ğ‘›. For the next result, think of each list ğ‘£1, â€¦, ğ‘£ğ‘› of ğ‘› vectors in ğ…ğ‘› as a list of ğ‘›-by-1column vectors. The notation (ğ‘£1 â‹¯ ğ‘£ğ‘› )then denotes the ğ‘›-by-ğ‘› square matrix whose ğ‘˜th column is ğ‘£ğ‘˜ for each ğ‘˜ = 1, â€¦, ğ‘›. 9.45 determinant is an alternating multilinear form Suppose that ğ‘› is a positive integer. The map that takes a list ğ‘£1, â€¦, ğ‘£ğ‘› of vectors in ğ…ğ‘› to det (ğ‘£1 â‹¯ ğ‘£ğ‘› )is an alternating ğ‘›-linear form on ğ…ğ‘›. Proof Let ğ‘’1, â€¦, ğ‘’ğ‘› be the standard basis of ğ…ğ‘› and suppose ğ‘£1, â€¦, ğ‘£ğ‘› is a list of vectors in ğ…ğ‘›. Let ğ‘‡ âˆˆ â„’(ğ…ğ‘›)be the operator such that ğ‘‡ğ‘’ğ‘˜ = ğ‘£ğ‘˜ for ğ‘˜ = 1, â€¦, ğ‘›. Thus ğ‘‡ is the operator whose matrix with respect to ğ‘’1, â€¦, ğ‘’ğ‘› is (ğ‘£1 â‹¯ ğ‘£ğ‘› ). Hence det (ğ‘£1 â‹¯ ğ‘£ğ‘› )= det ğ‘‡, by definition of the determinant of a matrix. Let ğ›¼ be an alternating ğ‘›-linear form on ğ…ğ‘› such that ğ›¼(ğ‘’1, â€¦, ğ‘’ğ‘›) = 1. Then det (ğ‘£1 â‹¯ ğ‘£ğ‘› )= det ğ‘‡ = (det ğ‘‡) ğ›¼(ğ‘’1, â€¦, ğ‘’ğ‘›) = ğ›¼(ğ‘‡ğ‘’1, â€¦, ğ‘‡ğ‘’ğ‘›) = ğ›¼(ğ‘£1, â€¦, ğ‘£ğ‘›), where the third line follows from the definition of the determinant of an operator. The equation above shows that the map that takes a list of vectors ğ‘£1, â€¦, ğ‘£ğ‘› in ğ…ğ‘› to det (ğ‘£1 â‹¯ ğ‘£ğ‘› )is the alternating ğ‘›-linear form ğ›¼ on ğ…ğ‘›. 356 Chapter 9 Multilinear Algebra and Determinants The previous result has several important consequences. For example, it immediately implies that a matrix with two identical columns has determinant 0. We will come back to other consequences later, but for now we want to give a formula for the determinant of a square matrix. Recall that if ğ´ is a matrix, then ğ´ğ‘—, ğ‘˜ denotes the entry in row ğ‘—, column ğ‘˜ of ğ´. 9.46 formula for determinant of a matrix Suppose that ğ‘› is a positive integer and ğ´ is an ğ‘›-by-ğ‘› square matrix. Then det ğ´ = âˆ‘ (ğ‘—1, â€¦, ğ‘—ğ‘›) âˆˆperm ğ‘› (sign(ğ‘—1, â€¦, ğ‘—ğ‘›))ğ´ğ‘—1, 1â‹¯ğ´ğ‘—ğ‘›, ğ‘›. Proof Apply 9.36 with ğ‘‰ = ğ…ğ‘› and ğ‘’1, â€¦, ğ‘’ğ‘› the standard basis of ğ…ğ‘› and ğ›¼ the alternating ğ‘›-linear form on ğ…ğ‘› that takes ğ‘£1, â€¦, ğ‘£ğ‘› to det (ğ‘£1 â‹¯ ğ‘£ğ‘› )[see 9.45]. If eachğ‘£ğ‘˜ is the ğ‘˜th column of ğ´, then each ğ‘ğ‘—, ğ‘˜ in 9.36 equals ğ´ğ‘—, ğ‘˜. Finally, ğ›¼(ğ‘’1, â€¦, ğ‘’ğ‘›) = det (ğ‘’1 â‹¯ ğ‘’ğ‘› )= det ğ¼ = 1. Thus the formula in 9.36 becomes the formula stated in this result. 9.47 example:explicit formula for determinant â€¢ If ğ´ is a 2-by-2matrix, then the formula in 9.46 becomes det ğ´ = ğ´1, 1ğ´2, 2 âˆ’ ğ´2, 1ğ´1, 2. â€¢ If ğ´ is a 3-by-3matrix, then the formula in 9.46 becomes det ğ´ =ğ´1, 1ğ´2, 2ğ´3, 3 âˆ’ ğ´2, 1ğ´1, 2ğ´3, 3 âˆ’ ğ´3, 1ğ´2, 2ğ´1, 3 âˆ’ ğ´1, 1ğ´3, 2ğ´2, 3 + ğ´3, 1ğ´1, 2ğ´2, 3 + ğ´2, 1ğ´3, 2ğ´1, 3. The sum in the formula in 9.46 contains ğ‘›! terms. Because ğ‘›! grows rapidly as ğ‘› increases, the formula in 9.46 is not a viable method to evaluate determinants even for moderately sized ğ‘›. For example, 10!is over three million, and 100!is approximately 10 158, leading to a sum that the fastest computer cannot evaluate. We will soon see some results that lead to faster evaluations of determinants than direct use of the sum in 9.46. 9.48 determinant of upper-triangular matrix Suppose that ğ´ is an upper-triangular matrix with ğœ†1, â€¦, ğœ†ğ‘› on the diagonal. Then det ğ´ = ğœ†1â‹¯ğœ†ğ‘›. Proof If (ğ‘—1, â€¦, ğ‘—ğ‘›) âˆˆ perm ğ‘› with (ğ‘—1, â€¦, ğ‘—ğ‘›) â‰  (1, â€¦, ğ‘›), then ğ‘—ğ‘˜ > ğ‘˜ for some ğ‘˜ âˆˆ {1, â€¦, ğ‘›}, which implies that ğ´ğ‘—ğ‘˜, ğ‘˜ = 0. Thus the only permutation that can make a nonzero contribution to the sum in 9.46 is the permutation (1, â€¦, ğ‘›). Because ğ´ğ‘˜, ğ‘˜ = ğœ†ğ‘˜ for each ğ‘˜ = 1, â€¦, ğ‘›, this implies that det ğ´ = ğœ†1â‹¯ğœ†ğ‘›. Section 9C Determinants 357 Properties of Determinants Our definition of the determinant leads to the following magical proof that the determinant is multiplicative. 9.49 determinant is multiplicative (a) Suppose ğ‘†, ğ‘‡ âˆˆ â„’(ğ‘‰). Then det(ğ‘†ğ‘‡) = (det ğ‘†)(det ğ‘‡). (b) Suppose ğ´ and ğµ are square matrices of the same size. Then det(ğ´ğµ) = (det ğ´)(det ğµ) Proof (a) Let ğ‘› = dim ğ‘‰. Suppose ğ›¼ âˆˆ ğ‘‰(ğ‘›) alt and ğ‘£1, â€¦, ğ‘£ğ‘› âˆˆ ğ‘‰. Then ğ›¼ğ‘†ğ‘‡(ğ‘£1, â€¦, ğ‘£ğ‘›) = ğ›¼(ğ‘†ğ‘‡ğ‘£1, â€¦, ğ‘†ğ‘‡ğ‘£ğ‘›) = (det ğ‘†)ğ›¼(ğ‘‡ğ‘£1, â€¦, ğ‘‡ğ‘£ğ‘›) = (det ğ‘†)(det ğ‘‡)ğ›¼(ğ‘£1, â€¦, ğ‘£ğ‘›), where the first equation follows from the definition ofğ›¼ğ‘†ğ‘‡, the second equation follows from the definition ofdet ğ‘†, and the third equation follows from the definition ofdet ğ‘‡. The equation above implies that det(ğ‘†ğ‘‡) = (det ğ‘†)(det ğ‘‡). (b) Let ğ‘†, ğ‘‡ âˆˆ â„’(ğ…ğ‘›)be such that â„³(ğ‘†) = ğ´ and â„³(ğ‘‡) = ğµ, where all matrices of operators in this proof are with respect to the standard basis of ğ…ğ‘›. Then â„³(ğ‘†ğ‘‡) = â„³(ğ‘†)â„³(ğ‘‡) = ğ´ğµ (see 3.43). Thus det(ğ´ğµ) = det(ğ‘†ğ‘‡) = (det ğ‘†)(det ğ‘‡) = (det ğ´)(det ğµ), where the second equality comes from the result in (a). The determinant of an operator determines whether the operator is invertible. 9.50 invertible âŸº nonzero determinant An operator ğ‘‡ âˆˆ â„’(ğ‘‰) is invertible if and only if det ğ‘‡ â‰  0. Furthermore, if ğ‘‡ is invertible, then det(ğ‘‡âˆ’1)= 1 det ğ‘‡ . Proof First suppose ğ‘‡ is invertible. Thus ğ‘‡ğ‘‡âˆ’1 = ğ¼. Now 9.49 implies that 1 =det ğ¼ = det(ğ‘‡ğ‘‡âˆ’1)= (det ğ‘‡)(det(ğ‘‡âˆ’1)). Hence det ğ‘‡ â‰  0and det(ğ‘‡âˆ’1)is the multiplicative inverse of det ğ‘‡. To prove the other direction, now suppose det ğ‘‡ â‰  0. Suppose ğ‘£ âˆˆ ğ‘‰ and ğ‘£ â‰  0. Let ğ‘£, ğ‘’2, â€¦, ğ‘’ğ‘› be a basis of ğ‘‰ and let ğ›¼ âˆˆ ğ‘‰(ğ‘›) alt be such that ğ›¼ â‰  0. Then ğ›¼(ğ‘£, ğ‘’2, â€¦, ğ‘’ğ‘›) â‰  0(by 9.39). Now ğ›¼(ğ‘‡ğ‘£, ğ‘‡ğ‘’2, â€¦, ğ‘‡ğ‘’ğ‘›) = (det ğ‘‡)ğ›¼(ğ‘£, ğ‘’2, â€¦, ğ‘’ğ‘›) â‰  0, Thus ğ‘‡ğ‘£ â‰  0. Hence ğ‘‡ is invertible. 358 Chapter 9 Multilinear Algebra and Determinants An ğ‘›-by-ğ‘› matrix ğ´ is invertible (see 3.80 for the definition of an invertible matrix) if and only if the operator on ğ…ğ‘› associated with ğ´ (via the standard basis of ğ…ğ‘›)is invertible. Thus the previous result shows that a square matrix ğ´ is invertible if and only if det ğ´ â‰  0. 9.51 eigenvalues and determinants Suppose ğ‘‡ âˆˆ â„’(ğ‘‰) and ğœ† âˆˆ ğ…. Then ğœ† is an eigenvalue of ğ‘‡ if and only if det(ğœ†ğ¼ âˆ’ ğ‘‡) = 0. Proof The number ğœ† is an eigenvalue of ğ‘‡ if and only if ğ‘‡ âˆ’ ğœ†ğ¼ is not invertible (see 5.7), which happens if and only if ğœ†ğ¼ âˆ’ ğ‘‡ is not invertible, which happens if and only if det(ğœ†ğ¼ âˆ’ ğ‘‡) = 0(by 9.50). Suppose ğ‘‡ âˆˆ â„’(ğ‘‰) and ğ‘†âˆ¶ ğ‘Š â†’ ğ‘‰ is an invertible linear map. To prove that det(ğ‘† âˆ’1ğ‘‡ğ‘†)= det ğ‘‡, we could try to use 9.49 and 9.50, writing det(ğ‘† âˆ’1ğ‘‡ğ‘†)= (det ğ‘† âˆ’1)(det ğ‘‡)(det ğ‘†) = det ğ‘‡. That proof works if ğ‘Š = ğ‘‰, but if ğ‘Š â‰  ğ‘‰ then it makes no sense because the determinant is defined only for linear maps from a vector space to itself, andğ‘† maps ğ‘Š to ğ‘‰, making det ğ‘† undefined. The proof given below works around this issue and is valid when ğ‘Š â‰  ğ‘‰. 9.52 determinant is a similarity invariant Suppose ğ‘‡ âˆˆ â„’(ğ‘‰) and ğ‘†âˆ¶ ğ‘Š â†’ ğ‘‰ is an invertible linear map. Then det(ğ‘† âˆ’1ğ‘‡ğ‘†)= det ğ‘‡. Proof Let ğ‘› = dim ğ‘Š = dim ğ‘‰. Suppose ğœ âˆˆ ğ‘Š(ğ‘›) alt . Defineğ›¼ âˆˆ ğ‘‰(ğ‘›) alt by ğ›¼(ğ‘£1, â€¦, ğ‘£ğ‘›) = ğœ(ğ‘† âˆ’1ğ‘£1, â€¦, ğ‘† âˆ’1ğ‘£ğ‘›) for ğ‘£1, â€¦, ğ‘£ğ‘› âˆˆ ğ‘‰. Suppose ğ‘¤1, â€¦, ğ‘¤ğ‘› âˆˆ ğ‘Š. Then ğœğ‘†âˆ’1ğ‘‡ğ‘†(ğ‘¤1, â€¦, ğ‘¤ğ‘›) = ğœ(ğ‘† âˆ’1ğ‘‡ğ‘†ğ‘¤1, â€¦, ğ‘† âˆ’1ğ‘‡ğ‘†ğ‘¤ğ‘›) = ğ›¼(ğ‘‡ğ‘†ğ‘¤1, â€¦, ğ‘‡ğ‘†ğ‘¤ğ‘›) = ğ›¼ğ‘‡(ğ‘†ğ‘¤1, â€¦, ğ‘†ğ‘¤ğ‘›) = (det ğ‘‡)ğ›¼(ğ‘†ğ‘¤1, â€¦, ğ‘†ğ‘¤ğ‘›) = (det ğ‘‡)ğœ(ğ‘¤1, â€¦, ğ‘¤ğ‘›). The equation above and the definition of the determinant of the operatorğ‘† âˆ’1ğ‘‡ğ‘† imply that det(ğ‘†âˆ’1ğ‘‡ğ‘†)= det ğ‘‡. Section 9C Determinants 359 For the special case in which ğ‘‰ = ğ…ğ‘› and ğ‘’1, â€¦, ğ‘’ğ‘› is the standard basis of ğ…ğ‘›, the next result is true by the definition of the determinant of a matrix. The left side of the equation in the next result does not depend on a choice of basis, which means that the right side is independent of the choice of basis. 9.53 determinant of operator equals determinant of its matrix Suppose ğ‘‡ âˆˆ â„’(ğ‘‰) and ğ‘’1, â€¦, ğ‘’ğ‘› is a basis of ğ‘‰. Then det ğ‘‡ = det â„³(ğ‘‡, (ğ‘’1, â€¦, ğ‘’ğ‘›)). Proof Let ğ‘“1, â€¦, ğ‘“ğ‘› be the standard basis of ğ…ğ‘›. Let ğ‘†âˆ¶ ğ…ğ‘› â†’ ğ‘‰ be the linear map such that ğ‘† ğ‘“ğ‘˜ = ğ‘’ğ‘˜ for each ğ‘˜ = 1, â€¦, ğ‘›. Thus â„³(ğ‘†, ( ğ‘“1, â€¦, ğ‘“ğ‘›), (ğ‘’1, â€¦, ğ‘’ğ‘›)) and â„³(ğ‘† âˆ’1, (ğ‘’1, â€¦, ğ‘’ğ‘›), ( ğ‘“1, â€¦, ğ‘“ğ‘›))both equal the ğ‘›-by-ğ‘› identity matrix. Hence 9.54 â„³(ğ‘†âˆ’1ğ‘‡ğ‘†, ( ğ‘“1, â€¦, ğ‘“ğ‘›))= â„³(ğ‘‡, (ğ‘’1, â€¦, ğ‘’ğ‘›)), as follows from two applications of 3.43. Thus det ğ‘‡ = det(ğ‘†âˆ’1ğ‘‡ğ‘†) = det â„³(ğ‘† âˆ’1ğ‘‡ğ‘†, ( ğ‘“1, â€¦, ğ‘“ğ‘›)) = det â„³(ğ‘‡, (ğ‘’1, â€¦, ğ‘’ğ‘›)), where the first line comes from9.52, the second line comes from the definition of the determinant of a matrix, and the third line follows from 9.54. The next result gives a more intuitive way to think about determinants than the definition or the formula in9.46. We could make the characterization in the result below the definition of the determinant of an operator on a finite-dimensional complex vector space, with the current definition then becoming a consequence of that definition. 9.55 if ğ… = ğ‚, then determinant equals product of eigenvalues Suppose ğ… = ğ‚ and ğ‘‡ âˆˆ â„’(ğ‘‰). Then det ğ‘‡ equals the product of the eigen- values of ğ‘‡, with each eigenvalue included as many times as its multiplicity. Proof There is a basis of ğ‘‰ with respect to which ğ‘‡ has an upper-triangular matrix with the diagonal entries of the matrix consisting of the eigenvalues of ğ‘‡, with each eigenvalue included as many times as its multiplicityâ€”see 8.37. Thus 9.53 and 9.48 imply that det ğ‘‡ equals the product of the eigenvalues of ğ‘‡, with each eigenvalue included as many times as its multiplicity. As the next result shows, the determinant interacts nicely with the transpose of a square matrix, with the dual of an operator, and with the adjoint of an operator on an inner product space. 360 Chapter 9 Multilinear Algebra and Determinants 9.56 determinant of transpose, dual, or adjoint (a) Suppose ğ´ is a square matrix. Then det ğ´ t = det ğ´. (b) Suppose ğ‘‡ âˆˆ â„’(ğ‘‰). Then det ğ‘‡â€² = det ğ‘‡. (c) Suppose ğ‘‰ is an inner product space and ğ‘‡ âˆˆ â„’(ğ‘‰). Then det(ğ‘‡âˆ—)= det ğ‘‡. Proof (a) Let ğ‘› be a positive integer. Defineğ›¼âˆ¶ (ğ…ğ‘›) ğ‘› â†’ ğ… by ğ›¼((ğ‘£1 â‹¯ ğ‘£ğ‘› ))= det((ğ‘£1 â‹¯ ğ‘£ğ‘› ) t) for all ğ‘£1, â€¦, ğ‘£ğ‘› âˆˆ ğ…ğ‘›. The formula in 9.46 for the determinant of a matrix shows that ğ›¼ is an ğ‘›-linear form on ğ…ğ‘›. Suppose ğ‘£1, â€¦, ğ‘£ğ‘› âˆˆ ğ…ğ‘› and ğ‘£ğ‘— = ğ‘£ğ‘˜ for some ğ‘— â‰  ğ‘˜. If ğµ is an ğ‘›-by-ğ‘› matrix, then (ğ‘£1 â‹¯ ğ‘£ğ‘› ) tğµ cannot equal the identity matrix because row ğ‘— and row ğ‘˜ of (ğ‘£1 â‹¯ ğ‘£ğ‘› ) tğµ are equal. Thus (ğ‘£1 â‹¯ ğ‘£ğ‘› ) t is not invertible, which implies that ğ›¼((ğ‘£1 â‹¯ ğ‘£ğ‘› ))= 0. Hence ğ›¼ is an alternating ğ‘›- linear form on ğ…ğ‘›. Note that ğ›¼ applied to the standard basis of ğ…ğ‘› equals 1. Because the vector space of alternating ğ‘›-linear forms on ğ…ğ‘› has dimension one (by 9.37), this implies that ğ›¼ is the determinant function. Thus (a) holds. (b) The equation det ğ‘‡â€² = det ğ‘‡ follows from (a) and 9.53 and 3.132. (c) Pick an orthonormal basis of ğ‘‰. The matrix of ğ‘‡âˆ— with respect to that basis is the conjugate transpose of the matrix of ğ‘‡ with respect to that basis (by 7.9). Thus 9.53, 9.46, and (a) imply that det(ğ‘‡âˆ—)= det ğ‘‡. 9.57 helpful results in evaluating determinants (a) If either two columns or two rows of a square matrix are equal, then the determinant of the matrix equals 0. (b) Suppose ğ´ is a square matrix and ğµ is the matrix obtained from ğ´ by swapping either two columns or two rows. Then det ğ´ = âˆ’ det ğµ. (c) If one column or one row of a square matrix is multiplied by a scalar, then the value of the determinant is multiplied by the same scalar. (d) If a scalar multiple of one column of a square matrix to added to another column, then the value of the determinant is unchanged. (e) If a scalar multiple of one row of a square matrix to added to another row, then the value of the determinant is unchanged. Section 9C Determinants 361 Proof All the assertions in this result follow from the result that the maps ğ‘£1, â€¦, ğ‘£ğ‘› â†¦ det (ğ‘£1 â‹¯ ğ‘£ğ‘› )and ğ‘£1, â€¦, ğ‘£ğ‘› â†¦ det (ğ‘£1 â‹¯ ğ‘£ğ‘› ) t are both alternating ğ‘›-linear forms on ğ…ğ‘› [see 9.45 and 9.56(a)]. For example, to prove (d) suppose ğ‘£1, â€¦, ğ‘£ğ‘› âˆˆ ğ…ğ‘› and ğ‘ âˆˆ ğ…. Then det(ğ‘£1 + ğ‘ğ‘£2 ğ‘£2 â‹¯ ğ‘£ğ‘› ) = det (ğ‘£1 ğ‘£2 â‹¯ ğ‘£ğ‘› )+ ğ‘ det (ğ‘£2 ğ‘£2 ğ‘£3 â‹¯ ğ‘£ğ‘› ) = det (ğ‘£1 ğ‘£2 â‹¯ ğ‘£ğ‘› ), where the first equation follows from the multilinearity property and the second equation follows from the alternating property. The equation above shows that adding a multiple of the second column to the first column does not change the value of the determinant. The same conclusion holds for any two columns. Thus (d) holds. The proof of (e) follows from (d) and from 9.56(a). The proofs of (a), (b), and (c) use similar tools and are left to the reader. For matrices whose entries are concrete numbers, the result above leads to a much faster way to evaluate the determinant than direct application of the formula in 9.46. Specifically, apply the Gaussian elimination procedure of swapping rows [by 9.48(b), this changes the determinant by a factor of âˆ’1], multiplying a row by a nonzero constant [by 9.48(c), this changes the determinant by the same constant], and adding a multiple of one row to another row [by 9.48(e), this does not change the determinant]to produce an upper-triangular matrix, whose determinant is the product of the diagonal entries (by 9.48). If your software keeps track of the number of row swaps and of the constants used when multiplying a row by a constant, then the determinant of the original matrix can be computed. Because a number ğœ† âˆˆ ğ… is an eigenvalue of an operator ğ‘‡ âˆˆ â„’(ğ‘‰) if and only if det(ğœ†ğ¼ âˆ’ ğ‘‡) = 0(by 9.51), you may be tempted to think that one way to find eigenvalues quickly is to choose a basis ofğ‘‰, let ğ´ = â„³(ğ‘‡), evaluate det(ğœ†ğ¼ âˆ’ ğ´), and then solve the equation det(ğœ†ğ¼ âˆ’ ğ´) = 0for ğœ†. However, that procedure is rarely efficient, except when dim ğ‘‰ = 2(or when dim ğ‘‰ equals 3or 4if you are willing to use the cubic or quartic formulas). One problem is that the procedure described in the paragraph above for evaluating a determinant does not work when the matrix includes a symbol (such as the ğœ† in ğœ†ğ¼ âˆ’ ğ´). This problem arises because decisions need to be made in the Gaussian elimination procedure about whether certain quantities equal 0, and those decisions become complicated in expressions involving a symbol ğœ†. Recall that an operator on a finite-dimensional inner product space is unitary if it preserves norms (see 7.51 and the paragraph following it). Every eigenvalue of a unitary operator has absolute value 1(by 7.54). Thus the product of the eigenvalues of a unitary operator has absolute value 1. Hence (at least in the case ğ… = ğ‚) the determinant of a unitary operator has absolute value 1(by 9.55). The next result gives a proof that works without the assumption that ğ… = ğ‚. 362 Chapter 9 Multilinear Algebra and Determinants 9.58 every unitary operator has determinant with absolute value 1 Suppose ğ‘‰ is an inner product space and ğ‘† âˆˆ â„’(ğ‘‰) is a unitary operator. Then |det ğ‘†| = 1. Proof Because ğ‘† is unitary, ğ¼ = ğ‘†âˆ—ğ‘† (see 7.53). Thus 1 =det(ğ‘†âˆ—ğ‘†) = (det ğ‘†âˆ—)(det ğ‘†) = (det ğ‘†)(det ğ‘†) = |det ğ‘†| 2, where the second equality comes from 9.49(a) and the third equality comes from 9.56(c). The equation above implies that |det ğ‘†| = 1. The determinant of a positive operator on an inner product space meshes well with the analogy that such operators correspond to the nonnegative real numbers. 9.59 every positive operator has nonnegative determinant Suppose ğ‘‰ is an inner product space and ğ‘‡ âˆˆ â„’(ğ‘‰) is a positive operator. Then det ğ‘‡ â‰¥ 0. Proof By the spectral theorem (7.29 or 7.31), ğ‘‰ has an orthonormal basis con- sisting of eigenvectors of ğ‘‡. Thus by the last bullet point of 9.42, det ğ‘‡ equals a product of the eigenvalues of ğ‘‡, possibly with repetitions. Each eigenvalue of ğ‘‡ is a nonnegative number (by 7.38). Thus we conclude that det ğ‘‡ â‰¥ 0. Suppose ğ‘‰ is an inner product space and ğ‘‡ âˆˆ â„’(ğ‘‰). Recall that the list of nonnegative square roots of the eigenvalues of ğ‘‡âˆ—ğ‘‡ (each included as many times as its multiplicity) is called the list of singular values of ğ‘‡ (see Section 7E). 9.60 |det ğ‘‡| = product of singular values of ğ‘‡ Suppose ğ‘‰ is an inner product space and ğ‘‡ âˆˆ â„’(ğ‘‰). Then |det ğ‘‡| = âˆšdet(ğ‘‡âˆ—ğ‘‡)= product of singular values of ğ‘‡. Proof We have |det ğ‘‡| 2 = (det ğ‘‡)(det ğ‘‡) = (det(ğ‘‡âˆ—))(det ğ‘‡) = det(ğ‘‡âˆ—ğ‘‡), where the middle equality comes from 9.56(c) and the last equality comes from 9.49(a). Taking square roots of both sides of the equation above shows that |det ğ‘‡| = âˆšdet(ğ‘‡âˆ—ğ‘‡). Let ğ‘ 1, â€¦, ğ‘ ğ‘› denote the list of singular values of ğ‘‡. Thus ğ‘ 1 2, â€¦, ğ‘ ğ‘› 2 is the list of eigenvalues of ğ‘‡âˆ—ğ‘‡ (with appropriate repetitions), corresponding to an orthonormal basis of ğ‘‰ consisting of eigenvectors of ğ‘‡âˆ—ğ‘‡. Hence the last bullet point of 9.42 implies that det(ğ‘‡âˆ—ğ‘‡)= ğ‘ 1 2â‹¯ğ‘ ğ‘› 2. Thus |det ğ‘‡| = ğ‘ 1â‹¯ğ‘ ğ‘›, as desired. Section 9C Determinants 363 An operator ğ‘‡ on a real inner product space changes volume by a factor of the product of the singular values (by 7.111). Thus the next result follows immediately from 7.111 and 9.60. This result explains why the absolute value of a determinant appears in the change of variables formula in multivariable calculus. 9.61 ğ‘‡ changes volume by factor of |det ğ‘‡| Suppose ğ‘‡ âˆˆ â„’(ğ‘ğ‘›)and Î© âŠ† ğ‘ ğ‘›. Then volume ğ‘‡(Î©) = |det ğ‘‡|(volume Î©). For operators on finite-dimensional complex vector spaces, we now connect the determinant to a polynomial that we have previously seen. 9.62 if ğ… = ğ‚, then characteristic polynomial of ğ‘‡ equals det(ğ‘§ğ¼ âˆ’ ğ‘‡) Suppose ğ… = ğ‚ and ğ‘‡ âˆˆ â„’(ğ‘‰). Let ğœ†1, â€¦, ğœ†ğ‘š denote the distinct eigenvalues of ğ‘‡, and let ğ‘‘1, â€¦, ğ‘‘ğ‘š denote their multiplicities. Then det(ğ‘§ğ¼ âˆ’ ğ‘‡) = (ğ‘§ âˆ’ ğœ†1) ğ‘‘1â‹¯(ğ‘§ âˆ’ ğœ†ğ‘š)ğ‘‘ğ‘š. Proof There exists a basis of ğ‘‰ with respect to which ğ‘‡ has an upper-triangular matrix with each ğœ†ğ‘˜ appearing on the diagonal exactly ğ‘‘ğ‘˜ times (by 8.37). With respect to this basis, ğ‘§ğ¼ âˆ’ ğ‘‡ has an upper-triangular matrix with ğ‘§ âˆ’ ğœ†ğ‘˜ appearing on the diagonal exactly ğ‘‘ğ‘˜ times for each ğ‘˜. Thus 9.48 gives the desired equation. Suppose ğ… = ğ‚ and ğ‘‡ âˆˆ â„’(ğ‘‰). The characteristic polynomial of ğ‘‡ was defined in8.26 as the polynomial on the right side of the equation in 9.62. We did not previously define the characteristic polynomial of an operator on a finite- dimensional real vector space because such operators may have no eigenvalues, making a definition using the right side of the equation in9.62 inappropriate. We now present a new definition of the characteristic polynomial, motivated by 9.62. This new definition is valid for both real and complex vector spaces. The equation in 9.62 shows that this new definition is equivalent to our previous definition whenğ… = ğ‚ (8.26). 9.63 definition:characteristic polynomial Suppose ğ‘‡ âˆˆ â„’(ğ‘‰). The polynomial defined by ğ‘§ â†¦ det(ğ‘§ğ¼ âˆ’ ğ‘‡) is called the characteristic polynomial of ğ‘‡. The formula in 9.46 shows that the characteristic polynomial of an opera- tor ğ‘‡ âˆˆ â„’(ğ‘‰) is a monic polynomial of degree dim ğ‘‰. The zeros in ğ… of the characteristic polynomial of ğ‘‡ are exactly the eigenvalues of ğ‘‡ (by 9.51). 364 Chapter 9 Multilinear Algebra and Determinants Previously we proved the Cayleyâ€“Hamilton theorem (8.29) in the complex case. Now we can extend that result to operators on real vector spaces. 9.64 Cayleyâ€“Hamilton theorem Suppose ğ‘‡ âˆˆ â„’(ğ‘‰) and ğ‘ is the characteristic polynomial of ğ‘‡. Then ğ‘(ğ‘‡) = 0. Proof If ğ… = ğ‚, then the equation ğ‘(ğ‘‡) = 0follows from 9.62 and 8.29. Now suppose ğ… = ğ‘. Fix a basis of ğ‘‰, and let ğ´ be the matrix of ğ‘‡ with respect to this basis. Let ğ‘† be the operator on ğ‚ dim ğ‘‰ such that the matrix of ğ‘† (with respect to the standard basis of ğ‚ dim ğ‘‰)is ğ´. For all ğ‘§ âˆˆ ğ‘ we have ğ‘(ğ‘§) = det(ğ‘§ğ¼ âˆ’ ğ‘‡) = det(ğ‘§ğ¼ âˆ’ ğ´) = det(ğ‘§ğ¼ âˆ’ ğ‘†). Thus ğ‘ is the characteristic polynomial of ğ‘†. The case ğ… = ğ‚ (first sentence of this proof) now implies that 0 = ğ‘(ğ‘†) = ğ‘(ğ´) = ğ‘(ğ‘‡). The Cayleyâ€“Hamilton theorem (9.64) implies that the characteristic polyno- mial of an operator ğ‘‡ âˆˆ â„’(ğ‘‰) is a polynomial multiple of the minimal polynomial of ğ‘‡ (by 5.29). Thus if the degree of the minimal polynomial of ğ‘‡ equals dim ğ‘‰, then the characteristic polynomial of ğ‘‡ equals the minimal polynomial of ğ‘‡. This happens for a very large percentage of operators, including over 99.999% of 4-by-4matrices with integer entries in [âˆ’100, 100](see the paragraph following 5.25). The last sentence in our next result was previously proved in the complex case (see 8.54). Now we can give a proof that works on both real and complex vector spaces. 9.65 characteristic polynomial, trace, and determinant Suppose ğ‘‡ âˆˆ â„’(ğ‘‰). Let ğ‘› = dim ğ‘‰. Then the characteristic polynomial of ğ‘‡ can be written as ğ‘§ ğ‘› âˆ’ (tr ğ‘‡)ğ‘§ ğ‘› âˆ’ 1 + â‹¯ + (âˆ’1) ğ‘›(det ğ‘‡). Proof The constant term of a polynomial function of ğ‘§ is the value of the poly- nomial when ğ‘§ = 0. Thus the constant term of the characteristic polynomial of ğ‘‡ equals det(âˆ’ğ‘‡), which equals (âˆ’1) ğ‘› det ğ‘‡ (by the third bullet point of 9.42). Fix a basis of ğ‘‰, and let ğ´ be the matrix of ğ‘‡ with respect to this basis. The matrix of ğ‘§ğ¼ âˆ’ ğ‘‡ with respect to this basis is ğ‘§ğ¼ âˆ’ ğ´. The term coming from the identity permutation {1, â€¦, ğ‘›} in the formula 9.46 for det(ğ‘§ğ¼ âˆ’ ğ´) is (ğ‘§ âˆ’ ğ´1, 1)â‹¯(ğ‘§ âˆ’ ğ´ğ‘›, ğ‘›). The coefficient of ğ‘§ ğ‘› âˆ’ 1 in the expression above is âˆ’(ğ´1, 1+â‹¯+ğ´ğ‘›, ğ‘›), which equals âˆ’ tr ğ‘‡. The terms in the formula for det(ğ‘§ğ¼ âˆ’ ğ´) coming from other elements of perm ğ‘› contain at most ğ‘› âˆ’ 2factors of the form ğ‘§ âˆ’ ğ´ğ‘˜, ğ‘˜ and thus do not contribute to the coefficient of ğ‘§ ğ‘› âˆ’ 1 in the characteristic polynomial of ğ‘‡. Section 9C Determinants 365 The next result was proved by Jacques Hadamard (1865â€“1963) in 1893. In the result below, think of the columns of the ğ‘›-by-ğ‘› matrix ğ´ as ele- ments of ğ…ğ‘›. The norms appearing below then arise from the standard inner product on ğ…ğ‘›. Recall that the notation ğ‘…â‹…, ğ‘˜ in the proof below means the ğ‘˜th column of the matrix ğ‘… (as was defined in3.44). 9.66 Hadamardâ€™s inequality Suppose ğ´ is an ğ‘›-by-ğ‘› matrix. Let ğ‘£1, â€¦, ğ‘£ğ‘› denote the columns of ğ´. Then |det ğ´| â‰¤ ğ‘› âˆ ğ‘˜ = 1 â€–ğ‘£ğ‘˜â€–. Proof If ğ´ is not invertible, then det ğ´ = 0and hence the desired inequality holds in this case. Thus assume that ğ´ is invertible. The QR factorization (7.58) tells us that there exist a unitary matrix ğ‘„ and an upper-triangular matrix ğ‘… whose diagonal contains only positive numbers such that ğ´ = ğ‘„ğ‘…. We have |det ğ´| = |det ğ‘„| |det ğ‘…| = |det ğ‘…| = ğ‘› âˆ ğ‘˜ = 1 ğ‘…ğ‘˜, ğ‘˜ â‰¤ ğ‘› âˆ ğ‘˜ = 1 â€–ğ‘…â‹…, ğ‘˜â€– = ğ‘› âˆ ğ‘˜ = 1 â€–ğ‘„ğ‘…â‹…, ğ‘˜â€– = ğ‘› âˆ ğ‘˜ = 1 â€–ğ‘£ğ‘˜â€–, where the first line comes from9.49(b), the second line comes from 9.58, the third line comes from 9.48, and the fifth line holds becauseğ‘„ is an isometry. To give a geometric interpretation to Hadamardâ€™s inequality, suppose ğ… = ğ‘. Let ğ‘‡ âˆˆ â„’(ğ‘ğ‘›)be the operator such that ğ‘‡ğ‘’ğ‘˜ = ğ‘£ğ‘˜ for each ğ‘˜ = 1, â€¦, ğ‘›, where ğ‘’1, â€¦, ğ‘’ğ‘› is the standard basis of ğ‘ğ‘›. Then ğ‘‡ maps the box ğ‘ƒ(ğ‘’1, â€¦, ğ‘’ğ‘›) onto the parallelepiped ğ‘ƒ(ğ‘£1, â€¦, ğ‘£ğ‘›) [see7.102 and 7.105 for a review of this notation and terminology]. Because the boxğ‘ƒ(ğ‘’1, â€¦, ğ‘’ğ‘›) has volume 1, this implies (by 9.61) that the parallelepiped ğ‘ƒ(ğ‘£1, â€¦, ğ‘£ğ‘›) has volume |det ğ‘‡|, which equals |det ğ´|. Thus Hadamardâ€™s inequality above can be interpreted to say that among all paral- lelepipeds whose edges have lengths â€–ğ‘£1â€–, â€¦, â€–ğ‘£ğ‘›â€–, the ones with largest volume have orthogonal edges (and thus have volume âˆ ğ‘› ğ‘˜ = 1 â€–ğ‘£ğ‘˜â€–). For a necessary and sufficient condition for Hadamardâ€™s inequality to be an equality, see Exercise 18. 366 Chapter 9 Multilinear Algebra and Determinants The matrix in the next result is called the Vandermonde matrix. Vandermonde matrices have important applications in polynomial interpolation, the discrete Fourier transform, and other areas of mathematics. The proof of the next result is a nice illustration of the power of switching between matrices and linear maps. 9.67 determinant of Vandermonde matrix Suppose ğ‘› > 1and ğ›½1, â€¦, ğ›½ğ‘› âˆˆ ğ…. Then det â›âœâœâœâœâœâœâœâœâœâœâœâœâœâœâœ â 1 ğ›½1 ğ›½1 2 â‹¯ ğ›½1 ğ‘› âˆ’ 1 1 ğ›½2 ğ›½2 2 â‹¯ ğ›½2 ğ‘› âˆ’ 1 â‹± 1 ğ›½ğ‘› ğ›½ğ‘› 2 â‹¯ ğ›½ğ‘› ğ‘› âˆ’ 1 ââŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸ â  = âˆ 1 â‰¤ ğ‘— < ğ‘˜ â‰¤ ğ‘›(ğ›½ğ‘˜ âˆ’ ğ›½ğ‘—). Proof Let 1, ğ‘§, â€¦, ğ‘§ğ‘› âˆ’ 1 be the standard basis of ğ’«ğ‘› âˆ’ 1(ğ…) and let ğ‘’1, â€¦, ğ‘’ğ‘› denote the standard basis of ğ…ğ‘›. Define a linear mapğ‘†âˆ¶ ğ’«ğ‘› âˆ’ 1(ğ…) â†’ ğ…ğ‘› by ğ‘†ğ‘ = (ğ‘(ğ›½1), â€¦, ğ‘(ğ›½ğ‘›)). Let ğ´ denote the Vandermonde matrix shown in the statement of this result. Note that ğ´ = â„³(ğ‘†, (1, ğ‘§, â€¦, ğ‘§ ğ‘› âˆ’ 1), (ğ‘’1, â€¦, ğ‘’ğ‘›)). Let ğ‘‡âˆ¶ ğ’«ğ‘› âˆ’ 1(ğ…) â†’ ğ’«ğ‘› âˆ’ 1(ğ…) be the operator on ğ’«ğ‘› âˆ’ 1(ğ…) such that ğ‘‡1 = 1 and ğ‘‡ğ‘§ ğ‘˜ = (ğ‘§ âˆ’ ğ›½1)(ğ‘§ âˆ’ ğ›½2)â‹¯(ğ‘§ âˆ’ ğ›½ğ‘˜) for ğ‘˜ = 1, â€¦, ğ‘› âˆ’ 1. Let ğµ = â„³(ğ‘‡, (1, ğ‘§, â€¦, ğ‘§ğ‘› âˆ’ 1), (1, ğ‘§, â€¦, ğ‘§ ğ‘› âˆ’ 1)). Then ğµ is an upper-triangular matrix all of whose diagonal entries equal 1. Thus det ğµ = 1(by 9.48). Let ğ¶ = â„³(ğ‘†ğ‘‡, (1, ğ‘§, â€¦, ğ‘§ğ‘› âˆ’ 1), (ğ‘’1, â€¦, ğ‘’ğ‘›)). Thus ğ¶ = ğ´ğµ (by 3.81), which implies that det ğ´ = (det ğ´)(det ğµ) = det ğ¶. The definitions ofğ¶, ğ‘†, and ğ‘‡ show that ğ¶ equals â›âœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœ â 1 0 0 â‹¯ 0 1 ğ›½2 âˆ’ ğ›½1 0 â‹¯ 0 1 ğ›½2 âˆ’ ğ›½1 (ğ›½3 âˆ’ ğ›½1)(ğ›½3 âˆ’ ğ›½2) â‹¯ 0 â‹± 1 ğ›½2 âˆ’ ğ›½1 (ğ›½3 âˆ’ ğ›½1)(ğ›½3 âˆ’ ğ›½2) â‹¯ (ğ›½ğ‘› âˆ’ ğ›½1)(ğ›½ğ‘› âˆ’ ğ›½2)â‹¯(ğ›½ğ‘› âˆ’ ğ›½ğ‘› âˆ’ 1) ââŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸ â  . Now det ğ´ = det ğ¶ = âˆ 1 â‰¤ ğ‘— < ğ‘˜ â‰¤ ğ‘›(ğ›½ğ‘˜ âˆ’ ğ›½ğ‘—), where we have used 9.56(a) and 9.48. Section 9C Determinants 367 Exercises 9C 1 Prove or give a counterexample: ğ‘†, ğ‘‡ âˆˆ â„’(ğ‘‰) âŸ¹ det(ğ‘†+ğ‘‡) = det ğ‘†+det ğ‘‡. 2 Suppose the first column of a square matrixğ´ consists of all zeros except possibly the first entryğ´1, 1. Let ğµ be the matrix obtained from ğ´ by deleting the first row and the first column ofğ´. Show that det ğ´ = ğ´1, 1 det ğµ. 3 Suppose ğ‘‡ âˆˆ â„’(ğ‘‰) is nilpotent. Prove that det(ğ¼ + ğ‘‡) = 1. 4 Suppose ğ‘† âˆˆ â„’(ğ‘‰). Prove that ğ‘† is unitary if and only if |det ğ‘†| = â€–ğ‘†â€– = 1. 5 Suppose ğ´ is a block upper-triangular matrix ğ´ = â›âœâœâœ â ğ´1 âˆ— â‹± 0 ğ´ğ‘š ââŸâŸâŸ â  , where each ğ´ğ‘˜ along the diagonal is a square matrix. Prove that det ğ´ = (det ğ´1)â‹¯(det ğ´ğ‘š). 6 Suppose ğ´ = (ğ‘£1 â‹¯ ğ‘£ğ‘› )is an ğ‘›-by-ğ‘› matrix, with ğ‘£ğ‘˜ denoting the ğ‘˜th column of ğ´. Show that if (ğ‘š1, â€¦, ğ‘šğ‘›) âˆˆ perm ğ‘›, then det (ğ‘£ğ‘š1 â‹¯ ğ‘£ğ‘šğ‘› )= (sign(ğ‘š1, â€¦, ğ‘šğ‘›))det ğ´. 7 Suppose ğ‘‡ âˆˆ â„’(ğ‘‰) is invertible. Let ğ‘ denote the characteristic polynomial of ğ‘‡ and let ğ‘ denote the characteristic polynomial of ğ‘‡âˆ’1. Prove that ğ‘(ğ‘§) = 1 ğ‘(0) ğ‘§ dim ğ‘‰ ğ‘( 1 ğ‘§ ) for all nonzero ğ‘§ âˆˆ ğ…. 8 Suppose ğ‘‡ âˆˆ â„’(ğ‘‰) is an operator with no eigenvalues (which implies that ğ… = ğ‘). Prove that det ğ‘‡ > 0. 9 Suppose that ğ‘‰ is a real vector space of even dimension, ğ‘‡ âˆˆ â„’(ğ‘‰), and det ğ‘‡ < 0. Prove that ğ‘‡ has at least two distinct eigenvalues. 10 Suppose ğ‘‰ is a real vector space of odd dimension and ğ‘‡ âˆˆ â„’(ğ‘‰). Without using the minimal polynomial, prove that ğ‘‡ has an eigenvalue. This result was previously proved without using determinants or the charac- teristic polynomialâ€”see 5.34. 11 Prove or give a counterexample: If ğ… = ğ‘, ğ‘‡ âˆˆ â„’(ğ‘‰), and det ğ‘‡ > 0, then ğ‘‡ has a square root. If ğ… = ğ‚, ğ‘‡ âˆˆ â„’(ğ‘‰), and det ğ‘‰ â‰  0, then ğ‘‰ has a square root (see 8.41). 368 Chapter 9 Multilinear Algebra and Determinants 12 Suppose ğ‘†, ğ‘‡ âˆˆ â„’(ğ‘‰) and ğ‘† is invertible. Defineğ‘âˆ¶ ğ… â†’ ğ… by ğ‘(ğ‘§) = det(ğ‘§ğ‘† âˆ’ ğ‘‡). Prove that ğ‘ is a polynomial of degree dim ğ‘‰ and that the coefficient of ğ‘§dim ğ‘‰ in this polynomial is det ğ‘†. 13 Suppose ğ… = ğ‚, ğ‘‡ âˆˆ â„’(ğ‘‰), and ğ‘› = dim ğ‘‰ > 2. Let ğœ†1, â€¦, ğœ†ğ‘› denote the eigenvalues of ğ‘‡, with each eigenvalue included as many times as its multiplicity. (a) Find a formula for the coefficient of ğ‘§ ğ‘› âˆ’ 2 in the characteristic polynomial of ğ‘‡ in terms of ğœ†1, â€¦, ğœ†ğ‘›. (b) Find a formula for the coefficient of ğ‘§ in the characteristic polynomial of ğ‘‡ in terms of ğœ†1, â€¦, ğœ†ğ‘›. 14 Suppose ğ‘‰ is an inner product space and ğ‘‡ is a positive operator on ğ‘‰. Prove that det âˆšğ‘‡ = âˆšdet ğ‘‡. 15 Suppose ğ‘‰ is an inner product space and ğ‘‡ âˆˆ â„’(ğ‘‰). Use the polar decom- position to give a proof that |det ğ‘‡| = âˆšdet(ğ‘‡âˆ—ğ‘‡) that is different from the proof given earlier (see 9.60). 16 Suppose ğ‘‡ âˆˆ â„’(ğ‘‰). Defineğ‘”âˆ¶ ğ… â†’ ğ… by ğ‘”(ğ‘¥) = det(ğ¼ + ğ‘¥ğ‘‡). Show that ğ‘”â€™(0) =tr ğ‘‡. Look for a clean solution to this exercise, without using the explicit but complicated formula for the determinant of a matrix. 17 Suppose ğ‘, ğ‘, ğ‘ are positive numbers. Find the volume of the ellipsoid {(ğ‘¥, ğ‘¦, ğ‘§) âˆˆ ğ‘3 âˆ¶ ğ‘¥2 ğ‘2 + ğ‘¦2 ğ‘2 + ğ‘§ 2 ğ‘2 < 1} by finding a setÎ© âŠ† ğ‘ 3 whose volume you know and an operator ğ‘‡ on ğ‘3 such that ğ‘‡(Î©) equals the ellipsoid above. 18 Suppose that ğ´ is an invertible square matrix. Prove that Hadamardâ€™s inequality (9.66) is an equality if and only if each column of ğ´ is orthogonal to the other columns. 19 Suppose ğ‘‰ is an inner product space, ğ‘’1, â€¦, ğ‘’ğ‘› is an orthonormal basis of ğ‘‰, and ğ‘‡ âˆˆ â„’(ğ‘‰) is a positive operator. (a) Prove that det ğ‘‡ â‰¤âˆğ‘› ğ‘˜ = 1âŸ¨ğ‘‡ğ‘’ğ‘˜, ğ‘’ğ‘˜âŸ©. (b) Prove that if ğ‘‡ is invertible, then the inequality in (a) is an equality if and only if ğ‘’ğ‘˜ is an eigenvector of ğ‘‡ for each ğ‘˜ = 1, â€¦, ğ‘›. Section 9C Determinants 369 20 Suppose ğ´ is an ğ‘›-by-ğ‘› matrix, and suppose ğ‘ is such that |ğ´ğ‘—, ğ‘˜| â‰¤ ğ‘for all ğ‘—, ğ‘˜ âˆˆ {1, â€¦, ğ‘›}. Prove that |det ğ´| â‰¤ ğ‘ ğ‘›ğ‘› ğ‘›/2. The formula for the determinant of a matrix (9.46) shows that |det ğ´| â‰¤ ğ‘ ğ‘›ğ‘›!. However, the estimate given by this exercise is much better. For example, if ğ‘ = 1and ğ‘› = 100, then ğ‘ğ‘›ğ‘›! â‰ˆ 10 158, but the estimate given by this exercise is the much smaller number 10 100. If ğ‘› is an integer power of 2, then the inequality above is sharp and cannot be improved. 21 Suppose ğ‘› is a positive integer and ğ›¿âˆ¶ ğ‚ ğ‘›, ğ‘› â†’ ğ‚ is a function such that ğ›¿(ğ´ğµ) = ğ›¿(ğ´) â‹… ğ›¿(ğµ) for all ğ´, ğµ âˆˆ ğ‚ğ‘›, ğ‘› and ğ›¿(ğ´) equals the product of the diagonal entries of ğ´ for each diagonal matrix ğ´ âˆˆ ğ‚ğ‘›, ğ‘›. Prove that ğ›¿(ğ´) = det ğ´ for all ğ´ âˆˆ ğ‚ğ‘›, ğ‘›. Recall that ğ‚ ğ‘›, ğ‘› denotes set of ğ‘›-by-ğ‘› matrices with entries in ğ‚. This exercise shows that the determinant is the unique function defined on square matrices that is multiplicative and has the desired behavior on diagonal matrices. This result is analogous to Exercise 10 in Section 8D, which shows that the trace is uniquely determined by its algebraic properties. I find that in my own elementary lectures, I have, for pedagogical reasons, pushed determinants more and more into the background. Too often I have had the expe- rience that, while the students acquired facility with the formulas, which are so useful in abbreviating long expressions, they often failed to gain familiarity with their meaning, and skill in manipulation prevented the student from going into all the details of the subject and so gaining a mastery. â€”Elementary Mathematics from an Advanced Standpoint: Geometry, Felix Klein 370 Chapter 9 Multilinear Algebra and Determinants 9D Tensor Products Tensor Product of Two Vector Spaces The motivation for our next topic comes from wanting to form the product of a vector ğ‘£ âˆˆ ğ‘‰ and a vector ğ‘¤ âˆˆ ğ‘Š. This product will be denoted by ğ‘£ âŠ— ğ‘¤, pronounced â€œğ‘£ tensor ğ‘¤â€, and will be an element of some new vector space called ğ‘‰ âŠ— ğ‘Š (also pronounced â€œğ‘‰ tensor ğ‘Š â€). We already have a vector space ğ‘‰ Ã— ğ‘Š (see Section 3E), called the product of ğ‘‰ and ğ‘Š. However, ğ‘‰ Ã— ğ‘Š will not serve our purposes here because it does not provide a natural way to multiply an element of ğ‘‰ by an element of ğ‘Š. We would like our tensor product to satisfy some of the usual properties of multiplication. For example, we would like the distributive property to be satisfied, meaning that if ğ‘£1, ğ‘£2, ğ‘£ âˆˆ ğ‘‰ and ğ‘¤1, ğ‘¤2, ğ‘¤ âˆˆ ğ‘Š, then (ğ‘£1 + ğ‘£2) âŠ— ğ‘¤ = ğ‘£1 âŠ— ğ‘¤ + ğ‘£2 âŠ— ğ‘¤ and ğ‘£ âŠ— (ğ‘¤1 + ğ‘¤2) = ğ‘£ âŠ— ğ‘¤1 + ğ‘£ âŠ— ğ‘¤2. To produce âŠ— in TEX, type \\otimes.We would also like scalar multiplica- tion to interact well with this new multi- plication, meaning that ğœ†(ğ‘£ âŠ— ğ‘¤) = (ğœ†ğ‘£) âŠ— ğ‘¤ = ğ‘£ âŠ— (ğœ†ğ‘¤) for all ğœ† âˆˆ ğ…, ğ‘£ âˆˆ ğ‘‰, and ğ‘¤ âˆˆ ğ‘Š. Furthermore, it would be nice if each basis of ğ‘‰ when combined with each basis of ğ‘Š produced a basis of ğ‘‰ âŠ— ğ‘Š. Specifically, ifğ‘’1, â€¦, ğ‘’ğ‘š is a basis of ğ‘‰ and ğ‘“1, â€¦, ğ‘“ğ‘› is a basis of ğ‘Š, then we would like a list (in any order) consisting of ğ‘’ğ‘— âŠ— ğ‘“ğ‘˜, as ğ‘— ranges from 1to ğ‘š and ğ‘˜ ranges from 1to ğ‘›, to be a basis of ğ‘‰âŠ— ğ‘Š. This implies that dim(ğ‘‰âŠ— ğ‘Š) should equal (dim ğ‘‰)(dim ğ‘Š). Recall that dim(ğ‘‰ Ã— ğ‘Š) = dim ğ‘‰ + dim ğ‘Š (see 3.92), which shows that the product ğ‘‰ Ã— ğ‘Š will not serve our purposes here. To produce a vector space whose dimension is (dim ğ‘‰)(dim ğ‘Š) in a natural fashion from ğ‘‰ and ğ‘Š, we look at the vector space of bilinear functionals, as defined below. 9.68 definition:bilinear functional on ğ‘‰ Ã— ğ‘Š, the vector space â„¬(ğ‘‰, ğ‘Š) â€¢ A bilinear functional on ğ‘‰ Ã— ğ‘Š is a function ğ›½âˆ¶ ğ‘‰ Ã— ğ‘Š â†’ ğ… such that ğ‘£ â†¦ ğ›½(ğ‘£, ğ‘¤) is a linear functional on ğ‘‰ for each ğ‘¤ âˆˆ ğ‘Š and ğ‘¤ â†¦ ğ›½(ğ‘£, ğ‘¤) is a linear functional on ğ‘Š for each ğ‘£ âˆˆ ğ‘‰. â€¢ The vector space of bilinear functionals on ğ‘‰ Ã— ğ‘Š is denoted by â„¬(ğ‘‰, ğ‘Š). If ğ‘Š = ğ‘‰, then a bilinear functional on ğ‘‰ Ã— ğ‘Š is a bilinear form; see 9.1. The operations of addition and scalar multiplication on â„¬(ğ‘‰, ğ‘Š) are defined to be the usual operations of addition and scalar multiplication of functions. As you can verify, these operations make â„¬(ğ‘‰, ğ‘Š) into a vector space whose additive identity is the zero function from ğ‘‰ Ã— ğ‘Š to ğ…. Section 9D Tensor Products 371 9.69 example:bilinear functionals â€¢ Suppose ğœ‘ âˆˆ ğ‘‰â€² and ğœ âˆˆ ğ‘Šâ€². Defineğ›½âˆ¶ ğ‘‰ Ã— ğ‘Š â†’ ğ… by ğ›½(ğ‘£, ğ‘¤) = ğœ‘(ğ‘£)ğœ(ğ‘¤). Then ğ›½ is a bilinear functional on ğ‘‰ Ã— ğ‘Š. â€¢ Suppose ğ‘£ âˆˆ ğ‘‰ and ğ‘¤ âˆˆ ğ‘Š. Defineğ›½âˆ¶ ğ‘‰â€² Ã— ğ‘Šâ€² â†’ ğ… by ğ›½(ğœ‘, ğœ) = ğœ‘(ğ‘£)ğœ(ğ‘¤). Then ğ›½ is a bilinear functional on ğ‘‰â€² Ã— ğ‘Šâ€². â€¢ Defineğ›½âˆ¶ ğ‘‰ Ã— ğ‘‰â€² â†’ ğ… by ğ›½(ğ‘£, ğœ‘) = ğœ‘(ğ‘£). Then ğ›½ is a bilinear functional on ğ‘‰ Ã— ğ‘‰â€². â€¢ Suppose ğœ‘ âˆˆ ğ‘‰â€². Defineğ›½âˆ¶ ğ‘‰ Ã— â„’(ğ‘‰) â†’ ğ… by ğ›½(ğ‘£, ğ‘‡) = ğœ‘(ğ‘‡ğ‘£). Then ğ›½ is a bilinear functional on ğ‘‰ Ã— â„’(ğ‘‰). â€¢ Suppose ğ‘š and ğ‘› are positive integers. Defineğ›½âˆ¶ ğ…ğ‘š, ğ‘›Ã—ğ…ğ‘›, ğ‘š â†’ ğ… by ğ›½(ğ´, ğµ) = tr(ğ´ğµ). Then ğ›½ is a bilinear functional on ğ…ğ‘š, ğ‘› Ã— ğ…ğ‘›, ğ‘š. 9.70 dimension of the vector space of bilinear functionals dim â„¬(ğ‘‰, ğ‘Š) = (dim ğ‘‰)(dim ğ‘Š). Proof Let ğ‘’1, â€¦, ğ‘’ğ‘š be a basis of ğ‘‰ and ğ‘“1, â€¦, ğ‘“ğ‘› be a basis of ğ‘Š. For a bilinear functional ğ›½ âˆˆ â„¬(ğ‘‰, ğ‘Š), let â„³(ğ›½) be the ğ‘š-by-ğ‘› matrix whose entry in row ğ‘—, column ğ‘˜ is ğ›½(ğ‘’ğ‘—, ğ‘“ğ‘˜). The map ğ›½ â†¦ â„³(ğ›½) is a linear map of â„¬(ğ‘‰, ğ‘Š) into ğ…ğ‘š, ğ‘›. For a matrix ğ¶ âˆˆ ğ…ğ‘š, ğ‘›, define a bilinear functionalğ›½ğ¶ on ğ‘‰ Ã— ğ‘Š by ğ›½ğ¶(ğ‘1ğ‘’1 + â‹¯ + ğ‘ğ‘šğ‘’ğ‘š, ğ‘1 ğ‘“1 + â‹¯ + ğ‘ğ‘› ğ‘“ğ‘›) = ğ‘› âˆ‘ ğ‘˜ = 1 ğ‘š âˆ‘ ğ‘— = 1 ğ¶ğ‘—, ğ‘˜ğ‘ğ‘—ğ‘ğ‘˜ for ğ‘1, â€¦, ğ‘ğ‘š, ğ‘1, â€¦, ğ‘ğ‘› âˆˆ ğ…. The linear map ğ›½ â†¦ â„³(ğ›½) from â„¬(ğ‘‰, ğ‘Š) to ğ…ğ‘š, ğ‘› and the linear map ğ¶ â†¦ ğ›½ğ¶ from ğ…ğ‘š, ğ‘› to â„¬(ğ‘‰, ğ‘Š) are inverses of each other because ğ›½â„³(ğ›½)= ğ›½ for all ğ›½ âˆˆ â„¬(ğ‘‰, ğ‘Š) and â„³(ğ›½ğ¶) = ğ¶ for all ğ¶ âˆˆ ğ…ğ‘š, ğ‘›, as you should verify. Thus both maps are isomorphisms and the two spaces that they connect have the same dimension. Hence dim â„¬(ğ‘‰, ğ‘Š) = dim ğ…ğ‘š, ğ‘› = ğ‘šğ‘› = (dim ğ‘‰)(dim ğ‘Š). Several different definitions ofğ‘‰ âŠ— ğ‘Š appear in the mathematical literature. These definitions are equivalent to each other, at least in the finite-dimensional context, because any two vector spaces of the same dimension are isomorphic. The result above states that â„¬(ğ‘‰, ğ‘Š) has the dimension that we seek, as do â„’(ğ‘‰, ğ‘Š) and ğ…dim ğ‘‰,dim ğ‘Š. Thus it may be tempting to defineğ‘‰âŠ— ğ‘Š to be â„¬(ğ‘‰, ğ‘Š) or â„’(ğ‘‰, ğ‘Š) or ğ…dim ğ‘‰,dim ğ‘Š. However, none of those definitions would lead to a basis-free definition ofğ‘£ âŠ— ğ‘¤ for ğ‘£ âˆˆ ğ‘‰ and ğ‘¤ âˆˆ ğ‘Š. The following definition, while it may seem a bit strange and abstract at first, has the huge advantage that it definesğ‘£ âŠ— ğ‘¤ in a basis-free fashion. We define ğ‘‰ âŠ— ğ‘Š to be the vector space of bilinear functionals on ğ‘‰â€² Ã— ğ‘Šâ€² instead of the more tempting choice of the vector space of bilinear functionals on ğ‘‰ Ã— ğ‘Š. 372 Chapter 9 Multilinear Algebra and Determinants 9.71 definition:tensor product, ğ‘‰ âŠ— ğ‘Š, ğ‘£ âŠ— ğ‘¤ â€¢ The tensor product ğ‘‰ âŠ— ğ‘Š is defined to beâ„¬(ğ‘‰â€², ğ‘Šâ€²). â€¢ For ğ‘£ âˆˆ ğ‘‰ and ğ‘¤ âˆˆ ğ‘Š, the tensor product ğ‘£ âŠ— ğ‘¤ is the element of ğ‘‰ âŠ— ğ‘Š defined by (ğ‘£ âŠ— ğ‘¤)(ğœ‘, ğœ) = ğœ‘(ğ‘£)ğœ(ğ‘¤) for all (ğœ‘, ğœ) âˆˆ ğ‘‰â€² Ã— ğ‘Šâ€². We can quickly prove that the definition ofğ‘‰âŠ—ğ‘Š gives it the desired dimension. 9.72 dimension of the tensor product of two vector spaces dim(ğ‘‰ âŠ— ğ‘Š) = (dim ğ‘‰)(dim ğ‘Š). Proof Because a vector space and its dual have the same dimension (by 3.111), we have dim ğ‘‰â€² = dim ğ‘‰ and dim ğ‘Šâ€² = dim ğ‘Š. Thus 9.70 tells us that the dimension of â„¬(ğ‘‰â€², ğ‘Šâ€²)equals (dim ğ‘‰)(dim ğ‘Š). To understand the definition of the tensor productğ‘£ âŠ— ğ‘¤ of two vectors ğ‘£ âˆˆ ğ‘‰ and ğ‘¤ âˆˆ ğ‘Š, focus on the kind of object it is. An element of ğ‘‰ âŠ— ğ‘Š is a bilinear functional on ğ‘‰â€²Ã— ğ‘Šâ€², and in particular it is a function from ğ‘‰â€²Ã— ğ‘Šâ€² to ğ…. Thus for each element of ğ‘‰â€² Ã— ğ‘Šâ€², it should produce an element of ğ…. The definition above has this behavior, because ğ‘£ âŠ— ğ‘¤ applied to a typical element (ğœ‘, ğœ) of ğ‘‰â€² Ã— ğ‘Šâ€² produces the number ğœ‘(ğ‘£)ğœ(ğ‘¤). The somewhat abstract nature of ğ‘£ âŠ— ğ‘¤ should not matter. The important point is the behavior of these objects. The next result shows that tensor products of vectors have the desired bilinearity properties. 9.73 bilinearity of tensor product Suppose ğ‘£, ğ‘£1, ğ‘£2 âˆˆ ğ‘‰ and ğ‘¤, ğ‘¤1, ğ‘¤2 âˆˆ ğ‘Š and ğœ† âˆˆ ğ…. Then (ğ‘£1 + ğ‘£2) âŠ— ğ‘¤ = ğ‘£1 âŠ— ğ‘¤ + ğ‘£2 âŠ— ğ‘¤ and ğ‘£ âŠ— (ğ‘¤1 + ğ‘¤2) = ğ‘£ âŠ— ğ‘¤1 + ğ‘£ âŠ— ğ‘¤2 and ğœ†(ğ‘£ âŠ— ğ‘¤) = (ğœ†ğ‘£) âŠ— ğ‘¤ = ğ‘£ âŠ— (ğœ†ğ‘¤). Proof Suppose (ğœ‘, ğœ) âˆˆ ğ‘‰â€² Ã— ğ‘Šâ€². Then ((ğ‘£1 + ğ‘£2) âŠ— ğ‘¤)(ğœ‘, ğœ) = ğœ‘(ğ‘£1 + ğ‘£2)ğœ(ğ‘¤) = ğœ‘(ğ‘£1)ğœ(ğ‘¤) + ğœ‘(ğ‘£2)ğœ(ğ‘¤) = (ğ‘£1 âŠ— ğ‘¤)(ğœ‘, ğœ) + (ğ‘£2 âŠ— ğ‘¤)(ğœ‘, ğœ) = (ğ‘£1 âŠ— ğ‘¤ + ğ‘£2 âŠ— ğ‘¤)(ğœ‘, ğœ). Thus (ğ‘£1 + ğ‘£2) âŠ— ğ‘¤ = ğ‘£1 âŠ— ğ‘¤ + ğ‘£2 âŠ— ğ‘¤. The other two equalities are proved similarly. Section 9D Tensor Products 373 Lists are, by definition, ordered. The order matters when, for example, we form the matrix of an operator with respect to a basis. For lists in this section with two indices, such as {ğ‘’ğ‘— âŠ— ğ‘“ğ‘˜}ğ‘— = 1, â€¦, ğ‘š; ğ‘˜ = 1, â€¦, ğ‘› in the next result, the ordering does not matter and we do not specify itâ€”just choose any convenient ordering. The linear independence of elements of ğ‘‰ âŠ— ğ‘Š in (a) of the result below captures the idea that there are no relationships among vectors in ğ‘‰ âŠ— ğ‘Š other than the relationships that come from bilinearity of the tensor product (see 9.73) and the relationships that may be present due to linear dependence of a list of vectors in ğ‘‰ or a list of vectors in ğ‘Š. 9.74 basis of ğ‘‰ âŠ— ğ‘Š Suppose ğ‘’1, â€¦, ğ‘’ğ‘š is a list of vectors in ğ‘‰ and ğ‘“1, â€¦, ğ‘“ğ‘› is a list of vectors in ğ‘Š. (a) If ğ‘’1, â€¦, ğ‘’ğ‘š and ğ‘“1, â€¦, ğ‘“ğ‘› are both linearly independent lists, then {ğ‘’ğ‘— âŠ— ğ‘“ğ‘˜}ğ‘— = 1, â€¦, ğ‘š; ğ‘˜ = 1, â€¦, ğ‘› is a linearly independent list in ğ‘‰ âŠ— ğ‘Š. (b) If ğ‘’1, â€¦, ğ‘’ğ‘š is a basis of ğ‘‰ and ğ‘“1, â€¦, ğ‘“ğ‘› is a basis of ğ‘Š, then the list {ğ‘’ğ‘— âŠ— ğ‘“ğ‘˜}ğ‘— = 1, â€¦, ğ‘š; ğ‘˜ = 1, â€¦, ğ‘› is a basis of ğ‘‰ âŠ— ğ‘Š. Proof To prove (a), suppose ğ‘’1, â€¦, ğ‘’ğ‘š and ğ‘“1, â€¦, ğ‘“ğ‘› are both linearly independent lists. This linear independence and the linear map lemma (3.4) imply that there exist ğœ‘1, â€¦, ğœ‘ğ‘š âˆˆ ğ‘‰â€² and ğœ1, â€¦, ğœğ‘› âˆˆ ğ‘Šâ€² such that ğœ‘ğ‘—(ğ‘’ğ‘˜) = â§{ â¨{â© 1 if ğ‘— = ğ‘˜, 0 if ğ‘— â‰  ğ‘˜ and ğœğ‘—( ğ‘“ğ‘˜) = â§{ â¨{â© 1 if ğ‘— = ğ‘˜, 0 if ğ‘— â‰  ğ‘˜, where ğ‘—, ğ‘˜ âˆˆ {1, â€¦, ğ‘š} in the first equation andğ‘—, ğ‘˜ âˆˆ {1, â€¦, ğ‘›} in the second equation. Suppose {ğ‘ğ‘—, ğ‘˜}ğ‘— = 1, â€¦, ğ‘š; ğ‘˜ = 1, â€¦, ğ‘› is a list of scalars such that 9.75 ğ‘› âˆ‘ ğ‘˜ = 1 ğ‘š âˆ‘ ğ‘— = 1 ğ‘ğ‘—, ğ‘˜(ğ‘’ğ‘— âŠ— ğ‘“ğ‘˜) = 0. Note that (ğ‘’ğ‘— âŠ— ğ‘“ğ‘˜)(ğœ‘ğ‘€, ğœğ‘) equals 1if ğ‘— = ğ‘€ and ğ‘˜ = ğ‘, and equals 0otherwise. Thus applying both sides of 9.75 to (ğœ‘ğ‘€, ğœğ‘) shows that ğ‘ğ‘€, ğ‘ = 0, proving that {ğ‘’ğ‘— âŠ— ğ‘“ğ‘˜}ğ‘— = 1, â€¦, ğ‘š; ğ‘˜ = 1, â€¦, ğ‘› is linearly independent. Now (b) follows from (a), the equation dim ğ‘‰ âŠ— ğ‘Š = (dim ğ‘‰)(dim ğ‘Š) [see 9.72], and the result that a linearly independent list of the right length is a basis (see 2.38). Every element of ğ‘‰ Ã— ğ‘Š is a finite sum of elements of the formğ‘£ âŠ— ğ‘¤, where ğ‘£ âˆˆ ğ‘‰ and ğ‘¤ âˆˆ ğ‘Š, as implied by (b) in the result above. However, if dim ğ‘‰ > 1 and dim ğ‘Š > 1, then Exercise 4 shows that {ğ‘£ âŠ— ğ‘¤ âˆ¶ (ğ‘£, ğ‘¤) âˆˆ ğ‘‰ Ã— ğ‘Š}â‰  ğ‘‰ âŠ— ğ‘Š. 374 Chapter 9 Multilinear Algebra and Determinants 9.76 example:tensor product of element of ğ…ğ‘š with element of ğ…ğ‘› Suppose ğ‘š and ğ‘› are positive integers. Let ğ‘’1, â€¦, ğ‘’ğ‘š denote the standard basis of ğ…ğ‘š and let ğ‘“1, â€¦, ğ‘“ğ‘› denote the standard basis of ğ…ğ‘›. Suppose ğ‘£ = (ğ‘£1, â€¦, ğ‘£ğ‘š) âˆˆ ğ…ğ‘š and ğ‘¤ = (ğ‘¤1, â€¦, ğ‘¤ğ‘›) âˆˆ ğ…ğ‘›. Then ğ‘£ âŠ— ğ‘¤ = (ğ‘š âˆ‘ ğ‘— = 1 ğ‘£ğ‘—ğ‘’ğ‘—)âŠ— ( ğ‘› âˆ‘ ğ‘˜ = 1 ğ‘¤ğ‘˜ ğ‘“ğ‘˜) = ğ‘› âˆ‘ ğ‘˜ = 1 ğ‘š âˆ‘ ğ‘— = 1(ğ‘£ğ‘—ğ‘¤ğ‘˜)(ğ‘’ğ‘— âŠ— ğ‘“ğ‘˜). Thus with respect to the basis {ğ‘’ğ‘— âŠ— ğ‘“ğ‘˜}ğ‘— = 1, â€¦, ğ‘š; ğ‘˜ = 1, â€¦, ğ‘› of ğ…ğ‘š âŠ— ğ…ğ‘› provided by 9.74(b), the coefficients of ğ‘£ âŠ— ğ‘¤ are the numbers {ğ‘£ğ‘—ğ‘¤ğ‘˜}ğ‘— = 1, â€¦, ğ‘š; ğ‘˜ = 1, â€¦, ğ‘›. If instead of writing these numbers in a list, we write them in an ğ‘š-by-ğ‘› matrix with ğ‘£ğ‘—ğ‘¤ğ‘˜ in row ğ‘—, column ğ‘˜, then we can identify ğ‘£ âŠ— ğ‘¤ with the ğ‘š-by-ğ‘› matrix â›âœâœâœâœâœâœ â ğ‘£1ğ‘¤1 â‹¯ ğ‘£1ğ‘¤ğ‘› â‹± ğ‘£ğ‘šğ‘¤1 â‹¯ ğ‘£ğ‘šğ‘¤ğ‘› ââŸâŸâŸâŸâŸâŸ â  . See Exercises 5 and 6 for practice in using the identification from the example above. We now define bilinear maps, which differ from bilinear functionals in that the target space can be an arbitrary vector space rather than just the scalar field. 9.77 definition:bilinear map A bilinear map from ğ‘‰ Ã— ğ‘Š to a vector space ğ‘ˆ is a function Î“âˆ¶ ğ‘‰ Ã— ğ‘Š â†’ ğ‘ˆ such that ğ‘£ â†¦ Î“(ğ‘£, ğ‘¤) is a linear map from ğ‘‰ to ğ‘ˆ for each ğ‘¤ âˆˆ ğ‘Š and ğ‘¤ â†¦ Î“(ğ‘£, ğ‘¤) is a linear map from ğ‘Š to ğ‘ˆ for each ğ‘£ âˆˆ ğ‘‰. 9.78 example:bilinear maps â€¢ Every bilinear functional on ğ‘‰ Ã— ğ‘Š is a bilinear map from ğ‘‰ Ã— ğ‘Š to ğ…. â€¢ The function Î“âˆ¶ ğ‘‰Ã— ğ‘Š â†’ ğ‘‰âŠ— ğ‘Š defined byÎ“(ğ‘£, ğ‘¤) = ğ‘£ âŠ— ğ‘¤ is a bilinear map from ğ‘‰ Ã— ğ‘Š to ğ‘‰ âŠ— ğ‘Š (by 9.73). â€¢ The function Î“âˆ¶ â„’(ğ‘‰) Ã— â„’(ğ‘‰) â†’ â„’(ğ‘‰) defined byÎ“(ğ‘†, ğ‘‡) = ğ‘†ğ‘‡ is a bilinear map from â„’(ğ‘‰) Ã— â„’(ğ‘‰) to â„’(ğ‘‰). â€¢ The function Î“âˆ¶ ğ‘‰ Ã— â„’(ğ‘‰, ğ‘Š) â†’ ğ‘Š defined byÎ“(ğ‘£, ğ‘‡) = ğ‘‡ğ‘£ is a bilinear map from ğ‘‰ Ã— â„’(ğ‘‰, ğ‘Š) to ğ‘Š. Section 9D Tensor Products 375 Tensor products allow us to convert bilinear maps on ğ‘‰Ã— ğ‘Š into linear maps on ğ‘‰âŠ— ğ‘Š (and vice versa), as shown by the next result. In the mathematical literature, (a) of the result below is called the â€œuniversal propertyâ€ of tensor products. 9.79 converting bilinear maps to linear maps Suppose ğ‘ˆ is a vector space. (a) Suppose Î“âˆ¶ ğ‘‰ Ã— ğ‘Š â†’ ğ‘ˆ is a bilinear map. Then there exists a unique linear map Ì‚Î“âˆ¶ ğ‘‰ âŠ— ğ‘Š â†’ ğ‘ˆ such that Ì‚Î“(ğ‘£ âŠ— ğ‘¤) = Î“(ğ‘£, ğ‘¤) for all (ğ‘£, ğ‘¤) âˆˆ ğ‘‰ Ã— ğ‘Š. (b) Conversely, suppose ğ‘‡âˆ¶ ğ‘‰ âŠ— ğ‘Š â†’ ğ‘ˆ is a linear map. There there exists a unique bilinear map ğ‘‡# âˆ¶ ğ‘‰ Ã— ğ‘Š â†’ ğ‘ˆ such that ğ‘‡#(ğ‘£, ğ‘¤) = ğ‘‡(ğ‘£ âŠ— ğ‘¤) for all (ğ‘£, ğ‘¤) âˆˆ ğ‘‰ Ã— ğ‘Š. Proof Let ğ‘’1, â€¦, ğ‘’ğ‘š be a basis of ğ‘‰ and let ğ‘“1, â€¦, ğ‘“ğ‘› be a basis of ğ‘Š. By the linear map lemma (3.4) and 9.74(b), there exists a unique linear map Ì‚Î“âˆ¶ ğ‘‰ âŠ— ğ‘Š â†’ ğ‘ˆ such that Ì‚Î“(ğ‘’ğ‘— âŠ— ğ‘“ğ‘˜) = Î“(ğ‘’ğ‘—, ğ‘“ğ‘˜) for all ğ‘— âˆˆ {1, â€¦, ğ‘š} and ğ‘˜ âˆˆ {1, â€¦, ğ‘›}. Now suppose (ğ‘£, ğ‘¤) âˆˆ ğ‘‰ Ã— ğ‘Š. There exist ğ‘1, â€¦, ğ‘ğ‘š, ğ‘1, â€¦, ğ‘ğ‘› âˆˆ ğ… such that ğ‘£ = ğ‘1ğ‘’1 + â‹¯ + ğ‘ğ‘šğ‘’ğ‘š and ğ‘¤ = ğ‘1 ğ‘“1 + â‹¯ + ğ‘ğ‘› ğ‘“ğ‘›. Thus Ì‚Î“(ğ‘£ âŠ— ğ‘¤) = Ì‚Î“( ğ‘› âˆ‘ ğ‘˜ = 1 ğ‘š âˆ‘ ğ‘— = 1(ğ‘ğ‘—ğ‘ğ‘˜)(ğ‘’ğ‘— âŠ— ğ‘“ğ‘˜)) = ğ‘› âˆ‘ ğ‘˜ = 1 ğ‘š âˆ‘ ğ‘— = 1 ğ‘ğ‘—ğ‘ğ‘˜ Ì‚Î“(ğ‘’ğ‘— âŠ— ğ‘“ğ‘˜) = ğ‘› âˆ‘ ğ‘˜ = 1 ğ‘š âˆ‘ ğ‘— = 1 ğ‘ğ‘—ğ‘ğ‘˜Î“(ğ‘’ğ‘—, ğ‘“ğ‘˜) = Î“(ğ‘£, ğ‘¤), as desired, where the second line holds because Ì‚Î“ is linear, the third line holds by the definition of Ì‚Î“, and the fourth line holds because Î“ is bilinear. The uniqueness of the linear map Ì‚Î“ satisfying Ì‚Î“(ğ‘£ âŠ— ğ‘¤) = Î“(ğ‘£, ğ‘¤) follows from 9.74(b), completing the proof of (a). To prove (b), define a functionğ‘‡# âˆ¶ ğ‘‰Ã— ğ‘Š â†’ ğ‘ˆ by ğ‘‡#(ğ‘£, ğ‘¤) = ğ‘‡(ğ‘£ âŠ— ğ‘¤) for all (ğ‘£, ğ‘¤) âˆˆ ğ‘‰ Ã— ğ‘Š. The bilinearity of the tensor product (see 9.73) and the linearity of ğ‘‡ imply that ğ‘‡# is bilinear. Clearly the choice of ğ‘‡# that satisfies the conditions is unique. 376 Chapter 9 Multilinear Algebra and Determinants To prove 9.79(a), we could not just define Ì‚Î“(ğ‘£ âŠ— ğ‘¤) = Î“(ğ‘£, ğ‘¤) for all ğ‘£ âˆˆ ğ‘‰ and ğ‘¤ âˆˆ ğ‘Š (and then extend Ì‚Î“ linearly to all of ğ‘‰ âŠ— ğ‘Š)because elements of ğ‘‰ âŠ— ğ‘Š do not have unique representations as finite sums of elements of the form ğ‘£ âŠ— ğ‘¤. Our proof used a basis of ğ‘‰ and a basis of ğ‘Š to get around this problem. Although our construction of Ì‚Î“ in the proof of 9.79(a) depended on a basis of ğ‘‰ and a basis of ğ‘Š, the equation Ì‚Î“(ğ‘£ âŠ— ğ‘¤) = Î“(ğ‘£, ğ‘¤) that holds for all ğ‘£ âˆˆ ğ‘‰ and ğ‘¤ âˆˆ ğ‘Š shows that Ì‚Î“ does not depend on the choice of bases for ğ‘‰ and ğ‘Š. Tensor Product of Inner Product Spaces The result below features three inner productsâ€”one on ğ‘‰âŠ— ğ‘Š, one on ğ‘‰, and one on ğ‘Š, although we use the same symbol âŸ¨â‹…, â‹…âŸ©for all three inner products. 9.80 inner product on tensor product of two inner product spaces Suppose ğ‘‰ and ğ‘Š are inner product spaces. Then there is a unique inner product on ğ‘‰ âŠ— ğ‘Š such that âŸ¨ğ‘£ âŠ— ğ‘¤, ğ‘¢ âŠ— ğ‘¥âŸ© = âŸ¨ğ‘£, ğ‘¢âŸ©âŸ¨ğ‘¤, ğ‘¥âŸ© for all ğ‘£, ğ‘¢ âˆˆ ğ‘‰ and ğ‘¤, ğ‘¥ âˆˆ ğ‘Š. Proof Suppose ğ‘’1, â€¦, ğ‘’ğ‘š is an orthonormal basis of ğ‘‰ and ğ‘“1, â€¦, ğ‘“ğ‘› is an ortho- normal basis of ğ‘Š. Define an inner product onğ‘‰ âŠ— ğ‘Š by 9.81 âŸ¨ ğ‘› âˆ‘ ğ‘˜ = 1 ğ‘š âˆ‘ ğ‘— = 1 ğ‘ğ‘—, ğ‘˜ ğ‘’ğ‘— âŠ— ğ‘“ğ‘˜, ğ‘› âˆ‘ ğ‘˜ = 1 ğ‘š âˆ‘ ğ‘— = 1 ğ‘ğ‘—, ğ‘˜ ğ‘’ğ‘— âŠ— ğ‘“ğ‘˜âŸ©= ğ‘› âˆ‘ ğ‘˜ = 1 ğ‘š âˆ‘ ğ‘— = 1 ğ‘ğ‘—, ğ‘˜ğ‘ğ‘—, ğ‘˜. The straightforward verification that9.81 defines an inner product onğ‘‰ Ã— ğ‘Š is left to the reader [use 9.74(b)]. Suppose that ğ‘£, ğ‘¢ âˆˆ ğ‘‰ and ğ‘¤, ğ‘¥ âˆˆ ğ‘Š. Let ğ‘£1, â€¦, ğ‘£ğ‘š âˆˆ ğ… be such that ğ‘£ = ğ‘£1ğ‘’1 + â‹¯ + ğ‘£ğ‘šğ‘’ğ‘š, with similar expressions for ğ‘¢, ğ‘¤, and ğ‘¥. Then âŸ¨ğ‘£ âŠ— ğ‘¤, ğ‘¢ âŠ— ğ‘¥âŸ© =âŸ¨ ğ‘š âˆ‘ ğ‘— = 1 ğ‘£ğ‘—ğ‘’ğ‘— âŠ— ğ‘› âˆ‘ ğ‘˜ = 1 ğ‘¤ğ‘˜ ğ‘“ğ‘˜, ğ‘š âˆ‘ ğ‘— = 1 ğ‘¢ğ‘—ğ‘’ğ‘— âŠ— ğ‘› âˆ‘ ğ‘˜ = 1 ğ‘¥ğ‘˜ ğ‘“ğ‘˜âŸ© = âŸ¨ ğ‘› âˆ‘ ğ‘˜ = 1 ğ‘š âˆ‘ ğ‘— = 1 ğ‘£ğ‘—ğ‘¤ğ‘˜ ğ‘’ğ‘— âŠ— ğ‘“ğ‘˜, ğ‘› âˆ‘ ğ‘˜ = 1 ğ‘š âˆ‘ ğ‘— = 1 ğ‘¢ğ‘—ğ‘¥ğ‘˜ ğ‘’ğ‘— âŠ— ğ‘“ğ‘˜âŸ© = ğ‘› âˆ‘ ğ‘˜ = 1 ğ‘š âˆ‘ ğ‘— = 1 ğ‘£ğ‘—ğ‘¢ğ‘—ğ‘¤ğ‘˜ğ‘¥ğ‘˜ = ( ğ‘š âˆ‘ ğ‘— = 1 ğ‘£ğ‘—ğ‘¢ğ‘—)( ğ‘› âˆ‘ ğ‘˜ = 1 ğ‘¤ğ‘˜ğ‘¥ğ‘˜) = âŸ¨ğ‘£, ğ‘¢âŸ©âŸ¨ğ‘¤, ğ‘¥âŸ©. There is only one inner product on ğ‘‰âŠ—ğ‘Š such that âŸ¨ğ‘£âŠ—ğ‘¤, ğ‘¢âŠ—ğ‘¥âŸ© = âŸ¨ğ‘£, ğ‘¢âŸ©âŸ¨ğ‘¤, ğ‘¥âŸ© for all ğ‘£, ğ‘¢ âˆˆ ğ‘‰ and ğ‘¤, ğ‘¥ âˆˆ ğ‘Š because every element of ğ‘‰ âŠ— ğ‘Š can be written as a linear combination of elements of the form ğ‘£ âŠ— ğ‘¤ [by 9.74(b)]. Section 9D Tensor Products 377 The definition below of a natural inner product onğ‘‰ âŠ— ğ‘Š is now justified by 9.80. We could not have simply definedâŸ¨ğ‘£ âŠ— ğ‘¤, ğ‘¢ âŠ— ğ‘¥âŸ©to be âŸ¨ğ‘£, ğ‘¢âŸ©âŸ¨ğ‘¤, ğ‘¥âŸ©(and then used additivity in each slot separately to extend the definition toğ‘‰ âŠ— ğ‘Š) without some proof because elements of ğ‘‰ âŠ— ğ‘Š do not have unique representations as finite sums of elements of the formğ‘£ âŠ— ğ‘¤. 9.82 definition: inner product on tensor product of two inner product spaces Suppose ğ‘‰ and ğ‘Š are inner product spaces. The inner product on ğ‘‰ âŠ— ğ‘Š is the unique function âŸ¨â‹…, â‹…âŸ©from (ğ‘‰ âŠ— ğ‘Š) Ã— (ğ‘‰ âŠ— ğ‘Š) to ğ… such that âŸ¨ğ‘£ âŠ— ğ‘¤, ğ‘¢ âŠ— ğ‘¥âŸ© = âŸ¨ğ‘£, ğ‘¢âŸ©âŸ¨ğ‘¤, ğ‘¥âŸ© for all ğ‘£, ğ‘¢ âˆˆ ğ‘‰ and ğ‘¤, ğ‘¥ âˆˆ ğ‘Š. Take ğ‘¢ = ğ‘£ and ğ‘¥ = ğ‘¤ in the equation above and then take square roots to show that â€–ğ‘£ âŠ— ğ‘¤â€– = â€–ğ‘£â€– â€–ğ‘¤â€– for all ğ‘£ âˆˆ ğ‘‰ and all ğ‘¤ âˆˆ ğ‘Š. The construction of the inner product in the proof of 9.80 depended on an orthonormal basis ğ‘’1, â€¦, ğ‘’ğ‘š of ğ‘‰ and an orthonormal basis ğ‘“1, â€¦, ğ‘“ğ‘› of ğ‘Š. Formula 9.81 implies that {ğ‘’ğ‘— âŠ— ğ‘“ğ‘˜}ğ‘— = 1, â€¦, ğ‘š; ğ‘˜ = 1, â€¦, ğ‘› is a doubly indexed orthonormal list in ğ‘‰âŠ— ğ‘Š and hence is an orthonormal basis of ğ‘‰âŠ— ğ‘Š [by 9.74(b)]. The importance of the next result arises because the orthonormal bases used there can be different from the orthonormal bases used to define the inner product in9.80. Although the notation for the bases is the same in the proof of 9.80 and in the result below, think of them as two different sets of orthonormal bases. 9.83 orthonormal basis of ğ‘‰ âŠ— ğ‘Š Suppose ğ‘‰ and ğ‘Š are inner product spaces, and ğ‘’1, â€¦, ğ‘’ğ‘š is an orthonormal basis of ğ‘‰ and ğ‘“1, â€¦, ğ‘“ğ‘› is an orthonormal basis of ğ‘Š. Then {ğ‘’ğ‘— âŠ— ğ‘“ğ‘˜}ğ‘— = 1, â€¦, ğ‘š; ğ‘˜ = 1, â€¦, ğ‘› is an orthonormal basis of ğ‘‰ âŠ— ğ‘Š. Proof We know that {ğ‘’ğ‘— âŠ— ğ‘“ğ‘˜}ğ‘— = 1, â€¦, ğ‘š; ğ‘˜ = 1, â€¦, ğ‘› is a basis of ğ‘‰ âŠ— ğ‘Š [by 9.74(b)]. Thus we only need to verify orthonormality. To do this, suppose ğ‘—, ğ‘€ âˆˆ {1, â€¦, ğ‘š} and ğ‘˜, ğ‘ âˆˆ {1, â€¦, ğ‘›}. Then âŸ¨ğ‘’ğ‘— âŠ— ğ‘“ğ‘˜, ğ‘’ğ‘ âŠ— ğ‘“ğ‘€âŸ© = âŸ¨ğ‘’ğ‘—, ğ‘’ğ‘âŸ©âŸ¨ ğ‘“ğ‘˜, ğ‘“ğ‘€âŸ© = â§{ â¨{â© 1 if ğ‘— = ğ‘ and ğ‘˜ = ğ‘€, 0 otherwise. Hence the doubly indexed list {ğ‘’ğ‘— âŠ— ğ‘“ğ‘˜}ğ‘— = 1, â€¦, ğ‘š; ğ‘˜ = 1, â€¦, ğ‘› is indeed an orthonormal basis of ğ‘‰ âŠ— ğ‘Š. See Exercise 11 for an example of how the inner product structure on ğ‘‰ âŠ— ğ‘Š interacts with operators on ğ‘‰ and ğ‘Š. 378 Chapter 9 Multilinear Algebra and Determinants Tensor Product of Multiple Vector Spaces We have been discussing properties of the tensor product of two finite-dimensional vector spaces. Now we turn our attention to the tensor product of multiple finite- dimensional vector spaces. This generalization requires no new ideas, only some slightly more complicated notation. Readers with a good understanding of the tensor product of two vector spaces should be able to make the extension to the tensor product of more than two vector spaces. Thus in this subsection, no proofs will be provided. The definitions and the statements of results that will be provided should be enough information to enable readers to fill in the details, using what has already been learned about the tensor product of two vector spaces. We begin with the following notational assumption. 9.84 notation: ğ‘‰1, â€¦, ğ‘‰ğ‘š For the rest of this subsection, ğ‘š denotes an integer greater than 1and ğ‘‰1, â€¦, ğ‘‰ğ‘š denote finite-dimensional vector spaces. The notion of an ğ‘š-linear functional, which we are about to define, generalizes the notion of a bilinear functional (see 9.68). Recall that the use of the word â€œfunctionalâ€ indicates that we are mapping into the scalar fieldğ…. Recall also that the terminology â€œğ‘š-linear formâ€ is used in the special caseğ‘‰1 = â‹¯ = ğ‘‰ğ‘š (see 9.25). The notation â„¬(ğ‘‰1, â€¦, ğ‘‰ğ‘š) generalizes our previous notation â„¬(ğ‘‰, ğ‘Š). 9.85 definition: ğ‘š-linear functional, the vector space â„¬(ğ‘‰1, â€¦, ğ‘‰ğ‘š) â€¢ An ğ‘š-linear functional on ğ‘‰1 Ã— â‹¯ Ã— ğ‘‰ğ‘š is a function ğ›½âˆ¶ ğ‘‰1 Ã— â‹¯ Ã— ğ‘‰ğ‘š â†’ ğ… that is a linear functional in each slot when the other slots are held fixed. â€¢ The vector space of ğ‘š-linear functionals on ğ‘‰1 Ã— â‹¯ Ã— ğ‘‰ğ‘š is denoted by â„¬(ğ‘‰1, â€¦, ğ‘‰ğ‘š). 9.86 example:ğ‘š-linear functional Suppose ğœ‘ğ‘˜ âˆˆ (ğ‘‰ğ‘˜)â€² for each ğ‘˜ âˆˆ {1, â€¦, ğ‘š}. Defineğ›½âˆ¶ ğ‘‰1 Ã— â‹¯ Ã— ğ‘‰ğ‘š â†’ ğ… by ğ›½(ğ‘£1, â€¦, ğ‘£ğ‘š) = ğœ‘1(ğ‘£1) Ã— â‹¯ Ã— ğœ‘ğ‘š(ğ‘£ğ‘š). Then ğ›½ is an ğ‘š-linear functional on ğ‘‰1 Ã— â‹¯ Ã— ğ‘‰ğ‘š. The next result can be proved by imitating the proof of 9.70. 9.87 dimension of the vector space of ğ‘š-linear functionals dim â„¬(ğ‘‰1, â€¦, ğ‘‰ğ‘š) = (dim ğ‘‰1) Ã— â‹¯ Ã— (dim ğ‘‰ğ‘š). Section 9D Tensor Products 379 Now we can define the tensor product of multiple vector spaces and the tensor product of elements of those vector spaces. The following definition is completely analogous to our previous definition (9.71) in the case ğ‘š = 2. 9.88 definition: tensor product, ğ‘‰1 âŠ— â‹¯ âŠ— ğ‘‰ğ‘š, ğ‘£1 âŠ— â‹¯ âŠ— ğ‘£ğ‘š â€¢ The tensor product ğ‘‰1 âŠ— â‹¯ âŠ— ğ‘‰ğ‘š is defined to beâ„¬(ğ‘‰1 â€², â€¦, ğ‘‰ğ‘š â€²). â€¢ For ğ‘£1 âˆˆ ğ‘‰1, â€¦, ğ‘£ğ‘š âˆˆ ğ‘‰ğ‘š, the tensor product ğ‘£1 âŠ— â‹¯ âŠ— ğ‘£ğ‘š is the element of ğ‘‰1 âŠ— â‹¯ âŠ— ğ‘‰ğ‘š defined by (ğ‘£1 âŠ— â‹¯ âŠ— ğ‘£ğ‘š)(ğœ‘1, â€¦, ğœ‘ğ‘š) = ğœ‘1(ğ‘£1)â‹¯ğœ‘ğ‘š(ğ‘£ğ‘š) for all (ğœ‘1 â€¦, ğœ‘ğ‘š) âˆˆ ğ‘‰1 â€² Ã— â‹¯ Ã— ğ‘‰ğ‘š â€². The next result can be proved by following the pattern of the proof of the analogous result when ğ‘š = 2(see 9.72). 9.89 dimension of the tensor product dim(ğ‘‰1 âŠ— â‹¯ âŠ— ğ‘‰ğ‘š) = (dim ğ‘‰1)â‹¯(dim ğ‘‰ğ‘š). Our next result generalizes 9.74. 9.90 basis of ğ‘‰1 âŠ— â‹¯ âŠ— ğ‘‰ğ‘š Suppose dim ğ‘‰ğ‘˜ = ğ‘›ğ‘˜ and ğ‘’ ğ‘˜ 1 , â€¦, ğ‘’ ğ‘˜ ğ‘›ğ‘˜ is a basis of ğ‘‰ğ‘˜ for ğ‘˜ = 1, â€¦, ğ‘š. Then {ğ‘’1 ğ‘—1 âŠ— â‹¯ âŠ— ğ‘’ğ‘š ğ‘—ğ‘š}ğ‘—1 = 1, â€¦, ğ‘›1; â‹¯; ğ‘—ğ‘š = 1, â€¦, ğ‘›ğ‘š is a basis of ğ‘‰1 âŠ— â‹¯ âŠ— ğ‘‰ğ‘š. Suppose ğ‘š = 2and ğ‘’1 1, â€¦, ğ‘’1 ğ‘›1 is a basis of ğ‘‰1 and ğ‘’2 1, â€¦, ğ‘’2 ğ‘›2 is a basis of ğ‘‰2. Then with respect to the basis {ğ‘’1 ğ‘—1 âŠ— ğ‘’2 ğ‘—2}ğ‘—1 = 1, â€¦, ğ‘›1; ğ‘—2 = 1, â€¦, ğ‘›2 in the result above, the coefficients of an element of ğ‘‰1 âŠ—ğ‘‰2 can be represented by an ğ‘›1-by-ğ‘›2 matrix that contains the coefficient of ğ‘’1 ğ‘—1 âŠ— ğ‘’2 ğ‘—2 in row ğ‘—1, column ğ‘—2. Thus we need a matrix, which is an array specified by two indices, to represent an element ofğ‘‰1 âŠ— ğ‘‰2. If ğ‘š > 2, then the result above shows that we need an array specified byğ‘š indices to represent an arbitrary element of ğ‘‰1 âŠ— â‹¯ âŠ— ğ‘‰ğ‘š. Thus tensor products may appear when we deal with objects specified by arrays with multiple indices. The next definition generalizes the notion of a bilinear map (see9.77). As with bilinear maps, the target space can be an arbitrary vector space. 9.91 definition: ğ‘š-linear map An ğ‘š-linear map from ğ‘‰1 Ã— â‹¯ Ã— ğ‘‰ğ‘š to a vector space ğ‘ˆ is a function Î“âˆ¶ ğ‘‰1 Ã— â‹¯ Ã— ğ‘‰ğ‘š â†’ ğ‘ˆ that is a linear map in each slot when the other slots are held fixed. 380 Chapter 9 Multilinear Algebra and Determinants The next result can be proved by following the pattern of the proof of 9.79. 9.92 converting ğ‘š-linear maps to linear maps Suppose ğ‘ˆ is a vector space. (a) Suppose that Î“âˆ¶ ğ‘‰1 Ã— â‹¯ Ã— ğ‘‰ğ‘š â†’ ğ‘ˆ is an ğ‘š-linear map. Then there exists a unique linear map Ì‚Î“âˆ¶ ğ‘‰1 âŠ— â‹¯ âŠ— ğ‘‰ğ‘š â†’ ğ‘ˆ such that Ì‚Î“(ğ‘£1 âŠ— â‹¯ âŠ— ğ‘£ğ‘š) = Î“(ğ‘£1, â€¦, ğ‘£ğ‘š) for all (ğ‘£1, â€¦, ğ‘£ğ‘š) âˆˆ ğ‘‰1 Ã— â‹¯ Ã— ğ‘‰ğ‘š. (b) Conversely, suppose ğ‘‡âˆ¶ ğ‘‰1 âŠ— â‹¯ âŠ— ğ‘‰ğ‘š â†’ ğ‘ˆ is a linear map. There there exists a unique ğ‘š-linear map ğ‘‡# âˆ¶ ğ‘‰1 Ã— â‹¯ Ã— ğ‘‰ğ‘š â†’ ğ‘ˆ such that ğ‘‡#(ğ‘£1, â€¦, ğ‘£ğ‘š) = ğ‘‡(ğ‘£1 âŠ— â‹¯ âŠ— ğ‘£ğ‘š) for all (ğ‘£1, â€¦, ğ‘£ğ‘š) âˆˆ ğ‘‰1 Ã— â‹¯ Ã— ğ‘‰ğ‘š. See Exercises 12 and 13 for tensor products of multiple inner product spaces. Exercises 9D 1 Suppose ğ‘£ âˆˆ ğ‘‰ and ğ‘¤ âˆˆ ğ‘Š. Prove that ğ‘£ âŠ— ğ‘¤ = 0if and only if ğ‘£ = 0or ğ‘¤ = 0. 2 Give an example of six distinct vectors ğ‘£1, ğ‘£2, ğ‘£3, ğ‘¤1, ğ‘¤2, ğ‘¤3 in ğ‘3 such that ğ‘£1 âŠ— ğ‘¤1 + ğ‘£2 âŠ— ğ‘¤2 + ğ‘£3 âŠ— ğ‘¤3 = 0 but none of ğ‘£1 âŠ— ğ‘¤1, ğ‘£2 âŠ— ğ‘¤2, ğ‘£3 âŠ— ğ‘¤3 is a scalar multiple of another element of this list. 3 Suppose that ğ‘£1, â€¦, ğ‘£ğ‘š is a linearly independent list in ğ‘‰. Suppose also that ğ‘¤1, â€¦, ğ‘¤ğ‘š is a list in ğ‘Š such that ğ‘£1 âŠ— ğ‘¤1 + â‹¯ + ğ‘£ğ‘š âŠ— ğ‘¤ğ‘š = 0. Prove that ğ‘¤1 = â‹¯ = ğ‘¤ğ‘š = 0. 4 Suppose dim ğ‘‰ > 1and dim ğ‘Š > 1. Prove that {ğ‘£ âŠ— ğ‘¤ âˆ¶ (ğ‘£, ğ‘¤) âˆˆ ğ‘‰ Ã— ğ‘Š} is not a subspace of ğ‘‰ âŠ— ğ‘Š. This exercise implies that if dim ğ‘‰ > 1and dim ğ‘Š > 1, then {ğ‘£ âŠ— ğ‘¤ âˆ¶ (ğ‘£, ğ‘¤) âˆˆ ğ‘‰ Ã— ğ‘Š}â‰  ğ‘‰ âŠ— ğ‘Š. Section 9D Tensor Products 381 5 Suppose ğ‘š and ğ‘› are positive integers. For ğ‘£ âˆˆ ğ…ğ‘š and ğ‘¤ âˆˆ ğ…ğ‘›, identify ğ‘£ âŠ— ğ‘¤ with an ğ‘š-by-ğ‘› matrix as in Example 9.76. With that identification, show that the set {ğ‘£ âŠ— ğ‘¤ âˆ¶ ğ‘£ âˆˆ ğ…ğ‘š and ğ‘¤ âˆˆ ğ…ğ‘›} is the set of ğ‘š-by-ğ‘› matrices (with entries in ğ…) that have rank at most one. 6 Suppose ğ‘š and ğ‘› are positive integers. Give a description, analogous to Exercise 5, of the set of ğ‘š-by-ğ‘› matrices (with entries in ğ…) that have rank at most two. 7 Suppose dim ğ‘‰ > 2and dim ğ‘Š > 2. Prove that {ğ‘£1 âŠ— ğ‘¤1 + ğ‘£2 âŠ— ğ‘¤2 âˆ¶ ğ‘£1, ğ‘£2 âˆˆ ğ‘‰ and ğ‘¤1, ğ‘¤2 âˆˆ ğ‘Š} â‰  ğ‘‰ âŠ— ğ‘Š. 8 Suppose ğ‘£1, â€¦, ğ‘£ğ‘š âˆˆ ğ‘‰ and ğ‘¤1, â€¦, ğ‘¤ğ‘š âˆˆ ğ‘Š are such that ğ‘£1 âŠ— ğ‘¤1 + â‹¯ + ğ‘£ğ‘š âŠ— ğ‘¤ğ‘š = 0. Suppose that ğ‘ˆ is a vector space and Î“âˆ¶ ğ‘‰Ã— ğ‘Š â†’ ğ‘ˆ is a bilinear map. Show that Î“(ğ‘£1, ğ‘¤1) + â‹¯ + Î“(ğ‘£ğ‘š, ğ‘¤ğ‘š) = 0. 9 Suppose ğ‘† âˆˆ â„’(ğ‘‰) and ğ‘‡ âˆˆ â„’(ğ‘Š). Prove that there exists a unique operator on ğ‘‰ âŠ— ğ‘Š that takes ğ‘£ âŠ— ğ‘¤ to ğ‘†ğ‘£ âŠ— ğ‘‡ğ‘¤ for all ğ‘£ âˆˆ ğ‘‰ and ğ‘¤ âˆˆ ğ‘Š. In an abuse of notation, the operator on ğ‘‰ âŠ— ğ‘Š given by this exercise is often called ğ‘† âŠ— ğ‘‡. 10 Suppose ğ‘† âˆˆ â„’(ğ‘‰) and ğ‘‡ âˆˆ â„’(ğ‘Š). Prove that ğ‘†âŠ—ğ‘‡ is an invertible operator on ğ‘‰ âŠ— ğ‘Š if and only if both ğ‘† and ğ‘‡ are invertible operators. Also, prove that if both ğ‘† and ğ‘‡ are invertible operators, then (ğ‘† âŠ— ğ‘‡)âˆ’1 = ğ‘†âˆ’1 âŠ— ğ‘‡âˆ’1, where we are using the notation from the comment after Exercise 9. 11 Suppose ğ‘‰ and ğ‘Š are inner product spaces. Prove that if ğ‘† âˆˆ â„’(ğ‘‰) and ğ‘‡ âˆˆ â„’(ğ‘Š), then (ğ‘† âŠ— ğ‘‡)âˆ— = ğ‘†âˆ— âŠ— ğ‘‡âˆ—, where we are using the notation from the comment after Exercise 9. 12 Suppose that ğ‘‰1, â€¦, ğ‘‰ğ‘š are finite-dimensional inner product spaces. Prove that there is a unique inner product on ğ‘‰1 âŠ— â‹¯ âŠ— ğ‘‰ğ‘š such that âŸ¨ğ‘£1 âŠ— â‹¯ âŠ— ğ‘£ğ‘š, ğ‘¢1 âŠ— â‹¯ âŠ— ğ‘¢ğ‘šâŸ© = âŸ¨ğ‘£1, ğ‘¢1âŸ©â‹¯âŸ¨ğ‘£ğ‘š, ğ‘¢ğ‘šâŸ© for all (ğ‘£1, â€¦, ğ‘£ğ‘š) and (ğ‘¢1, â€¦, ğ‘¢ğ‘š) in ğ‘‰1 Ã— â‹¯ Ã— ğ‘‰ğ‘š. Note that the equation above implies that â€–ğ‘£1 âŠ— â‹¯ âŠ— ğ‘£ğ‘šâ€– = â€–ğ‘£1â€– Ã— â‹¯ Ã— â€–ğ‘£ğ‘šâ€– for all (ğ‘£1, â€¦, ğ‘£ğ‘š) âˆˆ ğ‘‰1 Ã— â‹¯ Ã— ğ‘‰ğ‘š. 382 Chapter 9 Multilinear Algebra and Determinants 13 Suppose that ğ‘‰1, â€¦, ğ‘‰ğ‘š are finite-dimensional inner product spaces and ğ‘‰1 âŠ— â‹¯ âŠ— ğ‘‰ğ‘š is made into an inner product space using the inner product from Exercise 12. Suppose ğ‘’ ğ‘˜ 1 , â€¦, ğ‘’ ğ‘˜ ğ‘›ğ‘˜ is an orthonormal basis of ğ‘‰ğ‘˜ for each ğ‘˜ = 1, â€¦, ğ‘š. Show that the list {ğ‘’1 ğ‘—1 âŠ— â‹¯ âŠ— ğ‘’ğ‘š ğ‘—ğ‘š}ğ‘—1 = 1, â€¦, ğ‘›1; â‹¯; ğ‘—ğ‘š = 1, â€¦, ğ‘›ğ‘š is an orthonormal basis of ğ‘‰1 âŠ— â‹¯ âŠ— ğ‘‰ğ‘š. Photo Credits â€¢ page v: Photos by Carrie Heeter and Bishnu Sarangi. Public domain image. â€¢ page 1: Original painting by Pierre Louis Dumesnil; 1884 copy by Nils Forsberg. Public domain image downloaded on 29 March 2022 from https://commons.wikimedia.org/wiki/File:RenÃ©_Descartes_i_samtal_med_Sveriges_drottning,_Kristina.jpg. â€¢ page 27: Public domain image downloaded on 4 February 2022 from https://commons.wikimedia.org/wiki/File:IAS_Princeton.jpg. â€¢ page 51: Photo by Stefan SchÃ¤fer; Creative Commons Attribution Share Alike license. Downloaded on 28 March 2022 from https://commons.wikimedia.org/wiki/File:BurgDankwarderode2016.jpg. â€¢ page 119: Photo by Alireza Javaheri. Creative Commons Attribution license. Downloaded on 12 March 2023 from https://commons.wikimedia.org/wiki/File:Hakim_Omar_Khayam_-_panoramio.jpg. â€¢ page 132: Statue completed by Giovanni Paganucci in 1863. Photo by Hans-Peter Postel; Creative Commons Attribution license. Downloaded on 14 March 2022 from https://commons.wikimedia.org/wiki/File:Leonardo_da_Pisa.jpg. â€¢ page 181: Photo by Matthew Petroff; Creative Commons Attribution Share Alike license. Downloaded on 31 March 2022 from https://commons.wikimedia.org/wiki/File:George-peabody-library.jpg. â€¢ page 227: Photo by Petar MiloÅ¡eviÄ‡; Creative Commons Attribution Share Alike license. Downloaded on 30 March 2022 from https://en.wikipedia.org/wiki/Lviv. â€¢ page 297: Photo by David Iliff; Creative Commons Attribution Share Alike license. Downloaded on 30 March 2022 from https://en.wikipedia.org/wiki/File:Long_Room_Interior,_Trinity_College_Dublin,_Ireland_-_Diliff.jpg. â€¢ page 332: Photo by Daniel Schwen; Creative Commons Attribution Share Alike license. Downloaded on 9 July 2019 from https://commons.wikimedia.org/wiki/File:Mathematik_GÃ¶ttingen.jpg. 383 S. Axler, Linear Algebra Done Right, Undergraduate Texts in Mathematics, https://doi.org/10.1007/978-3-031-41026-0 Â© Sheldon Axler 2024 Symbol Index ğ´ âˆ’1, 91 ğ´ğ‘—, â‹… , 74 ğ´ğ‘—, ğ‘˜, 69 ğ´â‹…, ğ‘˜, 74 ğ›¼ğ‘‡, 354 ğ´âˆ—, 231 ğ´ t, 77 Ì‚Î“, 375, 380 ğµ, 287 â„¬(ğ‘‰1, â€¦, ğ‘‰ğ‘š), 378 â„¬(ğ‘‰, ğ‘Š), 370 ğ‚, 2 âˆ˜, 55 deg, 31 Î”, 196 det ğ´, 355 det ğ‘‡, 354 dim, 44 âŠ•, 21 ğ¸(ğ‘ 1 ğ‘“1, â€¦, ğ‘ ğ‘› ğ‘“ğ‘›), 287 ğ¸(ğœ†, ğ‘‡), 164 ğ…, 4 ğ…âˆ, 13 ğ…ğ‘š, ğ‘›, 72 ğ…ğ‘›, 6 ğ…ğ‘†, 13 ğº(ğœ†, ğ‘‡), 308 ğ¼, 52, 90 âŸº , 23 Im, 120 âˆ’âˆ, 31 â„’(ğ‘‰), 52 â„’(ğ‘‰, ğ‘Š), 52 â„³(ğ›½), 334 â„³(ğ‘‡), 69, 154 â„³(ğ‘£), 88 perm, 348 ğ’«(ğ…), 30 ğœ‹, 101 ğ’«ğ‘š(ğ…), 31 ğ‘(ğ‘‡), 137 ğ‘ƒğ‘ˆ, 214 ğ‘ğ›½, 341 , 7 ğ‘, 2 Re, 120 ğ‘† âŠ— ğ‘‡, 381 âŠŠ, 299 âˆšğ‘‡, 253 Ìƒğ‘‡, 102 ğ‘‡â€², 107 ğ‘‡âˆ—, 228 ğ‘‡âˆ’1, 82 ğ‘‡(Î©), 288 ğ‘‡â€ , 221 ğ‘‡ğ‘š, 137 â€–ğ‘‡â€–, 280 ğ‘‡#, 375, 380 tr ğ´, 326 tr ğ‘‡, 327 ğ‘‡|ğ‘ˆ, 133 ğ‘‡/ğ‘ˆ, 142 ğ‘ˆâŸ‚, 211 ğ‘ˆ0, 109 âŸ¨ğ‘¢, ğ‘£âŸ©, 184 ğ‘‰, 15 ğ‘‰â€², 105, 204 ğ‘‰/ğ‘ˆ, 99 âˆ’ğ‘£, 15 ğ‘‰1 âŠ— â‹¯ âŠ— ğ‘‰ğ‘š, 379 ğ‘£1 âŠ— â‹¯ âŠ— ğ‘£ğ‘š, 379 ğ‘‰(2), 334 ğ‘‰(2) alt , 339 ğ‘‰(2) sym, 337 ğ‘‰ğ‚, 17 ğ‘‰ğ‘š, 103, 346 ğ‘‰(ğ‘š), 346 ğ‘‰(ğ‘š) alt , 347 ğ‘‰ âŠ— ğ‘Š, 372 ğ‘£ âŠ— ğ‘¤, 372 ğ‘£ + ğ‘ˆ, 98 ||ğ‘£||, 186 ğ‘§, 120 |ğ‘§|, 120 384 S. Axler, Linear Algebra Done Right, Undergraduate Texts in Mathematics, https://doi.org/10.1007/978-3-031-41026-0 Â© Sheldon Axler 2024 Index Abbott, Edwin A., 6 absolute value, 120 addition in quotient space, 100 of complex numbers, 2 of functions, 13 of linear maps, 55 of matrices, 71 of subspaces, 19 of vectors, 12 of vectors in ğ…ğ‘›, 6 additive inverse in ğ‚, 3, 4 in ğ…ğ‘›, 9 in vector space, 12, 15 additivity, 52 adjoint of a linear map, 228 algebraic multiplicity, 311 alternating bilinear form, 339 alternating ğ‘š-linear form, 347 annihilator, 109 Apolloniusâ€™s identity, 195 Artin, Emil, 80 associativity, 3, 12, 56 backward shift, 53, 59, 84, 140 ball, 287 Banach, Stefan, 227 basis, 39 of eigenvectors, 165, 245, 246, 250 of generalized eigenvectors, 301 Bernstein polynomials, 49 Besselâ€™s inequality, 198 bilinear form, 333 bilinear functional, 370 bilinear map, 374 block diagonal matrix, 314 Bunyakovsky, Viktor, 189 ğ¶âˆ—-algebras, 295 Carroll, Lewis, 11 Cauchy, Augustin-Louis, 189 Cauchyâ€“Schwarz inequality, 189 Cayley, Arthur, 312 Cayleyâ€“Hamilton theorem, 364 on complex vector space, 312 change-of-basis formula for bilinear forms, 336 for operators, 93 characteristic polynomial, 363 on complex vector space, 311 ChatGPT, 196, 279 Cholesky factorization, 267 Cholesky, AndrÃ©-Louis, 267 Christina, Queen of Sweden, 1 closed under addition, 18 closed under scalar multiplication, 18 column rank of a matrix, 77, 114, 239 columnâ€“row factorization, 78 commutativity, 3, 7, 12, 25, 56, 73, 80 commuting operators, 138, 175â€“180, 209, 235, 248â€“249, 256 companion matrix, 152 complex conjugate, 120 complex number, 2 complex spectral theorem, 246 complex vector space, 13 385 S. Axler, Linear Algebra Done Right, Undergraduate Texts in Mathematics, https://doi.org/10.1007/978-3-031-41026-0 Â© Sheldon Axler 2024 386 Index complexification eigenvalues of, 140 generalized eigenvectors of, 318 minimal polynomial of, 153 multiplicity of eigenvalues, 318 of a linear map, 68 of a vector space, 17, 43 of an inner product space, 194 conjugate symmetry, 183 conjugate transpose of a matrix, 231 coordinate, 6 cube root of an operator, 248 De Moivreâ€™s theorem, 125 degree of a polynomial, 31 Descartes, RenÃ©, 1 determinant of matrix, 355 of operator, 354 of positive operator, 362 of unitary operator, 362 diagonal matrix, 163, 274 diagonal of a square matrix, 155 diagonalizable, 163, 172, 176, 245, 246, 294, 307, 316 differentiation linear map, 53, 56, 59, 61, 62, 67, 70, 79, 138, 208, 304 dimension, 44 of a sum of subspaces, 47 direct sum, 21, 42, 98 of a subspace and its orthogonal complement, 212 of null ğ‘‡dim ğ‘‰ and range ğ‘‡dim ğ‘‰, 299 discrete Fourier transform, 269 distributive property, 3, 12, 15, 56, 80 division algorithm for polynomials, 124 division of complex numbers, 4 dot product, 182 double dual space, 118 dual of a basis, 106 of a linear map, 107, 153, 162, 174 of a linear operator, 140 of a vector space, 105, 204 eigenspace, 164 eigenvalue of adjoint, 239 of dual of linear operator, 140 of operator, 134 of positive operator, 252 of self-adjoint operator, 233 of unitary operator, 262 on odd-dimensional space, 150, 318, 367 eigenvector, 135 ellipsoid, 287 Euclidean inner product, 184 Fibonacci, 132 Fibonacci sequence, 174 field,10 finite-dimensional vector space,30 Flatland, 6 forward shift, 140 Frankenstein, 50 Frobenius norm, 331 Fugledeâ€™s theorem, 248 fundamental theorem of algebra, 125 fundamental theorem of linear maps, 62 Gauss, Carl Friedrich, 51 Gaussian elimination, 51, 65, 361 generalized eigenspace, 308 generalized eigenvector, 300 geometric multiplicity, 311 Gershgorin disk, 170 Index 387 Gershgorin disk theorem, 171 Gershgorin, Semyon Aronovich, 171 Gram, JÃ¸rgen, 200 Gramâ€“Schmidt procedure, 200 graph of a linear map, 103 Hadamardâ€™s inequality, 365 Halmos, Paul, 27 Hamilton, William, 297 harmonic function, 196 Hilbert matrix, 256 Hilbertâ€“Schmidt norm, 331 homogeneity, 52 homogeneous system of linear equations, 65, 95 hyponormal operator, 241 identity matrix, 90 identity operator, 52, 56 imaginary part, 120 infinite-dimensional vector space,31 inhomogeneous system of linear equations, 65, 95 injective, 60 inner product, 183 inner product space, 184 Institute for Advanced Study, 27 invariant subspace, 133 inverse of a linear map, 82 of a matrix, 91 invertible linear map, 82 invertible matrix, 91 isometry, 258 isomorphic vector spaces, 86 isomorphism, 86 Jordan basis, 322 Jordan form, 324 Jordan, Camille, 324 kernel, 59 Khayyam, Omar, 119 Laplacian, 196 length of list, 5 Leonardo of Pisa, 132 linear combination, 28 linear dependence lemma, 33 linear equations, 64â€“65, 95 linear functional, 105, 204 linear map, 52 linear map lemma, 54 linear span, 29 linear subspace, 18 linear transformation, 52 linearly dependent, 33 linearly independent, 32 list, 5 of vectors, 28 lower-triangular matrix, 162, 267 Lviv, 227 LwÃ³w, 227 matrix, 69 multiplication, 73 of bilinear form, 334 of linear map, 69 of nilpotent operator, 305 of operator, 154 of product of linear maps, 74, 91 of ğ‘‡â€², 113 of ğ‘‡âˆ—, 232 of vector, 88 minimal polynomial and basis of generalized eigenvectors, 306 and characteristic polynomial, 312 and diagonalizability, 169 and generalized eigenspace decomposition, 316 and generalized eigenspaces, 317 and invertibility, 149 388 Index and upper-triangular matrices, 159, 203 computing, 145 definition of,145 gcd with its derivative, 173 no direct sum decomposition, 325 of adjoint, 241 of companion matrix, 152 of complexification,153 of dual map, 153 of nilpotent operator, 305, 324 of normal operator, 241 of quotient operator, 153 of restriction operator, 148 of self-adjoint operator, 244 polynomial multiple of, 148 zeros of, 146 minimizing distance, 217 ğ‘š-linear form, 346 ğ‘š-linear functional, 378 ğ‘š-linear map, 379 monic polynomial, 144 Moon, v, xvii Mooreâ€“Penrose inverse, 221 multilinear form, 346 multiplication, see product multiplicity of an eigenvalue, 310 nilpotent operator, 303, 322 Noether, Emmy, 332 nonsingular matrix, 91 norm, 182, 186 normal operator, 235 null space, 59 of powers of an operator, 298 of ğ‘‡â€², 111 of ğ‘‡âˆ—, 231 one-to-one, 60 onto, 62 operator, 133 orthogonal complement, 211 projection, 214 vectors, 187 orthonormal basis, 199 list, 197 parallelogram equality, 191 Parsevalâ€™s identity, 200 partial differentiation operator, 175 Peabody Library, 181 permutation, 348 photo credits, 383 point, 12 polar decomposition, 286 polynomial, 30 positive definite,266 positive operator, 251 positive semidefinite operator,251 principal axes, 287 product of complex numbers, 2 of linear maps, 55 of matrices, 73 of polynomials, 138 of scalar and linear map, 55 of scalar and vector, 12 of scalar and vector in ğ…ğ‘›, 9 of vector spaces, 96 pseudoinverse, 221, 250, 255, 275, 279 Pythagorean theorem, 187 QR factorization, 264, 365 quadratic form, 341 quotient map, 101 operator, 142, 153, 162, 173 space, 99 range, 61 Index 389 of powers of an operator, 306 of ğ‘‡â€², 112 of ğ‘‡âˆ—, 231 rank of a matrix, 79, 114, 239 real part, 120 real spectral theorem, 245 real vector space, 13 reverse triangle inequality, 129, 193, 294 Riesz representation theorem, 205, 210, 216, 224, 225 Riesz, Frigyes, 205 row rank of a matrix, 77, 114, 239 scalar, 4 scalar multiplication, 9, 12 in quotient space, 100 of linear maps, 55 of matrices, 71 Schmidt pair, 278 Schmidt, Erhard, 200, 278 Schurâ€™s theorem, 204 Schur, Issai, 204 Schwarz, Hermann, 189 self-adjoint operator, 233 Shelley, Mary Wollstonecraft, 50 sign of a permutation, 349 simultaneous diagonalization, 176 simultaneously upper triangularizable, 178 singular matrix, 91 singular value decomposition of adjoint, 275 of linear map, 273 of pseudoinverse, 275 singular values, 271, 362 skew operator, 240, 247, 269 span, 29 spans, 29 spectral theorem, 245, 246 square root of an operator, 248, 251, 253, 320 standard basis of ğ…ğ‘›, 39 of ğ’«ğ‘š(ğ…), 39 subspace, 18 subtraction of complex numbers, 4 sum, see addition sum of subspaces, 19 Supreme Court, 210 surjective, 62 SVD, see singular value decomposition Sylvester, James, 181 symmetric bilinear form, 337 symmetric matrix, 269, 337 tensor product, 372, 379 Through the Looking Glass, 11 trace of a matrix, 326 of an operator, 327 translate, 99 transpose of a matrix, 77, 231 triangle inequality, 121, 190, 281 tuple, 5 two-sided ideal, 58 unit circle in ğ‚, 262, 269 unitary matrix, 263 unitary operator, 260 University of Dublin, 297 University of GÃ¶ttingen, 332 upper-triangular matrix, 155â€“160, 264, 267, 314 Vandermonde matrix, 366 vector, 8, 12 vector space, 12 volume, 292, 363 of a box, 291 zero of a polynomial, 122 Colophon: Notes on Typesetting â€¢ This book was typeset in LuaLATEX by the author, who wrote the LATEX code to implement the bookâ€™s design. â€¢ The LATEX software used for this book was written by Leslie Lamport. The TEX software, which forms the base for LATEX, was written by Donald Knuth. â€¢ The main text font in this book is the Open Type Format version of TEX Gyre Termes, a font based on Times, which was designed by Stanley Morison and Victor Lardent for the British newspaper The Times in 1931. â€¢ The main math font in this book is the Open Type Format version of TEX Gyre Pagella Math, a font based on Palatino, which was designed by Hermann Zapf. â€¢ The sans serif font used for page headings and some other design elements is the Open Type Format version of TEX Gyre Heros, a font based on Helvetica, which was designed by Max Miedinger and Eduard Hoffmann. â€¢ The LuaLATEX packages fontspec and unicode-math, both written by Will Robertson, were used to manage fonts. â€¢ The LATEX package fontsize, written by Ivan Valbusa, was used to gracefully change the main fonts to 10.5 point size. â€¢ The figures in the book were produced byMathematica, using Mathematica code written by the author. Mathematica was created by Stephen Wolfram. The Mathematica package MaTeX, written by Szabolcs HorvÃ¡t, was used to place LATEX-generated labels in the Mathematica figures. â€¢ The LATEX package graphicx, written by David Carlisle and Sebastian Rahtz, was used to include photos and figures. â€¢ The LATEX package multicol, written by Frank Mittelbach, was used to get around LATEXâ€™s limitation that two-column format must start on a new page (needed for the Symbol Index and the Index). â€¢ The LATEX packages TikZ, written by Till Tantau, and tcolorbox, written by Thomas Sturm, were used to produce the definition boxes and result boxes. â€¢ The LATEX package color, written by David Carlisle, was used to add appropriate color to various design elements. â€¢ The LATEX package wrapfig, written by Donald Arseneau, was used to wrap text around the comment boxes. â€¢ The LATEX package microtype, written by Robert Schlicht, was used to reduce hyphenation and produce more pleasing right justification. 390 S. Axler, Linear Algebra Done Right, Undergraduate Texts in Mathematics, https://doi.org/10.1007/978-3-031-41026-0 Â© Sheldon Axler 2024","libVersion":"0.3.2","langs":""}