{"path":"My Books/Mathematics for Machine Learning.pdf","text":"MATHEMATICS FOR MACHINE LEARNING Marc Peter Deisenroth A. Aldo Faisal Cheng Soon OngMATHEMATICS FOR MACHINE LEARNINGDEISENROTH ET AL. The fundamental mathematical tools needed to understand machine learning include linear algebra, analytic geometry, matrix decompositions, vector calculus, optimization, probability and statistics. These topics are traditionally taught in disparate courses, making it hard for data science or computer science students, or professionals, to efﬁ ciently learn the mathematics. This self-contained textbook bridges the gap between mathematical and machine learning texts, introducing the mathematical concepts with a minimum of prerequisites. It uses these concepts to derive four central machine learning methods: linear regression, principal component analysis, Gaussian mixture models and support vector machines. For students and others with a mathematical background, these derivations provide a starting point to machine learning texts. For those learning the mathematics for the ﬁ rst time, the methods help build intuition and practical experience with applying mathematical concepts. Every chapter includes worked examples and exercises to test understanding. Programming tutorials are offered on the book’s web site. MARC PETER DEISENROTH is Senior Lecturer in Statistical Machine Learning at the Department of Computing, Împerial College London. A. ALDO FAISAL leads the Brain & Behaviour Lab at Imperial College London, where he is also Reader in Neurotechnology at the Department of Bioengineering and the Department of Computing. CHENG SOON ONG is Principal Research Scientist at the Machine Learning Research Group, Data61, CSIRO. He is also Adjunct Associate Professor at Australian National University. Cover image courtesy of Daniel Bosma / Moment / Getty Images Cover design by Holly JohnsonDeisenrith et al. 9781108455145 Cover. C M Y KContents Foreword 1 Part I Mathematical Foundations 9 1 Introduction and Motivation 11 1.1 Finding Words for Intuitions 12 1.2 Two Ways to Read This Book 13 1.3 Exercises and Feedback 16 2 Linear Algebra 17 2.1 Systems of Linear Equations 19 2.2 Matrices 22 2.3 Solving Systems of Linear Equations 27 2.4 Vector Spaces 35 2.5 Linear Independence 40 2.6 Basis and Rank 44 2.7 Linear Mappings 48 2.8 Afﬁne Spaces 61 2.9 Further Reading 63 Exercises 64 3 Analytic Geometry 70 3.1 Norms 71 3.2 Inner Products 72 3.3 Lengths and Distances 75 3.4 Angles and Orthogonality 76 3.5 Orthonormal Basis 78 3.6 Orthogonal Complement 79 3.7 Inner Product of Functions 80 3.8 Orthogonal Projections 81 3.9 Rotations 91 3.10 Further Reading 94 Exercises 96 4 Matrix Decompositions 98 4.1 Determinant and Trace 99 i This material will be published by Cambridge University Press as Mathematics for Machine Learn- ing by Marc Peter Deisenroth, A. Aldo Faisal, and Cheng Soon Ong. This pre-publication version is free to view and download for personal use only. Not for re-distribution, re-sale or use in deriva- tive works. c⃝by M. P. Deisenroth, A. A. Faisal, and C. S. Ong, 2019. https://mml-book.com. ii Contents 4.2 Eigenvalues and Eigenvectors 105 4.3 Cholesky Decomposition 114 4.4 Eigendecomposition and Diagonalization 115 4.5 Singular Value Decomposition 119 4.6 Matrix Approximation 129 4.7 Matrix Phylogeny 134 4.8 Further Reading 135 Exercises 137 5 Vector Calculus 139 5.1 Differentiation of Univariate Functions 141 5.2 Partial Differentiation and Gradients 146 5.3 Gradients of Vector-Valued Functions 149 5.4 Gradients of Matrices 155 5.5 Useful Identities for Computing Gradients 158 5.6 Backpropagation and Automatic Differentiation 159 5.7 Higher-Order Derivatives 164 5.8 Linearization and Multivariate Taylor Series 165 5.9 Further Reading 170 Exercises 170 6 Probability and Distributions 172 6.1 Construction of a Probability Space 172 6.2 Discrete and Continuous Probabilities 178 6.3 Sum Rule, Product Rule, and Bayes’ Theorem 183 6.4 Summary Statistics and Independence 186 6.5 Gaussian Distribution 197 6.6 Conjugacy and the Exponential Family 205 6.7 Change of Variables/Inverse Transform 214 6.8 Further Reading 221 Exercises 222 7 Continuous Optimization 225 7.1 Optimization Using Gradient Descent 227 7.2 Constrained Optimization and Lagrange Multipliers 233 7.3 Convex Optimization 236 7.4 Further Reading 246 Exercises 247 Part II Central Machine Learning Problems 249 8 When Models Meet Data 251 8.1 Data, Models, and Learning 251 8.2 Empirical Risk Minimization 258 8.3 Parameter Estimation 265 8.4 Probabilistic Modeling and Inference 272 8.5 Directed Graphical Models 278 Draft (2019-12-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com. Contents iii 8.6 Model Selection 283 9 Linear Regression 289 9.1 Problem Formulation 291 9.2 Parameter Estimation 292 9.3 Bayesian Linear Regression 303 9.4 Maximum Likelihood as Orthogonal Projection 313 9.5 Further Reading 315 10 Dimensionality Reduction with Principal Component Analysis 317 10.1 Problem Setting 318 10.2 Maximum Variance Perspective 320 10.3 Projection Perspective 325 10.4 Eigenvector Computation and Low-Rank Approximations 333 10.5 PCA in High Dimensions 335 10.6 Key Steps of PCA in Practice 336 10.7 Latent Variable Perspective 339 10.8 Further Reading 343 11 Density Estimation with Gaussian Mixture Models 348 11.1 Gaussian Mixture Model 349 11.2 Parameter Learning via Maximum Likelihood 350 11.3 EM Algorithm 360 11.4 Latent-Variable Perspective 363 11.5 Further Reading 368 12 Classiﬁcation with Support Vector Machines 370 12.1 Separating Hyperplanes 372 12.2 Primal Support Vector Machine 374 12.3 Dual Support Vector Machine 383 12.4 Kernels 388 12.5 Numerical Solution 390 12.6 Further Reading 392 References 395 Index 407 c⃝2019 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press. Foreword Machine learning is the latest in a long line of attempts to distill human knowledge and reasoning into a form that is suitable for constructing ma- chines and engineering automated systems. As machine learning becomes more ubiquitous and its software packages become easier to use, it is nat- ural and desirable that the low-level technical details are abstracted away and hidden from the practitioner. However, this brings with it the danger that a practitioner becomes unaware of the design decisions and, hence, the limits of machine learning algorithms. The enthusiastic practitioner who is interested to learn more about the magic behind successful machine learning algorithms currently faces a daunting set of pre-requisite knowledge: Programming languages and data analysis tools Large-scale computation and the associated frameworks Mathematics and statistics and how machine learning builds on it At universities, introductory courses on machine learning tend to spend early parts of the course covering some of these pre-requisites. For histori- cal reasons, courses in machine learning tend to be taught in the computer science department, where students are often trained in the ﬁrst two areas of knowledge, but not so much in mathematics and statistics. Current machine learning textbooks primarily focus on machine learn- ing algorithms and methodologies and assume that the reader is com- petent in mathematics and statistics. Therefore, these books only spend one or two chapters of background mathematics, either at the beginning of the book or as appendices. We have found many people who want to delve into the foundations of basic machine learning methods who strug- gle with the mathematical knowledge required to read a machine learning textbook. Having taught undergraduate and graduate courses at universi- ties, we ﬁnd that the gap between high school mathematics and the math- ematics level required to read a standard machine learning textbook is too big for many people. This book brings the mathematical foundations of basic machine learn- ing concepts to the fore and collects the information in a single place so that this skills gap is narrowed or even closed. 1 This material will be published by Cambridge University Press as Mathematics for Machine Learn- ing by Marc Peter Deisenroth, A. Aldo Faisal, and Cheng Soon Ong. This pre-publication version is free to view and download for personal use only. Not for re-distribution, re-sale or use in deriva- tive works. c⃝by M. P. Deisenroth, A. A. Faisal, and C. S. Ong, 2019. https://mml-book.com. 2 Foreword Why Another Book on Machine Learning? Machine learning builds upon the language of mathematics to express concepts that seem intuitively obvious but that are surprisingly difﬁcult to formalize. Once formalized properly, we can gain insights into the task we want to solve. One common complaint of students of mathematics around the globe is that the topics covered seem to have little relevance to practical problems. We believe that machine learning is an obvious and direct motivation for people to learn mathematics. This book is intended to be a guidebook to the vast mathematical lit- erature that forms the foundations of modern machine learning. We mo-“Math is linked in the popular mind with phobia and anxiety. You’d think we’re discussing spiders.” (Strogatz, 2014, page 281) tivate the need for mathematical concepts by directly pointing out their usefulness in the context of fundamental machine learning problems. In the interest of keeping the book short, many details and more advanced concepts have been left out. Equipped with the basic concepts presented here, and how they ﬁt into the larger context of machine learning, the reader can ﬁnd numerous resources for further study, which we provide at the end of the respective chapters. For readers with a mathematical back- ground, this book provides a brief but precisely stated glimpse of machine learning. In contrast to other books that focus on methods and models of machine learning (MacKay, 2003; Bishop, 2006; Alpaydin, 2010; Bar- ber, 2012; Murphy, 2012; Shalev-Shwartz and Ben-David, 2014; Rogers and Girolami, 2016) or programmatic aspects of machine learning (M¨uller and Guido, 2016; Raschka and Mirjalili, 2017; Chollet and Allaire, 2018), we provide only four representative examples of machine learning algo- rithms. Instead, we focus on the mathematical concepts behind the models themselves. We hope that readers will be able to gain a deeper understand- ing of the basic questions in machine learning and connect practical ques- tions arising from the use of machine learning with fundamental choices in the mathematical model. We do not aim to write a classical machine learning book. Instead, our intention is to provide the mathematical background, applied to four cen- tral machine learning problems, to make it easier to read other machine learning textbooks. Who Is the Target Audience? As applications of machine learning become widespread in society, we believe that everybody should have some understanding of its underlying principles. This book is written in an academic mathematical style, which enables us to be precise about the concepts behind machine learning. We encourage readers unfamiliar with this seemingly terse style to persevere and to keep the goals of each topic in mind. We sprinkle comments and remarks throughout the text, in the hope that it provides useful guidance with respect to the big picture. The book assumes the reader to have mathematical knowledge commonly Draft (2019-12-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com. Foreword 3 covered in high school mathematics and physics. For example, the reader should have seen derivatives and integrals before, and geometric vectors in two or three dimensions. Starting from there, we generalize these con- cepts. Therefore, the target audience of the book includes undergraduate university students, evening learners and learners participating in online machine learning courses. In analogy to music, there are three types of interaction that people have with machine learning: Astute Listener The democratization of machine learning by the pro- vision of open-source software, online tutorials and cloud-based tools al- lows users to not worry about the speciﬁcs of pipelines. Users can focus on extracting insights from data using off-the-shelf tools. This enables non- tech-savvy domain experts to beneﬁt from machine learning. This is sim- ilar to listening to music; the user is able to choose and discern between different types of machine learning, and beneﬁts from it. More experi- enced users are like music critics, asking important questions about the application of machine learning in society such as ethics, fairness, and pri- vacy of the individual. We hope that this book provides a foundation for thinking about the certiﬁcation and risk management of machine learning systems, and allows them to use their domain expertise to build better machine learning systems. Experienced Artist Skilled practitioners of machine learning can plug and play different tools and libraries into an analysis pipeline. The stereo- typical practitioner would be a data scientist or engineer who understands machine learning interfaces and their use cases, and is able to perform wonderful feats of prediction from data. This is similar to a virtuoso play- ing music, where highly skilled practitioners can bring existing instru- ments to life and bring enjoyment to their audience. Using the mathe- matics presented here as a primer, practitioners would be able to under- stand the beneﬁts and limits of their favorite method, and to extend and generalize existing machine learning algorithms. We hope that this book provides the impetus for more rigorous and principled development of machine learning methods. Fledgling Composer As machine learning is applied to new domains, developers of machine learning need to develop new methods and extend existing algorithms. They are often researchers who need to understand the mathematical basis of machine learning and uncover relationships be- tween different tasks. This is similar to composers of music who, within the rules and structure of musical theory, create new and amazing pieces. We hope this book provides a high-level overview of other technical books for people who want to become composers of machine learning. There is a great need in society for new researchers who are able to propose and explore novel approaches for attacking the many challenges of learning from data. c⃝2019 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press. 4 Foreword Acknowledgments We are grateful to many people who looked at early drafts of the book and suffered through painful expositions of concepts. We tried to implement their ideas that we did not vehemently disagree with. We would like to especially acknowledge Christfried Webers for his careful reading of many parts of the book, and his detailed suggestions on structure and presen- tation. Many friends and colleagues have also been kind enough to pro- vide their time and energy on different versions of each chapter. We have been lucky to beneﬁt from the generosity of the online community, who have suggested improvements via github.com, which greatly improved the book. The following people have found bugs, proposed clariﬁcations and sug- gested relevant literature, either via github.com or personal communica- tion. Their names are sorted alphabetically. Abdul-Ganiy Usman Adam Gaier Adele Jackson Aditya Menon Alasdair Tran Aleksandar Krnjaic Alexander Makrigiorgos Alfredo Canziani Ali Shafti Amr Khalifa Andrew Tanggara Angus Gruen Antal A. Buss Antoine Toisoul Le Cann Areg Sarvazyan Artem Artemev Artyom Stepanov Bill Kromydas Bob Williamson Boon Ping Lim Chao Qu Cheng Li Chris Sherlock Christopher Gray Daniel McNamara Daniel Wood Darren Siegel David Johnston Dawei Chen Ellen Broad Fengkuangtian Zhu Fiona Condon Georgios Theodorou He Xin Irene Raissa Kameni Jakub Nabaglo James Hensman Jamie Liu Jean Kaddour Jean-Paul Ebejer Jerry Qiang Jitesh Sindhare John Lloyd Jonas Ngnawe Jon Martin Justin Hsi Kai Arulkumaran Kamil Dreczkowski Lily Wang Lionel Tondji Ngoupeyou Lydia Kn¨uﬁng Mahmoud Aslan Mark Hartenstein Mark van der Wilk Markus Hegland Martin Hewing Matthew Alger Matthew Lee Draft (2019-12-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com. Foreword 5 Maximus McCann Mengyan Zhang Michael Bennett Michael Pedersen Minjeong Shin Mohammad Malekzadeh Naveen Kumar Nico Montali Oscar Armas Patrick Henriksen Patrick Wieschollek Pattarawat Chormai Paul Kelly Petros Christodoulou Piotr Januszewski Pranav Subramani Quyu Kong Ragib Zaman Rui Zhang Ryan-Rhys Grifﬁths Salomon Kabongo Samuel Ogunmola Sandeep Mavadia Sarvesh Nikumbh Sebastian Raschka Senanayak Sesh Kumar Karri Seung-Heon Baek Shahbaz Chaudhary Shakir Mohamed Shawn Berry Sheikh Abdul Raheem Ali Sheng Xue Sridhar Thiagarajan Syed Nouman Hasany Szymon Brych Thomas B¨uhler Timur Sharapov Tom Melamed Vincent Adam Vincent Dutordoir Vu Minh Wasim Aftab Wen Zhi Wojciech Stokowiec Xiaonan Chong Xiaowei Zhang Yazhou Hao Yicheng Luo Young Lee Yu Lu Yun Cheng Yuxiao Huang Zac Cranko Zijian Cao Zoe Nolan Contributors through github, whose real names were not listed on their github proﬁle, are: SamDataMad bumptiousmonkey idoamihai deepakiim insad HorizonP cs-maillist kudo23 empet victorBigand 17SKYE jessjing1995 We are also very grateful to Parameswaran Raman and the many anony- mous reviewers, organized by Cambridge University Press, who read one or more chapters of earlier versions of the manuscript, and provided con- structive criticism that led to considerable improvements. A special men- tion goes to Dinesh Singh Negi, our LATEX support, for detailed and prompt advice about LATEX-related issues. Last but not least, we are very grateful to our editor Lauren Cowles, who has been patiently guiding us through the gestation process of this book. c⃝2019 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press. 6 Foreword Table of Symbols Symbol Typical meaning a, b, c, α, β, γ Scalars are lowercase x, y, z Vectors are bold lowercase A, B, C Matrices are bold uppercase x ⊤, A⊤ Transpose of a vector or matrix A −1 Inverse of a matrix ⟨x, y⟩ Inner product of x and y x ⊤y Dot product of x and y B = (b1, b2, b3) (Ordered) tuple B = [b1, b2, b3] Matrix of column vectors stacked horizontally B = {b1, b2, b3} Set of vectors (unordered) Z, N Integers and natural numbers, respectively R, C Real and complex numbers, respectively R n n-dimensional vector space of real numbers ∀x Universal quantiﬁer: for all x ∃x Existential quantiﬁer: there exists x a := b a is deﬁned as b a =: b b is deﬁned as a a ∝ b a is proportional to b, i.e., a = constant · b g ◦ f Function composition: “g after f ” ⇐⇒ If and only if =⇒ Implies A, C Sets a ∈ A a is an element of the set A ∅ Empty set D Number of dimensions; indexed by d = 1, . . . , D N Number of data points; indexed by n = 1, . . . , N I m Identity matrix of size m × m 0m,n Matrix of zeros of size m × n 1m,n Matrix of ones of size m × n ei Standard/canonical vector (where i is the component that is 1) dim Dimensionality of vector space rk(A) Rank of matrix A Im(Φ) Image of linear mapping Φ ker(Φ) Kernel (null space) of a linear mapping Φ span[b1] Span (generating set) of b1 tr(A) Trace of A det(A) Determinant of A | · | Absolute value or determinant (depending on context) ∥·∥ Norm; Euclidean unless speciﬁed λ Eigenvalue or Lagrange multiplier Eλ Eigenspace corresponding to eigenvalue λ Draft (2019-12-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com. Foreword 7 Symbol Typical meaning θ Parameter vector ∂f ∂x Partial derivative of f with respect to x df dx Total derivative of f with respect to x ∇ Gradient L Lagrangian L Negative log-likelihood( n k) Binomial coefﬁcient, n choose k VX[x] Variance of x with respect to the random variable X EX[x] Expectation of x with respect to the random variable X CovX,Y [x, y] Covariance between x and y. X ⊥⊥ Y | Z X is conditionally independent of Y given Z X ∼ p Random variable X is distributed according to p N (µ, Σ ) Gaussian distribution with mean µ and covariance Σ Ber(µ) Bernoulli distribution with parameter µ Bin(N, µ) Binomial distribution with parameters N, µ Beta(α, β) Beta distribution with parameters α, β Table of Abbreviations and Acronyms Acronym Meaning e.g. Exempli gratia (Latin: for example) GMM Gaussian mixture model i.e. Id est (Latin: this means) i.i.d. Independent, identically distributed MAP Maximum a posteriori MLE Maximum likelihood estimation/estimator ONB Orthonormal basis PCA Principal component analysis PPCA Probabilistic principal component analysis REF Row-echelon form SPD Symmetric, positive deﬁnite SVM Support vector machine c⃝2019 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press. Part I Mathematical Foundations 9 This material will be published by Cambridge University Press as Mathematics for Machine Learn- ing by Marc Peter Deisenroth, A. Aldo Faisal, and Cheng Soon Ong. This pre-publication version is free to view and download for personal use only. Not for re-distribution, re-sale or use in deriva- tive works. c⃝by M. P. Deisenroth, A. A. Faisal, and C. S. Ong, 2019. https://mml-book.com. 1 Introduction and Motivation Machine learning is about designing algorithms that automatically extract valuable information from data. The emphasis here is on “automatic”, i.e., machine learning is concerned about general-purpose methodologies that can be applied to many datasets, while producing something that is mean- ingful. There are three concepts that are at the core of machine learning: data, a model, and learning. Since machine learning is inherently data driven, data is at the core data of machine learning. The goal of machine learning is to design general- purpose methodologies to extract valuable patterns from data, ideally without much domain-speciﬁc expertise. For example, given a large corpus of documents (e.g., books in many libraries), machine learning methods can be used to automatically ﬁnd relevant topics that are shared across documents (Hoffman et al., 2010). To achieve this goal, we design mod- els that are typically related to the process that generates data, similar to model the dataset we are given. For example, in a regression setting, the model would describe a function that maps inputs to real-valued outputs. To paraphrase Mitchell (1997): A model is said to learn from data if its per- formance on a given task improves after the data is taken into account. The goal is to ﬁnd good models that generalize well to yet unseen data, which we may care about in the future. Learning can be understood as a learning way to automatically ﬁnd patterns and structure in data by optimizing the parameters of the model. While machine learning has seen many success stories, and software is readily available to design and train rich and ﬂexible machine learning systems, we believe that the mathematical foundations of machine learn- ing are important in order to understand fundamental principles upon which more complicated machine learning systems are built. Understand- ing these principles can facilitate creating new machine learning solutions, understanding and debugging existing approaches, and learning about the inherent assumptions and limitations of the methodologies we are work- ing with. 11 This material will be published by Cambridge University Press as Mathematics for Machine Learn- ing by Marc Peter Deisenroth, A. Aldo Faisal, and Cheng Soon Ong. This pre-publication version is free to view and download for personal use only. Not for re-distribution, re-sale or use in deriva- tive works. c⃝by M. P. Deisenroth, A. A. Faisal, and C. S. Ong, 2019. https://mml-book.com. 12 Introduction and Motivation 1.1 Finding Words for Intuitions A challenge we face regularly in machine learning is that concepts and words are slippery, and a particular component of the machine learning system can be abstracted to different mathematical concepts. For example, the word “algorithm” is used in at least two different senses in the con- text of machine learning. In the ﬁrst sense, we use the phrase “machine learning algorithm” to mean a system that makes predictions based on in- put data. We refer to these algorithms as predictors. In the second sense,predictor we use the exact same phrase “machine learning algorithm” to mean a system that adapts some internal parameters of the predictor so that it performs well on future unseen input data. Here we refer to this adapta- tion as training a system.training This book will not resolve the issue of ambiguity, but we want to high- light upfront that, depending on the context, the same expressions can mean different things. However, we attempt to make the context sufﬁ- ciently clear to reduce the level of ambiguity. The ﬁrst part of this book introduces the mathematical concepts and foundations needed to talk about the three main components of a machine learning system: data, models, and learning. We will brieﬂy outline these components here, and we will revisit them again in Chapter 8 once we have discussed the necessary mathematical concepts. While not all data is numerical, it is often useful to consider data in a number format. In this book, we assume that data has already been appropriately converted into a numerical representation suitable for read- ing into a computer program. Therefore, we think of data as vectors. Asdata as vectors another illustration of how subtle words are, there are (at least) three different ways to think about vectors: a vector as an array of numbers (a computer science view), a vector as an arrow with a direction and magni- tude (a physics view), and a vector as an object that obeys addition and scaling (a mathematical view). A model is typically used to describe a process for generating data, sim-model ilar to the dataset at hand. Therefore, good models can also be thought of as simpliﬁed versions of the real (unknown) data-generating process, capturing aspects that are relevant for modeling the data and extracting hidden patterns from it. A good model can then be used to predict what would happen in the real world without performing real-world experi- ments. We now come to the crux of the matter, the learning component oflearning machine learning. Assume we are given a dataset and a suitable model. Training the model means to use the data available to optimize some pa- rameters of the model with respect to a utility function that evaluates how well the model predicts the training data. Most training methods can be thought of as an approach analogous to climbing a hill to reach its peak. In this analogy, the peak of the hill corresponds to a maximum of some Draft (2019-12-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com. 1.2 Two Ways to Read This Book 13 desired performance measure. However, in practice, we are interested in the model to perform well on unseen data. Performing well on data that we have already seen (training data) may only mean that we found a good way to memorize the data. However, this may not generalize well to unseen data, and, in practical applications, we often need to expose our machine learning system to situations that it has not encountered before. Let us summarize the main concepts of machine learning that we cover in this book: We represent data as vectors. We choose an appropriate model, either using the probabilistic or opti- mization view. We learn from available data by using numerical optimization methods with the aim that the model performs well on data not used for training. 1.2 Two Ways to Read This Book We can consider two strategies for understanding the mathematics for machine learning: Bottom-up: Building up the concepts from foundational to more ad- vanced. This is often the preferred approach in more technical ﬁelds, such as mathematics. This strategy has the advantage that the reader at all times is able to rely on their previously learned concepts. Unfor- tunately, for a practitioner many of the foundational concepts are not particularly interesting by themselves, and the lack of motivation means that most foundational deﬁnitions are quickly forgotten. Top-down: Drilling down from practical needs to more basic require- ments. This goal-driven approach has the advantage that the readers know at all times why they need to work on a particular concept, and there is a clear path of required knowledge. The downside of this strat- egy is that the knowledge is built on potentially shaky foundations, and the readers have to remember a set of words that they do not have any way of understanding. We decided to write this book in a modular way to separate foundational (mathematical) concepts from applications so that this book can be read in both ways. The book is split into two parts, where Part I lays the math- ematical foundations and Part II applies the concepts from Part I to a set of fundamental machine learning problems, which form four pillars of machine learning as illustrated in Figure 1.1: regression, dimensionality reduction, density estimation, and classiﬁcation. Chapters in Part I mostly build upon the previous ones, but it is possible to skip a chapter and work backward if necessary. Chapters in Part II are only loosely coupled and can be read in any order. There are many pointers forward and backward c⃝2019 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press. 14 Introduction and Motivation Figure 1.1 The foundations and four pillars of machine learning.Classiﬁcation Density Estimation Regression Dimensionality Reduction Machine Learning V ector Calculus Probability & Distributions Optimization Analytic Geometry Matrix DecompositionLinear Algebra between the two parts of the book to link mathematical concepts with machine learning algorithms. Of course there are more than two ways to read this book. Most readers learn using a combination of top-down and bottom-up approaches, some- times building up basic mathematical skills before attempting more com- plex concepts, but also choosing topics based on applications of machine learning. Part I Is about Mathematics The four pillars of machine learning we cover in this book (see Figure 1.1) require a solid mathematical foundation, which is laid out in Part I. We represent numerical data as vectors and represent a table of such data as a matrix. The study of vectors and matrices is called linear algebra, which we introduce in Chapter 2. The collection of vectors as a matrix islinear algebra also described there. Given two vectors representing two objects in the real world, we want to make statements about their similarity. The idea is that vectors that are similar should be predicted to have similar outputs by our machine learning algorithm (our predictor). To formalize the idea of similarity be- tween vectors, we need to introduce operations that take two vectors as input and return a numerical value representing their similarity. The con- struction of similarity and distances is central to analytic geometry and isanalytic geometry discussed in Chapter 3. In Chapter 4, we introduce some fundamental concepts about matri- ces and matrix decomposition. Some operations on matrices are extremelymatrix decomposition useful in machine learning, and they allow for an intuitive interpretation of the data and more efﬁcient learning. We often consider data to be noisy observations of some true underly- ing signal. We hope that by applying machine learning we can identify the signal from the noise. This requires us to have a language for quantify- ing what “noise” means. We often would also like to have predictors that Draft (2019-12-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com. 1.2 Two Ways to Read This Book 15 allow us to express some sort of uncertainty, e.g., to quantify the conﬁ- dence we have about the value of the prediction at a particular test data point. Quantiﬁcation of uncertainty is the realm of probability theory and probability theory is covered in Chapter 6. To train machine learning models, we typically ﬁnd parameters that maximize some performance measure. Many optimization techniques re- quire the concept of a gradient, which tells us the direction in which to search for a solution. Chapter 5 is about vector calculus and details the vector calculus concept of gradients, which we subsequently use in Chapter 7, where we talk about optimization to ﬁnd maxima/minima of functions. optimization Part II Is about Machine Learning The second part of the book introduces four pillars of machine learning as shown in Figure 1.1. We illustrate how the mathematical concepts in- troduced in the ﬁrst part of the book are the foundation for each pillar. Broadly speaking, chapters are ordered by difﬁculty (in ascending order). In Chapter 8, we restate the three components of machine learning (data, models, and parameter estimation) in a mathematical fashion. In addition, we provide some guidelines for building experimental set-ups that guard against overly optimistic evaluations of machine learning sys- tems. Recall that the goal is to build a predictor that performs well on unseen data. In Chapter 9, we will have a close look at linear regression, where our linear regression objective is to ﬁnd functions that map inputs x ∈ R D to corresponding ob- served function values y ∈ R, which we can interpret as the labels of their respective inputs. We will discuss classical model ﬁtting (parameter esti- mation) via maximum likelihood and maximum a posteriori estimation, as well as Bayesian linear regression, where we integrate the parameters out instead of optimizing them. Chapter 10 focuses on dimensionality reduction, the second pillar in Fig- dimensionality reductionure 1.1, using principal component analysis. The key objective of dimen- sionality reduction is to ﬁnd a compact, lower-dimensional representation of high-dimensional data x ∈ R D, which is often easier to analyze than the original data. Unlike regression, dimensionality reduction is only con- cerned about modeling the data – there are no labels associated with a data point x. In Chapter 11, we will move to our third pillar: density estimation. The density estimation objective of density estimation is to ﬁnd a probability distribution that de- scribes a given dataset. We will focus on Gaussian mixture models for this purpose, and we will discuss an iterative scheme to ﬁnd the parameters of this model. As in dimensionality reduction, there are no labels associated with the data points x ∈ RD. However, we do not seek a low-dimensional representation of the data. Instead, we are interested in a density model that describes the data. Chapter 12 concludes the book with an in-depth discussion of the fourth c⃝2019 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press. 16 Introduction and Motivation pillar: classiﬁcation. We will discuss classiﬁcation in the context of supportclassiﬁcation vector machines. Similar to regression (Chapter 9), we have inputs x and corresponding labels y. However, unlike regression, where the labels were real-valued, the labels in classiﬁcation are integers, which requires special care. 1.3 Exercises and Feedback We provide some exercises in Part I, which can be done mostly by pen and paper. For Part II, we provide programming tutorials (jupyter notebooks) to explore some properties of the machine learning algorithms we discuss in this book. We appreciate that Cambridge University Press strongly supports our aim to democratize education and learning by making this book freely available for download at https://mml-book.com where tutorials, errata, and additional materials can be found. Mistakes can be reported and feedback provided using the preceding URL. Draft (2019-12-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com. 2 Linear Algebra When formalizing intuitive concepts, a common approach is to construct a set of objects (symbols) and a set of rules to manipulate these objects. This is known as an algebra. Linear algebra is the study of vectors and certain algebra rules to manipulate vectors. The vectors many of us know from school are called “geometric vectors”, which are usually denoted by a small arrow above the letter, e.g., −→x and −→y . In this book, we discuss more general concepts of vectors and use a bold letter to represent them, e.g., x and y. In general, vectors are special objects that can be added together and multiplied by scalars to produce another object of the same kind. From an abstract mathematical viewpoint, any object that satisﬁes these two properties can be considered a vector. Here are some examples of such vector objects: 1. Geometric vectors. This example of a vector may be familiar from high school mathematics and physics. Geometric vectors – see Figure 2.1(a) – are directed segments, which can be drawn (at least in two dimen- sions). Two geometric vectors → x, → y can be added, such that → x + → y = → z is another geometric vector. Furthermore, multiplication by a scalar λ → x, λ ∈ R, is also a geometric vector. In fact, it is the original vector scaled by λ. Therefore, geometric vectors are instances of the vector concepts introduced previously. Interpreting vectors as geometric vec- tors enables us to use our intuitions about direction and magnitude to reason about mathematical operations. 2. Polynomials are also vectors; see Figure 2.1(b): Two polynomials can Figure 2.1 Different types of vectors. Vectors can be surprising objects, including (a) geometric vectors and (b) polynomials. → x → y → x + → y (a) Geometric vectors. −2 0 2 x −6 −4 −2 0 2 4y (b) Polynomials. 17 This material will be published by Cambridge University Press as Mathematics for Machine Learn- ing by Marc Peter Deisenroth, A. Aldo Faisal, and Cheng Soon Ong. This pre-publication version is free to view and download for personal use only. Not for re-distribution, re-sale or use in deriva- tive works. c⃝by M. P. Deisenroth, A. A. Faisal, and C. S. Ong, 2019. https://mml-book.com. 18 Linear Algebra be added together, which results in another polynomial; and they can be multiplied by a scalar λ ∈ R, and the result is a polynomial as well. Therefore, polynomials are (rather unusual) instances of vectors. Note that polynomials are very different from geometric vectors. While geometric vectors are concrete “drawings”, polynomials are abstract concepts. However, they are both vectors in the sense previously de- scribed. 3. Audio signals are vectors. Audio signals are represented as a series of numbers. We can add audio signals together, and their sum is a new audio signal. If we scale an audio signal, we also obtain an audio signal. Therefore, audio signals are a type of vector, too. 4. Elements of R n (tuples of n real numbers) are vectors. Rn is more abstract than polynomials, and it is the concept we focus on in this book. For instance, a =   1 2 3   ∈ R 3 (2.1) is an example of a triplet of numbers. Adding two vectors a, b ∈ Rn component-wise results in another vector: a + b = c ∈ Rn. Moreover, multiplying a ∈ Rn by λ ∈ R results in a scaled vector λa ∈ Rn. Considering vectors as elements of Rn has an additional beneﬁt thatBe careful to check whether array operations actually perform vector operations when implementing on a computer. it loosely corresponds to arrays of real numbers on a computer. Many programming languages support array operations, which allow for con- venient implementation of algorithms that involve vector operations. Linear algebra focuses on the similarities between these vector concepts. We can add them together and multiply them by scalars. We will largely Pavel Grinfeld’s series on linear algebra: http://tinyurl. com/nahclwm Gilbert Strang’s course on linear algebra: http://tinyurl. com/29p5q8j 3Blue1Brown series on linear algebra: https://tinyurl. com/h5g4kps focus on vectors in R n since most algorithms in linear algebra are for- mulated in Rn. We will see in Chapter 8 that we often consider data to be represented as vectors in Rn. In this book, we will focus on ﬁnite- dimensional vector spaces, in which case there is a 1:1 correspondence between any kind of vector and Rn. When it is convenient, we will use intuitions about geometric vectors and consider array-based algorithms. One major idea in mathematics is the idea of “closure”. This is the ques- tion: What is the set of all things that can result from my proposed oper- ations? In the case of vectors: What is the set of vectors that can result by starting with a small set of vectors, and adding them to each other and scaling them? This results in a vector space (Section 2.4). The concept of a vector space and its properties underlie much of machine learning. The concepts introduced in this chapter are summarized in Figure 2.2. This chapter is mostly based on the lecture notes and books by Drumm and Weil (2001), Strang (2003), Hogben (2013), Liesen and Mehrmann (2015), as well as Pavel Grinfeld’s Linear Algebra series. Other excellent Draft (2019-12-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com. 2.1 Systems of Linear Equations 19 Figure 2.2 A mind map of the concepts introduced in this chapter, along with where they are used in other parts of the book. Vector Vector space Matrix Chapter 5 Vector calculus Group System of linear equations Matrix inverse Gaussian elimination Linear/afﬁne mapping Linear independence Basis Chapter 10 Dimensionality reduction Chapter 12 Classiﬁcation Chapter 3 Analytic geometry composesclosure Abelian with +represents representssolvedby solves property ofmaximalset resources are Gilbert Strang’s Linear Algebra course at MIT and the Linear Algebra Series by 3Blue1Brown. Linear algebra plays an important role in machine learning and gen- eral mathematics. The concepts introduced in this chapter are further ex- panded to include the idea of geometry in Chapter 3. In Chapter 5, we will discuss vector calculus, where a principled knowledge of matrix op- erations is essential. In Chapter 10, we will use projections (to be intro- duced in Section 3.8) for dimensionality reduction with principal compo- nent analysis (PCA). In Chapter 9, we will discuss linear regression, where linear algebra plays a central role for solving least-squares problems. 2.1 Systems of Linear Equations Systems of linear equations play a central part of linear algebra. Many problems can be formulated as systems of linear equations, and linear algebra gives us the tools for solving them. Example 2.1 A company produces products N1, . . . , Nn for which resources R1, . . . , Rm are required. To produce a unit of product Nj, aij units of resource Ri are needed, where i = 1, . . . , m and j = 1, . . . , n. The objective is to ﬁnd an optimal production plan, i.e., a plan of how many units xj of product Nj should be produced if a total of bi units of resource Ri are available and (ideally) no resources are left over. If we produce x1, . . . , xn units of the corresponding products, we need c⃝2019 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press. 20 Linear Algebra a total of ai1x1 + · · · + ainxn (2.2) many units of resource Ri. An optimal production plan (x1, . . . , xn) ∈ Rn, therefore, has to satisfy the following system of equations: a11x1 + · · · + a1nxn = b1 ... am1x1 + · · · + amnxn = bm , (2.3) where aij ∈ R and bi ∈ R. Equation (2.3) is the general form of a system of linear equations, andsystem of linear equations x1, . . . , xn are the unknowns of this system. Every n-tuple (x1, . . . , xn) ∈ R n that satisﬁes (2.3) is a solution of the linear equation system.solution Example 2.2 The system of linear equations x1 + x2 + x3 = 3 (1) x1 − x2 + 2x3 = 2 (2) 2x1 + 3x3 = 1 (3) (2.4) has no solution: Adding the ﬁrst two equations yields 2x1 +3x3 = 5, which contradicts the third equation (3). Let us have a look at the system of linear equations x1 + x2 + x3 = 3 (1) x1 − x2 + 2x3 = 2 (2) x2 + x3 = 2 (3) . (2.5) From the ﬁrst and third equation, it follows that x1 = 1. From (1)+(2), we get 2x1 + 3x3 = 5, i.e., x3 = 1. From (3), we then get that x2 = 1. Therefore, (1, 1, 1) is the only possible and unique solution (verify that (1, 1, 1) is a solution by plugging in). As a third example, we consider x1 + x2 + x3 = 3 (1) x1 − x2 + 2x3 = 2 (2) 2x1 + 3x3 = 5 (3) . (2.6) Since (1)+(2)=(3), we can omit the third equation (redundancy). From (1) and (2), we get 2x1 = 5−3x3 and 2x2 = 1+x3. We deﬁne x3 = a ∈ R as a free variable, such that any triplet ( 5 2 − 3 2 a, 1 2 + 1 2 a, a ) , a ∈ R (2.7) Draft (2019-12-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com. 2.1 Systems of Linear Equations 21 Figure 2.3 The solution space of a system of two linear equations with two variables can be geometrically interpreted as the intersection of two lines. Every linear equation represents a line. 2x1 − 4x2 = 14x1 + 4x2 = 5x1x2 is a solution of the system of linear equations, i.e., we obtain a solution set that contains inﬁnitely many solutions. In general, for a real-valued system of linear equations we obtain either no, exactly one, or inﬁnitely many solutions. Linear regression (Chapter 9) solves a version of Example 2.1 when we cannot solve the system of linear equations. Remark (Geometric Interpretation of Systems of Linear Equations). In a system of linear equations with two variables x1, x2, each linear equation deﬁnes a line on the x1x2-plane. Since a solution to a system of linear equations must satisfy all equations simultaneously, the solution set is the intersection of these lines. This intersection set can be a line (if the linear equations describe the same line), a point, or empty (when the lines are parallel). An illustration is given in Figure 2.3 for the system 4x1 + 4x2 = 5 2x1 − 4x2 = 1 (2.8) where the solution space is the point (x1, x2) = (1, 1 4 ). Similarly, for three variables, each linear equation determines a plane in three-dimensional space. When we intersect these planes, i.e., satisfy all linear equations at the same time, we can obtain a solution set that is a plane, a line, a point or empty (when the planes have no common intersection). ♦ For a systematic approach to solving systems of linear equations, we will introduce a useful compact notation. We collect the coefﬁcients aij into vectors and collect the vectors into matrices. In other words, we write the system from (2.3) in the following form: x1    a11 ... am1    + x2    a12 ... am2    + · · · + xn    a1n ... amn    =    b1 ... bm    (2.9) c⃝2019 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press. 22 Linear Algebra ⇐⇒    a11 · · · a1n ... ... am1 · · · amn       x1 ... xn    =    b1 ... bm    . (2.10) In the following, we will have a close look at these matrices and de- ﬁne computation rules. We will return to solving linear equations in Sec- tion 2.3. 2.2 Matrices Matrices play a central role in linear algebra. They can be used to com- pactly represent systems of linear equations, but they also represent linear functions (linear mappings) as we will see later in Section 2.7. Before we discuss some of these interesting topics, let us ﬁrst deﬁne what a matrix is and what kind of operations we can do with matrices. We will see more properties of matrices in Chapter 4. Deﬁnition 2.1 (Matrix). With m, n ∈ N a real-valued (m, n) matrix A ismatrix an m·n-tuple of elements aij, i = 1, . . . , m, j = 1, . . . , n, which is ordered according to a rectangular scheme consisting of m rows and n columns: A =      a11 a12 · · · a1n a21 a22 · · · a2n ... ... ... am1 am2 · · · amn      , aij ∈ R . (2.11) By convention (1, n)-matrices are called rows and (m, 1)-matrices are calledrow columns. These special matrices are also called row/column vectors.column row vector column vector Figure 2.4 By stacking its columns, a matrix A can be represented as a long vector a. re-shape A ∈ R4×2 a ∈ R 8 Rm×n is the set of all real-valued (m, n)-matrices. A ∈ R m×n can be equivalently represented as a ∈ Rmn by stacking all n columns of the matrix into a long vector; see Figure 2.4. 2.2.1 Matrix Addition and Multiplication The sum of two matrices A ∈ R m×n, B ∈ R m×n is deﬁned as the element- wise sum, i.e., A + B :=    a11 + b11 · · · a1n + b1n ... ... am1 + bm1 · · · amn + bmn    ∈ R m×n . (2.12) For matrices A ∈ Rm×n, B ∈ R n×k, the elements cij of the productNote the size of the matrices. C = AB ∈ R m×k are computed as C = np.einsum(’il, lj’, A, B) cij = n∑ l=1 ailblj, i = 1, . . . , m, j = 1, . . . , k. (2.13) Draft (2019-12-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com. 2.2 Matrices 23 This means, to compute element cij we multiply the elements of the ith There are n columns in A and n rows in B so that we can compute ailblj for l = 1, . . . , n. Commonly, the dot product between two vectors a, b is denoted by a⊤b or ⟨a, b⟩. row of A with the jth column of B and sum them up. Later in Section 3.2, we will call this the dot product of the corresponding row and column. In cases, where we need to be explicit that we are performing multiplication, we use the notation A · B to denote multiplication (explicitly showing “·”). Remark. Matrices can only be multiplied if their “neighboring” dimensions match. For instance, an n × k-matrix A can be multiplied with a k × m- matrix B, but only from the left side: A︸︷︷︸ n×k B︸︷︷︸ k×m = C︸︷︷︸ n×m (2.14) The product BA is not deﬁned if m ̸= n since the neighboring dimensions do not match. ♦ Remark. Matrix multiplication is not deﬁned as an element-wise operation on matrix elements, i.e., cij ̸= aijbij (even if the size of A, B was cho- sen appropriately). This kind of element-wise multiplication often appears in programming languages when we multiply (multi-dimensional) arrays with each other, and is called a Hadamard product. ♦ Hadamard product Example 2.3 For A = [ 1 2 3 3 2 1 ] ∈ R2×3, B =   0 2 1 −1 0 1   ∈ R3×2, we obtain AB = [ 1 2 3 3 2 1 ]   0 2 1 −1 0 1   = [ 2 3 2 5 ] ∈ R 2×2, (2.15) BA =   0 2 1 −1 0 1   [ 1 2 3 3 2 1 ] =   6 4 2 −2 0 2 3 2 1   ∈ R 3×3 . (2.16) Figure 2.5 Even if both matrix multiplications AB and BA are deﬁned, the dimensions of the results can be different. From this example, we can already see that matrix multiplication is not commutative, i.e., AB ̸= BA; see also Figure 2.5 for an illustration. Deﬁnition 2.2 (Identity Matrix). In R n×n, we deﬁne the identity matrix identity matrix I n :=           1 0 · · · 0 · · · 0 0 1 · · · 0 · · · 0 ... ... . . . ... . . . ... 0 0 · · · 1 · · · 0 ... ... . . . ... . . . ... 0 0 · · · 0 · · · 1           ∈ Rn×n (2.17) c⃝2019 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press. 24 Linear Algebra as the n × n-matrix containing 1 on the diagonal and 0 everywhere else. Now that we deﬁned matrix multiplication, matrix addition and the identity matrix, let us have a look at some properties of matrices: associativity Associativity: ∀A ∈ R m×n, B ∈ Rn×p, C ∈ Rp×q : (AB)C = A(BC) (2.18) distributivity Distributivity: ∀A, B ∈ Rm×n, C, D ∈ R n×p : (A + B)C = AC + BC (2.19a) A(C + D) = AC + AD (2.19b) Multiplication with the identity matrix: ∀A ∈ Rm×n : I mA = AI n = A (2.20) Note that I m ̸= I n for m ̸= n. 2.2.2 Inverse and Transpose Deﬁnition 2.3 (Inverse). Consider a square matrix A ∈ R n×n. Let matrixA square matrix possesses the same number of columns and rows. B ∈ R n×n have the property that AB = I n = BA. B is called the inverse of A and denoted by A−1. inverse Unfortunately, not every matrix A possesses an inverse A −1. If this inverse does exist, A is called regular/invertible/nonsingular, otherwiseregular invertible nonsingular singular/noninvertible. When the matrix inverse exists, it is unique. In Sec- singular noninvertible tion 2.3, we will discuss a general way to compute the inverse of a matrix by solving a system of linear equations. Remark (Existence of the Inverse of a 2 × 2-matrix). Consider a matrix A := [ a11 a12 a21 a22 ] ∈ R 2×2 . (2.21) If we multiply A with B := [ a22 −a12 −a21 a11 ] (2.22) we obtain AB = [ a11a22 − a12a21 0 0 a11a22 − a12a21 ] = (a11a22 − a12a21)I . (2.23) Therefore, A −1 = 1 a11a22 − a12a21 [ a22 −a12 −a21 a11 ] (2.24) if and only if a11a22 − a12a21 ̸= 0. In Section 4.1, we will see that a11a22 − a12a21 is the determinant of a 2×2-matrix. Furthermore, we can generally use the determinant to check whether a matrix is invertible. ♦ Draft (2019-12-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com. 2.2 Matrices 25 Example 2.4 (Inverse Matrix) The matrices A =   1 2 1 4 4 5 6 7 7   , B =   −7 −7 6 2 1 −1 4 5 −4   (2.25) are inverse to each other since AB = I = BA. Deﬁnition 2.4 (Transpose). For A ∈ R m×n the matrix B ∈ R n×m with bij = aji is called the transpose of A. We write B = A⊤. transpose The main diagonal (sometimes called “principal diagonal”, “primary diagonal”, “leading diagonal”, or “major diagonal”) of a matrix A is the collection of entries Aij where i = j. In general, A⊤ can be obtained by writing the columns of A as the rows of A⊤. The following are important properties of inverses and transposes: The scalar case of (2.28) is 1 2+4 = 1 6 ̸= 1 2 + 1 4 . AA−1 = I = A−1A (2.26) (AB) −1 = B−1A −1 (2.27) (A + B) −1 ̸= A−1 + B−1 (2.28) (A⊤) ⊤ = A (2.29) (A + B) ⊤ = A⊤ + B⊤ (2.30) (AB) ⊤ = B⊤A⊤ (2.31) Deﬁnition 2.5 (Symmetric Matrix). A matrix A ∈ Rn×n is symmetric if symmetric matrix A = A⊤. Note that only (n, n)-matrices can be symmetric. Generally, we call (n, n)-matrices also square matrices because they possess the same num- square matrix ber of rows and columns. Moreover, if A is invertible, then so is A⊤, and (A−1)⊤ = (A⊤) −1 =: A−⊤. Remark (Sum and Product of Symmetric Matrices). The sum of symmet- ric matrices A, B ∈ Rn×n is always symmetric. However, although their product is always deﬁned, it is generally not symmetric: [ 1 0 0 0 ] [ 1 1 1 1 ] = [ 1 1 0 0 ] . (2.32) ♦ 2.2.3 Multiplication by a Scalar Let us look at what happens to matrices when they are multiplied by a scalar λ ∈ R. Let A ∈ R m×n and λ ∈ R. Then λA = K, Kij = λ aij. Practically, λ scales each element of A. For λ, ψ ∈ R, the following holds: associativity Associativity: (λψ)C = λ(ψC), C ∈ Rm×n c⃝2019 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press. 26 Linear Algebra λ(BC) = (λB)C = B(λC) = (BC)λ, B ∈ R m×n, C ∈ R n×k. Note that this allows us to move scalar values around. (λC) ⊤ = C ⊤λ⊤ = C ⊤λ = λC ⊤ since λ = λ⊤ for all λ ∈ R.distributivity Distributivity: (λ + ψ)C = λC + ψC, C ∈ Rm×n λ(B + C) = λB + λC, B, C ∈ Rm×n Example 2.5 (Distributivity) If we deﬁne C := [ 1 2 3 4 ] , (2.33) then for any λ, ψ ∈ R we obtain (λ + ψ)C = [ (λ + ψ)1 (λ + ψ)2 (λ + ψ)3 (λ + ψ)4 ] = [ λ + ψ 2λ + 2ψ 3λ + 3ψ 4λ + 4ψ ] (2.34a) = [ λ 2λ 3λ 4λ ] + [ ψ 2ψ 3ψ 4ψ ] = λC + ψC . (2.34b) 2.2.4 Compact Representations of Systems of Linear Equations If we consider the system of linear equations 2x1 + 3x2 + 5x3 = 1 4x1 − 2x2 − 7x3 = 8 9x1 + 5x2 − 3x3 = 2 (2.35) and use the rules for matrix multiplication, we can write this equation system in a more compact form as   2 3 5 4 −2 −7 9 5 −3     x1 x2 x3   =   1 8 2   . (2.36) Note that x1 scales the ﬁrst column, x2 the second one, and x3 the third one. Generally, a system of linear equations can be compactly represented in their matrix form as Ax = b; see (2.3), and the product Ax is a (linear) combination of the columns of A. We will discuss linear combinations in more detail in Section 2.5. Draft (2019-12-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com. 2.3 Solving Systems of Linear Equations 27 2.3 Solving Systems of Linear Equations In (2.3), we introduced the general form of an equation system, i.e., a11x1 + · · · + a1nxn = b1 ... am1x1 + · · · + amnxn = bm , (2.37) where aij ∈ R and bi ∈ R are known constants and xj are unknowns, i = 1, . . . , m, j = 1, . . . , n. Thus far, we saw that matrices can be used as a compact way of formulating systems of linear equations so that we can write Ax = b, see (2.10). Moreover, we deﬁned basic matrix operations, such as addition and multiplication of matrices. In the following, we will focus on solving systems of linear equations and provide an algorithm for ﬁnding the inverse of a matrix. 2.3.1 Particular and General Solution Before discussing how to generally solve systems of linear equations, let us have a look at an example. Consider the system of equations [ 1 0 8 −4 0 1 2 12 ]     x1 x2 x3 x4     = [ 42 8 ] . (2.38) The system has two equations and four unknowns. Therefore, in general we would expect inﬁnitely many solutions. This system of equations is in a particularly easy form, where the ﬁrst two columns consist of a 1 and a 0. Remember that we want to ﬁnd scalars x1, . . . , x4, such that∑4 i=1 xici = b, where we deﬁne ci to be the ith column of the matrix and b the right-hand-side of (2.38). A solution to the problem in (2.38) can be found immediately by taking 42 times the ﬁrst column and 8 times the second column so that b = [ 42 8 ] = 42 [ 1 0 ] + 8 [ 0 1 ] . (2.39) Therefore, a solution is [42, 8, 0, 0] ⊤. This solution is called a particular particular solution solution or special solution. However, this is not the only solution of this special solution system of linear equations. To capture all the other solutions, we need to be creative in generating 0 in a non-trivial way using the columns of the matrix: Adding 0 to our special solution does not change the special solution. To do so, we express the third column using the ﬁrst two columns (which are of this very simple form) [ 8 2 ] = 8 [ 1 0 ] + 2 [ 0 1 ] (2.40) c⃝2019 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press. 28 Linear Algebra so that 0 = 8c1 + 2c2 − 1c3 + 0c4 and (x1, x2, x3, x4) = (8, 2, −1, 0). In fact, any scaling of this solution by λ1 ∈ R produces the 0 vector, i.e., [ 1 0 8 −4 0 1 2 12 ]    λ1     8 2 −1 0         = λ1(8c1 + 2c2 − c3) = 0 . (2.41) Following the same line of reasoning, we express the fourth column of the matrix in (2.38) using the ﬁrst two columns and generate another set of non-trivial versions of 0 as [ 1 0 8 −4 0 1 2 12 ]    λ2     −4 12 0 −1         = λ2(−4c1 + 12c2 − c4) = 0 (2.42) for any λ2 ∈ R. Putting everything together, we obtain all solutions of the equation system in (2.38), which is called the general solution, as the setgeneral solution    x ∈ R 4 : x =     42 8 0 0     + λ1     8 2 −1 0     + λ2     −4 12 0 −1     , λ1, λ2 ∈ R    . (2.43) Remark. The general approach we followed consisted of the following three steps: 1. Find a particular solution to Ax = b. 2. Find all solutions to Ax = 0. 3. Combine the solutions from steps 1. and 2. to the general solution. Neither the general nor the particular solution is unique. ♦ The system of linear equations in the preceding example was easy to solve because the matrix in (2.38) has this particularly convenient form, which allowed us to ﬁnd the particular and the general solution by in- spection. However, general equation systems are not of this simple form. Fortunately, there exists a constructive algorithmic way of transforming any system of linear equations into this particularly simple form: Gaussian elimination. Key to Gaussian elimination are elementary transformations of systems of linear equations, which transform the equation system into a simple form. Then, we can apply the three steps to the simple form that we just discussed in the context of the example in (2.38). 2.3.2 Elementary Transformations Key to solving a system of linear equations are elementary transformationselementary transformations that keep the solution set the same, but that transform the equation system into a simpler form: Draft (2019-12-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com. 2.3 Solving Systems of Linear Equations 29 Exchange of two equations (rows in the matrix representing the system of equations) Multiplication of an equation (row) with a constant λ ∈ R\\{0} Addition of two equations (rows) Example 2.6 For a ∈ R, we seek all solutions of the following system of equations: −2x1 + 4x2 − 2x3 − x4 + 4x5 = −3 4x1 − 8x2 + 3x3 − 3x4 + x5 = 2 x1 − 2x2 + x3 − x4 + x5 = 0 x1 − 2x2 − 3x4 + 4x5 = a . (2.44) We start by converting this system of equations into the compact matrix notation Ax = b. We no longer mention the variables x explicitly and build the augmented matrix (in the form [ A | b] ) augmented matrix     −2 4 −2 −1 4 −3 4 −8 3 −3 1 2 1 −2 1 −1 1 0 1 −2 0 −3 4 a     Swap with R3 Swap with R1 where we used the vertical line to separate the left-hand side from the right-hand side in (2.44). We use ⇝ to indicate a transformation of the augmented matrix using elementary transformations. The augmented matrix [A | b ] compactly represents the system of linear equations Ax = b. Swapping Rows 1 and 3 leads to     1 −2 1 −1 1 0 4 −8 3 −3 1 2 −2 4 −2 −1 4 −3 1 −2 0 −3 4 a     −4R1 +2R1 −R1 When we now apply the indicated transformations (e.g., subtract Row 1 four times from Row 2), we obtain     1 −2 1 −1 1 0 0 0 −1 1 −3 2 0 0 0 −3 6 −3 0 0 −1 −2 3 a     −R2 − R3 ⇝     1 −2 1 −1 1 0 0 0 −1 1 −3 2 0 0 0 −3 6 −3 0 0 0 0 0 a + 1     ·(−1) ·(− 1 3 ) ⇝     1 −2 1 −1 1 0 0 0 1 −1 3 −2 0 0 0 1 −2 1 0 0 0 0 0 a + 1     c⃝2019 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press. 30 Linear Algebra This (augmented) matrix is in a convenient form, the row-echelon formrow-echelon form (REF). Reverting this compact notation back into the explicit notation with the variables we seek, we obtain x1 − 2x2 + x3 − x4 + x5 = 0 x3 − x4 + 3x5 = −2 x4 − 2x5 = 1 0 = a + 1 . (2.45) Only for a = −1 this system can be solved. A particular solution isparticular solution       x1 x2 x3 x4 x5       =       2 0 −1 1 0       . (2.46) The general solution, which captures the set of all possible solutions, isgeneral solution    x ∈ R 5 : x =       2 0 −1 1 0       + λ1       2 1 0 0 0       + λ2       2 0 −1 2 1       , λ1, λ2 ∈ R    . (2.47) In the following, we will detail a constructive way to obtain a particular and general solution of a system of linear equations. Remark (Pivots and Staircase Structure). The leading coefﬁcient of a row (ﬁrst nonzero number from the left) is called the pivot and is alwayspivot strictly to the right of the pivot of the row above it. Therefore, any equa- tion system in row-echelon form always has a “staircase” structure. ♦ Deﬁnition 2.6 (Row-Echelon Form). A matrix is in row-echelon form ifrow-echelon form All rows that contain only zeros are at the bottom of the matrix; corre- spondingly, all rows that contain at least one nonzero element are on top of rows that contain only zeros. Looking at nonzero rows only, the ﬁrst nonzero number from the left (also called the pivot or the leading coefﬁcient) is always strictly to thepivot leading coefﬁcient right of the pivot of the row above it. In other texts, it is sometimes required that the pivot is 1. Remark (Basic and Free Variables). The variables corresponding to the pivots in the row-echelon form are called basic variables and the other basic variable variables are free variables. For example, in (2.45), x1, x3, x4 are basic free variable variables, whereas x2, x5 are free variables. ♦ Remark (Obtaining a Particular Solution). The row-echelon form makes Draft (2019-12-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com. 2.3 Solving Systems of Linear Equations 31 our lives easier when we need to determine a particular solution. To do this, we express the right-hand side of the equation system using the pivot columns, such that b = ∑P i=1 λipi, where pi, i = 1, . . . , P , are the pivot columns. The λi are determined easiest if we start with the rightmost pivot column and work our way to the left. In the previous example, we would try to ﬁnd λ1, λ2, λ3 so that λ1     1 0 0 0     + λ2     1 1 0 0     + λ3     −1 −1 1 0     =     0 −2 1 0     . (2.48) From here, we ﬁnd relatively directly that λ3 = 1, λ2 = −1, λ1 = 2. When we put everything together, we must not forget the non-pivot columns for which we set the coefﬁcients implicitly to 0. Therefore, we get the particular solution x = [2, 0, −1, 1, 0] ⊤. ♦ Remark (Reduced Row Echelon Form). An equation system is in reduced reduced row-echelon formrow-echelon form (also: row-reduced echelon form or row canonical form) if It is in row-echelon form. Every pivot is 1. The pivot is the only nonzero entry in its column. ♦ The reduced row-echelon form will play an important role later in Sec- tion 2.3.3 because it allows us to determine the general solution of a sys- tem of linear equations in a straightforward way. Gaussian eliminationRemark (Gaussian Elimination). Gaussian elimination is an algorithm that performs elementary transformations to bring a system of linear equations into reduced row-echelon form. ♦ Example 2.7 (Reduced Row Echelon Form) Verify that the following matrix is in reduced row-echelon form (the pivots are in bold): A =   1 3 0 0 3 0 0 1 0 9 0 0 0 1 −4   . (2.49) The key idea for ﬁnding the solutions of Ax = 0 is to look at the non- pivot columns, which we will need to express as a (linear) combination of the pivot columns. The reduced row echelon form makes this relatively straightforward, and we express the non-pivot columns in terms of sums and multiples of the pivot columns that are on their left: The second col- umn is 3 times the ﬁrst column (we can ignore the pivot columns on the right of the second column). Therefore, to obtain 0, we need to subtract c⃝2019 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press. 32 Linear Algebra the second column from three times the ﬁrst column. Now, we look at the ﬁfth column, which is our second non-pivot column. The ﬁfth column can be expressed as 3 times the ﬁrst pivot column, 9 times the second pivot column, and −4 times the third pivot column. We need to keep track of the indices of the pivot columns and translate this into 3 times the ﬁrst col- umn, 0 times the second column (which is a non-pivot column), 9 times the third column (which is our second pivot column), and −4 times the fourth column (which is the third pivot column). Then we need to subtract the ﬁfth column to obtain 0. In the end, we are still solving a homogeneous equation system. To summarize, all solutions of Ax = 0, x ∈ R5 are given by    x ∈ R5 : x = λ1       3 −1 0 0 0       + λ2       3 0 9 −4 −1       , λ1, λ2 ∈ R    . (2.50) 2.3.3 The Minus-1 Trick In the following, we introduce a practical trick for reading out the solu- tions x of a homogeneous system of linear equations Ax = 0, where A ∈ R k×n, x ∈ R n. To start, we assume that A is in reduced row-echelon form without any rows that just contain zeros, i.e., A =          0 · · · 0 1 ∗ · · · ∗ 0 ∗ · · · ∗ 0 ∗ · · · ∗ ... ... 0 0 · · · 0 1 ∗ · · · ∗ ... ... ... ... ... ... ... ... 0 ... ... ... ... ... ... ... ... ... ... ... ... ... 0 ... ... 0 · · · 0 0 0 · · · 0 0 0 · · · 0 1 ∗ · · · ∗          , (2.51) where ∗ can be an arbitrary real number, with the constraints that the ﬁrst nonzero entry per row must be 1 and all other entries in the corresponding column must be 0. The columns j1, . . . , jk with the pivots (marked in bold) are the standard unit vectors e1, . . . , ek ∈ R k. We extend this matrix to an n × n-matrix ˜A by adding n − k rows of the form [ 0 · · · 0 −1 0 · · · 0 ] (2.52) so that the diagonal of the augmented matrix ˜A contains either 1 or −1. Then, the columns of ˜A that contain the −1 as pivots are solutions of Draft (2019-12-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com. 2.3 Solving Systems of Linear Equations 33 the homogeneous equation system Ax = 0. To be more precise, these columns form a basis (Section 2.6.1) of the solution space of Ax = 0, which we will later call the kernel or null space (see Section 2.7.3). kernel null space Example 2.8 (Minus-1 Trick) Let us revisit the matrix in (2.49), which is already in REF: A =   1 3 0 0 3 0 0 1 0 9 0 0 0 1 −4   . (2.53) We now augment this matrix to a 5 × 5 matrix by adding rows of the form (2.52) at the places where the pivots on the diagonal are missing and obtain ˜A =       1 3 0 0 3 0 −1 0 0 0 0 0 1 0 9 0 0 0 1 −4 0 0 0 0 −1       . (2.54) From this form, we can immediately read out the solutions of Ax = 0 by taking the columns of ˜A, which contain −1 on the diagonal:    x ∈ R5 : x = λ1       3 −1 0 0 0       + λ2       3 0 9 −4 −1       , λ1, λ2 ∈ R    , (2.55) which is identical to the solution in (2.50) that we obtained by “insight”. Calculating the Inverse To compute the inverse A−1 of A ∈ R n×n, we need to ﬁnd a matrix X that satisﬁes AX = I n. Then, X = A−1. We can write this down as a set of simultaneous linear equations AX = I n, where we solve for X = [x1| · · · |xn]. We use the augmented matrix notation for a compact representation of this set of systems of linear equations and obtain [ A|I n] ⇝ · · · ⇝ [ I n|A−1] . (2.56) This means that if we bring the augmented equation system into reduced row-echelon form, we can read out the inverse on the right-hand side of the equation system. Hence, determining the inverse of a matrix is equiv- alent to solving systems of linear equations. c⃝2019 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press. 34 Linear Algebra Example 2.9 (Calculating an Inverse Matrix by Gaussian Elimination) To determine the inverse of A =     1 0 2 0 1 1 0 0 1 2 0 1 1 1 1 1     (2.57) we write down the augmented matrix     1 0 2 0 1 0 0 0 1 1 0 0 0 1 0 0 1 2 0 1 0 0 1 0 1 1 1 1 0 0 0 1     and use Gaussian elimination to bring it into reduced row-echelon form     1 0 0 0 −1 2 −2 2 0 1 0 0 1 −1 2 −2 0 0 1 0 1 −1 1 −1 0 0 0 1 −1 0 −1 2     , such that the desired inverse is given as its right-hand side: A−1 =     −1 2 −2 2 1 −1 2 −2 1 −1 1 −1 −1 0 −1 2     . (2.58) We can verify that (2.58) is indeed the inverse by performing the multi- plication AA−1 and observing that we recover I 4. 2.3.4 Algorithms for Solving a System of Linear Equations In the following, we brieﬂy discuss approaches to solving a system of lin- ear equations of the form Ax = b. We make the assumption that a solu- tion exists. Should there be no solution, we need to resort to approximate solutions, which we do not cover in this chapter. One way to solve the ap- proximate problem is using the approach of linear regression, which we discuss in detail in Chapter 9. In special cases, we may be able to determine the inverse A−1, such that the solution of Ax = b is given as x = A−1b. However, this is only possible if A is a square matrix and invertible, which is often not the case. Otherwise, under mild assumptions (i.e., A needs to have linearly independent columns) we can use the transformation Ax = b ⇐⇒ A ⊤Ax = A ⊤b ⇐⇒ x = (A⊤A) −1A⊤b (2.59) Draft (2019-12-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com. 2.4 Vector Spaces 35 and use the Moore-Penrose pseudo-inverse (A⊤A)−1A⊤ to determine the Moore-Penrose pseudo-inversesolution (2.59) that solves Ax = b, which also corresponds to the mini- mum norm least-squares solution. A disadvantage of this approach is that it requires many computations for the matrix-matrix product and comput- ing the inverse of A⊤A. Moreover, for reasons of numerical precision it is generally not recommended to compute the inverse or pseudo-inverse. In the following, we therefore brieﬂy discuss alternative approaches to solving systems of linear equations. Gaussian elimination plays an important role when computing deter- minants (Section 4.1), checking whether a set of vectors is linearly inde- pendent (Section 2.5), computing the inverse of a matrix (Section 2.2.2), computing the rank of a matrix (Section 2.6.2), and determining a basis of a vector space (Section 2.6.1). Gaussian elimination is an intuitive and constructive way to solve a system of linear equations with thousands of variables. However, for systems with millions of variables, it is impracti- cal as the required number of arithmetic operations scales cubically in the number of simultaneous equations. In practice, systems of many linear equations are solved indirectly, by ei- ther stationary iterative methods, such as the Richardson method, the Ja- cobi method, the Gauß-Seidel method, and the successive over-relaxation method, or Krylov subspace methods, such as conjugate gradients, gener- alized minimal residual, or biconjugate gradients. We refer to the books by Stoer and Burlirsch (2002), Strang (2003), and Liesen and Mehrmann (2015) for further details. Let x∗ be a solution of Ax = b. The key idea of these iterative methods is to set up an iteration of the form x(k+1) = Cx (k) + d (2.60) for suitable C and d that reduces the residual error ∥x (k+1) − x∗∥ in every iteration and converges to x∗. We will introduce norms ∥ · ∥, which allow us to compute similarities between vectors, in Section 3.1. 2.4 Vector Spaces Thus far, we have looked at systems of linear equations and how to solve them (Section 2.3). We saw that systems of linear equations can be com- pactly represented using matrix-vector notation (2.10). In the following, we will have a closer look at vector spaces, i.e., a structured space in which vectors live. In the beginning of this chapter, we informally characterized vectors as objects that can be added together and multiplied by a scalar, and they remain objects of the same type. Now, we are ready to formalize this, and we will start by introducing the concept of a group, which is a set of elements and an operation deﬁned on these elements that keeps some structure of the set intact. c⃝2019 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press. 36 Linear Algebra 2.4.1 Groups Groups play an important role in computer science. Besides providing a fundamental framework for operations on sets, they are heavily used in cryptography, coding theory, and graphics. Deﬁnition 2.7 (Group). Consider a set G and an operation ⊗ : G ×G → G deﬁned on G. Then G := (G, ⊗) is called a group if the following hold:group closure 1. Closure of G under ⊗: ∀x, y ∈ G : x ⊗ y ∈ G associativity 2. Associativity: ∀x, y, z ∈ G : (x ⊗ y) ⊗ z = x ⊗ (y ⊗ z)neutral element 3. Neutral element: ∃e ∈ G ∀x ∈ G : x ⊗ e = x and e ⊗ x = xinverse element 4. Inverse element: ∀x ∈ G ∃y ∈ G : x ⊗ y = e and y ⊗ x = e. We often write x−1 to denote the inverse element of x. Remark. The inverse element is deﬁned with respect to the operation ⊗ and does not necessarily mean 1 x . ♦ If additionally ∀x, y ∈ G : x ⊗ y = y ⊗ x, then G = (G, ⊗) is an AbelianAbelian group group (commutative). Example 2.10 (Groups) Let us have a look at some examples of sets with associated operations and see whether they are groups: (Z, +) is a group. (N0, +) is not a group: Although (N0, +) possesses a neutral elementN0 := N ∪ {0} (0), the inverse elements are missing. (Z, ·) is not a group: Although (Z, ·) contains a neutral element (1), the inverse elements for any z ∈ Z, z ̸= ±1, are missing. (R, ·) is not a group since 0 does not possess an inverse element. (R\\{0}, ·) is Abelian. (R n, +), (Z n, +), n ∈ N are Abelian if + is deﬁned componentwise, i.e., (x1, · · · , xn) + (y1, · · · , yn) = (x1 + y1, · · · , xn + yn). (2.61) Then, (x1, · · · , xn)−1 := (−x1, · · · , −xn) is the inverse element and e = (0, · · · , 0) is the neutral element. (R m×n, +), the set of m × n-matrices is Abelian (with componentwise addition as deﬁned in (2.61)). Let us have a closer look at (Rn×n, ·), i.e., the set of n × n-matrices with matrix multiplication as deﬁned in (2.13). – Closure and associativity follow directly from the deﬁnition of matrix multiplication. – Neutral element: The identity matrix I n is the neutral element with respect to matrix multiplication “·” in (R n×n, ·). Draft (2019-12-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com. 2.4 Vector Spaces 37 – Inverse element: If the inverse exists (A is regular), then A−1 is the inverse element of A ∈ R n×n, and in exactly this case (R n×n, ·) is a group, called the general linear group. Deﬁnition 2.8 (General Linear Group). The set of regular (invertible) matrices A ∈ R n×n is a group with respect to matrix multiplication as deﬁned in (2.13) and is called general linear group GL(n, R). However, general linear group since matrix multiplication is not commutative, the group is not Abelian. 2.4.2 Vector Spaces When we discussed groups, we looked at sets G and inner operations on G, i.e., mappings G × G → G that only operate on elements in G. In the following, we will consider sets that in addition to an inner operation + also contain an outer operation ·, the multiplication of a vector x ∈ G by a scalar λ ∈ R. We can think of the inner operation as a form of addition, and the outer operation as a form of scaling. Note that the inner/outer operations have nothing to do with inner/outer products. Deﬁnition 2.9 (Vector Space). A real-valued vector space V = (V, +, ·) is vector space a set V with two operations + : V × V → V (2.62) · : R × V → V (2.63) where 1. (V, +) is an Abelian group 2. Distributivity: 1. ∀λ ∈ R, x, y ∈ V : λ · (x + y) = λ · x + λ · y 2. ∀λ, ψ ∈ R, x ∈ V : (λ + ψ) · x = λ · x + ψ · x 3. Associativity (outer operation): ∀λ, ψ ∈ R, x ∈ V : λ·(ψ ·x) = (λψ)·x 4. Neutral element with respect to the outer operation: ∀x ∈ V : 1·x = x The elements x ∈ V are called vectors. The neutral element of (V, +) is vector the zero vector 0 = [0, . . . , 0] ⊤, and the inner operation + is called vector vector addition addition. The elements λ ∈ R are called scalars and the outer operation scalar · is a multiplication by scalars. Note that a scalar product is something multiplication by scalarsdifferent, and we will get to this in Section 3.2. Remark. A “vector multiplication” ab, a, b ∈ R n, is not deﬁned. Theoret- ically, we could deﬁne an element-wise multiplication, such that c = ab with cj = ajbj. This “array multiplication” is common to many program- ming languages but makes mathematically limited sense using the stan- dard rules for matrix multiplication: By treating vectors as n × 1 matrices c⃝2019 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press. 38 Linear Algebra (which we usually do), we can use the matrix multiplication as deﬁned in (2.13). However, then the dimensions of the vectors do not match. Only the following multiplications for vectors are deﬁned: ab⊤ ∈ R n×n (outerouter product product), a ⊤b ∈ R (inner/scalar/dot product). ♦ Example 2.11 (Vector Spaces) Let us have a look at some important examples: V = Rn, n ∈ N is a vector space with operations deﬁned as follows: – Addition: x+y = (x1, . . . , xn)+(y1, . . . , yn) = (x1 +y1, . . . , xn +yn) for all x, y ∈ R n – Multiplication by scalars: λx = λ(x1, . . . , xn) = (λx1, . . . , λxn) for all λ ∈ R, x ∈ Rn V = Rm×n, m, n ∈ N is a vector space with – Addition: A + B =    a11 + b11 · · · a1n + b1n ... ... am1 + bm1 · · · amn + bmn    is deﬁned ele- mentwise for all A, B ∈ V – Multiplication by scalars: λA =    λa11 · · · λa1n ... ... λam1 · · · λamn    as deﬁned in Section 2.2. Remember that R m×n is equivalent to Rmn. V = C, with the standard deﬁnition of addition of complex numbers. Remark. In the following, we will denote a vector space (V, +, ·) by V when + and · are the standard vector addition and scalar multiplication. Moreover, we will use the notation x ∈ V for vectors in V to simplify notation. ♦ Remark. The vector spaces R n, R n×1, R 1×n are only different in the way we write vectors. In the following, we will not make a distinction between R n and Rn×1, which allows us to write n-tuples as column vectorscolumn vector x =    x1 ... xn    . (2.64) This simpliﬁes the notation regarding vector space operations. However, we do distinguish between R n×1 and R 1×n (the row vectors) to avoid con-row vector fusion with matrix multiplication. By default, we write x to denote a col- umn vector, and a row vector is denoted by x ⊤, the transpose of x. ♦transpose Draft (2019-12-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com. 2.4 Vector Spaces 39 2.4.3 Vector Subspaces In the following, we will introduce vector subspaces. Intuitively, they are sets contained in the original vector space with the property that when we perform vector space operations on elements within this subspace, we will never leave it. In this sense, they are “closed”. Vector subspaces are a key idea in machine learning. For example, Chapter 10 demonstrates how to use vector subspaces for dimensionality reduction. Deﬁnition 2.10 (Vector Subspace). Let V = (V, +, ·) be a vector space and U ⊆ V, U ̸= ∅. Then U = (U, +, ·) is called vector subspace of V (or vector subspace linear subspace) if U is a vector space with the vector space operations + linear subspace and · restricted to U × U and R × U . We write U ⊆ V to denote a subspace U of V . If U ⊆ V and V is a vector space, then U naturally inherits many prop- erties directly from V because they hold for all x ∈ V, and in particular for all x ∈ U ⊆ V. This includes the Abelian group properties, the distribu- tivity, the associativity and the neutral element. To determine whether (U, +, ·) is a subspace of V we still do need to show 1. U ̸= ∅, in particular: 0 ∈ U 2. Closure of U : a. With respect to the outer operation: ∀λ ∈ R ∀x ∈ U : λx ∈ U . b. With respect to the inner operation: ∀x, y ∈ U : x + y ∈ U . Example 2.12 (Vector Subspaces) Let us have a look at some examples: For every vector space V , the trivial subspaces are V itself and {0}. Only example D in Figure 2.6 is a subspace of R2 (with the usual inner/ outer operations). In A and C, the closure property is violated; B does not contain 0. The solution set of a homogeneous system of linear equations Ax = 0 with n unknowns x = [x1, . . . , xn] ⊤ is a subspace of Rn. The solution of an inhomogeneous system of linear equations Ax = b, b ̸= 0 is not a subspace of Rn. The intersection of arbitrarily many subspaces is a subspace itself. Figure 2.6 Not all subsets of R2 are subspaces. In A and C, the closure property is violated; B does not contain 0. Only D is a subspace. 0 0 0 0 A B C D c⃝2019 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press. 40 Linear Algebra Remark. Every subspace U ⊆ (R n, +, ·) is the solution space of a homo- geneous system of homogeneous linear equations Ax = 0 for x ∈ Rn. ♦ 2.5 Linear Independence In the following, we will have a close look at what we can do with vectors (elements of the vector space). In particular, we can add vectors together and multiply them with scalars. The closure property guarantees that we end up with another vector in the same vector space. It is possible to ﬁnd a set of vectors with which we can represent every vector in the vector space by adding them together and scaling them. This set of vectors is a basis, and we will discuss them in Section 2.6.1. Before we get there, we will need to introduce the concepts of linear combinations and linear independence. Deﬁnition 2.11 (Linear Combination). Consider a vector space V and a ﬁnite number of vectors x1, . . . , xk ∈ V . Then, every v ∈ V of the form v = λ1x1 + · · · + λkxk = k∑ i=1 λixi ∈ V (2.65) with λ1, . . . , λk ∈ R is a linear combination of the vectors x1, . . . , xk.linear combination The 0-vector can always be written as the linear combination of k vec- tors x1, . . . , xk because 0 = ∑k i=1 0xi is always true. In the following, we are interested in non-trivial linear combinations of a set of vectors to represent 0, i.e., linear combinations of vectors x1, . . . , xk, where not all coefﬁcients λi in (2.65) are 0. Deﬁnition 2.12 (Linear (In)dependence). Let us consider a vector space V with k ∈ N and x1, . . . , xk ∈ V . If there is a non-trivial linear com- bination, such that 0 = ∑k i=1 λixi with at least one λi ̸= 0, the vectors x1, . . . , xk are linearly dependent. If only the trivial solution exists, i.e.,linearly dependent λ1 = . . . = λk = 0 the vectors x1, . . . , xk are linearly independent.linearly independent Linear independence is one of the most important concepts in linear algebra. Intuitively, a set of linearly independent vectors consists of vectors that have no redundancy, i.e., if we remove any of those vectors from the set, we will lose something. Throughout the next sections, we will formalize this intuition more. Example 2.13 (Linearly Dependent Vectors) A geographic example may help to clarify the concept of linear indepen- dence. A person in Nairobi (Kenya) describing where Kigali (Rwanda) is Draft (2019-12-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com. 2.5 Linear Independence 41 might say ,“You can get to Kigali by ﬁrst going 506 km Northwest to Kam- pala (Uganda) and then 374 km Southwest.”. This is sufﬁcient information to describe the location of Kigali because the geographic coordinate sys- tem may be considered a two-dimensional vector space (ignoring altitude and the Earth’s curved surface). The person may add, “It is about 751 km West of here.” Although this last statement is true, it is not necessary to ﬁnd Kigali given the previous information (see Figure 2.7 for an illus- tration). In this example, the “506 km Northwest” vector (blue) and the “374 km Southwest” vector (purple) are linearly independent. This means the Southwest vector cannot be described in terms of the Northwest vec- tor, and vice versa. However, the third “751 km West” vector (black) is a linear combination of the other two vectors, and it makes the set of vec- tors linearly dependent. Equivalently, given “751 km West” and “374 km Southwest” can be linearly combined to obtain “506 km Northwest”. Figure 2.7 Geographic example (with crude approximations to cardinal directions) of linearly dependent vectors in a two-dimensional space (plane). 506 km Northwest751 km West374 km Southwest374 km SouthwestKampalaNairobiKigali Remark. The following properties are useful to ﬁnd out whether vectors are linearly independent: k vectors are either linearly dependent or linearly independent. There is no third option. If at least one of the vectors x1, . . . , xk is 0 then they are linearly de- pendent. The same holds if two vectors are identical. The vectors {x1, . . . , xk : xi ̸= 0, i = 1, . . . , k}, k ⩾ 2, are linearly dependent if and only if (at least) one of them is a linear combination of the others. In particular, if one vector is a multiple of another vector, i.e., xi = λxj, λ ∈ R then the set {x1, . . . , xk : xi ̸= 0, i = 1, . . . , k} is linearly dependent. A practical way of checking whether vectors x1, . . . , xk ∈ V are linearly independent is to use Gaussian elimination: Write all vectors as columns c⃝2019 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press. 42 Linear Algebra of a matrix A and perform Gaussian elimination until the matrix is in row echelon form (the reduced row-echelon form is unnecessary here): – The pivot columns indicate the vectors, which are linearly indepen- dent of the vectors on the left. Note that there is an ordering of vec- tors when the matrix is built. – The non-pivot columns can be expressed as linear combinations of the pivot columns on their left. For instance, the row-echelon form [ 1 3 0 0 0 2 ] (2.66) tells us that the ﬁrst and third columns are pivot columns. The sec- ond column is a non-pivot column because it is three times the ﬁrst column. All column vectors are linearly independent if and only if all columns are pivot columns. If there is at least one non-pivot column, the columns (and, therefore, the corresponding vectors) are linearly dependent. ♦ Example 2.14 Consider R4 with x1 =     1 2 −3 4     , x2 =     1 1 0 2     , x3 =     −1 −2 1 1     . (2.67) To check whether they are linearly dependent, we follow the general ap- proach and solve λ1x1 + λ2x2 + λ3x3 = λ1     1 2 −3 4     + λ2     1 1 0 2     + λ3     −1 −2 1 1     = 0 (2.68) for λ1, . . . , λ3. We write the vectors xi, i = 1, 2, 3, as the columns of a matrix and apply elementary row operations until we identify the pivot columns:     1 1 −1 2 1 −2 −3 0 1 4 2 1     ⇝ · · · ⇝     1 1 −1 0 1 0 0 0 1 0 0 0     . (2.69) Here, every column of the matrix is a pivot column. Therefore, there is no non-trivial solution, and we require λ1 = 0, λ2 = 0, λ3 = 0 to solve the equation system. Hence, the vectors x1, x2, x3 are linearly independent. Draft (2019-12-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com. 2.5 Linear Independence 43 Remark. Consider a vector space V with k linearly independent vectors b1, . . . , bk and m linear combinations x1 = k∑ i=1 λi1bi , ... xm = k∑ i=1 λimbi . (2.70) Deﬁning B = [b1, . . . , bk] as the matrix whose columns are the linearly independent vectors b1, . . . , bk, we can write xj = Bλj , λj =    λ1j ... λkj    , j = 1, . . . , m , (2.71) in a more compact form. We want to test whether x1, . . . , xm are linearly independent. For this purpose, we follow the general approach of testing when ∑m j=1 ψjxj = 0. With (2.71), we obtain m∑ j=1 ψjxj = m∑ j=1 ψjBλj = B m∑ j=1 ψjλj . (2.72) This means that {x1, . . . , xm} are linearly independent if and only if the column vectors {λ1, . . . , λm} are linearly independent. ♦ Remark. In a vector space V , m linear combinations of k vectors x1, . . . , xk are linearly dependent if m > k. ♦ Example 2.15 Consider a set of linearly independent vectors b1, b2, b3, b4 ∈ R n and x1 = b1 − 2b2 + b3 − b4 x2 = −4b1 − 2b2 + 4b4 x3 = 2b1 + 3b2 − b3 − 3b4 x4 = 17b1 − 10b2 + 11b3 + b4 . (2.73) Are the vectors x1, . . . , x4 ∈ Rn linearly independent? To answer this question, we investigate whether the column vectors        1 −2 1 −1     ,     −4 −2 0 4     ,     2 3 −1 −3     ,     17 −10 11 1        (2.74) c⃝2019 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press. 44 Linear Algebra are linearly independent. The reduced row-echelon form of the corre- sponding linear equation system with coefﬁcient matrix A =     1 −4 2 17 −2 −2 3 −10 1 0 −1 11 −1 4 −3 1     (2.75) is given as     1 0 0 −7 0 1 0 −15 0 0 1 −18 0 0 0 0     . (2.76) We see that the corresponding linear equation system is non-trivially solv- able: The last column is not a pivot column, and x4 = −7x1−15x2−18x3. Therefore, x1, . . . , x4 are linearly dependent as x4 can be expressed as a linear combination of x1, . . . , x3. 2.6 Basis and Rank In a vector space V , we are particularly interested in sets of vectors A that possess the property that any vector v ∈ V can be obtained by a linear combination of vectors in A. These vectors are special vectors, and in the following, we will characterize them. 2.6.1 Generating Set and Basis Deﬁnition 2.13 (Generating Set and Span). Consider a vector space V = (V, +, ·) and set of vectors A = {x1, . . . , xk} ⊆ V. If every vector v ∈ V can be expressed as a linear combination of x1, . . . , xk, A is called a generating set of V . The set of all linear combinations of vectors in A isgenerating set called the span of A. If A spans the vector space V , we write V = span[A]span or V = span[x1, . . . , xk]. Generating sets are sets of vectors that span vector (sub)spaces, i.e., every vector can be represented as a linear combination of the vectors in the generating set. Now, we will be more speciﬁc and characterize the smallest generating set that spans a vector (sub)space. Deﬁnition 2.14 (Basis). Consider a vector space V = (V, +, ·) and A ⊆ V. A generating set A of V is called minimal if there exists no smaller setminimal ˜A ⊆ A ⊆ V that spans V . Every linearly independent generating set of V is minimal and is called a basis of V .basis Draft (2019-12-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com. 2.6 Basis and Rank 45 Let V = (V, +, ·) be a vector space and B ⊆ V, B ̸= ∅. Then, the following statements are equivalent: A basis is a minimal generating set and a maximal linearly independent set of vectors. B is a basis of V . B is a minimal generating set. B is a maximal linearly independent set of vectors in V , i.e., adding any other vector to this set will make it linearly dependent. Every vector x ∈ V is a linear combination of vectors from B, and every linear combination is unique, i.e., with x = k∑ i=1 λibi = k∑ i=1 ψibi (2.77) and λi, ψi ∈ R, bi ∈ B it follows that λi = ψi, i = 1, . . . , k. Example 2.16 In R3, the canonical/standard basis is canonical basis B =      1 0 0   ,   0 1 0   ,   0 0 1      . (2.78) Different bases in R 3 are B1 =      1 0 0   ,   1 1 0   ,   1 1 1      , B2 =      0.5 0.8 0.4   ,   1.8 0.3 0.3   ,   −2.2 −1.3 3.5      . (2.79) The set A =        1 2 3 4     ,     2 −1 0 2     ,     1 1 0 −4        (2.80) is linearly independent, but not a generating set (and no basis) of R 4: For instance, the vector [1, 0, 0, 0] ⊤ cannot be obtained by a linear com- bination of elements in A. Remark. Every vector space V possesses a basis B. The preceding exam- ples show that there can be many bases of a vector space V , i.e., there is no unique basis. However, all bases possess the same number of elements, the basis vectors. ♦ basis vector We only consider ﬁnite-dimensional vector spaces V . In this case, the dimension of V is the number of basis vectors of V , and we write dim(V ). dimension If U ⊆ V is a subspace of V , then dim(U ) ⩽ dim(V ) and dim(U ) = c⃝2019 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press. 46 Linear Algebra dim(V ) if and only if U = V . Intuitively, the dimension of a vector space can be thought of as the number of independent directions in this vector space.The dimension of a vector space corresponds to the number of its basis vectors. Remark. The dimension of a vector space is not necessarily the number of elements in a vector. For instance, the vector space V = span[ [ 0 1 ] ] is one-dimensional, although the basis vector possesses two elements. ♦ Remark. A basis of a subspace U = span[x1, . . . , xm] ⊆ R n can be found by executing the following steps: 1. Write the spanning vectors as columns of a matrix A 2. Determine the row-echelon form of A. 3. The spanning vectors associated with the pivot columns are a basis of U . ♦ Example 2.17 (Determining a Basis) For a vector subspace U ⊆ R 5, spanned by the vectors x1 =       1 2 −1 −1 −1       , x2 =       2 −1 1 2 −2       , x3 =       3 −4 3 5 −3       , x4 =       −1 8 −5 −6 1       ∈ R 5, (2.81) we are interested in ﬁnding out which vectors x1, . . . , x4 are a basis for U . For this, we need to check whether x1, . . . , x4 are linearly independent. Therefore, we need to solve 4∑ i=1 λixi = 0 , (2.82) which leads to a homogeneous system of equations with matrix [ x1, x2, x3, x4] =       1 2 3 −1 2 −1 −4 8 −1 1 3 −5 −1 2 5 −6 −1 −2 −3 1       . (2.83) With the basic transformation rules for systems of linear equations, we obtain the row-echelon form       1 2 3 −1 2 −1 −4 8 −1 1 3 −5 −1 2 5 −6 −1 −2 −3 1       ⇝ · · · ⇝       1 2 3 −1 0 1 2 −2 0 0 0 1 0 0 0 0 0 0 0 0       . Draft (2019-12-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com. 2.6 Basis and Rank 47 Since the pivot columns indicate which set of vectors is linearly indepen- dent, we see from the row-echelon form that x1, x2, x4 are linearly inde- pendent (because the system of linear equations λ1x1 + λ2x2 + λ4x4 = 0 can only be solved with λ1 = λ2 = λ4 = 0). Therefore, {x1, x2, x4} is a basis of U . 2.6.2 Rank The number of linearly independent columns of a matrix A ∈ R m×n equals the number of linearly independent rows and is called the rank rank of A and is denoted by rk(A). Remark. The rank of a matrix has some important properties: rk(A) = rk(A⊤), i.e., the column rank equals the row rank. The columns of A ∈ Rm×n span a subspace U ⊆ R m with dim(U ) = rk(A). Later we will call this subspace the image or range. A basis of U can be found by applying Gaussian elimination to A to identify the pivot columns. The rows of A ∈ R m×n span a subspace W ⊆ R n with dim(W ) = rk(A). A basis of W can be found by applying Gaussian elimination to A⊤. For all A ∈ Rn×n it holds that A is regular (invertible) if and only if rk(A) = n. For all A ∈ Rm×n and all b ∈ R m it holds that the linear equation system Ax = b can be solved if and only if rk(A) = rk(A|b), where A|b denotes the augmented system. For A ∈ R m×n the subspace of solutions for Ax = 0 possesses dimen- sion n − rk(A). Later, we will call this subspace the kernel or the null kernel null spacespace. A matrix A ∈ R m×n has full rank if its rank equals the largest possible full rank rank for a matrix of the same dimensions. This means that the rank of a full-rank matrix is the lesser of the number of rows and columns, i.e., rk(A) = min(m, n). A matrix is said to be rank deﬁcient if it does not rank deﬁcient have full rank. ♦ Example 2.18 (Rank) A =   1 0 1 0 1 1 0 0 0  . A has two linearly independent rows/columns so that rk(A) = 2. c⃝2019 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press. 48 Linear Algebra A =   1 2 1 −2 −3 1 3 5 0   . We use Gaussian elimination to determine the rank:   1 2 1 −2 −3 1 3 5 0   ⇝ · · · ⇝   1 2 1 0 1 3 0 0 0   . (2.84) Here, we see that the number of linearly independent rows and columns is 2, such that rk(A) = 2. 2.7 Linear Mappings In the following, we will study mappings on vector spaces that preserve their structure, which will allow us to deﬁne the concept of a coordinate. In the beginning of the chapter, we said that vectors are objects that can be added together and multiplied by a scalar, and the resulting object is still a vector. We wish to preserve this property when applying the mapping: Consider two real vector spaces V, W . A mapping Φ : V → W preserves the structure of the vector space if Φ(x + y) = Φ(x) + Φ(y) (2.85) Φ(λx) = λΦ(x) (2.86) for all x, y ∈ V and λ ∈ R. We can summarize this in the following deﬁnition: Deﬁnition 2.15 (Linear Mapping). For vector spaces V, W , a mapping Φ : V → W is called a linear mapping (or vector space homomorphism/linear mapping vector space homomorphism linear transformation) if linear transformation ∀x, y ∈ V ∀λ, ψ ∈ R : Φ(λx + ψy) = λΦ(x) + ψΦ(y) . (2.87) It turns out that we can represent linear mappings as matrices (Sec- tion 2.7.1). Recall that we can also collect a set of vectors as columns of a matrix. When working with matrices, we have to keep in mind what the matrix represents: a linear mapping or a collection of vectors. We will see more about linear mappings in Chapter 4. Before we continue, we will brieﬂy introduce special mappings. Deﬁnition 2.16 (Injective, Surjective, Bijective). Consider a mapping Φ : V → W, where V, W can be arbitrary sets. Then Φ is called injective Injective if ∀x, y ∈ V : Φ(x) = Φ(y) =⇒ x = y.surjective Surjective if Φ(V) = W.bijective Bijective if it is injective and surjective. Draft (2019-12-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com. 2.7 Linear Mappings 49 If Φ is surjective, then every element in W can be “reached” from V using Φ. A bijective Φ can be “undone”, i.e., there exists a mapping Ψ : W → V so that Ψ ◦ Φ(x) = x. This mapping Ψ is then called the inverse of Φ and normally denoted by Φ −1. With these deﬁnitions, we introduce the following special cases of linear mappings between vector spaces V and W : isomorphism Isomorphism: Φ : V → W linear and bijective endomorphism Endomorphism: Φ : V → V linear automorphism Automorphism: Φ : V → V linear and bijective We deﬁne idV : V → V , x ↦→ x as the identity mapping or identity identity mapping identity automorphism automorphism in V . Example 2.19 (Homomorphism) The mapping Φ : R2 → C, Φ(x) = x1 + ix2, is a homomorphism: Φ ([ x1 x2 ] + [ y1 y2 ]) = (x1 + y1) + i(x2 + y2) = x1 + ix2 + y1 + iy2 = Φ ([x1 x2 ]) + Φ ([y1 y2 ]) Φ (λ [ x1 x2 ]) = λx1 + λix2 = λ(x1 + ix2) = λΦ ([ x1 x2 ]) . (2.88) This also justiﬁes why complex numbers can be represented as tuples in R 2: There is a bijective linear mapping that converts the elementwise addi- tion of tuples in R 2 into the set of complex numbers with the correspond- ing addition. Note that we only showed linearity, but not the bijection. Theorem 2.17 (Theorem 3.59 in Axler (2015)). Finite-dimensional vector spaces V and W are isomorphic if and only if dim(V ) = dim(W ). Theorem 2.17 states that there exists a linear, bijective mapping be- tween two vector spaces of the same dimension. Intuitively, this means that vector spaces of the same dimension are kind of the same thing, as they can be transformed into each other without incurring any loss. Theorem 2.17 also gives us the justiﬁcation to treat R m×n (the vector space of m × n-matrices) and Rmn (the vector space of vectors of length mn) the same, as their dimensions are mn, and there exists a linear, bi- jective mapping that transforms one into the other. Remark. Consider vector spaces V, W, X. Then: For linear mappings Φ : V → W and Ψ : W → X, the mapping Ψ ◦ Φ : V → X is also linear. If Φ : V → W is an isomorphism, then Φ−1 : W → V is an isomor- phism, too. c⃝2019 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press. 50 Linear Algebra Figure 2.8 Two different coordinate systems deﬁned by two sets of basis vectors. A vector x has different coordinate representations depending on which coordinate system is chosen. x x e1 e2 b1 b2 If Φ : V → W, Ψ : V → W are linear, then Φ + Ψ and λΦ, λ ∈ R, are linear, too. ♦ 2.7.1 Matrix Representation of Linear Mappings Any n-dimensional vector space is isomorphic to Rn (Theorem 2.17). We consider a basis {b1, . . . , bn} of an n-dimensional vector space V . In the following, the order of the basis vectors will be important. Therefore, we write B = (b1, . . . , bn) (2.89) and call this n-tuple an ordered basis of V .ordered basis Remark (Notation). We are at the point where notation gets a bit tricky. Therefore, we summarize some parts here. B = (b1, . . . , bn) is an ordered basis, B = {b1, . . . , bn} is an (unordered) basis, and B = [b1, . . . , bn] is a matrix whose columns are the vectors b1, . . . , bn. ♦ Deﬁnition 2.18 (Coordinates). Consider a vector space V and an ordered basis B = (b1, . . . , bn) of V . For any x ∈ V we obtain a unique represen- tation (linear combination) x = α1b1 + . . . + αnbn (2.90) of x with respect to B. Then α1, . . . , αn are the coordinates of x withcoordinate respect to B, and the vector α =    α1 ... αn    ∈ Rn (2.91) is the coordinate vector/coordinate representation of x with respect to thecoordinate vector coordinate representation ordered basis B. Draft (2019-12-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com. 2.7 Linear Mappings 51 A basis effectively deﬁnes a coordinate system. We are familiar with the Cartesian coordinate system in two dimensions, which is spanned by the canonical basis vectors e1, e2. In this coordinate system, a vector x ∈ R 2 has a representation that tells us how to linearly combine e1 and e2 to obtain x. However, any basis of R2 deﬁnes a valid coordinate system, and the same vector x from before may have a different coordinate rep- resentation in the (b1, b2) basis. In Figure 2.8, the coordinates of x with respect to the standard basis (e1, e2) is [2, 2] ⊤. However, with respect to the basis (b1, b2) the same vector x is represented as [1.09, 0.72] ⊤, i.e., x = 1.09b1 + 0.72b2. In the following sections, we will discover how to obtain this representation. Example 2.20 Let us have a look at a geometric vector x ∈ R 2 with coordinates [2, 3] ⊤ Figure 2.9 Different coordinate representations of a vector x, depending on the choice of basis. e1 e2 b2 b1 x = −1 2b1 + 5 2b2 x = 2e1 + 3e2 with respect to the standard basis (e1, e2) of R 2. This means, we can write x = 2e1 + 3e2. However, we do not have to choose the standard basis to represent this vector. If we use the basis vectors b1 = [1, −1] ⊤, b2 = [1, 1] ⊤ we will obtain the coordinates 1 2 [−1, 5] ⊤ to represent the same vector with respect to (b1, b2) (see Figure 2.9). Remark. For an n-dimensional vector space V and an ordered basis B of V , the mapping Φ : R n → V , Φ(ei) = bi, i = 1, . . . , n, is linear (and because of Theorem 2.17 an isomorphism), where (e1, . . . , en) is the standard basis of R n. ♦ Now we are ready to make an explicit connection between matrices and linear mappings between ﬁnite-dimensional vector spaces. Deﬁnition 2.19 (Transformation Matrix). Consider vector spaces V, W with corresponding (ordered) bases B = (b1, . . . , bn) and C = (c1, . . . , cm). Moreover, we consider a linear mapping Φ : V → W . For j ∈ {1, . . . , n}, Φ(bj) = α1jc1 + · · · + αmjcm = m∑ i=1 αijci (2.92) is the unique representation of Φ(bj) with respect to C. Then, we call the m × n-matrix AΦ, whose elements are given by AΦ(i, j) = αij , (2.93) the transformation matrix of Φ (with respect to the ordered bases B of V transformation matrixand C of W ). The coordinates of Φ(bj) with respect to the ordered basis C of W are the j-th column of AΦ. Consider (ﬁnite-dimensional) vector spaces V, W with ordered bases B, C and a linear mapping Φ : V → W with c⃝2019 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press. 52 Linear Algebra transformation matrix AΦ. If ˆx is the coordinate vector of x ∈ V with respect to B and ˆy the coordinate vector of y = Φ(x) ∈ W with respect to C, then ˆy = AΦ ˆx . (2.94) This means that the transformation matrix can be used to map coordinates with respect to an ordered basis in V to coordinates with respect to an ordered basis in W . Example 2.21 (Transformation Matrix) Consider a homomorphism Φ : V → W and ordered bases B = (b1, . . . , b3) of V and C = (c1, . . . , c4) of W . With Φ(b1) = c1 − c2 + 3c3 − c4 Φ(b2) = 2c1 + c2 + 7c3 + 2c4 Φ(b3) = 3c2 + c3 + 4c4 (2.95) the transformation matrix AΦ with respect to B and C satisﬁes Φ(bk) =∑4 i=1 αikci for k = 1, . . . , 3 and is given as AΦ = [α1, α2, α3] =     1 2 0 −1 1 3 3 7 1 −1 2 4     , (2.96) where the αj, j = 1, 2, 3, are the coordinate vectors of Φ(bj) with respect to C. Example 2.22 (Linear Transformations of Vectors) Figure 2.10 Three examples of linear transformations of the vectors shown as dots in (a); (b) Rotation by 45◦; (c) Stretching of the horizontal coordinates by 2; (d) Combination of reﬂection, rotation and stretching. (a) Original data. (b) Rotation by 45◦. (c) Stretch along the horizontal axis. (d) General linear mapping. We consider three linear transformations of a set of vectors in R 2 with the transformation matrices A1 = [ cos( π 4 ) − sin( π 4 ) sin( π 4 ) cos( π 4 ) ] , A2 = [ 2 0 0 1 ] , A3 = 1 2 [ 3 −1 1 −1 ] . (2.97) Draft (2019-12-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com. 2.7 Linear Mappings 53 Figure 2.10 gives three examples of linear transformations of a set of vec- tors. Figure 2.10(a) shows 400 vectors in R 2, each of which is represented by a dot at the corresponding (x1, x2)-coordinates. The vectors are ar- ranged in a square. When we use matrix A1 in (2.97) to linearly transform each of these vectors, we obtain the rotated square in Figure 2.10(b). If we apply the linear mapping represented by A2, we obtain the rectangle in Figure 2.10(c) where each x1-coordinate is stretched by 2. Figure 2.10(d) shows the original square from Figure 2.10(a) when linearly transformed using A3, which is a combination of a reﬂection, a rotation, and a stretch. 2.7.2 Basis Change In the following, we will have a closer look at how transformation matrices of a linear mapping Φ : V → W change if we change the bases in V and W . Consider two ordered bases B = (b1, . . . , bn), ˜B = (˜b1, . . . , ˜bn) (2.98) of V and two ordered bases C = (c1, . . . , cm), ˜C = (˜c1, . . . , ˜cm) (2.99) of W . Moreover, AΦ ∈ R m×n is the transformation matrix of the linear mapping Φ : V → W with respect to the bases B and C, and ˜AΦ ∈ Rm×n is the corresponding transformation mapping with respect to ˜B and ˜C. In the following, we will investigate how A and ˜A are related, i.e., how/ whether we can transform AΦ into ˜AΦ if we choose to perform a basis change from B, C to ˜B, ˜C. Remark. We effectively get different coordinate representations of the identity mapping idV . In the context of Figure 2.9, this would mean to map coordinates with respect to (e1, e2) onto coordinates with respect to (b1, b2) without changing the vector x. By changing the basis and corre- spondingly the representation of vectors, the transformation matrix with respect to this new basis can have a particularly simple form that allows for straightforward computation. ♦ Example 2.23 (Basis Change) Consider a transformation matrix A = [ 2 1 1 2 ] (2.100) with respect to the canonical basis in R 2. If we deﬁne a new basis B = ( [ 1 1 ] , [ 1 −1 ] ) (2.101) c⃝2019 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press. 54 Linear Algebra we obtain a diagonal transformation matrix ˜A = [ 3 0 0 1 ] (2.102) with respect to B, which is easier to work with than A. In the following, we will look at mappings that transform coordinate vectors with respect to one basis into coordinate vectors with respect to a different basis. We will state our main result ﬁrst and then provide an explanation. Theorem 2.20 (Basis Change). For a linear mapping Φ : V → W , ordered bases B = (b1, . . . , bn), ˜B = (˜b1, . . . , ˜bn) (2.103) of V and C = (c1, . . . , cm), ˜C = (˜c1, . . . , ˜cm) (2.104) of W , and a transformation matrix AΦ of Φ with respect to B and C, the corresponding transformation matrix ˜AΦ with respect to the bases ˜B and ˜C is given as ˜AΦ = T −1AΦS . (2.105) Here, S ∈ Rn×n is the transformation matrix of idV that maps coordinates with respect to ˜B onto coordinates with respect to B, and T ∈ R m×m is the transformation matrix of idW that maps coordinates with respect to ˜C onto coordinates with respect to C. Proof Following Drumm and Weil (2001), we can write the vectors of the new basis ˜B of V as a linear combination of the basis vectors of B, such that ˜bj = s1jb1 + · · · + snjbn = n∑ i=1 sijbi , j = 1, . . . , n . (2.106) Similarly, we write the new basis vectors ˜C of W as a linear combination of the basis vectors of C, which yields ˜ck = t1kc1 + · · · + tmkcm = m∑ l=1 tlkcl , k = 1, . . . , m . (2.107) We deﬁne S = ((sij)) ∈ R n×n as the transformation matrix that maps coordinates with respect to ˜B onto coordinates with respect to B and T = ((tlk)) ∈ Rm×m as the transformation matrix that maps coordinates with respect to ˜C onto coordinates with respect to C. In particular, the jth column of S is the coordinate representation of ˜bj with respect to B and Draft (2019-12-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com. 2.7 Linear Mappings 55 the kth column of T is the coordinate representation of ˜ck with respect to C. Note that both S and T are regular. We are going to look at Φ(˜bj) from two perspectives. First, applying the mapping Φ, we get that for all j = 1, . . . , n Φ(˜bj) = m∑ k=1 ˜akj˜ck ︸ ︷︷ ︸ ∈W (2.107) = m∑ k=1 ˜akj m∑ l=1 tlkcl = m∑ l=1 ( m∑ k=1 tlk˜akj ) cl , (2.108) where we ﬁrst expressed the new basis vectors ˜ck ∈ W as linear com- binations of the basis vectors cl ∈ W and then swapped the order of summation. Alternatively, when we express the ˜bj ∈ V as linear combinations of bj ∈ V , we arrive at Φ(˜bj) (2.106) = Φ ( n∑ i=1 sijbi ) = n∑ i=1 sijΦ(bi) = n∑ i=1 sij m∑ l=1 alicl (2.109a) = m∑ l=1 ( n∑ i=1 alisij ) cl , j = 1, . . . , n , (2.109b) where we exploited the linearity of Φ. Comparing (2.108) and (2.109b), it follows for all j = 1, . . . , n and l = 1, . . . , m that m∑ k=1 tlk˜akj = n∑ i=1 alisij (2.110) and, therefore, T ˜AΦ = AΦS ∈ R m×n , (2.111) such that ˜AΦ = T −1AΦS , (2.112) which proves Theorem 2.20. Theorem 2.20 tells us that with a basis change in V (B is replaced with ˜B) and W (C is replaced with ˜C), the transformation matrix AΦ of a linear mapping Φ : V → W is replaced by an equivalent matrix ˜AΦ with ˜AΦ = T −1AΦS. (2.113) Figure 2.11 illustrates this relation: Consider a homomorphism Φ : V → W and ordered bases B, ˜B of V and C, ˜C of W . The mapping ΦCB is an instantiation of Φ and maps basis vectors of B onto linear combinations of basis vectors of C. Assume that we know the transformation matrix AΦ of ΦCB with respect to the ordered bases B, C. When we perform a basis change from B to ˜B in V and from C to ˜C in W , we can determine the c⃝2019 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press. 56 Linear Algebra Figure 2.11 For a homomorphism Φ : V → W and ordered bases B, ˜B of V and C, ˜C of W (marked in blue), we can express the mapping Φ ˜C ˜B with respect to the bases ˜B, ˜C equivalently as a composition of the homomorphisms Φ ˜C ˜B = Ξ ˜CC ◦ ΦCB ◦ ΨB ˜B with respect to the bases in the subscripts. The corresponding transformation matrices are in red. V W B ˜B ˜C C Φ ΦCB Φ ˜C ˜B ΨB ˜B ΞC ˜CS T ˜AΦ AΦ V W B ˜B ˜C C Φ ΦCB Φ ˜C ˜B ΨB ˜B Ξ ˜CC = Ξ −1 C ˜CS T −1 ˜AΦ AΦ Vector spaces Ordered bases corresponding transformation matrix ˜AΦ as follows: First, we ﬁnd the ma- trix representation of the linear mapping ΨB ˜B : V → V that maps coordi- nates with respect to the new basis ˜B onto the (unique) coordinates with respect to the “old” basis B (in V ). Then, we use the transformation ma- trix AΦ of ΦCB : V → W to map these coordinates onto the coordinates with respect to C in W . Finally, we use a linear mapping Ξ ˜CC : W → W to map the coordinates with respect to C onto coordinates with respect to ˜C. Therefore, we can express the linear mapping Φ ˜C ˜B as a composition of linear mappings that involve the “old” basis: Φ ˜C ˜B = Ξ ˜CC ◦ ΦCB ◦ ΨB ˜B = Ξ −1 C ˜C ◦ ΦCB ◦ ΨB ˜B . (2.114) Concretely, we use ΨB ˜B = idV and ΞC ˜C = idW , i.e., the identity mappings that map vectors onto themselves, but with respect to a different basis. Deﬁnition 2.21 (Equivalence). Two matrices A, ˜A ∈ R m×n are equivalentequivalent if there exist regular matrices S ∈ Rn×n and T ∈ R m×m, such that ˜A = T −1AS. Deﬁnition 2.22 (Similarity). Two matrices A, ˜A ∈ R n×n are similar ifsimilar there exists a regular matrix S ∈ Rn×n with ˜A = S−1AS Remark. Similar matrices are always equivalent. However, equivalent ma- trices are not necessarily similar. ♦ Remark. Consider vector spaces V, W, X. From the remark that follows Theorem 2.17, we already know that for linear mappings Φ : V → W and Ψ : W → X the mapping Ψ ◦ Φ : V → X is also linear. With transformation matrices AΦ and AΨ of the corresponding mappings, the overall transformation matrix is AΨ◦Φ = AΨAΦ. ♦ In light of this remark, we can look at basis changes from the perspec- tive of composing linear mappings: AΦ is the transformation matrix of a linear mapping ΦCB : V → W with respect to the bases B, C. ˜AΦ is the transformation matrix of the linear mapping Φ ˜C ˜B : V → W with respect to the bases ˜B, ˜C. S is the transformation matrix of a linear mapping ΨB ˜B : V → V (automorphism) that represents ˜B in terms of B. Normally, Ψ = idV is the identity mapping in V . Draft (2019-12-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com. 2.7 Linear Mappings 57 T is the transformation matrix of a linear mapping ΞC ˜C : W → W (automorphism) that represents ˜C in terms of C. Normally, Ξ = idW is the identity mapping in W . If we (informally) write down the transformations just in terms of bases, then AΦ : B → C, ˜AΦ : ˜B → ˜C, S : ˜B → B, T : ˜C → C and T −1 : C → ˜C, and ˜B → ˜C = ˜B → B→ C → ˜C (2.115) ˜AΦ = T −1AΦS . (2.116) Note that the execution order in (2.116) is from right to left because vec- tors are multiplied at the right-hand side so that x ↦→ Sx ↦→ AΦ(Sx) ↦→ T −1(AΦ(Sx) ) = ˜AΦx. Example 2.24 (Basis Change) Consider a linear mapping Φ : R 3 → R 4 whose transformation matrix is AΦ =     1 2 0 −1 1 3 3 7 1 −1 2 4     (2.117) with respect to the standard bases B = (   1 0 0   ,   0 1 0   ,   0 0 1  ) , C = (     1 0 0 0     ,     0 1 0 0     ,     0 0 1 0     ,     0 0 0 1    ). (2.118) We seek the transformation matrix ˜AΦ of Φ with respect to the new bases ˜B = (   1 1 0   ,   0 1 1   ,   1 0 1  ) ∈ R3, ˜C = (     1 1 0 0     ,     1 0 1 0     ,     0 1 1 0     ,     1 0 0 1    ) . (2.119) Then, S =   1 0 1 1 1 0 0 1 1   , T =     1 1 0 1 1 0 1 0 0 1 1 0 0 0 0 1     , (2.120) where the ith column of S is the coordinate representation of ˜bi in terms of the basis vectors of B. Since B is the standard basis, the co- ordinate representation is straightforward to ﬁnd. For a general basis B, we would need to solve a linear equation system to ﬁnd the λi such that c⃝2019 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press. 58 Linear Algebra ∑3 i=1 λibi = ˜bj, j = 1, . . . , 3. Similarly, the jth column of T is the coordi- nate representation of ˜cj in terms of the basis vectors of C. Therefore, we obtain ˜AΦ = T −1AΦS = 1 2     1 1 −1 −1 1 −1 1 −1 −1 1 1 1 0 0 0 2         3 2 1 0 4 2 10 8 4 1 6 3     (2.121a) =     −4 −4 −2 6 0 0 4 8 4 1 6 3     . (2.121b) In Chapter 4, we will be able to exploit the concept of a basis change to ﬁnd a basis with respect to which the transformation matrix of an en- domorphism has a particularly simple (diagonal) form. In Chapter 10, we will look at a data compression problem and ﬁnd a convenient basis onto which we can project the data while minimizing the compression loss. 2.7.3 Image and Kernel The image and kernel of a linear mapping are vector subspaces with cer- tain important properties. In the following, we will characterize them more carefully. Deﬁnition 2.23 (Image and Kernel). For Φ : V → W , we deﬁne the kernel/null spacekernel null space ker(Φ) := Φ −1(0W ) = {v ∈ V : Φ(v) = 0W } (2.122) and the image/rangeimage range Im(Φ) := Φ(V ) = {w ∈ W |∃v ∈ V : Φ(v) = w} . (2.123) We also call V and W also the domain and codomain of Φ, respectively.domain codomain Intuitively, the kernel is the set of vectors in v ∈ V that Φ maps onto the neutral element 0W ∈ W . The image is the set of vectors w ∈ W that can be “reached” by Φ from any vector in V . An illustration is given in Figure 2.12. Remark. Consider a linear mapping Φ : V → W , where V, W are vector spaces. It always holds that Φ(0V ) = 0W and, therefore, 0V ∈ ker(Φ). In particular, the null space is never empty. Im(Φ) ⊆ W is a subspace of W , and ker(Φ) ⊆ V is a subspace of V . Draft (2019-12-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com. 2.7 Linear Mappings 59 Figure 2.12 Kernel and image of a linear mapping Φ : V → W . Im(Φ) 0W ker(Φ) 0V Φ : V → W V W Φ is injective (one-to-one) if and only if ker(Φ) = {0}. ♦ Remark (Null Space and Column Space). Let us consider A ∈ R m×n and a linear mapping Φ : Rn → Rm, x ↦→ Ax. For A = [a1, . . . , an], where ai are the columns of A, we obtain Im(Φ) = {Ax : x ∈ R n} = { n∑ i=1 xiai : x1, . . . , xn ∈ R } (2.124a) = span[a1, . . . , an] ⊆ Rm , (2.124b) i.e., the image is the span of the columns of A, also called the column column space space. Therefore, the column space (image) is a subspace of R m, where m is the “height” of the matrix. rk(A) = dim(Im(Φ)). The kernel/null space ker(Φ) is the general solution to the homoge- neous system of linear equations Ax = 0 and captures all possible linear combinations of the elements in Rn that produce 0 ∈ Rm. The kernel is a subspace of R n, where n is the “width” of the matrix. The kernel focuses on the relationship among the columns, and we can use it to determine whether/how we can express a column as a linear combination of other columns. ♦ Example 2.25 (Image and Kernel of a Linear Mapping) The mapping Φ : R4 → R 2,     x1 x2 x3 x4     ↦→ [ 1 2 −1 0 1 0 0 1 ]     x1 x2 x3 x4     = [ x1 + 2x2 − x3 x1 + x4 ] (2.125a) c⃝2019 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press. 60 Linear Algebra = x1 [ 1 1 ] + x2 [ 2 0 ] + x3 [ −1 0 ] + x4 [ 0 1 ] (2.125b) is linear. To determine Im(Φ), we can take the span of the columns of the transformation matrix and obtain Im(Φ) = span[ [ 1 1 ] , [ 2 0 ] , [ −1 0 ] , [ 0 1 ] ] . (2.126) To compute the kernel (null space) of Φ, we need to solve Ax = 0, i.e., we need to solve a homogeneous equation system. To do this, we use Gaussian elimination to transform A into reduced row-echelon form: [ 1 2 −1 0 1 0 0 1 ] ⇝ · · · ⇝ [ 1 0 0 1 0 1 − 1 2 − 1 2 ] . (2.127) This matrix is in reduced row-echelon form, and we can use the Minus- 1 Trick to compute a basis of the kernel (see Section 2.3.3). Alternatively, we can express the non-pivot columns (columns 3 and 4) as linear com- binations of the pivot columns (columns 1 and 2). The third column a3 is equivalent to − 1 2 times the second column a2. Therefore, 0 = a3 + 1 2 a2. In the same way, we see that a4 = a1− 1 2 a2 and, therefore, 0 = a1− 1 2 a2−a4. Overall, this gives us the kernel (null space) as ker(Φ) = span[     0 1 2 1 0     ,     −1 1 2 0 1    ] . (2.128) rank-nullity theorem Theorem 2.24 (Rank-Nullity Theorem). For vector spaces V, W and a lin- ear mapping Φ : V → W it holds that dim(ker(Φ)) + dim(Im(Φ)) = dim(V ) . (2.129) The rank-nullity theorem is also referred to as the fundamental theoremfundamental theorem of linear mappings of linear mappings (Axler, 2015, theorem 3.22). The following are direct consequences of Theorem 2.24: If dim(Im(Φ)) < dim(V ), then ker(Φ) is non-trivial, i.e., the kernel contains more than 0V and dim(ker(Φ)) ⩾ 1. If AΦ is the transformation matrix of Φ with respect to an ordered basis and dim(Im(Φ)) < dim(V ), then the system of linear equations AΦx = 0 has inﬁnitely many solutions. If dim(V ) = dim(W ), then the following three-way equivalence holds: – Φ is injective – Φ is surjective – Φ is bijective since Im(Φ) ⊆ W . Draft (2019-12-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com. 2.8 Afﬁne Spaces 61 2.8 Afﬁne Spaces In the following, we will have a closer look at spaces that are offset from the origin, i.e., spaces that are no longer vector subspaces. Moreover, we will brieﬂy discuss properties of mappings between these afﬁne spaces, which resemble linear mappings. Remark. In the machine learning literature, the distinction between linear and afﬁne is sometimes not clear so that we can ﬁnd references to afﬁne spaces/mappings as linear spaces/mappings. ♦ 2.8.1 Afﬁne Subspaces Deﬁnition 2.25 (Afﬁne Subspace). Let V be a vector space, x0 ∈ V and U ⊆ V a subspace. Then the subset L = x0 + U := {x0 + u : u ∈ U } (2.130a) = {v ∈ V |∃u ∈ U : v = x0 + u} ⊆ V (2.130b) is called afﬁne subspace or linear manifold of V . U is called direction or afﬁne subspace linear manifold direction direction space, and x0 is called support point. In Chapter 12, we refer to direction space support point such a subspace as a hyperplane. hyperplane Note that the deﬁnition of an afﬁne subspace excludes 0 if x0 /∈ U . Therefore, an afﬁne subspace is not a (linear) subspace (vector subspace) of V for x0 /∈ U . Examples of afﬁne subspaces are points, lines, and planes in R 3, which do not (necessarily) go through the origin. Remark. Consider two afﬁne subspaces L = x0 + U and ˜L = ˜x0 + ˜U of a vector space V . Then, L ⊆ ˜L if and only if U ⊆ ˜U and x0 − ˜x0 ∈ ˜U . Afﬁne subspaces are often described by parameters: Consider a k-dimen- sional afﬁne space L = x0 + U of V . If (b1, . . . , bk) is an ordered basis of U , then every element x ∈ L can be uniquely described as x = x0 + λ1b1 + . . . + λkbk , (2.131) where λ1, . . . , λk ∈ R. This representation is called parametric equation parametric equation of L with directional vectors b1, . . . , bk and parameters λ1, . . . , λk. ♦ parameters Example 2.26 (Afﬁne Subspaces) One-dimensional afﬁne subspaces are called lines and can be written line as y = x0 + λx1, where λ ∈ R, where U = span[x1] ⊆ Rn is a one-dimensional subspace of Rn. This means that a line is deﬁned by a support point x0 and a vector x1 that deﬁnes the direction. See Fig- ure 2.13 for an illustration. c⃝2019 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press. 62 Linear Algebra Two-dimensional afﬁne subspaces of Rn are called planes. The para-plane metric equation for planes is y = x0 + λ1x1 + λ2x2, where λ1, λ2 ∈ R and U = span[x1, x2] ⊆ Rn. This means that a plane is deﬁned by a support point x0 and two linearly independent vectors x1, x2 that span the direction space. In R n, the (n − 1)-dimensional afﬁne subspaces are called hyperplanes,hyperplane and the corresponding parametric equation is y = x0 + ∑n−1 i=1 λixi, where x1, . . . , xn−1 form a basis of an (n − 1)-dimensional subspace U of Rn. This means that a hyperplane is deﬁned by a support point x0 and (n − 1) linearly independent vectors x1, . . . , xn−1 that span the direction space. In R 2, a line is also a hyperplane. In R3, a plane is also a hyperplane. Figure 2.13 Vectors y on a line lie in an afﬁne subspace L with support point x0 and direction u. 0 x0 u y L = x0 + λu Remark (Inhomogeneous systems of linear equations and afﬁne subspaces). For A ∈ Rm×n and b ∈ R m, the solution of the linear equation sys- tem Ax = b is either the empty set or an afﬁne subspace of Rn of dimension n − rk(A). In particular, the solution of the linear equation λ1x1 + . . . + λnxn = b, where (λ1, . . . , λn) ̸= (0, . . . , 0), is a hyperplane in R n. In Rn, every k-dimensional afﬁne subspace is the solution of a linear inhomogeneous equation system Ax = b, where A ∈ R m×n, b ∈ Rm and rk(A) = n − k. Recall that for homogeneous equation systems Ax = 0 the solution was a vector subspace, which we can also think of as a special afﬁne space with support point x0 = 0. ♦ 2.8.2 Afﬁne Mappings Similar to linear mappings between vector spaces, which we discussed in Section 2.7, we can deﬁne afﬁne mappings between two afﬁne spaces. Linear and afﬁne mappings are closely related. Therefore, many properties that we already know from linear mappings, e.g., that the composition of linear mappings is a linear mapping, also hold for afﬁne mappings. Deﬁnition 2.26 (Afﬁne Mapping). For two vector spaces V, W , a linear Draft (2019-12-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com. 2.9 Further Reading 63 mapping Φ : V → W , and a ∈ W , the mapping φ : V → W (2.132) x ↦→ a + Φ(x) (2.133) is an afﬁne mapping from V to W . The vector a is called the translation afﬁne mapping translation vectorvector of φ. Every afﬁne mapping φ : V → W is also the composition of a linear mapping Φ : V → W and a translation τ : W → W in W , such that φ = τ ◦ Φ. The mappings Φ and τ are uniquely determined. The composition φ′ ◦ φ of afﬁne mappings φ : V → W , φ′ : W → X is afﬁne. Afﬁne mappings keep the geometric structure invariant. They also pre- serve the dimension and parallelism. 2.9 Further Reading There are many resources for learning linear algebra, including the text- books by Strang (2003), Golan (2007), Axler (2015), and Liesen and Mehrmann (2015). There are also several online resources that we men- tioned in the introduction to this chapter. We only covered Gaussian elim- ination here, but there are many other approaches for solving systems of linear equations, and we refer to numerical linear algebra textbooks by Stoer and Burlirsch (2002), Golub and Van Loan (2012), and Horn and Johnson (2013) for an in-depth discussion. In this book, we distinguish between the topics of linear algebra (e.g., vectors, matrices, linear independence, basis) and topics related to the geometry of a vector space. In Chapter 3, we will introduce the inner product, which induces a norm. These concepts allow us to deﬁne angles, lengths and distances, which we will use for orthogonal projections. Pro- jections turn out to be key in many machine learning algorithms, such as linear regression and principal component analysis, both of which we will cover in Chapters 9 and 10, respectively. c⃝2019 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press. 64 Linear Algebra Exercises 2.1 We consider (R\\{−1}, ⋆), where a ⋆ b := ab + a + b, a, b ∈ R\\{−1} (2.134) a. Show that (R\\{−1}, ⋆) is an Abelian group. b. Solve 3 ⋆ x ⋆ x = 15 in the Abelian group (R\\{−1}, ⋆), where ⋆ is deﬁned in (2.134). 2.2 Let n be in N\\{0}. Let k, x be in Z. We deﬁne the congruence class ¯k of the integer k as the set k = {x ∈ Z | x − k = 0 (modn)} = {x ∈ Z | (∃a ∈ Z) : (x − k = n · a)} . We now deﬁne Z/nZ (sometimes written Zn) as the set of all congruence classes modulo n. Euclidean division implies that this set is a ﬁnite set con- taining n elements: Zn = {0, 1, . . . , n − 1} For all a, b ∈ Zn, we deﬁne a ⊕ b := a + b a. Show that (Zn, ⊕) is a group. Is it Abelian? b. We now deﬁne another operation ⊗ for all a and b in Zn as a ⊗ b = a × b , (2.135) where a × b represents the usual multiplication in Z. Let n = 5. Draw the times table of the elements of Z5\\{0} under ⊗, i.e., calculate the products a ⊗ b for all a and b in Z5\\{0}. Hence, show that Z5\\{0} is closed under ⊗ and possesses a neutral element for ⊗. Display the inverse of all elements in Z5\\{0} under ⊗. Conclude that (Z5\\{0}, ⊗) is an Abelian group. c. Show that (Z8\\{0}, ⊗) is not a group. d. We recall that the B´ezout theorem states that two integers a and b are relatively prime (i.e., gcd(a, b) = 1) if and only if there exist two integers u and v such that au + bv = 1. Show that (Zn\\{0}, ⊗) is a group if and only if n ∈ N\\{0} is prime. 2.3 Consider the set G of 3 × 3 matrices deﬁned as follows: G =     1 x z 0 1 y 0 0 1   ∈ R3×3 ∣ ∣ ∣ ∣ ∣ ∣ x, y, z ∈ R    (2.136) We deﬁne · as the standard matrix multiplication. Is (G, ·) a group? If yes, is it Abelian? Justify your answer. 2.4 Compute the following matrix products, if possible: Draft (2019-12-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com. Exercises 65 a.  1 2 4 5 7 8    1 1 0 0 1 1 1 0 1   b.  1 2 3 4 5 6 7 8 9     1 1 0 0 1 1 1 0 1   c.  1 1 0 0 1 1 1 0 1     1 2 3 4 5 6 7 8 9   d. [ 1 2 1 2 4 1 −1 −4 ]     0 3 1 −1 2 1 5 2     e.     0 3 1 −1 2 1 5 2     [ 1 2 1 2 4 1 −1 −4 ] 2.5 Find the set S of all solutions in x of the following inhomogeneous linear systems Ax = b, where A and b are deﬁned as follows: a. A =     1 1 −1 −1 2 5 −7 −5 2 −1 1 3 5 2 −4 2     , b =     1 −2 4 6     b. A =     1 −1 0 0 1 1 1 0 −3 0 2 −1 0 1 −1 −1 2 0 −2 −1     , b =     3 6 5 −1     2.6 Using Gaussian elimination, ﬁnd all solutions of the inhomogeneous equa- tion system Ax = b with A =   0 1 0 0 1 0 0 0 0 1 1 0 0 1 0 0 0 1   , b =   2 −1 1   c⃝2019 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press. 66 Linear Algebra 2.7 Find all solutions in x =  x1 x2 x3   ∈ R3 of the equation system Ax = 12x, where A =  6 4 3 6 0 9 0 8 0   and ∑3 i=1 xi = 1. 2.8 Determine the inverses of the following matrices if possible: a. A =  2 3 4 3 4 5 4 5 6   b. A =     1 0 1 0 0 1 1 0 1 1 0 1 1 1 1 0     2.9 Which of the following sets are subspaces of R3? a. A = {(λ, λ + µ3, λ − µ3) | λ, µ ∈ R} b. B = {(λ2, −λ2, 0) | λ ∈ R} c. Let γ be in R. C = {(ξ1, ξ2, ξ3) ∈ R3 | ξ1 − 2ξ2 + 3ξ3 = γ} d. D = {(ξ1, ξ2, ξ3) ∈ R3 | ξ2 ∈ Z} 2.10 Are the following sets of vectors linearly independent? a. x1 =   2 −1 3   , x2 =   1 1 −2   , x3 =   3 −3 8   b. x1 =       1 2 1 0 0       , x2 =       1 1 0 1 1       , x3 =       1 0 0 1 1       2.11 Write y =   1 −2 5   as linear combination of x1 =  1 1 1   , x2 =   1 2 3   , x3 =   2 −1 1   Draft (2019-12-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com. Exercises 67 2.12 Consider two subspaces of R4: U1 = span[     1 1 −3 1     ,     2 −1 0 −1     ,     −1 1 −1 1    ] , U2 = span[     −1 −2 2 1     ,     2 −2 0 0     ,     −3 6 −2 −1    ] . Determine a basis of U1 ∩ U2. 2.13 Consider two subspaces U1 and U2, where U1 is the solution space of the homogeneous equation system A1x = 0 and U2 is the solution space of the homogeneous equation system A2x = 0 with A1 =     1 0 1 1 −2 −1 2 1 3 1 0 1     , A2 =     3 −3 0 1 2 3 7 −5 2 3 −1 2     . a. Determine the dimension of U1, U2. b. Determine bases of U1 and U2. c. Determine a basis of U1 ∩ U2. 2.14 Consider two subspaces U1 and U2, where U1 is spanned by the columns of A1 and U2 is spanned by the columns of A2 with A1 =     1 0 1 1 −2 −1 2 1 3 1 0 1     , A2 =     3 −3 0 1 2 3 7 −5 2 3 −1 2     . a. Determine the dimension of U1, U2 b. Determine bases of U1 and U2 c. Determine a basis of U1 ∩ U2 2.15 Let F = {(x, y, z) ∈ R3 | x+y−z = 0} and G = {(a−b, a+b, a−3b) | a, b ∈ R}. a. Show that F and G are subspaces of R3. b. Calculate F ∩ G without resorting to any basis vector. c. Find one basis for F and one for G, calculate F ∩G using the basis vectors previously found and check your result with the previous question. 2.16 Are the following mappings linear? a. Let a, b ∈ R. Φ : L 1([a, b]) → R f ↦→ Φ(f ) = ∫ b a f (x)dx , where L 1([a, b]) denotes the set of integrable functions on [a, b]. b. Φ : C1 → C0 f ↦→ Φ(f ) = f ′ , where for k ⩾ 1, Ck denotes the set of k times continuously differen- tiable functions, and C0 denotes the set of continuous functions. c⃝2019 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press. 68 Linear Algebra c. Φ : R → R x ↦→ Φ(x) = cos(x) d. Φ : R3 → R2 x ↦→ [ 1 2 3 1 4 3 ] x e. Let θ be in [0, 2π[. Φ : R2 → R2 x ↦→ [ cos(θ) sin(θ) − sin(θ) cos(θ) ] x 2.17 Consider the linear mapping Φ : R3 → R4 Φ     x1 x2 x3     =     3x1 + 2x2 + x3 x1 + x2 + x3 x1 − 3x2 2x1 + 3x2 + x3     Find the transformation matrix AΦ. Determine rk(AΦ). Compute the kernel and image of Φ. What are dim(ker(Φ)) and dim(Im(Φ))? 2.18 Let E be a vector space. Let f and g be two automorphisms on E such that f ◦ g = idE (i.e., f ◦ g is the identity mapping idE). Show that ker(f ) = ker(g ◦ f ), Im(g) = Im(g ◦ f ) and that ker(f ) ∩ Im(g) = {0E}. 2.19 Consider an endomorphism Φ : R3 → R3 whose transformation matrix (with respect to the standard basis in R3) is AΦ =  1 1 0 1 −1 0 1 1 1   . 1. Determine ker(Φ) and Im(Φ). 2. Determine the transformation matrix ˜AΦ with respect to the basis B = (   1 1 1   ,  1 2 1   ,  1 0 0  ) , i.e., perform a basis change toward the new basis B. 2.20 Let us consider b1, b2, b′ 1, b′ 2, 4 vectors of R2 expressed in the standard basis of R2 as b1 = [2 1 ] , b2 = [ −1 −1 ] , b ′ 1 = [ 2 −2 ] , b ′ 2 = [ 1 1 ] and let us deﬁne two ordered bases B = (b1, b2) and B′ = (b′ 1, b ′ 2) of R2. Draft (2019-12-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com. Exercises 69 1. Show that B and B′ are two bases of R2 and draw those basis vectors. 2. Compute the matrix P 1 that performs a basis change from B′ to B. 3. We consider c1, c2, c3, three vectors of R3 deﬁned in the standard basis of R as c1 =   1 2 −1   , c2 =   0 −1 2   , c3 =   1 0 −1   and we deﬁne C = (c1, c2, c3). a. Show that C is a basis of R3, e.g., by using determinants (see Sec- tion 4.1). b. Let us call C′ = (c ′ 1, c′ 2, c ′ 3) the standard basis of R3. Determine the matrix P 2 that performs the basis change from C to C′. 4. We consider a homomorphism Φ : R2 −→ R3, such that Φ(b1 + b2) = c2 + c3 Φ(b1 − b2) = 2c1 − c2 + 3c3 where B = (b1, b2) and C = (c1, c2, c3) are ordered bases of R2 and R3, respectively. Determine the transformation matrix AΦ of Φ with respect to the ordered bases B and C. 5. Determine A ′, the transformation matrix of Φ with respect to the bases B′ and C′. 6. Let us consider the vector x ∈ R2 whose coordinates in B′ are [2, 3] ⊤. In other words, x = 2b′ 1 + 3b′ 2. a. Calculate the coordinates of x in B. b. Based on that, compute the coordinates of Φ(x) expressed in C. c. Then, write Φ(x) in terms of c ′ 1, c′ 2, c ′ 3. d. Use the representation of x in B′ and the matrix A ′ to ﬁnd this result directly. c⃝2019 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press. 3 Analytic Geometry In Chapter 2, we studied vectors, vector spaces, and linear mappings at a general but abstract level. In this chapter, we will add some geomet- ric interpretation and intuition to all of these concepts. In particular, we will look at geometric vectors and compute their lengths and distances or angles between two vectors. To be able to do this, we equip the vec- tor space with an inner product that induces the geometry of the vector space. Inner products and their corresponding norms and metrics capture the intuitive notions of similarity and distances, which we use to develop the support vector machine in Chapter 12. We will then use the concepts of lengths and angles between vectors to discuss orthogonal projections, which will play a central role when we discuss principal component anal- ysis in Chapter 10 and regression via maximum likelihood estimation in Chapter 9. Figure 3.1 gives an overview of how concepts in this chapter are related and how they are connected to other chapters of the book. Figure 3.1 A mind map of the concepts introduced in this chapter, along with when they are used in other parts of the book. Inner product Norm Lengths Orthogonal projection Angles Rotations Chapter 4 Matrix decomposition Chapter 10 Dimensionality reduction Chapter 9 Regression Chapter 12 Classiﬁcation induces 70 This material will be published by Cambridge University Press as Mathematics for Machine Learn- ing by Marc Peter Deisenroth, A. Aldo Faisal, and Cheng Soon Ong. This pre-publication version is free to view and download for personal use only. Not for re-distribution, re-sale or use in deriva- tive works. c⃝by M. P. Deisenroth, A. A. Faisal, and C. S. Ong, 2019. https://mml-book.com. 3.1 Norms 71 Figure 3.3 For different norms, the red lines indicate the set of vectors with norm 1. Left: Manhattan norm; Right: Euclidean distance. 1111∥x ∥1 = 1∥x ∥2 = 1 3.1 Norms When we think of geometric vectors, i.e., directed line segments that start at the origin, then intuitively the length of a vector is the distance of the “end” of this directed line segment from the origin. In the following, we will discuss the notion of the length of vectors using the concept of a norm. Deﬁnition 3.1 (Norm). A norm on a vector space V is a function norm ∥ · ∥ : V → R , (3.1) x ↦→ ∥x∥ , (3.2) which assigns each vector x its length ∥x∥ ∈ R, such that for all λ ∈ R length and x, y ∈ V the following hold: absolutely homogeneousAbsolutely homogeneous: ∥λx∥ = |λ|∥x∥ triangle inequalityTriangle inequality: ∥x + y∥ ⩽ ∥x∥ + ∥y∥ positive deﬁnitePositive deﬁnite: ∥x∥ ⩾ 0 and ∥x∥ = 0 ⇐⇒ x = 0 Figure 3.2 Triangle inequality. abc ≤ a + b In geometric terms, the triangle inequality states that for any triangle, the sum of the lengths of any two sides must be greater than or equal to the length of the remaining side; see Figure 3.2 for an illustration. Deﬁnition 3.1 is in terms of a general vector space V (Section 2.4), but in this book we will only consider a ﬁnite-dimensional vector space Rn. Recall that for a vector x ∈ R n we denote the elements of the vector using a subscript, that is, xi is the ith element of the vector x. Example 3.1 (Manhattan Norm) The Manhattan norm on Rn is deﬁned for x ∈ R n as Manhattan norm ∥x∥1 := n∑ i=1 |xi| , (3.3) where | · | is the absolute value. The left panel of Figure 3.3 shows all vectors x ∈ R 2 with ∥x∥1 = 1. The Manhattan norm is also called ℓ1 ℓ1 norm norm. c⃝2019 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press. 72 Analytic Geometry Example 3.2 (Euclidean Norm) The Euclidean norm of x ∈ Rn is deﬁned asEuclidean norm ∥x∥2 := √ √ √ √ n∑ i=1 x2 i = √x⊤x (3.4) and computes the Euclidean distance of x from the origin. The right panelEuclidean distance of Figure 3.3 shows all vectors x ∈ R2 with ∥x∥2 = 1. The Euclidean norm is also called ℓ2 norm.ℓ2 norm Remark. Throughout this book, we will use the Euclidean norm (3.4) by default if not stated otherwise. ♦ 3.2 Inner Products Inner products allow for the introduction of intuitive geometrical con- cepts, such as the length of a vector and the angle or distance between two vectors. A major purpose of inner products is to determine whether vectors are orthogonal to each other. 3.2.1 Dot Product We may already be familiar with a particular type of inner product, the scalar product/dot product in Rn, which is given byscalar product dot product x ⊤y = n∑ i=1 xiyi . (3.5) We will refer to this particular inner product as the dot product in this book. However, inner products are more general concepts with speciﬁc properties, which we will now introduce. 3.2.2 General Inner Products Recall the linear mapping from Section 2.7, where we can rearrange the mapping with respect to addition and multiplication with a scalar. A bi-bilinear mapping linear mapping Ω is a mapping with two arguments, and it is linear in each argument, i.e., when we look at a vector space V then it holds that for all x, y, z ∈ V, λ, ψ ∈ R that Ω(λx + ψy, z) = λΩ(x, z) + ψΩ(y, z) (3.6) Ω(x, λy + ψz) = λΩ(x, y) + ψΩ(x, z) . (3.7) Here, (3.6) asserts that Ω is linear in the ﬁrst argument, and (3.7) asserts that Ω is linear in the second argument (see also (2.87)). Draft (2019-12-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com. 3.2 Inner Products 73 Deﬁnition 3.2. Let V be a vector space and Ω : V × V → R be a bilinear mapping that takes two vectors and maps them onto a real number. Then Ω is called symmetric if Ω(x, y) = Ω(y, x) for all x, y ∈ V , i.e., the symmetric order of the arguments does not matter. Ω is called positive deﬁnite if positive deﬁnite ∀x ∈ V \\{0} : Ω(x, x) > 0 , Ω(0, 0) = 0 . (3.8) Deﬁnition 3.3. Let V be a vector space and Ω : V × V → R be a bilinear mapping that takes two vectors and maps them onto a real number. Then A positive deﬁnite, symmetric bilinear mapping Ω : V ×V → R is called an inner product on V . We typically write ⟨x, y⟩ instead of Ω(x, y). inner product The pair (V, ⟨·, ·⟩) is called an inner product space or (real) vector space inner product space vector space with inner product with inner product. If we use the dot product deﬁned in (3.5), we call (V, ⟨·, ·⟩) a Euclidean vector space. Euclidean vector spaceWe will refer to these spaces as inner product spaces in this book. Example 3.3 (Inner Product That Is Not the Dot Product) Consider V = R2. If we deﬁne ⟨x, y⟩ := x1y1 − (x1y2 + x2y1) + 2x2y2 (3.9) then ⟨·, ·⟩ is an inner product but different from the dot product. The proof will be an exercise. 3.2.3 Symmetric, Positive Deﬁnite Matrices Symmetric, positive deﬁnite matrices play an important role in machine learning, and they are deﬁned via the inner product. In Section 4.3, we will return to symmetric, positive deﬁnite matrices in the context of matrix decompositions. The idea of symmetric positive semideﬁnite matrices is key in the deﬁnition of kernels (Section 12.4). Consider an n-dimensional vector space V with an inner product ⟨·, ·⟩ : V × V → R (see Deﬁnition 3.3) and an ordered basis B = (b1, . . . , bn) of V . Recall from Section 2.6.1 that any vectors x, y ∈ V can be written as linear combinations of the basis vectors so that x = ∑n i=1 ψibi ∈ V and y = ∑n j=1 λjbj ∈ V for suitable ψi, λj ∈ R. Due to the bilinearity of the inner product, it holds for all x, y ∈ V that ⟨x, y⟩ = 〈 n∑ i=1 ψibi, n∑ j=1 λjbj 〉 = n∑ i=1 n∑ j=1 ψi ⟨bi, bj⟩ λj = ˆx⊤Aˆy , (3.10) where Aij := ⟨bi, bj⟩ and ˆx, ˆy are the coordinates of x and y with respect to the basis B. This implies that the inner product ⟨·, ·⟩ is uniquely deter- mined through A. The symmetry of the inner product also means that A c⃝2019 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press. 74 Analytic Geometry is symmetric. Furthermore, the positive deﬁniteness of the inner product implies that ∀x ∈ V \\{0} : x ⊤Ax > 0 . (3.11) Deﬁnition 3.4 (Symmetric, Positive Deﬁnite Matrix). A symmetric matrix A ∈ R n×n that satisﬁes (3.11) is called symmetric, positive deﬁnite, orsymmetric, positive deﬁnite just positive deﬁnite. If only ⩾ holds in (3.11), then A is called symmetric, positive deﬁnite symmetric, positive semideﬁnite positive semideﬁnite. Example 3.4 (Symmetric, Positive Deﬁnite Matrices) Consider the matrices A1 = [ 9 6 6 5 ] , A2 = [ 9 6 6 3 ] . (3.12) A1 is positive deﬁnite because it is symmetric and x⊤A1x = [ x1 x2] [ 9 6 6 5 ] [ x1 x2 ] (3.13a) = 9x2 1 + 12x1x2 + 5x2 2 = (3x1 + 2x2) 2 + x2 2 > 0 (3.13b) for all x ∈ V \\{0}. In contrast, A2 is symmetric but not positive deﬁnite because x ⊤A2x = 9x2 1 + 12x1x2 + 3x2 2 = (3x1 + 2x2) 2 − x2 2 can be less than 0, e.g., for x = [2, −3] ⊤. If A ∈ Rn×n is symmetric, positive deﬁnite, then ⟨x, y⟩ = ˆx ⊤Aˆy (3.14) deﬁnes an inner product with respect to an ordered basis B, where ˆx and ˆy are the coordinate representations of x, y ∈ V with respect to B. Theorem 3.5. For a real-valued, ﬁnite-dimensional vector space V and an ordered basis B of V , it holds that ⟨·, ·⟩ : V × V → R is an inner product if and only if there exists a symmetric, positive deﬁnite matrix A ∈ R n×n with ⟨x, y⟩ = ˆx ⊤Aˆy . (3.15) The following properties hold if A ∈ Rn×n is symmetric and positive deﬁnite: The null space (kernel) of A consists only of 0 because x ⊤Ax > 0 for all x ̸= 0. This implies that Ax ̸= 0 if x ̸= 0. The diagonal elements aii of A are positive because aii = e ⊤ i Aei > 0, where ei is the ith vector of the standard basis in R n. Draft (2019-12-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com. 3.3 Lengths and Distances 75 3.3 Lengths and Distances In Section 3.1, we already discussed norms that we can use to compute the length of a vector. Inner products and norms are closely related in the sense that any inner product induces a norm Inner products induce norms. ∥x∥ := √⟨x, x⟩ (3.16) in a natural way, such that we can compute lengths of vectors using the in- ner product. However, not every norm is induced by an inner product. The Manhattan norm (3.3) is an example of a norm without a corresponding inner product. In the following, we will focus on norms that are induced by inner products and introduce geometric concepts, such as lengths, dis- tances, and angles. Remark (Cauchy-Schwarz Inequality). For an inner product vector space (V, ⟨·, ·⟩) the induced norm ∥ · ∥ satisﬁes the Cauchy-Schwarz inequality Cauchy-Schwarz inequality | ⟨x, y⟩ | ⩽ ∥x∥∥y∥ . (3.17) ♦ Example 3.5 (Lengths of Vectors Using Inner Products) In geometry, we are often interested in lengths of vectors. We can now use an inner product to compute them using (3.16). Let us take x = [1, 1] ⊤ ∈ R 2. If we use the dot product as the inner product, with (3.16) we obtain ∥x∥ = √x⊤x = √12 + 12 = √2 (3.18) as the length of x. Let us now choose a different inner product: ⟨x, y⟩ := x⊤ [ 1 − 1 2 − 1 2 1 ] y = x1y1 − 1 2 (x1y2 + x2y1) + x2y2 . (3.19) If we compute the norm of a vector, then this inner product returns smaller values than the dot product if x1 and x2 have the same sign (and x1x2 > 0); otherwise, it returns greater values than the dot product. With this inner product, we obtain ⟨x, x⟩ = x2 1 − x1x2 + x2 2 = 1 − 1 + 1 = 1 =⇒ ∥x∥ = √1 = 1 , (3.20) such that x is “shorter” with this inner product than with the dot product. Deﬁnition 3.6 (Distance and Metric). Consider an inner product space (V, ⟨·, ·⟩). Then d(x, y) := ∥x − y∥ = √⟨x − y, x − y⟩ (3.21) is called the distance between x and y for x, y ∈ V . If we use the dot distance product as the inner product, then the distance is called Euclidean distance. Euclidean distance c⃝2019 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press. 76 Analytic Geometry The mapping d : V × V → R (3.22) (x, y) ↦→ d(x, y) (3.23) is called a metric.metric Remark. Similar to the length of a vector, the distance between vectors does not require an inner product: a norm is sufﬁcient. If we have a norm induced by an inner product, the distance may vary depending on the choice of the inner product. ♦ A metric d satisﬁes the following: 1. d is positive deﬁnite, i.e., d(x, y) ⩾ 0 for all x, y ∈ V and d(x, y) =positive deﬁnite 0 ⇐⇒ x = y . 2. d is symmetric, i.e., d(x, y) = d(y, x) for all x, y ∈ V .symmetric triangle inequality 3. Triangle inequality: d(x, z) ⩽ d(x, y) + d(y, z) for all x, y, z ∈ V . Remark. At ﬁrst glance, the lists of properties of inner products and met- rics look very similar. However, by comparing Deﬁnition 3.3 with Deﬁni- tion 3.6 we observe that ⟨x, y⟩ and d(x, y) behave in opposite directions. Very similar x and y will result in a large value for the inner product and a small value for the metric. ♦ 3.4 Angles and Orthogonality Figure 3.4 When restricted to [0, π] then f (ω) = cos(ω) returns a unique number in the interval [−1, 1]. 0 π/2 π ω −1 0 1cos(ω) In addition to enabling the deﬁnition of lengths of vectors, as well as the distance between two vectors, inner products also capture the geometry of a vector space by deﬁning the angle ω between two vectors. We use the Cauchy-Schwarz inequality (3.17) to deﬁne angles ω in inner prod- uct spaces between two vectors x, y, and this notion coincides with our intuition in R 2 and R3. Assume that x ̸= 0, y ̸= 0. Then −1 ⩽ ⟨x, y⟩ ∥x∥ ∥y∥ ⩽ 1 . (3.24) Therefore, there exists a unique ω ∈ [0, π], illustrated in Figure 3.4, with cos ω = ⟨x, y⟩ ∥x∥ ∥y∥ . (3.25) The number ω is the angle between the vectors x and y. Intuitively, theangle angle between two vectors tells us how similar their orientations are. For example, using the dot product, the angle between x and y = 4x, i.e., y is a scaled version of x, is 0: Their orientation is the same. Draft (2019-12-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com. 3.4 Angles and Orthogonality 77 Example 3.6 (Angle between Vectors) Let us compute the angle between x = [1, 1] ⊤ ∈ R 2 and y = [1, 2] ⊤ ∈ R 2; Figure 3.5 The angle ω between two vectors x, y is computed using the inner product. y x 10 1 ω see Figure 3.5, where we use the dot product as the inner product. Then we get cos ω = ⟨x, y⟩ √⟨x, x⟩ ⟨y, y⟩ = x ⊤y √x⊤xy⊤y = 3 √10 , (3.26) and the angle between the two vectors is arccos( 3√10 ) ≈ 0.32 rad, which corresponds to about 18 ◦. A key feature of the inner product is that it also allows us to characterize vectors that are orthogonal. Deﬁnition 3.7 (Orthogonality). Two vectors x and y are orthogonal if and orthogonal only if ⟨x, y⟩ = 0, and we write x ⊥ y. If additionally ∥x∥ = 1 = ∥y∥, i.e., the vectors are unit vectors, then x and y are orthonormal. orthonormal An implication of this deﬁnition is that the 0-vector is orthogonal to every vector in the vector space. Remark. Orthogonality is the generalization of the concept of perpendic- ularity to bilinear forms that do not have to be the dot product. In our context, geometrically, we can think of orthogonal vectors as having a right angle with respect to a speciﬁc inner product. ♦ Example 3.7 (Orthogonal Vectors) Figure 3.6 The angle ω between two vectors x, y can change depending on the inner product. y x −1 10 1 ω Consider two vectors x = [1, 1] ⊤, y = [−1, 1] ⊤ ∈ R 2; see Figure 3.6. We are interested in determining the angle ω between them using two different inner products. Using the dot product as the inner product yields an angle ω between x and y of 90 ◦, such that x ⊥ y. However, if we choose the inner product ⟨x, y⟩ = x ⊤ [ 2 0 0 1 ] y , (3.27) c⃝2019 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press. 78 Analytic Geometry we get that the angle ω between x and y is given by cos ω = ⟨x, y⟩ ∥x∥∥y∥ = − 1 3 =⇒ ω ≈ 1.91 rad ≈ 109.5 ◦ , (3.28) and x and y are not orthogonal. Therefore, vectors that are orthogonal with respect to one inner product do not have to be orthogonal with re- spect to a different inner product. Deﬁnition 3.8 (Orthogonal Matrix). A square matrix A ∈ R n×n is an orthogonal matrix if and only if its columns are orthonormal so thatorthogonal matrix AA⊤ = I = A ⊤A , (3.29) which implies that A−1 = A⊤ , (3.30) i.e., the inverse is obtained by simply transposing the matrix.It is convention to call these matrices “orthogonal” but a more precise description would be “orthonormal”. Transformations by orthogonal matrices are special because the length of a vector x is not changed when transforming it using an orthogonal matrix A. For the dot product, we obtain Transformations with orthogonal matrices preserve distances and angles. ∥Ax∥ 2 = (Ax) ⊤(Ax) = x⊤A⊤Ax = x⊤Ix = x ⊤x = ∥x∥2 . (3.31) Moreover, the angle between any two vectors x, y, as measured by their inner product, is also unchanged when transforming both of them using an orthogonal matrix A. Assuming the dot product as the inner product, the angle of the images Ax and Ay is given as cos ω = (Ax)⊤(Ay) ∥Ax∥ ∥Ay∥ = x⊤A⊤Ay √x⊤A⊤Axy⊤A⊤Ay = x ⊤y ∥x∥ ∥y∥ , (3.32) which gives exactly the angle between x and y. This means that orthog- onal matrices A with A⊤ = A−1 preserve both angles and distances. It turns out that orthogonal matrices deﬁne transformations that are rota- tions (with the possibility of ﬂips). In Section 3.9, we will discuss more details about rotations. 3.5 Orthonormal Basis In Section 2.6.1, we characterized properties of basis vectors and found that in an n-dimensional vector space, we need n basis vectors, i.e., n vectors that are linearly independent. In Sections 3.3 and 3.4, we used inner products to compute the length of vectors and the angle between vectors. In the following, we will discuss the special case where the basis vectors are orthogonal to each other and where the length of each basis vector is 1. We will call this basis then an orthonormal basis. Draft (2019-12-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com. 3.6 Orthogonal Complement 79 Let us introduce this more formally. Deﬁnition 3.9 (Orthonormal Basis). Consider an n-dimensional vector space V and a basis {b1, . . . , bn} of V . If ⟨bi, bj⟩ = 0 for i ̸= j (3.33) ⟨bi, bi⟩ = 1 (3.34) for all i, j = 1, . . . , n then the basis is called an orthonormal basis (ONB). orthonormal basis ONBIf only (3.33) is satisﬁed, then the basis is called an orthogonal basis. Note orthogonal basisthat (3.34) implies that every basis vector has length/norm 1. Recall from Section 2.6.1 that we can use Gaussian elimination to ﬁnd a basis for a vector space spanned by a set of vectors. Assume we are given a set {˜b1, . . . , ˜bn} of non-orthogonal and unnormalized basis vectors. We concatenate them into a matrix ˜B = [˜b1, . . . , ˜bn] and apply Gaussian elim- ination to the augmented matrix (Section 2.3.2) [ ˜B ˜B⊤| ˜B] to obtain an orthonormal basis. This constructive way to iteratively build an orthonor- mal basis {b1, . . . , bn} is called the Gram-Schmidt process (Strang, 2003). Example 3.8 (Orthonormal Basis) The canonical/standard basis for a Euclidean vector space Rn is an or- thonormal basis, where the inner product is the dot product of vectors. In R2, the vectors b1 = 1 √ 2 [ 1 1 ] , b2 = 1 √2 [ 1 −1 ] (3.35) form an orthonormal basis since b⊤ 1 b2 = 0 and ∥b1∥ = 1 = ∥b2∥. We will exploit the concept of an orthonormal basis in Chapter 12 and Chapter 10 when we discuss support vector machines and principal com- ponent analysis. 3.6 Orthogonal Complement Having deﬁned orthogonality, we will now look at vector spaces that are orthogonal to each other. This will play an important role in Chapter 10, when we discuss linear dimensionality reduction from a geometric per- spective. Consider a D-dimensional vector space V and an M -dimensional sub- space U ⊆ V . Then its orthogonal complement U ⊥ is a (D−M )-dimensional orthogonal complementsubspace of V and contains all vectors in V that are orthogonal to every vector in U . Furthermore, U ∩ U ⊥ = {0} so that any vector x ∈ V can be c⃝2019 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press. 80 Analytic Geometry Figure 3.7 A plane U in a three-dimensional vector space can be described by its normal vector, which spans its orthogonal complement U ⊥. e3 e1 e2 w U uniquely decomposed into x = M∑ m=1 λmbm + D−M∑ j=1 ψjb⊥ j , λm, ψj ∈ R , (3.36) where (b1, . . . , bM ) is a basis of U and (b⊥ 1 , . . . , b⊥ D−M ) is a basis of U ⊥. Therefore, the orthogonal complement can also be used to describe a plane U (two-dimensional subspace) in a three-dimensional vector space. More speciﬁcally, the vector w with ∥w∥ = 1, which is orthogonal to the plane U , is the basis vector of U ⊥. Figure 3.7 illustrates this setting. All vectors that are orthogonal to w must (by construction) lie in the plane U . The vector w is called the normal vector of U .normal vector Generally, orthogonal complements can be used to describe hyperplanes in n-dimensional vector and afﬁne spaces. 3.7 Inner Product of Functions Thus far, we looked at properties of inner products to compute lengths, angles and distances. We focused on inner products of ﬁnite-dimensional vectors. In the following, we will look at an example of inner products of a different type of vectors: inner products of functions. The inner products we discussed so far were deﬁned for vectors with a ﬁnite number of entries. We can think of a vector x ∈ R n as function with n function values. The concept of an inner product can be generalized to vectors with an inﬁnite number of entries (countably inﬁnite) and also continuous-valued functions (uncountably inﬁnite). Then the sum over individual components of vectors (see Equation (3.5) for example) turns into an integral. An inner product of two functions u : R → R and v : R → R can be deﬁned as the deﬁnite integral ⟨u, v⟩ := ∫ b a u(x)v(x)dx (3.37) Draft (2019-12-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com. 3.8 Orthogonal Projections 81 for lower and upper limits a, b < ∞, respectively. As with our usual inner product, we can deﬁne norms and orthogonality by looking at the inner product. If (3.37) evaluates to 0, the functions u and v are orthogonal. To make the preceding inner product mathematically precise, we need to take care of measures and the deﬁnition of integrals, leading to the deﬁnition of a Hilbert space. Furthermore, unlike inner products on ﬁnite-dimensional vectors, inner products on functions may diverge (have inﬁnite value). All this requires diving into some more intricate details of real and functional analysis, which we do not cover in this book. Example 3.9 (Inner Product of Functions) If we choose u = sin(x) and v = cos(x), the integrand f (x) = u(x)v(x) Figure 3.8 f (x) = sin(x) cos(x). −2.5 0.0 2.5 x −0.5 0.0 0.5sin(x)cos(x) of (3.37), is shown in Figure 3.8. We see that this function is odd, i.e., f (−x) = −f (x). Therefore, the integral with limits a = −π, b = π of this product evaluates to 0. Therefore, sin and cos are orthogonal functions. Remark. It also holds that the collection of functions {1, cos(x), cos(2x), cos(3x), . . . } (3.38) is orthogonal if we integrate from −π to π, i.e., any pair of functions are orthogonal to each other. The collection of functions in (3.38) spans a large subspace of the functions that are even and periodic on [−π, π), and projecting functions onto this subspace is the fundamental idea behind Fourier series. ♦ In Section 6.4.6, we will have a look at a second type of unconventional inner products: the inner product of random variables. 3.8 Orthogonal Projections Projections are an important class of linear transformations (besides rota- tions and reﬂections) and play an important role in graphics, coding the- ory, statistics and machine learning. In machine learning, we often deal with data that is high-dimensional. High-dimensional data is often hard to analyze or visualize. However, high-dimensional data quite often pos- sesses the property that only a few dimensions contain most information, and most other dimensions are not essential to describe key properties of the data. When we compress or visualize high-dimensional data, we will lose information. To minimize this compression loss, we ideally ﬁnd the most informative dimensions in the data. As discussed in Chapter 1, “Feature” is a common expression for data representation. data can be represented as vectors, and in this chapter, we will discuss some of the fundamental tools for data compression. More speciﬁcally, we can project the original high-dimensional data onto a lower-dimensional feature space and work in this lower-dimensional space to learn more about the dataset and extract relevant patterns. For example, machine c⃝2019 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press. 82 Analytic Geometry Figure 3.9 Orthogonal projection (orange dots) of a two-dimensional dataset (blue dots) onto a one-dimensional subspace (straight line). −4 −2 0 2 4 x1 −2 −1 0 1 2x2 learning algorithms, such as principal component analysis (PCA) by Pear- son (1901) and Hotelling (1933) and deep neural networks (e.g., deep auto-encoders (Deng et al., 2010)), heavily exploit the idea of dimension- ality reduction. In the following, we will focus on orthogonal projections, which we will use in Chapter 10 for linear dimensionality reduction and in Chapter 12 for classiﬁcation. Even linear regression, which we discuss in Chapter 9, can be interpreted using orthogonal projections. For a given lower-dimensional subspace, orthogonal projections of high-dimensional data retain as much information as possible and minimize the difference/ error between the original data and the corresponding projection. An il- lustration of such an orthogonal projection is given in Figure 3.9. Before we detail how to obtain these projections, let us deﬁne what a projection actually is. Deﬁnition 3.10 (Projection). Let V be a vector space and U ⊆ V a subspace of V . A linear mapping π : V → U is called a projection ifprojection π2 = π ◦ π = π. Since linear mappings can be expressed by transformation matrices (see Section 2.7), the preceding deﬁnition applies equally to a special kind of transformation matrices, the projection matrices P π, which exhibit theprojection matrix property that P 2 π = P π. In the following, we will derive orthogonal projections of vectors in the inner product space (R n, ⟨·, ·⟩) onto subspaces. We will start with one- dimensional subspaces, which are also called lines. If not mentioned oth-line erwise, we assume the dot product ⟨x, y⟩ = x⊤y as the inner product. 3.8.1 Projection onto One-Dimensional Subspaces (Lines) Assume we are given a line (one-dimensional subspace) through the ori- gin with basis vector b ∈ R n. The line is a one-dimensional subspace U ⊆ R n spanned by b. When we project x ∈ Rn onto U , we seek the vector πU (x) ∈ U that is closest to x. Using geometric arguments, let Draft (2019-12-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com. 3.8 Orthogonal Projections 83 Figure 3.10 Examples of projections onto one-dimensional subspaces. b x πU (x) ω (a) Projection of x ∈ R2 onto a subspace U with basis vector b. cos ωω sin ω b x (b) Projection of a two-dimensional vector x with ∥x∥ = 1 onto a one-dimensional subspace spanned by b. us characterize some properties of the projection πU (x) (Figure 3.10(a) serves as an illustration): The projection πU (x) is closest to x, where “closest” implies that the distance ∥x − πU (x)∥ is minimal. It follows that the segment πU (x) − x from πU (x) to x is orthogonal to U , and therefore the basis vector b of U . The orthogonality condition yields ⟨πU (x) − x, b⟩ = 0 since angles between vectors are deﬁned via the inner product. λ is then the coordinate of πU (x) with respect to b. The projection πU (x) of x onto U must be an element of U and, there- fore, a multiple of the basis vector b that spans U . Hence, πU (x) = λb, for some λ ∈ R. In the following three steps, we determine the coordinate λ, the projection πU (x) ∈ U , and the projection matrix P π that maps any x ∈ R n onto U : 1. Finding the coordinate λ. The orthogonality condition yields ⟨x − πU (x), b⟩ = 0 πU (x)=λb ⇐⇒ ⟨x − λb, b⟩ = 0 . (3.39) We can now exploit the bilinearity of the inner product and arrive at With a general inner product, we get λ = ⟨x, b⟩ if ∥b∥ = 1.⟨x, b⟩ − λ ⟨b, b⟩ = 0 ⇐⇒ λ = ⟨x, b⟩ ⟨b, b⟩ = ⟨b, x⟩ ∥b∥2 . (3.40) In the last step, we exploited the fact that inner products are symmet- ric. If we choose ⟨·, ·⟩ to be the dot product, we obtain λ = b⊤x b ⊤b = b⊤x ∥b∥2 . (3.41) If ∥b∥ = 1, then the coordinate λ of the projection is given by b⊤x. c⃝2019 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press. 84 Analytic Geometry 2. Finding the projection point πU (x) ∈ U . Since πU (x) = λb, we imme- diately obtain with (3.40) that πU (x) = λb = ⟨x, b⟩ ∥b∥2 b = b⊤x ∥b∥2 b , (3.42) where the last equality holds for the dot product only. We can also compute the length of πU (x) by means of Deﬁnition 3.1 as ∥πU (x)∥ = ∥λb∥ = |λ| ∥b∥ . (3.43) Hence, our projection is of length |λ| times the length of b. This also adds the intuition that λ is the coordinate of πU (x) with respect to the basis vector b that spans our one-dimensional subspace U . If we use the dot product as an inner product, we get ∥πU (x)∥ (3.42) = |b⊤x| ∥b∥2 ∥b∥ (3.25) = | cos ω| ∥x∥ ∥b∥ ∥b∥ ∥b∥2 = | cos ω| ∥x∥ . (3.44) Here, ω is the angle between x and b. This equation should be familiar from trigonometry: If ∥x∥ = 1, then x lies on the unit circle. It follows that the projection onto the horizontal axis spanned by b is exactlyThe horizontal axis is a one-dimensional subspace. cos ω, and the length of the corresponding vector πU (x) = |cos ω|. An illustration is given in Figure 3.10(b). 3. Finding the projection matrix P π. We know that a projection is a lin- ear mapping (see Deﬁnition 3.10). Therefore, there exists a projection matrix P π, such that πU (x) = P πx. With the dot product as inner product and πU (x) = λb = bλ = b b⊤x ∥b∥2 = bb⊤ ∥b∥2 x , (3.45) we immediately see that P π = bb⊤ ∥b∥2 . (3.46) Note that bb⊤ (and, consequently, P π) is a symmetric matrix (of rankProjection matrices are always symmetric. 1), and ∥b∥ 2 = ⟨b, b⟩ is a scalar. The projection matrix P π projects any vector x ∈ R n onto the line through the origin with direction b (equivalently, the subspace U spanned by b). Remark. The projection πU (x) ∈ Rn is still an n-dimensional vector and not a scalar. However, we no longer require n coordinates to represent the projection, but only a single one if we want to express it with respect to the basis vector b that spans the subspace U : λ. ♦ Draft (2019-12-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com. 3.8 Orthogonal Projections 85 Figure 3.11 Projection onto a two-dimensional subspace U with basis b1, b2. The projection πU (x) of x ∈ R3 onto U can be expressed as a linear combination of b1, b2 and the displacement vector x − πU (x) is orthogonal to both b1 and b2. 0 x b1 b2 U πU (x) x − πU (x) Example 3.10 (Projection onto a Line) Find the projection matrix P π onto the line through the origin spanned by b = [ 1 2 2]⊤. b is a direction and a basis of the one-dimensional subspace (line through origin). With (3.46), we obtain P π = bb⊤ b⊤b = 1 9   1 2 2   [ 1 2 2 ] = 1 9   1 2 2 2 4 4 2 4 4   . (3.47) Let us now choose a particular x and see whether it lies in the subspace spanned by b. For x = [ 1 1 1]⊤, the projection is πU (x) = P πx = 1 9   1 2 2 2 4 4 2 4 4     1 1 1   = 1 9   5 10 10   ∈ span[   1 2 2  ] . (3.48) Note that the application of P π to πU (x) does not change anything, i.e., P ππU (x) = πU (x). This is expected because according to Deﬁnition 3.10, we know that a projection matrix P π satisﬁes P 2 πx = P πx for all x. Remark. With the results from Chapter 4, we can show that πU (x) is an eigenvector of P π, and the corresponding eigenvalue is 1. ♦ 3.8.2 Projection onto General Subspaces If U is given by a set of spanning vectors, which are not a basis, make sure you determine a basis b1, . . . , bm before proceeding. In the following, we look at orthogonal projections of vectors x ∈ Rn onto lower-dimensional subspaces U ⊆ Rn with dim(U ) = m ⩾ 1. An illustration is given in Figure 3.11. Assume that (b1, . . . , bm) is an ordered basis of U . Any projection πU (x) onto U is necessarily an element of U . Therefore, they can be represented c⃝2019 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press. 86 Analytic Geometry as linear combinations of the basis vectors b1, . . . , bm of U , such that πU (x) = ∑m i=1 λibi.The basis vectors form the columns of B ∈ Rn×m, where B = [b1, . . . , bm]. As in the 1D case, we follow a three-step procedure to ﬁnd the projec- tion πU (x) and the projection matrix P π: 1. Find the coordinates λ1, . . . , λm of the projection (with respect to the basis of U ), such that the linear combination πU (x) = m∑ i=1 λibi = Bλ , (3.49) B = [b1, . . . , bm] ∈ R n×m, λ = [λ1, . . . , λm]⊤ ∈ R m , (3.50) is closest to x ∈ R n. As in the 1D case, “closest” means “minimum distance”, which implies that the vector connecting πU (x) ∈ U and x ∈ Rn must be orthogonal to all basis vectors of U . Therefore, we obtain m simultaneous conditions (assuming the dot product as the inner product) ⟨b1, x − πU (x)⟩ = b⊤ 1 (x − πU (x)) = 0 (3.51) ... ⟨bm, x − πU (x)⟩ = b⊤ m(x − πU (x)) = 0 (3.52) which, with πU (x) = Bλ, can be written as b⊤ 1 (x − Bλ) = 0 (3.53) ... b⊤ m(x − Bλ) = 0 (3.54) such that we obtain a homogeneous linear equation system    b ⊤ 1 ... b⊤ m     x − Bλ   = 0 ⇐⇒ B⊤(x − Bλ) = 0 (3.55) ⇐⇒ B⊤Bλ = B⊤x . (3.56) The last expression is called normal equation. Since b1, . . . , bm are anormal equation basis of U and, therefore, linearly independent, B⊤B ∈ Rm×m is reg- ular and can be inverted. This allows us to solve for the coefﬁcients/ coordinates λ = (B⊤B)−1B⊤x . (3.57) The matrix (B⊤B) −1B⊤ is also called the pseudo-inverse of B, whichpseudo-inverse can be computed for non-square matrices B. It only requires that B⊤B is positive deﬁnite, which is the case if B is full rank. In practical ap- plications (e.g., linear regression), we often add a “jitter term” ϵI to Draft (2019-12-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com. 3.8 Orthogonal Projections 87 B⊤B to guarantee increased numerical stability and positive deﬁnite- ness. This “ridge” can be rigorously derived using Bayesian inference. See Chapter 9 for details. 2. Find the projection πU (x) ∈ U . We already established that πU (x) = Bλ. Therefore, with (3.57) πU (x) = B(B⊤B) −1B⊤x . (3.58) 3. Find the projection matrix P π. From (3.58), we can immediately see that the projection matrix that solves P πx = πU (x) must be P π = B(B⊤B)−1B⊤ . (3.59) Remark. The solution for projecting onto general subspaces includes the 1D case as a special case: If dim(U ) = 1, then B⊤B ∈ R is a scalar and we can rewrite the projection matrix in (3.59) P π = B(B⊤B)−1B⊤ as P π = BB⊤ B⊤B , which is exactly the projection matrix in (3.46). ♦ Example 3.11 (Projection onto a Two-dimensional Subspace) For a subspace U = span[   1 1 1   ,   0 1 2  ] ⊆ R 3 and x =   6 0 0   ∈ R 3 ﬁnd the coordinates λ of x in terms of the subspace U , the projection point πU (x) and the projection matrix P π. First, we see that the generating set of U is a basis (linear indepen- dence) and write the basis vectors of U into a matrix B =   1 0 1 1 1 2  . Second, we compute the matrix B⊤B and the vector B⊤x as B⊤B = [ 1 1 1 0 1 2 ]   1 0 1 1 1 2   = [ 3 3 3 5 ] , B⊤x = [ 1 1 1 0 1 2 ]   6 0 0   = [ 6 0 ] . (3.60) Third, we solve the normal equation B⊤Bλ = B⊤x to ﬁnd λ: [ 3 3 3 5 ] [ λ1 λ2 ] = [ 6 0 ] ⇐⇒ λ = [ 5 −3 ] . (3.61) Fourth, the projection πU (x) of x onto U , i.e., into the column space of B, can be directly computed via πU (x) = Bλ =   5 2 −1   . (3.62) c⃝2019 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press. 88 Analytic Geometry The corresponding projection error is the norm of the difference vectorprojection error between the original vector and its projection onto U , i.e.,The projection error is also called the reconstruction error. ∥x − πU (x)∥ = ∥ ∥ ∥ [ 1 −2 1]⊤∥ ∥ ∥ = √6 . (3.63) Fifth, the projection matrix (for any x ∈ R3) is given by P π = B(B⊤B) −1B⊤ = 1 6   5 2 −1 2 2 2 −1 2 5   . (3.64) To verify the results, we can (a) check whether the displacement vector πU (x) − x is orthogonal to all basis vectors of U , and (b) verify that P π = P 2 π (see Deﬁnition 3.10). Remark. The projections πU (x) are still vectors in Rn although they lie in an m-dimensional subspace U ⊆ Rn. However, to represent a projected vector we only need the m coordinates λ1, . . . , λm with respect to the basis vectors b1, . . . , bm of U . ♦ Remark. In vector spaces with general inner products, we have to pay attention when computing angles and distances, which are deﬁned by means of the inner product. ♦ We can ﬁnd approximate solutions to unsolvable linear equation systems using projections. Projections allow us to look at situations where we have a linear system Ax = b without a solution. Recall that this means that b does not lie in the span of A, i.e., the vector b does not lie in the subspace spanned by the columns of A. Given that the linear equation cannot be solved exactly, we can ﬁnd an approximate solution. The idea is to ﬁnd the vector in the subspace spanned by the columns of A that is closest to b, i.e., we compute the orthogonal projection of b onto the subspace spanned by the columns of A. This problem arises often in practice, and the solution is called the least-squares solution (assuming the dot product as the inner product) ofleast-squares solution an overdetermined system. This is discussed further in Section 9.4. Using reconstruction errors (3.63) is one possible approach to derive principal component analysis (Section 10.3). Remark. We just looked at projections of vectors x onto a subspace U with basis vectors {b1, . . . , bk}. If this basis is an ONB, i.e., (3.33) and (3.34) are satisﬁed, the projection equation (3.58) simpliﬁes greatly to πU (x) = BB⊤x (3.65) since B⊤B = I with coordinates λ = B⊤x . (3.66) This means that we no longer have to compute the inverse from (3.58), which saves computation time. ♦ Draft (2019-12-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com. 3.8 Orthogonal Projections 89 3.8.3 Gram-Schmidt Orthogonalization Projections are at the core of the Gram-Schmidt method that allows us to constructively transform any basis (b1, . . . , bn) of an n-dimensional vector space V into an orthogonal/orthonormal basis (u1, . . . , un) of V . This basis always exists (Liesen and Mehrmann, 2015) and span[b1, . . . , bn] = span[u1, . . . , un]. The Gram-Schmidt orthogonalization method iteratively Gram-Schmidt orthogonalizationconstructs an orthogonal basis (u1, . . . , un) from any basis (b1, . . . , bn) of V as follows: u1 := b1 (3.67) uk := bk − πspan[u1,...,uk−1](bk) , k = 2, . . . , n . (3.68) In (3.68), the kth basis vector bk is projected onto the subspace spanned by the ﬁrst k − 1 constructed orthogonal vectors u1, . . . , uk−1; see Sec- tion 3.8.2. This projection is then subtracted from bk and yields a vector uk that is orthogonal to the (k − 1)-dimensional subspace spanned by u1, . . . , uk−1. Repeating this procedure for all n basis vectors b1, . . . , bn yields an orthogonal basis (u1, . . . , un) of V . If we normalize the uk, we obtain an ONB where ∥uk∥ = 1 for k = 1, . . . , n. Example 3.12 (Gram-Schmidt Orthogonalization) Figure 3.12 Gram-Schmidt orthogonalization. (a) non-orthogonal basis (b1, b2) of R2; (b) ﬁrst constructed basis vector u1 and orthogonal projection of b2 onto span[u1]; (c) orthogonal basis (u1, u2) of R2. b1 b2 0 (a) Original non-orthogonal basis vectors b1, b2. u1 b2 0 πspan[u1](b2) (b) First new basis vector u1 = b1 and projection of b2 onto the subspace spanned by u1. u1 b2 0 πspan[u1](b2) u2 (c) Orthogonal basis vectors u1 and u2 = b2 − πspan[u1](b2). Consider a basis (b1, b2) of R2, where b1 = [ 2 0 ] , b2 = [ 1 1 ] ; (3.69) see also Figure 3.12(a). Using the Gram-Schmidt method, we construct an orthogonal basis (u1, u2) of R 2 as follows (assuming the dot product as the inner product): u1 := b1 = [ 2 0 ] , (3.70) u2 := b2 − πspan[u1](b2) (3.45) = b2 − u1u ⊤ 1 ∥u1∥2 b2 = [ 1 1 ] − [ 1 0 0 0 ] [ 1 1 ] = [ 0 1 ] . (3.71) c⃝2019 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press. 90 Analytic Geometry Figure 3.13 Projection onto an afﬁne space. (a) original setting; (b) setting shifted by −x0 so that x − x0 can be projected onto the direction space U ; (c) projection is translated back to x0 + πU (x − x0), which gives the ﬁnal orthogonal projection πL(x). L x0 x b2 b10 (a) Setting. b10 x − x0 U = L − x0 πU (x − x0) b2 (b) Reduce problem to pro- jection πU onto vector sub- space. L x0 x b2 b10 πL(x) (c) Add support point back in to get afﬁne projection πL. These steps are illustrated in Figures 3.12(b) and (c). We immediately see that u1 and u2 are orthogonal, i.e., u ⊤ 1 u2 = 0. 3.8.4 Projection onto Afﬁne Subspaces Thus far, we discussed how to project a vector onto a lower-dimensional subspace U . In the following, we provide a solution to projecting a vector onto an afﬁne subspace. Consider the setting in Figure 3.13(a). We are given an afﬁne space L = x0 + U , where b1, b2 are basis vectors of U . To determine the orthogonal projection πL(x) of x onto L, we transform the problem into a problem that we know how to solve: the projection onto a vector subspace. In order to get there, we subtract the support point x0 from x and from L, so that L − x0 = U is exactly the vector subspace U . We can now use the orthogonal projections onto a subspace we discussed in Section 3.8.2 and obtain the projection πU (x − x0), which is illustrated in Figure 3.13(b). This projection can now be translated back into L by adding x0, such that we obtain the orthogonal projection onto an afﬁne space L as πL(x) = x0 + πU (x − x0) , (3.72) where πU (·) is the orthogonal projection onto the subspace U , i.e., the direction space of L; see Figure 3.13(c). From Figure 3.13, it is also evident that the distance of x from the afﬁne space L is identical to the distance of x − x0 from U , i.e., d(x, L) = ∥x − πL(x)∥ = ∥x − (x0 + πU (x − x0))∥ (3.73a) = d(x − x0, πU (x − x0)) . (3.73b) We will use projections onto an afﬁne subspace to derive the concept of a separating hyperplane in Section 12.1. Draft (2019-12-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com. 3.9 Rotations 91 Figure 3.14 A rotation rotates objects in a plane about the origin. If the rotation angle is positive, we rotate counterclockwise. Original Rotated by 112.5◦ Figure 3.15 The robotic arm needs to rotate its joints in order to pick up objects or to place them correctly. Figure taken from (Deisenroth et al., 2015). 3.9 Rotations Length and angle preservation, as discussed in Section 3.4, are the two characteristics of linear mappings with orthogonal transformation matri- ces. In the following, we will have a closer look at speciﬁc orthogonal transformation matrices, which describe rotations. A rotation is a linear mapping (more speciﬁcally, an automorphism of rotation a Euclidean vector space) that rotates a plane by an angle θ about the origin, i.e., the origin is a ﬁxed point. For a positive angle θ > 0, by com- mon convention, we rotate in a counterclockwise direction. An example is shown in Figure 3.14, where the transformation matrix is R = [ −0.38 −0.92 0.92 −0.38 ] . (3.74) Important application areas of rotations include computer graphics and robotics. For example, in robotics, it is often important to know how to rotate the joints of a robotic arm in order to pick up or place an object, see Figure 3.15. c⃝2019 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press. 92 Analytic Geometry Figure 3.16 Rotation of the standard basis in R2 by an angle θ. e1 e2 θ θ Φ(e2) = [− sin θ, cos θ] ⊤ Φ(e1) = [cos θ, sin θ] ⊤ cos θ sin θ − sin θ cos θ 3.9.1 Rotations in R2 Consider the standard basis {e1 = [ 1 0 ] , e2 = [ 0 1 ]} of R 2, which deﬁnes the standard coordinate system in R2. We aim to rotate this coordinate system by an angle θ as illustrated in Figure 3.16. Note that the rotated vectors are still linearly independent and, therefore, are a basis of R 2. This means that the rotation performs a basis change. Rotations Φ are linear mappings so that we can express them by a rotation matrix R(θ). Trigonometry (see Figure 3.16) allows us to de-rotation matrix termine the coordinates of the rotated axes (the image of Φ) with respect to the standard basis in R 2. We obtain Φ(e1) = [ cos θ sin θ ] , Φ(e2) = [ − sin θ cos θ ] . (3.75) Therefore, the rotation matrix that performs the basis change into the rotated coordinates R(θ) is given as R(θ) = [ Φ(e1) Φ(e2) ] = [ cos θ − sin θ sin θ cos θ ] . (3.76) 3.9.2 Rotations in R3 In contrast to the R2 case, in R 3 we can rotate any two-dimensional plane about a one-dimensional axis. The easiest way to specify the general rota- tion matrix is to specify how the images of the standard basis e1, e2, e3 are supposed to be rotated, and making sure these images Re1, Re2, Re3 are orthonormal to each other. We can then obtain a general rotation matrix R by combining the images of the standard basis. To have a meaningful rotation angle, we have to deﬁne what “coun- terclockwise” means when we operate in more than two dimensions. We use the convention that a “counterclockwise” (planar) rotation about an axis refers to a rotation about an axis when we look at the axis “head on, from the end toward the origin”. In R 3, there are therefore three (planar) rotations about the three standard basis vectors (see Figure 3.17): Draft (2019-12-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com. 3.9 Rotations 93 Figure 3.17 Rotation of a vector (gray) in R3 by an angle θ about the e3-axis. The rotated vector is shown in blue. e1 e2 e3 θ Rotation about the e1-axis R1(θ) = [ Φ(e1) Φ(e2) Φ(e3) ] =   1 0 0 0 cos θ − sin θ 0 sin θ cos θ   . (3.77) Here, the e1 coordinate is ﬁxed, and the counterclockwise rotation is performed in the e2e3 plane. Rotation about the e2-axis R2(θ) =   cos θ 0 sin θ 0 1 0 − sin θ 0 cos θ   . (3.78) If we rotate the e1e3 plane about the e2 axis, we need to look at the e2 axis from its “tip” toward the origin. Rotation about the e3-axis R3(θ) =   cos θ − sin θ 0 sin θ cos θ 0 0 0 1   . (3.79) Figure 3.17 illustrates this. 3.9.3 Rotations in n Dimensions The generalization of rotations from 2D and 3D to n-dimensional Eu- clidean vector spaces can be intuitively described as ﬁxing n − 2 dimen- sions and restrict the rotation to a two-dimensional plane in the n-dimen- sional space. As in the three-dimensional case, we can rotate any plane (two-dimensional subspace of R n). Deﬁnition 3.11 (Givens Rotation). Let V be an n-dimensional Euclidean vector space and Φ : V → V an automorphism with transformation ma- c⃝2019 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press. 94 Analytic Geometry trix Rij(θ) :=       I i−1 0 · · · · · · 0 0 cos θ 0 − sin θ 0 0 0 I j−i−1 0 0 0 sin θ 0 cos θ 0 0 · · · · · · 0 I n−j       ∈ Rn×n , (3.80) for 1 ⩽ i < j ⩽ n and θ ∈ R. Then Rij(θ) is called a Givens rotation.Givens rotation Essentially, Rij(θ) is the identity matrix I n with rii = cos θ , rij = − sin θ , rji = sin θ , rjj = cos θ . (3.81) In two dimensions (i.e., n = 2), we obtain (3.76) as a special case. 3.9.4 Properties of Rotations Rotations exhibit a number of useful properties, which can be derived by considering them as orthogonal matrices (Deﬁnition 3.8): Rotations preserve distances, i.e., ∥x−y∥ = ∥Rθ(x)−Rθ(y)∥. In other words, rotations leave the distance between any two points unchanged after the transformation. Rotations preserve angles, i.e., the angle between Rθx and Rθy equals the angle between x and y. Rotations in three (or more) dimensions are generally not commuta- tive. Therefore, the order in which rotations are applied is important, even if they rotate about the same point. Only in two dimensions vector rotations are commutative, such that R(φ)R(θ) = R(θ)R(φ) for all φ, θ ∈ [0, 2π). They form an Abelian group (with multiplication) only if they rotate about the same point (e.g., the origin). 3.10 Further Reading In this chapter, we gave a brief overview of some of the important concepts of analytic geometry, which we will use in later chapters of the book. For a broader and more in-depth overview of some the concepts we presented, we refer to the following excellent books: Axler (2015) and Boyd and Vandenberghe (2018). Inner products allow us to determine speciﬁc bases of vector (sub)spaces, where each vector is orthogonal to all others (orthogonal bases) using the Gram-Schmidt method. These bases are important in optimization and numerical algorithms for solving linear equation systems. For instance, Krylov subspace methods, such as conjugate gradients or the generalized minimal residual method (GMRES), minimize residual errors that are or- thogonal to each other (Stoer and Burlirsch, 2002). In machine learning, inner products are important in the context of Draft (2019-12-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com. 3.10 Further Reading 95 kernel methods (Sch¨olkopf and Smola, 2002). Kernel methods exploit the fact that many linear algorithms can be expressed purely by inner prod- uct computations. Then, the “kernel trick” allows us to compute these inner products implicitly in a (potentially inﬁnite-dimensional) feature space, without even knowing this feature space explicitly. This allowed the “non-linearization” of many algorithms used in machine learning, such as kernel-PCA (Sch¨olkopf et al., 1997) for dimensionality reduction. Gaus- sian processes (Rasmussen and Williams, 2006) also fall into the category of kernel methods and are the current state of the art in probabilistic re- gression (ﬁtting curves to data points). The idea of kernels is explored further in Chapter 12. Projections are often used in computer graphics, e.g., to generate shad- ows. In optimization, orthogonal projections are often used to (iteratively) minimize residual errors. This also has applications in machine learning, e.g., in linear regression where we want to ﬁnd a (linear) function that minimizes the residual errors, i.e., the lengths of the orthogonal projec- tions of the data onto the linear function (Bishop, 2006). We will investi- gate this further in Chapter 9. PCA (Pearson, 1901; Hotelling, 1933) also uses projections to reduce the dimensionality of high-dimensional data. We will discuss this in more detail in Chapter 10. c⃝2019 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press. 96 Analytic Geometry Exercises 3.1 Show that ⟨·, ·⟩ deﬁned for all x = [x1, x2] ⊤ ∈ R2 and y = [y1, y2]⊤ ∈ R2 by ⟨x, y⟩ := x1y1 − (x1y2 + x2y1) + 2(x2y2) is an inner product. 3.2 Consider R2 with ⟨·, ·⟩ deﬁned for all x and y in R2 as ⟨x, y⟩ := x⊤ [ 2 0 1 2 ] ︸ ︷︷ ︸ =:A y . Is ⟨·, ·⟩ an inner product? 3.3 Compute the distance between x =  1 2 3   , y =   −1 −1 0   using a. ⟨x, y⟩ := x⊤y b. ⟨x, y⟩ := x⊤Ay , A :=  2 1 0 1 3 −1 0 −1 2   3.4 Compute the angle between x = [ 1 2 ] , y = [ −1 −1 ] using a. ⟨x, y⟩ := x⊤y b. ⟨x, y⟩ := x⊤By , B := [ 2 1 1 3 ] 3.5 Consider the Euclidean vector space R5 with the dot product. A subspace U ⊆ R5 and x ∈ R5 are given by U = span[       0 −1 2 0 2       ,       1 −3 1 −1 2       ,       −3 4 1 2 1       ,       −1 −3 5 0 7      ] , x =       −1 −9 −1 4 1       a. Determine the orthogonal projection πU (x) of x onto U b. Determine the distance d(x, U ) 3.6 Consider R3 with the inner product ⟨x, y⟩ := x⊤   2 1 0 1 2 −1 0 −1 2   y . Furthermore, we deﬁne e1, e2, e3 as the standard/canonical basis in R3. Draft (2019-12-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com. Exercises 97 a. Determine the orthogonal projection πU (e2) of e2 onto U = span[e1, e3] . Hint: Orthogonality is deﬁned through the inner product. b. Compute the distance d(e2, U ). c. Draw the scenario: standard basis vectors and πU (e2) 3.7 Let V be a vector space and π an endomorphism of V . a. Prove that π is a projection if and only if idV − π is a projection, where idV is the identity endomorphism on V . b. Assume now that π is a projection. Calculate Im(idV −π) and ker(idV −π) as a function of Im(π) and ker(π). 3.8 Using the Gram-Schmidt method, turn the basis B = (b1, b2) of a two- dimensional subspace U ⊆ R3 into an ONB C = (c1, c2) of U , where b1 :=  1 1 1   , b2 :=  −1 2 0   . 3.9 Let n ∈ N ∗ and let x1, . . . , xn > 0 be n positive real numbers so that x1 + · · · + xn = 1. Use the Cauchy-Schwarz inequality and show that a. ∑n i=1 x 2 i ⩾ 1 n b. ∑n i=1 1 xi ⩾ n 2 Hint: Think about the dot product on Rn. Then, choose speciﬁc vectors x, y ∈ Rn and apply the Cauchy-Schwarz inequality. 3.10 Rotate the vectors x1 := [ 2 3 ] , x2 := [ 0 −1 ] by 30 ◦. c⃝2019 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press. 4 Matrix Decompositions In Chapters 2 and 3, we studied ways to manipulate and measure vectors, projections of vectors, and linear mappings. Mappings and transforma- tions of vectors can be conveniently described as operations performed by matrices. Moreover, data is often represented in matrix form as well, e.g., where the rows of the matrix represent different people and the columns describe different features of the people, such as weight, height, and socio- economic status. In this chapter, we present three aspects of matrices: how to summarize matrices, how matrices can be decomposed, and how these decompositions can be used for matrix approximations. We ﬁrst consider methods that allow us to describe matrices with just a few numbers that characterize the overall properties of matrices. We will do this in the sections on determinants (Section 4.1) and eigenval- ues (Section 4.2) for the important special case of square matrices. These characteristic numbers have important mathematical consequences and allow us to quickly grasp what useful properties a matrix has. From here we will proceed to matrix decomposition methods: An analogy for ma- trix decomposition is the factoring of numbers, such as the factoring of 21 into prime numbers 7 · 3. For this reason matrix decomposition is also often referred to as matrix factorization. Matrix decompositions are usedmatrix factorization to describe a matrix by means of a different representation using factors of interpretable matrices. We will ﬁrst cover a square-root-like operation for symmetric, positive deﬁnite matrices, the Cholesky decomposition (Section 4.3). From here we will look at two related methods for factorizing matrices into canoni- cal forms. The ﬁrst one is known as matrix diagonalization (Section 4.4), which allows us to represent the linear mapping using a diagonal trans- formation matrix if we choose an appropriate basis. The second method, singular value decomposition (Section 4.5), extends this factorization to non-square matrices, and it is considered one of the fundamental concepts in linear algebra. These decompositions are helpful, as matrices represent- ing numerical data are often very large and hard to analyze. We conclude the chapter with a systematic overview of the types of matrices and the characteristic properties that distinguish them in the form of a matrix tax- onomy (Section 4.7). The methods that we cover in this chapter will become important in 98 This material will be published by Cambridge University Press as Mathematics for Machine Learn- ing by Marc Peter Deisenroth, A. Aldo Faisal, and Cheng Soon Ong. This pre-publication version is free to view and download for personal use only. Not for re-distribution, re-sale or use in deriva- tive works. c⃝by M. P. Deisenroth, A. A. Faisal, and C. S. Ong, 2019. https://mml-book.com. 4.1 Determinant and Trace 99 Figure 4.1 A mind map of the concepts introduced in this chapter, along with where they are used in other parts of the book. Determinant Invertibility Cholesky Eigenvalues Eigenvectors Orthogonal matrix Diagonalization SVD Chapter 6 Probability & distributions Chapter 10 Dimensionality reduction tests used inusedinusedindetermines used in used inusedin constructs used inusedinusedin both subsequent mathematical chapters, such as Chapter 6, but also in applied chapters, such as dimensionality reduction in Chapters 10 or den- sity estimation in Chapter 11. This chapter’s overall structure is depicted in the mind map of Figure 4.1. 4.1 Determinant and Trace The determinant notation |A| must not be confused with the absolute value. Determinants are important concepts in linear algebra. A determinant is a mathematical object in the analysis and solution of systems of linear equations. Determinants are only deﬁned for square matrices A ∈ R n×n, i.e., matrices with the same number of rows and columns. In this book, we write the determinant as det(A) or sometimes as |A| so that det(A) = ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ a11 a12 . . . a1n a21 a22 . . . a2n ... . . . ... an1 an2 . . . ann ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ . (4.1) The determinant of a square matrix A ∈ R n×n is a function that maps A determinant c⃝2019 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press. 100 Matrix Decompositions onto a real number. Before providing a deﬁnition of the determinant for general n × n matrices, let us have a look at some motivating examples, and deﬁne determinants for some special matrices. Example 4.1 (Testing for Matrix Invertibility) Let us begin with exploring if a square matrix A is invertible (see Sec- tion 2.2.2). For the smallest cases, we already know when a matrix is invertible. If A is a 1 × 1 matrix, i.e., it is a scalar number, then A = a =⇒ A−1 = 1 a . Thus a 1 a = 1 holds, if and only if a ̸= 0. For 2 × 2 matrices, by the deﬁnition of the inverse (Deﬁnition 2.3), we know that AA−1 = I. Then, with (2.24), the inverse of A is A −1 = 1 a11a22 − a12a21 [ a22 −a12 −a21 a11 ] . (4.2) Hence, A is invertible if and only if a11a22 − a12a21 ̸= 0 . (4.3) This quantity is the determinant of A ∈ R2×2, i.e., det(A) = ∣ ∣ ∣ ∣ ∣a11 a12 a21 a22 ∣ ∣ ∣ ∣ ∣ = a11a22 − a12a21 . (4.4) Example 4.1 points already at the relationship between determinants and the existence of inverse matrices. The next theorem states the same result for n × n matrices. Theorem 4.1. For any square matrix A ∈ R n×n it holds that A is invertible if and only if det(A) ̸= 0. We have explicit (closed-form) expressions for determinants of small matrices in terms of the elements of the matrix. For n = 1, det(A) = det(a11) = a11 . (4.5) For n = 2, det(A) = ∣ ∣ ∣ ∣a11 a12 a21 a22 ∣ ∣ ∣ ∣ = a11a22 − a12a21 , (4.6) which we have observed in the preceding example. For n = 3 (known as Sarrus’ rule), ∣ ∣ ∣ ∣ ∣ ∣ a11 a12 a13 a21 a22 a23 a31 a32 a33 ∣ ∣ ∣ ∣ ∣ ∣ = a11a22a33 + a21a32a13 + a31a12a23 (4.7) − a31a22a13 − a11a32a23 − a21a12a33 . Draft (2019-12-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com. 4.1 Determinant and Trace 101 For a memory aid of the product terms in Sarrus’ rule, try tracing the elements of the triple products in the matrix. We call a square matrix T an upper-triangular matrix if Tij = 0 for upper-triangular matrixi > j, i.e., the matrix is zero below its diagonal. Analogously, we deﬁne a lower-triangular matrix as a matrix with zeros above its diagonal. For a tri- lower-triangular matrixangular matrix T ∈ R n×n, the determinant is the product of the diagonal elements, i.e., det(T ) = n∏ i=1 Tii . (4.8) The determinant is the signed volume of the parallelepiped formed by the columns of the matrix. Figure 4.2 The area of the parallelogram (shaded region) spanned by the vectors b and g is |det([b, g])|. b g Figure 4.3 The volume of the parallelepiped (shaded volume) spanned by vectors r, b, g is |det([r, b, g])|. b g r Example 4.2 (Determinants as Measures of Volume) The notion of a determinant is natural when we consider it as a mapping from a set of n vectors spanning an object in R n. It turns out that the de- terminant det(A) is the signed volume of an n-dimensional parallelepiped formed by columns of the matrix A. For n = 2, the columns of the matrix form a parallelogram; see Fig- ure 4.2. As the angle between vectors gets smaller, the area of a parallel- ogram shrinks, too. Consider two vectors b, g that form the columns of a matrix A = [b, g]. Then, the absolute value of the determinant of A is the area of the parallelogram with vertices 0, b, g, b + g. In particular, if b, g are linearly dependent so that b = λg for some λ ∈ R, they no longer form a two-dimensional parallelogram. Therefore, the corresponding area is 0. On the contrary, if b, g are linearly independent and are multiples of the canonical basis vectors e1, e2 then they can be written as b = [b 0 ] and g = [0 g ] , and the determinant is ∣ ∣ ∣ ∣b 0 0 g ∣ ∣ ∣ ∣ = bg − 0 = bg. The sign of the determinant indicates the orientation of the spanning vectors b, g with respect to the standard basis (e1, e2). In our ﬁgure, ﬂip- ping the order to g, b swaps the columns of A and reverses the orientation of the shaded area. This becomes the familiar formula: area = height × length. This intuition extends to higher dimensions. In R3, we consider three vectors r, b, g ∈ R 3 spanning the edges of a parallelepiped, i.e., a solid with faces that are parallel parallelograms (see Figure 4.3). The ab- The sign of the determinant indicates the orientation of the spanning vectors. solute value of the determinant of the 3 × 3 matrix [r, b, g] is the volume of the solid. Thus, the determinant acts as a function that measures the signed volume formed by column vectors composed in a matrix. Consider the three linearly independent vectors r, g, b ∈ R3 given as r =   2 0 −8   , g =   6 1 0   , b =   1 4 −1   . (4.9) c⃝2019 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press. 102 Matrix Decompositions Writing these vectors as the columns of a matrix A = [r, g, b] =   2 6 1 0 1 4 −8 0 −1   (4.10) allows us to compute the desired volume as V = |det(A)| = 186 . (4.11) Computing the determinant of an n × n matrix requires a general algo- rithm to solve the cases for n > 3, which we are going to explore in the fol- lowing. Theorem 4.2 below reduces the problem of computing the deter- minant of an n×n matrix to computing the determinant of (n−1)×(n−1) matrices. By recursively applying the Laplace expansion (Theorem 4.2), we can therefore compute determinants of n × n matrices by ultimately computing determinants of 2 × 2 matrices. Laplace expansion Theorem 4.2 (Laplace Expansion). Consider a matrix A ∈ Rn×n. Then, for all j = 1, . . . , n: 1. Expansion along column jdet(Ak,j ) is called a minor and (−1)k+j det(Ak,j ) a cofactor. det(A) = n∑ k=1(−1) k+jakj det(Ak,j) . (4.12) 2. Expansion along row j det(A) = n∑ k=1(−1) k+jajk det(Aj,k) . (4.13) Here Ak,j ∈ R(n−1)×(n−1) is the submatrix of A that we obtain when delet- ing row k and column j. Example 4.3 (Laplace Expansion) Let us compute the determinant of A =   1 2 3 3 1 2 0 0 1   (4.14) using the Laplace expansion along the ﬁrst row. Applying (4.13) yields ∣ ∣ ∣ ∣ ∣ ∣ 1 2 3 3 1 2 0 0 1 ∣ ∣ ∣ ∣ ∣ ∣ = (−1) 1+1 · 1 ∣ ∣ ∣ ∣1 2 0 1 ∣ ∣ ∣ ∣ + (−1) 1+2 · 2 ∣ ∣ ∣ ∣3 2 0 1 ∣ ∣ ∣ ∣ + (−1) 1+3 · 3 ∣ ∣ ∣ ∣3 1 0 0 ∣ ∣ ∣ ∣ . (4.15) Draft (2019-12-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com. 4.1 Determinant and Trace 103 We use (4.6) to compute the determinants of all 2 × 2 matrices and obtain det(A) = 1(1 − 0) − 2(3 − 0) + 3(0 − 0) = −5 . (4.16) For completeness we can compare this result to computing the determi- nant using Sarrus’ rule (4.7): det(A) = 1·1·1+3·0·3+0·2·2−0·1·3−1·0·2−3·2·1 = 1−6 = −5 . (4.17) For A ∈ R n×n the determinant exhibits the following properties: The determinant of a matrix product is the product of the corresponding determinants, det(AB) = det(A)det(B). Determinants are invariant to transposition, i.e., det(A) = det(A⊤). If A is regular (invertible), then det(A−1) = 1 det(A) . Similar matrices (Deﬁnition 2.22) possess the same determinant. There- fore, for a linear mapping Φ : V → V all transformation matrices AΦ of Φ have the same determinant. Thus, the determinant is invariant to the choice of basis of a linear mapping. Adding a multiple of a column/row to another one does not change det(A). Multiplication of a column/row with λ ∈ R scales det(A) by λ. In particular, det(λA) = λn det(A). Swapping two rows/columns changes the sign of det(A). Because of the last three properties, we can use Gaussian elimination (see Section 2.1) to compute det(A) by bringing A into row-echelon form. We can stop Gaussian elimination when we have A in a triangular form where the elements below the diagonal are all 0. Recall from (4.8) that the determinant of a triangular matrix is the product of the diagonal elements. Theorem 4.3. A square matrix A ∈ Rn×n has det(A) ̸= 0 if and only if rk(A) = n. In other words, A is invertible if and only if it is full rank. When mathematics was mainly performed by hand, the determinant calculation was considered an essential way to analyze matrix invertibil- ity. However, contemporary approaches in machine learning use direct numerical methods that superseded the explicit calculation of the deter- minant. For example, in Chapter 2, we learned that inverse matrices can be computed by Gaussian elimination. Gaussian elimination can thus be used to compute the determinant of a matrix. Determinants will play an important theoretical role for the following sections, especially when we learn about eigenvalues and eigenvectors (Section 4.2) through the characteristic polynomial. Deﬁnition 4.4. The trace of a square matrix A ∈ Rn×n is deﬁned as trace c⃝2019 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press. 104 Matrix Decompositions tr(A) := n∑ i=1 aii , (4.18) i.e. , the trace is the sum of the diagonal elements of A. The trace satisﬁes the following properties: tr(A + B) = tr(A) + tr(B) for A, B ∈ Rn×n tr(αA) = αtr(A) , α ∈ R for A ∈ Rn×n tr(I n) = n tr(AB) = tr(BA) for A ∈ Rn×k, B ∈ R k×n It can be shown that only one function satisﬁes these four properties to- gether – the trace (Gohberg et al., 2012). The properties of the trace of matrix products are more general. Specif- ically, the trace is invariant under cyclic permutations, i.e.,The trace is invariant under cyclic permutations. tr(AKL) = tr(KLA) (4.19) for matrices A ∈ R a×k, K ∈ R k×l, L ∈ R l×a. This property generalizes to products of an arbitrary number of matrices. As a special case of (4.19), it follows that for two vectors x, y ∈ R n tr(xy⊤) = tr(y⊤x) = y⊤x ∈ R . (4.20) Given a linear mapping Φ : V → V , where V is a vector space, we deﬁne the trace of this map by using the trace of matrix representation of Φ. For a given basis of V , we can describe Φ by means of the transfor- mation matrix A. Then the trace of Φ is the trace of A. For a different basis of V , it holds that the corresponding transformation matrix B of Φ can be obtained by a basis change of the form S−1AS for suitable S (see Section 2.7.2). For the corresponding trace of Φ, this means tr(B) = tr(S−1AS) (4.19) = tr(ASS−1) = tr(A) . (4.21) Hence, while matrix representations of linear mappings are basis depen- dent the trace of a linear mapping Φ is independent of the basis. In this section, we covered determinants and traces as functions char- acterizing a square matrix. Taking together our understanding of determi- nants and traces we can now deﬁne an important equation describing a matrix A in terms of a polynomial, which we will use extensively in the following sections. Deﬁnition 4.5 (Characteristic Polynomial). For λ ∈ R and a square ma- trix A ∈ R n×n pA(λ) := det(A − λI) (4.22a) = c0 + c1λ + c2λ 2 + · · · + cn−1λn−1 + (−1) nλn , (4.22b) c0, . . . , cn−1 ∈ R, is the characteristic polynomial of A. In particular,characteristic polynomial Draft (2019-12-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com. 4.2 Eigenvalues and Eigenvectors 105 c0 = det(A) , (4.23) cn−1 = (−1) n−1tr(A) . (4.24) The characteristic polynomial (4.22a) will allow us to compute eigen- values and eigenvectors, covered in the next section. 4.2 Eigenvalues and Eigenvectors We will now get to know a new way to characterize a matrix and its associ- ated linear mapping. Recall from Section 2.7.1 that every linear mapping has a unique transformation matrix given an ordered basis. We can in- terpret linear mappings and their associated transformation matrices by performing an “eigen” analysis. As we will see, the eigenvalues of a lin- Eigen is a German word meaning “characteristic”, “self”, or “own”. ear mapping will tell us how a special set of vectors, the eigenvectors, is transformed by the linear mapping. Deﬁnition 4.6. Let A ∈ Rn×n be a square matrix. Then λ ∈ R is an eigenvalue of A and x ∈ Rn\\{0} is the corresponding eigenvector of A if eigenvalue eigenvector Ax = λx . (4.25) We call (4.25) the eigenvalue equation. eigenvalue equation Remark. In the linear algebra literature and software, it is often a conven- tion that eigenvalues are sorted in descending order, so that the largest eigenvalue and associated eigenvector are called the ﬁrst eigenvalue and its associated eigenvector, and the second largest called the second eigen- value and its associated eigenvector, and so on. However, textbooks and publications may have different or no notion of orderings. We do not want to presume an ordering in this book if not stated explicitly. ♦ The following statements are equivalent: λ is an eigenvalue of A ∈ R n×n. There exists an x ∈ Rn\\{0} with Ax = λx, or equivalently, (A − λI n)x = 0 can be solved non-trivially, i.e., x ̸= 0. rk(A − λI n) < n. det(A − λI n) = 0. Deﬁnition 4.7 (Collinearity and Codirection). Two vectors that point in the same direction are called codirected. Two vectors are collinear if they codirected collinearpoint in the same or the opposite direction. Remark (Non-uniqueness of eigenvectors). If x is an eigenvector of A associated with eigenvalue λ, then for any c ∈ R\\{0} it holds that cx is an eigenvector of A with the same eigenvalue since A(cx) = cAx = cλx = λ(cx) . (4.26) Thus, all vectors that are collinear to x are also eigenvectors of A. ♦ c⃝2019 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press. 106 Matrix Decompositions Theorem 4.8. λ ∈ R is eigenvalue of A ∈ Rn×n if and only if λ is a root of the characteristic polynomial pA(λ) of A. Deﬁnition 4.9. Let a square matrix A have an eigenvalue λi. The algebraicalgebraic multiplicity multiplicity of λi is the number of times the root appears in the character- istic polynomial. Deﬁnition 4.10 (Eigenspace and Eigenspectrum). For A ∈ Rn×n, the set of all eigenvectors of A associated with an eigenvalue λ spans a subspace of R n, which is called the eigenspace of A with respect to λ and is denotedeigenspace by Eλ. The set of all eigenvalues of A is called the eigenspectrum, or justeigenspectrum spectrum, of A.spectrum If λ is an eigenvalue of A ∈ Rn×n, then the corresponding eigenspace Eλ is the solution space of the homogeneous system of linear equations (A−λI)x = 0. Geometrically, the eigenvector corresponding to a nonzero eigenvalue points in a direction that is stretched by the linear mapping. The eigenvalue is the factor by which it is stretched. If the eigenvalue is negative, the direction of the stretching is ﬂipped. Example 4.4 (The Case of the Identity Matrix) The identity matrix I ∈ R n×n has characteristic polynomial pI(λ) = det(I − λI) = (1 − λ)n = 0, which has only one eigenvalue λ = 1 that oc- curs n times. Moreover, Ix = λx = 1x holds for all vectors x ∈ R n\\{0}. Because of this, the sole eigenspace E1 of the identity matrix spans n di- mensions, and all n standard basis vectors of Rn are eigenvectors of I. Useful properties regarding eigenvalues and eigenvectors include the following: A matrix A and its transpose A⊤ possess the same eigenvalues, but not necessarily the same eigenvectors. The eigenspace Eλ is the null space of A − λI since Ax = λx ⇐⇒ Ax − λx = 0 (4.27a) ⇐⇒ (A − λI)x = 0 ⇐⇒ x ∈ ker(A − λI). (4.27b) Similar matrices (see Deﬁnition 2.22) possess the same eigenvalues. Therefore, a linear mapping Φ has eigenvalues that are independent of the choice of basis of its transformation matrix. This makes eigenvalues, together with the determinant and the trace, key characteristic param- eters of a linear mapping as they are all invariant under basis change. Symmetric, positive deﬁnite matrices always have positive, real eigen- values. Draft (2019-12-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com. 4.2 Eigenvalues and Eigenvectors 107 Example 4.5 (Computing Eigenvalues, Eigenvectors, and Eigenspaces) Let us ﬁnd the eigenvalues and eigenvectors of the 2 × 2 matrix A = [ 4 2 1 3 ] . (4.28) Step 1: Characteristic Polynomial. From our deﬁnition of the eigen- vector x ̸= 0 and eigenvalue λ of A, there will be a vector such that Ax = λx, i.e., (A − λI)x = 0. Since x ̸= 0, this requires that the kernel (null space) of A − λI contains more elements than just 0. This means that A − λI is not invertible and therefore det(A − λI) = 0. Hence, we need to compute the roots of the characteristic polynomial (4.22a) to ﬁnd the eigenvalues. Step 2: Eigenvalues. The characteristic polynomial is pA(λ) = det(A − λI) (4.29a) = det ([ 4 2 1 3 ] − [ λ 0 0 λ ]) = ∣ ∣ ∣ ∣4 − λ 2 1 3 − λ ∣ ∣ ∣ ∣ (4.29b) = (4 − λ)(3 − λ) − 2 · 1 . (4.29c) We factorize the characteristic polynomial and obtain p(λ) = (4 − λ)(3 − λ) − 2 · 1 = 10 − 7λ + λ2 = (2 − λ)(5 − λ) (4.30) giving the roots λ1 = 2 and λ2 = 5. Step 3: Eigenvectors and Eigenspaces. We ﬁnd the eigenvectors that correspond to these eigenvalues by looking at vectors x such that [ 4 − λ 2 1 3 − λ ] x = 0 . (4.31) For λ = 5 we obtain [ 4 − 5 2 1 3 − 5 ] [ x1 x2 ] = [ −1 2 1 −2 ] [ x1 x2 ] = 0 . (4.32) We solve this homogeneous system and obtain a solution space E5 = span[ [ 2 1 ] ] . (4.33) This eigenspace is one-dimensional as it possesses a single basis vector. Analogously, we ﬁnd the eigenvector for λ = 2 by solving the homoge- neous system of equations [ 4 − 2 2 1 3 − 2 ] x = [ 2 2 1 1 ] x = 0 . (4.34) c⃝2019 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press. 108 Matrix Decompositions This means any vector x = [ x1 x2 ] , where x2 = −x1, such as [ 1 −1 ] , is an eigenvector with eigenvalue 2. The corresponding eigenspace is given as E2 = span[ [ 1 −1 ] ] . (4.35) The two eigenspaces E5 and E2 in Example 4.5 are one-dimensional as they are each spanned by a single vector. However, in other cases we may have multiple identical eigenvalues (see Deﬁnition 4.9) and the eigenspace may have more than one dimension. Deﬁnition 4.11. Let λi be an eigenvalue of a square matrix A. Then the geometric multiplicity of λi is the number of linearly independent eigen-geometric multiplicity vectors associated with λi. In other words, it is the dimensionality of the eigenspace spanned by the eigenvectors associated with λi. Remark. A speciﬁc eigenvalue’s geometric multiplicity must be at least one because every eigenvalue has at least one associated eigenvector. An eigenvalue’s geometric multiplicity cannot exceed its algebraic multiplic- ity, but it may be lower. ♦ Example 4.6 The matrix A = [ 2 1 0 2 ] has two repeated eigenvalues λ1 = λ2 = 2 and an algebraic multiplicity of 2. The eigenvalue has, however, only one distinct unit eigenvector x1 = [ 1 0 ] and, thus, geometric multiplicity 1. Graphical Intuition in Two Dimensions Let us gain some intuition for determinants, eigenvectors, and eigenval- ues using different linear mappings. Figure 4.4 depicts ﬁve transformation matrices A1, . . . , A5 and their impact on a square grid of points, centered at the origin:In geometry, the area-preserving properties of this type of shearing parallel to an axis is also known as Cavalieri’s principle of equal areas for parallelograms (Katz, 2004). A1 = [ 1 2 0 0 2 ] . The direction of the two eigenvectors correspond to the canonical basis vectors in R2, i.e., to two cardinal axes. The vertical axis is extended by a factor of 2 (eigenvalue λ1 = 2), and the horizontal axis is compressed by factor 1 2 (eigenvalue λ2 = 1 2 ). The mapping is area preserving (det(A1) = 1 = 2 · 1 2 ). A2 = [ 1 1 2 0 1 ] corresponds to a shearing mapping , i.e., it shears the points along the horizontal axis to the right if they are on the positive Draft (2019-12-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com. 4.2 Eigenvalues and Eigenvectors 109 Figure 4.4 Determinants and eigenspaces. Overview of ﬁve linear mappings and their associated transformation matrices Ai ∈ R2×2 projecting 400 color-coded points x ∈ R2 (left column) onto target points Aix (right column). The central column depicts the ﬁrst eigenvector, stretched by its associated eigenvalue λ1, and the second eigenvector stretched by its eigenvalue λ2. Each row depicts the effect of one of ﬁve transformation matrices Ai with respect to the standard basis . det(A) = 1.0 λ1 = 2.0 λ2 = 0.5 det(A) = 1.0 λ1 = 1.0 λ2 = 1.0 det(A) = 1.0 λ1 = (0.87-0.5j) λ2 = (0.87+0.5j) det(A) = 0.0 λ1 = 0.0 λ2 = 2.0 det(A) = 0.75 λ1 = 0.5 λ2 = 1.5 half of the vertical axis, and to the left vice versa. This mapping is area preserving (det(A2) = 1). The eigenvalue λ1 = 1 = λ2 is repeated and the eigenvectors are collinear (drawn here for emphasis in two opposite directions). This indicates that the mapping acts only along one direction (the horizontal axis). A3 = [ cos( π 6 ) − sin( π 6 ) sin( π 6 ) cos( π 6 ) ] = 1 2 [√3 −1 1 √3 ] The matrix A3 rotates the points by π 6 rad = 30◦ counter-clockwise and has only complex eigen- values, reﬂecting that the mapping is a rotation (hence, no eigenvectors are drawn). A rotation has to be volume preserving, and so the deter- minant is 1. For more details on rotations, we refer to Section 3.9. A4 = [ 1 −1 −1 1 ] represents a mapping in the standard basis that col- lapses a two-dimensional domain onto one dimension. Since one eigen- c⃝2019 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press. 110 Matrix Decompositions value is 0, the space in direction of the (blue) eigenvector corresponding to λ1 = 0 collapses, while the orthogonal (red) eigenvector stretches space by a factor λ2 = 2. Therefore, the area of the image is 0. A5 = [1 1 2 1 2 1 ] is a shear-and-stretch mapping that scales space by 75% since | det(A5)| = 3 4 . It stretches space along the (blue) eigenvector of λ2 by a factor 1.5 and compresses it along the orthogonal (blue) eigenvector by a factor 0.5. Example 4.7 (Eigenspectrum of a Biological Neural Network) Figure 4.5 Caenorhabditis elegans neural network (Kaiser and Hilgetag, 2006). (a) Symmetrized connectivity matrix; (b) Eigenspectrum. 0 50 100 150 200 250 neuron index 0 50 100 150 200 250neuronindex (a) Connectivity matrix. 0 100 200 index of sorted eigenvalue −10 −5 0 5 10 15 20 25eigenvalue (b) Eigenspectrum. Methods to analyze and learn from network data are an essential com- ponent of machine learning methods. The key to understanding networks is the connectivity between network nodes, especially if two nodes are connected to each other or not. In data science applications, it is often useful to study the matrix that captures this connectivity data. We build a connectivity/adjacency matrix A ∈ R 277×277 of the complete neural network of the worm C.Elegans. Each row/column represents one of the 277 neurons of this worm’s brain. The connectivity matrix A has a value of aij = 1 if neuron i talks to neuron j through a synapse, and aij = 0 otherwise. The connectivity matrix is not symmetric, which im- plies that eigenvalues may not be real valued. Therefore, we compute a symmetrized version of the connectivity matrix as Asym := A + A⊤. This new matrix Asym is shown in Figure 4.5(a) and has a nonzero value aij if and only if two neurons are connected (white pixels), irrespective of the direction of the connection. In Figure 4.5(b), we show the correspond- ing eigenspectrum of Asym. The horizontal axis shows the index of the eigenvalues, sorted in descending order. The vertical axis shows the corre- sponding eigenvalue. The S-like shape of this eigenspectrum is typical for many biological neural networks. The underlying mechanism responsible for this is an area of active neuroscience research. Draft (2019-12-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com. 4.2 Eigenvalues and Eigenvectors 111 Theorem 4.12. The eigenvectors x1, . . . , xn of a matrix A ∈ Rn×n with n distinct eigenvalues λ1, . . . , λn are linearly independent. This theorem states that eigenvectors of a matrix with n distinct eigen- values form a basis of Rn. Deﬁnition 4.13. A square matrix A ∈ Rn×n is defective if it possesses defective fewer than n linearly independent eigenvectors. A non-defective matrix A ∈ R n×n does not necessarily require n dis- tinct eigenvalues, but it does require that the eigenvectors form a basis of R n. Looking at the eigenspaces of a defective matrix, it follows that the sum of the dimensions of the eigenspaces is less than n. Speciﬁcally, a de- fective matrix has at least one eigenvalue λi with an algebraic multiplicity m > 1 and a geometric multiplicity of less than m. Remark. A defective matrix cannot have n distinct eigenvalues, as distinct eigenvalues have linearly independent eigenvectors (Theorem 4.12). ♦ Theorem 4.14. Given a matrix A ∈ Rm×n, we can always obtain a sym- metric, positive semideﬁnite matrix S ∈ R n×n by deﬁning S := A ⊤A . (4.36) Remark. If rk(A) = n, then S := A⊤A is symmetric, positive deﬁnite. ♦ Understanding why Theorem 4.14 holds is insightful for how we can use symmetrized matrices: Symmetry requires S = S⊤, and by insert- ing (4.36) we obtain S = A⊤A = A⊤(A⊤) ⊤ = (A⊤A) ⊤ = S⊤. More- over, positive semideﬁniteness (Section 3.2.3) requires that x ⊤Sx ⩾ 0 and inserting (4.36) we obtain x⊤Sx = x ⊤A⊤Ax = (x ⊤A⊤)(Ax) = (Ax) ⊤(Ax) ⩾ 0, because the dot product computes a sum of squares (which are themselves non-negative). spectral theorem Theorem 4.15 (Spectral Theorem). If A ∈ Rn×n is symmetric, there ex- ists an orthonormal basis of the corresponding vector space V consisting of eigenvectors of A, and each eigenvalue is real. A direct implication of the spectral theorem is that the eigendecompo- sition of a symmetric matrix A exists (with real eigenvalues), and that we can ﬁnd an ONB of eigenvectors so that A = P DP ⊤, where D is diagonal and the columns of P contain the eigenvectors. Example 4.8 Consider the matrix A =   3 2 2 2 3 2 2 2 3   . (4.37) c⃝2019 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press. 112 Matrix Decompositions The characteristic polynomial of A is pA(λ) = −(λ − 1) 2(λ − 7) , (4.38) so that we obtain the eigenvalues λ1 = 1 and λ2 = 7, where λ1 is a repeated eigenvalue. Following our standard procedure for computing eigenvectors, we obtain the eigenspaces E1 = span[   −1 1 0   ︸ ︷︷ ︸ =:x1 ,   −1 0 1   ︸ ︷︷ ︸ =:x2 ], E7 = span[   1 1 1   ︸︷︷︸ =:x3 ] . (4.39) We see that x3 is orthogonal to both x1 and x2. However, since x⊤ 1 x2 = 1 ̸= 0, they are not orthogonal. The spectral theorem (Theorem 4.15) states that there exists an orthogonal basis, but the one we have is not orthogonal. However, we can construct one. To construct such a basis, we exploit the fact that x1, x2 are eigenvec- tors associated with the same eigenvalue λ. Therefore, for any α, β ∈ R it holds that A(αx1 + βx2) = Ax1α + Ax2β = λ(αx1 + βx2) , (4.40) i.e., any linear combination of x1 and x2 is also an eigenvector of A as- sociated with λ. The Gram-Schmidt algorithm (Section 3.8.3) is a method for iteratively constructing an orthogonal/orthonormal basis from a set of basis vectors using such linear combinations. Therefore, even if x1 and x2 are not orthogonal, we can apply the Gram-Schmidt algorithm and ﬁnd eigenvectors associated with λ1 = 1 that are orthogonal to each other (and to x3). In our example, we will obtain x′ 1 =   −1 1 0   , x ′ 2 = 1 2   −1 −1 2   , (4.41) which are orthogonal to each other, orthogonal to x3, and eigenvectors of A associated with λ1 = 1. Before we conclude our considerations of eigenvalues and eigenvectors it is useful to tie these matrix characteristics together with the concepts of the determinant and the trace. Theorem 4.16. The determinant of a matrix A ∈ Rn×n is the product of its eigenvalues, i.e., det(A) = n∏ i=1 λi , (4.42) where λi are (possibly repeated) eigenvalues of A. Draft (2019-12-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com. 4.2 Eigenvalues and Eigenvectors 113 Figure 4.6 Geometric interpretation of eigenvalues. The eigenvectors of A get stretched by the corresponding eigenvalues. The area of the unit square changes by |λ1λ2|, the circumference changes by a factor 2(|λ1| + |λ2|). x1 x2 v1 v2 A Theorem 4.17. The trace of a matrix A ∈ Rn×n is the sum of its eigenval- ues, i.e., tr(A) = n∑ i=1 λi , (4.43) where λi are (possibly repeated) eigenvalues of A. Let us provide a geometric intuition of these two theorems. Consider a matrix A ∈ R 2×2 that possesses two linearly independent eigenvectors x1, x2. For this example, we assume (x1, x2) are an ONB of R 2 so that they are orthogonal and the area of the square they span is 1; see Figure 4.6. From Section 4.1, we know that the determinant computes the change of area of unit square under the transformation A. In this example, we can compute the change of area explicitly: Mapping the eigenvectors using A gives us vectors v1 = Ax1 = λ1x1 and v2 = Ax2 = λ2x2, i.e., the new vectors vi are scaled versions of the eigenvectors xi, and the scaling factors are the corresponding eigenvalues λi. v1, v2 are still orthogonal, and the area of the rectangle they span is |λ1λ2|. Given that x1, x2 (in our example) are orthonormal, we can directly compute the circumference of the unit square as 2(1 + 1). Mapping the eigenvectors using A creates a rectangle whose circumference is 2(|λ1| + |λ2|). Therefore, the sum of the absolute values of the eigenvalues tells us how the circumference of the unit square changes under the transforma- tion matrix A. Example 4.9 (Google’s PageRank – Webpages as Eigenvectors) Google uses the eigenvector corresponding to the maximal eigenvalue of a matrix A to determine the rank of a page for search. The idea for the PageRank algorithm, developed at Stanford University by Larry Page and Sergey Brin in 1996, was that the importance of any web page can be ap- proximated by the importance of pages that link to it. For this, they write down all web sites as a huge directed graph that shows which page links to which. PageRank computes the weight (importance) xi ⩾ 0 of a web site ai by counting the number of pages pointing to ai. Moreover, PageR- ank takes into account the importance of the web sites that link to ai. The navigation behavior of a user is then modeled by a transition matrix A of this graph that tells us with what (click) probability somebody will end up c⃝2019 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press. 114 Matrix Decompositions on a different web site. The matrix A has the property that for any ini- tial rank/importance vector x of a web site the sequence x, Ax, A2x, . . . converges to a vector x ∗. This vector is called the PageRank and satisﬁesPageRank Ax∗ = x ∗, i.e., it is an eigenvector (with corresponding eigenvalue 1) of A. After normalizing x∗, such that ∥x ∗∥ = 1, we can interpret the entries as probabilities. More details and different perspectives on PageRank can be found in the original technical report (Page et al., 1999). 4.3 Cholesky Decomposition There are many ways to factorize special types of matrices that we en- counter often in machine learning. In the positive real numbers, we have the square-root operation that gives us a decomposition of the number into identical components, e.g., 9 = 3 · 3. For matrices, we need to be careful that we compute a square-root-like operation on positive quanti- ties. For symmetric, positive deﬁnite matrices (see Section 3.2.3), we can choose from a number of square-root equivalent operations. The CholeskyCholesky decomposition decomposition/Cholesky factorization provides a square-root equivalent op- Cholesky factorization eration on symmetric, positive deﬁnite matrices that is useful in practice. Theorem 4.18 (Cholesky Decomposition). A symmetric, positive deﬁnite matrix A can be factorized into a product A = LL⊤, where L is a lower- triangular matrix with positive diagonal elements:    a11 · · · a1n ... . . . ... an1 · · · ann    =    l11 · · · 0 ... . . . ... ln1 · · · lnn       l11 · · · ln1 ... . . . ... 0 · · · lnn    . (4.44) L is called the Cholesky factor of A, and L is unique.Cholesky factor Example 4.10 (Cholesky Factorization) Consider a symmetric, positive deﬁnite matrix A ∈ R3×3. We are inter- ested in ﬁnding its Cholesky factorization A = LL⊤, i.e., A =   a11 a21 a31 a21 a22 a32 a31 a32 a33   = LL⊤ =   l11 0 0 l21 l22 0 l31 l32 l33     l11 l21 l31 0 l22 l32 0 0 l33   . (4.45) Multiplying out the right-hand side yields A =   l2 11 l21l11 l31l11 l21l11 l2 21 + l2 22 l31l21 + l32l22 l31l11 l31l21 + l32l22 l2 31 + l2 32 + l2 33   . (4.46) Draft (2019-12-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com. 4.4 Eigendecomposition and Diagonalization 115 Comparing the left-hand side of (4.45) and the right-hand side of (4.46) shows that there is a simple pattern in the diagonal elements lii: l11 = √a11 , l22 = √a22 − l2 21 , l33 = √ a33 − (l2 31 + l2 32) . (4.47) Similarly for the elements below the diagonal (lij, where i > j), there is also a repeating pattern: l21 = 1 l11 a21 , l31 = 1 l11 a31 , l32 = 1 l22 (a32 − l31l21) . (4.48) Thus, we constructed the Cholesky decomposition for any symmetric, pos- itive deﬁnite 3 × 3 matrix. The key realization is that we can backward calculate what the components lij for the L should be, given the values aij for A and previously computed values of lij. The Cholesky decomposition is an important tool for the numerical computations underlying machine learning. Here, symmetric positive def- inite matrices require frequent manipulation, e.g., the covariance matrix of a multivariate Gaussian variable (see Section 6.5) is symmetric, positive deﬁnite. The Cholesky factorization of this covariance matrix allows us to generate samples from a Gaussian distribution. It also allows us to perform a linear transformation of random variables, which is heavily exploited when computing gradients in deep stochastic models, such as the varia- tional auto-encoder (Jimenez Rezende et al., 2014; Kingma and Welling, 2014). The Cholesky decomposition also allows us to compute determi- nants very efﬁciently. Given the Cholesky decomposition A = LL⊤, we know that det(A) = det(L) det(L⊤) = det(L) 2. Since L is a triangular matrix, the determinant is simply the product of its diagonal entries so that det(A) = ∏ i l2 ii. Thus, many numerical software packages use the Cholesky decomposition to make computations more efﬁcient. 4.4 Eigendecomposition and Diagonalization A diagonal matrix is a matrix that has value zero on all off-diagonal ele- diagonal matrix ments, i.e., they are of the form D =    c1 · · · 0 ... . . . ... 0 · · · cn    . (4.49) They allow fast computation of determinants, powers, and inverses. The determinant is the product of its diagonal entries, a matrix power Dk is given by each diagonal element raised to the power k, and the inverse D−1 is the reciprocal of its diagonal elements if all of them are nonzero. In this section, we will discuss how to transform matrices into diagonal c⃝2019 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press. 116 Matrix Decompositions form. This is an important application of the basis change we discussed in Section 2.7.2 and eigenvalues from Section 4.2. Recall that two matrices A, D are similar (Deﬁnition 2.22) if there ex- ists an invertible matrix P , such that D = P −1AP . More speciﬁcally, we will look at matrices A that are similar to diagonal matrices D that con- tain the eigenvalues of A on the diagonal. Deﬁnition 4.19 (Diagonalizable). A matrix A ∈ R n×n is diagonalizablediagonalizable if it is similar to a diagonal matrix, i.e., if there exists an invertible matrix P ∈ R n×n such that D = P −1AP . In the following, we will see that diagonalizing a matrix A ∈ Rn×n is a way of expressing the same linear mapping but in another basis (see Section 2.6.1), which will turn out to be a basis that consists of the eigen- vectors of A. Let A ∈ R n×n, let λ1, . . . , λn be a set of scalars, and let p1, . . . , pn be a set of vectors in R n. We deﬁne P := [p1, . . . , pn] and let D ∈ R n×n be a diagonal matrix with diagonal entries λ1, . . . , λn. Then we can show that AP = P D (4.50) if and only if λ1, . . . , λn are the eigenvalues of A and p1, . . . , pn are cor- responding eigenvectors of A. We can see that this statement holds because AP = A[p1, . . . , pn] = [Ap1, . . . , Apn] , (4.51) P D = [p1, . . . , pn]    λ1 0 . . . 0 λn    = [λ1p1, . . . , λnpn] . (4.52) Thus, (4.50) implies that Ap1 = λ1p1 (4.53) ... Apn = λnpn . (4.54) Therefore, the columns of P must be eigenvectors of A. Our deﬁnition of diagonalization requires that P ∈ R n×n is invertible, i.e., P has full rank (Theorem 4.3). This requires us to have n linearly independent eigenvectors p1, . . . , pn, i.e., the pi form a basis of R n. Theorem 4.20 (Eigendecomposition). A square matrix A ∈ Rn×n can be factored into A = P DP −1 , (4.55) where P ∈ R n×n and D is a diagonal matrix whose diagonal entries are the eigenvalues of A, if and only if the eigenvectors of A form a basis of Rn. Draft (2019-12-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com. 4.4 Eigendecomposition and Diagonalization 117 Figure 4.7 Intuition behind the eigendecomposition as sequential transformations. Top-left to bottom-left: P −1 performs a basis change (here drawn in R2 and depicted as a rotation-like operation), mapping the eigenvectors into the standard basis. Bottom-left to bottom-right: D performs a scaling along the remapped orthogonal eigenvectors, depicted here by a circle being stretched to an ellipse. Bottom-right to top-right: P undoes the basis change (depicted as a reverse rotation) and restores the original coordinate frame. p2 p1 λ2p2 λ1p1 e2 e1 λ2e2 λ1e1 A P −1 D P Theorem 4.20 implies that only non-defective matrices can be diagonal- ized and that the columns of P are the n eigenvectors of A. For symmetric matrices we can obtain even stronger outcomes for the eigenvalue decom- position. Theorem 4.21. A symmetric matrix S ∈ Rn×n can always be diagonalized. Theorem 4.21 follows directly from the spectral theorem 4.15. More- over, the spectral theorem states that we can ﬁnd an ONB of eigenvectors of Rn. This makes P an orthogonal matrix so that D = P ⊤AP . Remark. The Jordan normal form of a matrix offers a decomposition that works for defective matrices (Lang, 1987) but is beyond the scope of this book. ♦ Geometric Intuition for the Eigendecomposition We can interpret the eigendecomposition of a matrix as follows (see also Figure 4.7): Let A be the transformation matrix of a linear mapping with respect to the standard basis. P −1 performs a basis change from the stan- dard basis into the eigenbasis. This identiﬁes the eigenvectors pi (red and orange arrows in Figure 4.7) onto the standard basis vectors ei. Then, the diagonal D scales the vectors along these axes by the eigenvalues λi. Fi- nally, P transforms these scaled vectors back into the standard/canonical coordinates yielding λipi. Example 4.11 (Eigendecomposition) Let us compute the eigendecomposition of A = [ 2 1 1 2 ] . Step 1: Compute eigenvalues and eigenvectors. The characteristic c⃝2019 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press. 118 Matrix Decompositions polynomial of A is det(A − λI) = det ([2 − λ 1 1 2 − λ ]) (4.56a) = (2 − λ) 2 − 1 = λ 2 − 4λ + 3 = (λ − 3)(λ − 1) . (4.56b) Therefore, the eigenvalues of A are λ1 = 1 and λ2 = 3 (the roots of the characteristic polynomial), and the associated (normalized) eigenvectors are obtained via [ 2 1 1 2 ] p1 = 1p1 , [ 2 1 1 2 ] p2 = 3p2 . (4.57) This yields p1 = 1 √2 [ 1 −1 ] , p2 = 1 √2 [ 1 1 ] . (4.58) Step 2: Check for existence. The eigenvectors p1, p2 form a basis of R 2. Therefore, A can be diagonalized. Step 3: Construct the matrix P to diagonalize A. We collect the eigenvectors of A in P so that P = [p1, p2] = 1 √2 [ 1 1 −1 1 ] . (4.59) We then obtain P −1AP = [ 1 0 0 3 ] = D . (4.60) Equivalently, we get (exploiting that P −1 = P ⊤ since the eigenvectors p1 and p2 in this example form an ONB) [ 2 1 1 2 ] ︸ ︷︷ ︸ A = 1 √2 [ 1 1 −1 1 ] ︸ ︷︷ ︸ P [ 1 0 0 3 ] ︸ ︷︷ ︸ D 1 √2 [ 1 −1 1 1 ] ︸ ︷︷ ︸ P ⊤ . (4.61) Diagonal matrices D can efﬁciently be raised to a power. Therefore, we can ﬁnd a matrix power for a matrix A ∈ Rn×n via the eigenvalue decomposition (if it exists) so that Ak = (P DP −1)k = P DkP −1 . (4.62) Computing Dk is efﬁcient because we apply this operation individually to any diagonal element. Assume that the eigendecomposition A = P DP −1 exists. Then, det(A) = det(P DP −1) = det(P ) det(D) det(P −1) (4.63a) Draft (2019-12-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com. 4.5 Singular Value Decomposition 119 = det(D) = ∏ i dii (4.63b) allows for an efﬁcient computation of the determinant of A. The eigenvalue decomposition requires square matrices. It would be useful to perform a decomposition on general matrices. In the next sec- tion, we introduce a more general matrix decomposition technique, the singular value decomposition. 4.5 Singular Value Decomposition The singular value decomposition (SVD) of a matrix is a central matrix decomposition method in linear algebra. It has been referred to as the “fundamental theorem of linear algebra” (Strang, 1993) because it can be applied to all matrices, not only to square matrices, and it always exists. Moreover, as we will explore in the following, the SVD of a matrix A, which represents a linear mapping Φ : V → W , quantiﬁes the change between the underlying geometry of these two vector spaces. We recom- mend the work by Kalman (1996) and Roy and Banerjee (2014) for a deeper overview of the mathematics of the SVD. SVD theorem Theorem 4.22 (SVD Theorem). Let Am×n be a rectangular matrix of rank r ∈ [0, min(m, n)]. The SVD of A is a decomposition of the form SVD singular value decomposition = UA V ⊤Σm nm mm n nn (4.64) with an orthogonal matrix U ∈ R m×m with column vectors ui, i = 1, . . . , m, and an orthogonal matrix V ∈ R n×n with column vectors vj, j = 1, . . . , n. Moreover, Σ is an m × n matrix with Σii = σi ⩾ 0 and Σij = 0, i ̸= j. The diagonal entries σi, i = 1, . . . , r, of Σ are called the singular values, singular values ui are called the left-singular vectors, and vj are called the right-singular left-singular vectors right-singular vectors vectors. By convention, the singular values are ordered, i.e., σ1 ⩾ σ2 ⩾ σr ⩾ 0. The singular value matrix Σ is unique, but it requires some attention. singular value matrixObserve that the Σ ∈ Rm×n is rectangular. In particular, Σ is of the same size as A. This means that Σ has a diagonal submatrix that contains the singular values and needs additional zero padding. Speciﬁcally, if m > n, then the matrix Σ has diagonal structure up to row n and then consists of c⃝2019 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press. 120 Matrix Decompositions Figure 4.8 Intuition behind the SVD of a matrix A ∈ R3×2 as sequential transformations. Top-left to bottom-left: V ⊤ performs a basis change in R2. Bottom-left to bottom-right: Σ scales and maps from R2 to R3. The ellipse in the bottom-right lives in R3. The third dimension is orthogonal to the surface of the elliptical disk. Bottom-right to top-right: U performs a basis change within R3. V2 V1 σ2u2 σ1u1 e2 e1 σ2e2 σ1e1 A V ⊤ Σ U 0⊤ row vectors from n + 1 to m below so that Σ =           σ1 0 0 0 . . . 0 0 0 σn 0 . . . 0 ... ... 0 . . . 0           . (4.65) If m < n, the matrix Σ has a diagonal structure up to column m and columns that consist of 0 from m + 1 to n: Σ =    σ1 0 0 0 . . . 0 0 . . . 0 0 0 0 0 σm 0 . . . 0    . (4.66) Remark. The SVD exists for any matrix A ∈ R m×n. ♦ 4.5.1 Geometric Intuitions for the SVD The SVD offers geometric intuitions to describe a transformation matrix A. In the following, we will discuss the SVD as sequential linear trans- formations performed on the bases. In Example 4.12, we will then apply transformation matrices of the SVD to a set of vectors in R 2, which allows us to visualize the effect of each transformation more clearly. The SVD of a matrix can be interpreted as a decomposition of a corre- sponding linear mapping (recall Section 2.7.1) Φ : Rn → R m into three operations; see Figure 4.8. The SVD intuition follows superﬁcially a simi- lar structure to our eigendecomposition intuition, see Figure 4.7: Broadly speaking, the SVD performs a basis change via V ⊤ followed by a scal- ing and augmentation (or reduction) in dimensionality via the singular Draft (2019-12-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com. 4.5 Singular Value Decomposition 121 value matrix Σ. Finally, it performs a second basis change via U . The SVD entails a number of important details and caveats, which is why we will review our intuition in more detail. It is useful to revise basis changes (Section 2.7.2), orthogonal matrices (Deﬁnition 3.8) and orthonormal bases (Section 3.5). Assume we are given a transformation matrix of a linear mapping Φ : R n → Rm with respect to the standard bases B and C of R n and Rm, respectively. Moreover, assume a second basis ˜B of R n and ˜C of Rm. Then 1. The matrix V performs a basis change in the domain Rn from ˜B (rep- resented by the red and orange vectors v1 and v2 in the top-left of Fig- ure 4.8) to the standard basis B. V ⊤ = V −1 performs a basis change from B to ˜B. The red and orange vectors are now aligned with the canonical basis in the bottom-left of Figure 4.8. 2. Having changed the coordinate system to ˜B, Σ scales the new coordi- nates by the singular values σi (and adds or deletes dimensions), i.e., Σ is the transformation matrix of Φ with respect to ˜B and ˜C, rep- resented by the red and orange vectors being stretched and lying in the e1-e2 plane, which is now embedded in a third dimension in the bottom-right of Figure 4.8. 3. U performs a basis change in the codomain R m from ˜C into the canoni- cal basis of Rm, represented by a rotation of the red and orange vectors out of the e1-e2 plane. This is shown in the top-right of Figure 4.8. The SVD expresses a change of basis in both the domain and codomain. This is in contrast with the eigendecomposition that operates within the same vector space, where the same basis change is applied and then un- done. What makes the SVD special is that these two different bases are simultaneously linked by the singular value matrix Σ. Example 4.12 (Vectors and the SVD) Consider a mapping of a square grid of vectors X ∈ R2 that ﬁt in a box of size 2 × 2 centered at the origin. Using the standard basis, we map these vectors using A =   1 −0.8 0 1 1 0   = U ΣV ⊤ (4.67a) =   −0.79 0 −0.62 0.38 −0.78 −0.49 −0.48 −0.62 0.62     1.62 0 0 1.0 0 0   [ −0.78 0.62 −0.62 −0.78 ] . (4.67b) We start with a set of vectors X (colored dots; see top-left panel of Fig- ure 4.9) arranged in a grid. We then apply V ⊤ ∈ R2×2, which rotates X . The rotated vectors are shown in the bottom-left panel of Figure 4.9. We now map these vectors using the singular value matrix Σ to the codomain R 3 (see the bottom-right panel in Figure 4.9). Note that all vectors lie in c⃝2019 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press. 122 Matrix Decompositions the x1-x2 plane. The third coordinate is always 0. The vectors in the x1-x2 plane have been stretched by the singular values. The direct mapping of the vectors X by A to the codomain R 3 equals the transformation of X by U ΣV ⊤, where U performs a rotation within the codomain R 3 so that the mapped vectors are no longer restricted to the x1-x2 plane; they still are on a plane as shown in the top-right panel of Figure 4.9. Figure 4.9 SVD and mapping of vectors (represented by discs). The panels follow the same anti-clockwise structure of Figure 4.8. −1.5 −1.0 −0.5 0.0 0.5 1.0 1.5 x1 −1.5 −1.0 −0.5 0.0 0.5 1.0 1.5x2 x1 -1.5 -0.5 0.5 1.5 x 2 -1.5 -0.5 0.5 1.5x3 -1.0 -0.5 0.0 0.5 1.0 −1.5 −1.0 −0.5 0.0 0.5 1.0 1.5 x1 −1.5 −1.0 −0.5 0.0 0.5 1.0 1.5x2 x1 -1.5 -0.5 0.5 1.5 x 2 -1.5 -0.5 0.5 1.5x30 4.5.2 Construction of the SVD We will next discuss why the SVD exists and show how to compute it in detail. The SVD of a general matrix shares some similarities with the eigendecomposition of a square matrix. Remark. Compare the eigendecomposition of an SPD matrix S = S⊤ = P DP ⊤ (4.68) Draft (2019-12-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com. 4.5 Singular Value Decomposition 123 with the corresponding SVD S = U ΣV ⊤ . (4.69) If we set U = P = V , D = Σ , (4.70) we see that the SVD of SPD matrices is their eigendecomposition. ♦ In the following, we will explore why Theorem 4.22 holds and how the SVD is constructed. Computing the SVD of A ∈ Rm×n is equivalent to ﬁnding two sets of orthonormal bases U = (u1, . . . , um) and V = (v1, . . . , vn) of the codomain R m and the domain R n, respectively. From these ordered bases, we will construct the matrices U and V . Our plan is to start with constructing the orthonormal set of right- singular vectors v1, . . . , vn ∈ Rn. We then construct the orthonormal set of left-singular vectors u1, . . . , um ∈ Rm. Thereafter, we will link the two and require that the orthogonality of the vi is preserved under the trans- formation of A. This is important because we know that the images Avi form a set of orthogonal vectors. We will then normalize these images by scalar factors, which will turn out to be the singular values. Let us begin with constructing the right-singular vectors. The spectral theorem (Theorem 4.15) tells us that a symmetric matrix possesses an ONB of eigenvectors, which also means it can be diagonalized. More- over, from Theorem 4.14 we can always construct a symmetric, positive semideﬁnite matrix A ⊤A ∈ R n×n from any rectangular matrix A ∈ R m×n. Thus, we can always diagonalize A⊤A and obtain A⊤A = P DP ⊤ = P    λ1 · · · 0 ... . . . ... 0 · · · λn    P ⊤ , (4.71) where P is an orthogonal matrix, which is composed of the orthonormal eigenbasis. The λi ⩾ 0 are the eigenvalues of A⊤A. Let us assume the SVD of A exists and inject (4.64) into (4.71). This yields A⊤A = (U ΣV ⊤)⊤(U ΣV ⊤) = V Σ ⊤U ⊤U ΣV ⊤ , (4.72) where U , V are orthogonal matrices. Therefore, with U ⊤U = I we ob- tain A⊤A = V Σ ⊤ΣV ⊤ = V    σ2 1 0 0 0 . . . 0 0 0 σ2 n    V ⊤ . (4.73) Comparing now (4.71) and (4.73), we identify V ⊤ = P ⊤ , (4.74) σ2 i = λi . (4.75) c⃝2019 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press. 124 Matrix Decompositions Therefore, the eigenvectors of A ⊤A that compose P are the right-singular vectors V of A (see (4.74)). The eigenvalues of A⊤A are the squared singular values of Σ (see (4.75)). To obtain the left-singular vectors U , we follow a similar procedure. We start by computing the SVD of the symmetric matrix AA⊤ ∈ Rm×m (instead of the previous A⊤A ∈ R n×n). The SVD of A yields AA⊤ = (U ΣV ⊤)(U ΣV ⊤) ⊤ = U ΣV ⊤V Σ ⊤U ⊤ (4.76a) = U    σ2 1 0 0 0 . . . 0 0 0 σ2 m    U ⊤ . (4.76b) The spectral theorem tells us that AA⊤ = SDS⊤ can be diagonalized and we can ﬁnd an ONB of eigenvectors of AA⊤, which are collected in S. The orthonormal eigenvectors of AA⊤ are the left-singular vectors U and form an orthonormal basis set in the codomain of the SVD. This leaves the question of the structure of the matrix Σ. Since AA⊤ and A ⊤A have the same nonzero eigenvalues (see page 106) the nonzero entries of the Σ matrices in the SVD for both cases have to be the same. The last step is to link up all the parts we touched upon so far. We have an orthonormal set of right-singular vectors in V . To ﬁnish the construc- tion of the SVD, we connect them with the orthonormal vectors U . To reach this goal, we use the fact the images of the vi under A have to be orthogonal, too. We can show this by using the results from Section 3.4. We require that the inner product between Avi and Avj must be 0 for i ̸= j. For any two orthogonal eigenvectors vi, vj, i ̸= j, it holds that (Avi)⊤(Avj) = v⊤ i (A⊤A)vj = v⊤ i (λjvj) = λjv⊤ i vj = 0 . (4.77) For the case m ⩾ r, it holds that {Av1, . . . , Avr} is a basis of an r- dimensional subspace of Rm. To complete the SVD construction, we need left-singular vectors that are orthonormal: We normalize the images of the right-singular vectors Avi and obtain ui := Avi ∥Avi∥ = 1 √λi Avi = 1 σi Avi , (4.78) where the last equality was obtained from (4.75) and (4.76b), showing us that the eigenvalues of AA⊤ are such that σ2 i = λi. Therefore, the eigenvectors of A⊤A, which we know are the right- singular vectors vi, and their normalized images under A, the left-singular vectors ui, form two self-consistent ONBs that are connected through the singular value matrix Σ. Let us rearrange (4.78) to obtain the singular value equationsingular value equation Avi = σiui , i = 1, . . . , r . (4.79) Draft (2019-12-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com. 4.5 Singular Value Decomposition 125 This equation closely resembles the eigenvalue equation (4.25), but the vectors on the left- and the right-hand sides are not the same. For n > m, (4.79) holds only for i ⩽ m and (4.79) says nothing about the ui for i > m. However, we know by construction that they are or- thonormal. Conversely, for m > n, (4.79) holds only for i ⩽ n. For i > n, we have Avi = 0 and we still know that the vi form an orthonormal set. This means that the SVD also supplies an orthonormal basis of the kernel (null space) of A, the set of vectors x with Ax = 0 (see Section 2.7.3). Moreover, concatenating the vi as the columns of V and the ui as the columns of U yields AV = U Σ , (4.80) where Σ has the same dimensions as A and a diagonal structure for rows 1, . . . , r. Hence, right-multiplying with V ⊤ yields A = U ΣV ⊤, which is the SVD of A. Example 4.13 (Computing the SVD) Let us ﬁnd the singular value decomposition of A = [ 1 0 1 −2 1 0 ] . (4.81) The SVD requires us to compute the right-singular vectors vj, the singular values σk, and the left-singular vectors ui. Step 1: Right-singular vectors as the eigenbasis of A⊤A. We start by computing A ⊤A =   1 −2 0 1 1 0   [ 1 0 1 −2 1 0 ] =   5 −2 1 −2 1 0 1 0 1   . (4.82) We compute the singular values and right-singular vectors vj through the eigenvalue decomposition of A ⊤A, which is given as A ⊤A =    5√30 0 −1√6 −2√30 1√5 −2√6 1√30 2√5 1√6      6 0 0 0 1 0 0 0 0      5√30 −2√30 1√30 0 1√5 2√5 −1√6 −2√6 1√6    = P DP ⊤ , (4.83) and we obtain the right-singular vectors as the columns of P so that V = P =    5√30 0 −1√6 −2√30 1√5 −2√6 1√30 2√5 1√6    . (4.84) Step 2: Singular-value matrix. As the singular values σi are the square roots of the eigenvalues of c⃝2019 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press. 126 Matrix Decompositions A⊤A we obtain them straight from D. Since rk(A) = 2, there are only two nonzero singular values: σ1 = √6 and σ2 = 1. The singular value matrix must be the same size as A, and we obtain Σ = [√6 0 0 0 1 0 ] . (4.85) Step 3: Left-singular vectors as the normalized image of the right- singular vectors. We ﬁnd the left-singular vectors by computing the image of the right- singular vectors under A and normalizing them by dividing them by their corresponding singular value. We obtain u1 = 1 σ1 Av1 = 1 √ 6 [ 1 0 1 −2 1 0 ]    5√30 −2√30 1√30    = [ 1√5 − 2√5 ] , (4.86) u2 = 1 σ2 Av2 = 1 1 [ 1 0 1 −2 1 0 ]   0 1√5 2√5   = [ 2√5 1√5 ] , (4.87) U = [u1, u2] = 1 √5 [ 1 2 −2 1 ] . (4.88) Note that on a computer the approach illustrated here has poor numerical behavior, and the SVD of A is normally computed without resorting to the eigenvalue decomposition of A⊤A. 4.5.3 Eigenvalue Decomposition vs. Singular Value Decomposition Let us consider the eigendecomposition A = P DP −1 and the SVD A = U ΣV ⊤ and review the core elements of the past sections. The SVD always exists for any matrix R m×n. The eigendecomposition is only deﬁned for square matrices R n×n and only exists if we can ﬁnd a basis of eigenvectors of Rn. The vectors in the eigendecomposition matrix P are not necessarily orthogonal, i.e., the change of basis is not a simple rotation and scaling. On the other hand, the vectors in the matrices U and V in the SVD are orthonormal, so they do represent rotations. Both the eigendecomposition and the SVD are compositions of three linear mappings: 1. Change of basis in the domain 2. Independent scaling of each new basis vector and mapping from do- main to codomain 3. Change of basis in the codomain Draft (2019-12-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com. 4.5 Singular Value Decomposition 127 Figure 4.10 Movie ratings of three people for four movies and its SVD decomposition.5 4 1 5 5 0 0 0 5 1 0 4        AliBeatrixChandra Star Wars Blade Runner Amelie Delicatessen = −0.6710 0.0236 0.4647 −0.5774 −0.7197 0.2054 −0.4759 0.4619 −0.0939 −0.7705 −0.5268 −0.3464 −0.1515 −0.6030 0.5293 −0.5774           9.6438 0 0 0 6.3639 0 0 0 0.7056 0 0 0         −0.7367 −0.6515 −0.1811 0.0852 0.1762 −0.9807 0.6708 −0.7379 −0.0743       A key difference between the eigendecomposition and the SVD is that in the SVD, domain and codomain can be vector spaces of different dimensions. In the SVD, the left- and right-singular vector matrices U and V are generally not inverse of each other (they perform basis changes in dif- ferent vector spaces). In the eigendecomposition, the basis change ma- trices P and P −1 are inverses of each other. In the SVD, the entries in the diagonal matrix Σ are all real and non- negative, which is not generally true for the diagonal matrix in the eigendecomposition. The SVD and the eigendecomposition are closely related through their projections – The left-singular vectors of A are eigenvectors of AA⊤ – The right-singular vectors of A are eigenvectors of A⊤A. – The nonzero singular values of A are the square roots of the nonzero eigenvalues of AA⊤ and are equal to the nonzero eigenvalues of A⊤A. For symmetric matrices A ∈ Rn×n, the eigenvalue decomposition and the SVD are one and the same, which follows from the spectral theo- rem 4.15. Example 4.14 (Finding Structure in Movie Ratings and Consumers) Let us add a practical interpretation of the SVD by analyzing data on people and their preferred movies. Consider three viewers (Ali, Beatrix, Chandra) rating four different movies (Star Wars, Blade Runner, Amelie, Delicatessen). Their ratings are values between 0 (worst) and 5 (best) and encoded in a data matrix A ∈ R 4×3 as shown in Figure 4.10. Each row c⃝2019 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press. 128 Matrix Decompositions represents a movie and each column a user. Thus, the column vectors of movie ratings, one for each viewer, are xAli, xBeatrix, xChandra. Factoring A using the SVD offers us a way to capture the relationships of how people rate movies, and especially if there is a structure linking which people like which movies. Applying the SVD to our data matrix A makes a number of assumptions: 1. All viewers rate movies consistently using the same linear mapping. 2. There are no errors or noise in the ratings. 3. We interpret the left-singular vectors ui as stereotypical movies and the right-singular vectors vj as stereotypical viewers. We then make the assumption that any viewer’s speciﬁc movie preferences can be expressed as a linear combination of the vj. Similarly, any movie’s like-ability can be expressed as a linear combination of the ui. Therefore, a vector in the domain of the SVD can be interpreted as a viewer in the “space” of stereotypical viewers, and a vector in the codomain of the SVD correspondingly as a movie in the “space” of stereotypical movies. Let usThese two “spaces” are only meaningfully spanned by the respective viewer and movie data if the data itself covers a sufﬁcient diversity of viewers and movies. inspect the SVD of our movie-user matrix. The ﬁrst left-singular vector u1 has large absolute values for the two science ﬁction movies and a large ﬁrst singular value (red shading in Figure 4.10). Thus, this groups a type of users with a speciﬁc set of movies (science ﬁction theme). Similarly, the ﬁrst right-singular v1 shows large absolute values for Ali and Beatrix, who give high ratings to science ﬁction movies (green shading in Figure 4.10). This suggests that v1 reﬂects the notion of a science ﬁction lover. Similarly, u2, seems to capture a French art house ﬁlm theme, and v2 indicates that Chandra is close to an idealized lover of such movies. An idealized science ﬁction lover is a purist and only loves science ﬁction movies, so a science ﬁction lover v1 gives a rating of zero to everything but science ﬁction themed – this logic is implied the diagonal substructure for the singular value matrix Σ. A speciﬁc movie is therefore represented by how it decomposes (linearly) into its stereotypical movies. Likewise, a person would be represented by how they decompose (via linear combi- nation) into movie themes. It is worth, to brieﬂy discuss SVD terminology and conventions, as there are different versions used in the literature. The mathematics remains in- variant to these differences, but these differences can be confusing. For convenience in notation and abstraction, we use an SVD notation where the SVD is described as having two square left- and right-singular vector matrices, but a non-square singular value matrix. Our deﬁni- tion (4.64) for the SVD is sometimes called the full SVD.full SVD Some authors deﬁne the SVD a bit differently and focus on square sin- Draft (2019-12-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com. 4.6 Matrix Approximation 129 gular matrices. Then, for A ∈ Rm×n and m ⩾ n, A m×n = U m×n Σ n×n V ⊤ n×n . (4.89) Sometimes this formulation is called the reduced SVD (e.g., Datta (2010)) reduced SVD or the SVD (e.g., Press et al. (2007)). This alternative format changes merely how the matrices are constructed but leaves the mathematical structure of the SVD unchanged. The convenience of this alternative formulation is that Σ is diagonal, as in the eigenvalue decomposition. In Section 4.6, we will learn about matrix approximation techniques using the SVD, which is also called the truncated SVD. truncated SVD It is possible to deﬁne the SVD of a rank-r matrix A so that U is an m × r matrix, Σ a diagonal matrix r × r, and V an r × n matrix. This construction is very similar to our deﬁnition, and ensures that the diagonal matrix Σ has only nonzero entries along the diagonal. The main convenience of this alternative notation is that Σ is diagonal, as in the eigenvalue decomposition. A restriction that the SVD for A only applies to m × n matrices with m > n is practically unnecessary. When m < n, the SVD decomposition will yield Σ with more zero columns than rows and, consequently, the singular values σm+1, . . . , σn are 0. The SVD is used in a variety of applications in machine learning from least-squares problems in curve ﬁtting to solving systems of linear equa- tions. These applications harness various important properties of the SVD, its relation to the rank of a matrix, and its ability to approximate matrices of a given rank with lower-rank matrices. Substituting a matrix with its SVD has often the advantage of making calculation more robust to nu- merical rounding errors. As we will explore in the next section, the SVD’s ability to approximate matrices with “simpler” matrices in a principled manner opens up machine learning applications ranging from dimension- ality reduction and topic modeling to data compression and clustering. 4.6 Matrix Approximation We considered the SVD as a way to factorize A = U ΣV ⊤ ∈ R m×n into the product of three matrices, where U ∈ Rm×m and V ∈ R n×n are or- thogonal and Σ contains the singular values on its main diagonal. Instead of doing the full SVD factorization, we will now investigate how the SVD allows us to represent a matrix A as a sum of simpler (low-rank) matrices Ai, which lends itself to a matrix approximation scheme that is cheaper to compute than the full SVD. We construct a rank-1 matrix Ai ∈ R m×n as Ai := uiv⊤ i , (4.90) which is formed by the outer product of the ith orthogonal column vector c⃝2019 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press. 130 Matrix Decompositions Figure 4.11 Image processing with the SVD. (a) The original grayscale image is a 1, 432 × 1, 910 matrix of values between 0 (black) and 1 (white). (b)–(f) Rank-1 matrices A1, . . . , A5 and their corresponding singular values σ1, . . . , σ5. The grid-like structure of each rank-1 matrix is imposed by the outer-product of the left and right-singular vectors. (a) Original image A. (b) A1, σ1 ≈ 228, 052. (c) A2, σ2 ≈ 40, 647. (d) A3, σ3 ≈ 26, 125. (e) A4, σ4 ≈ 20, 232. (f) A5, σ5 ≈ 15, 436. of U and V . Figure 4.11 shows an image of Stonehenge, which can be represented by a matrix A ∈ R 1432×1910, and some outer products Ai, as deﬁned in (4.90). A matrix A ∈ R m×n of rank r can be written as a sum of rank-1 matrices Ai so that A = r∑ i=1 σiuiv⊤ i = r∑ i=1 σiAi , (4.91) where the outer-product matrices Ai are weighted by the ith singular value σi. We can see why (4.91) holds: The diagonal structure of the singular value matrix Σ multiplies only matching left- and right-singular vectors uiv⊤ i and scales them by the corresponding singular value σi. All terms Σijuiv⊤ j vanish for i ̸= j because Σ is a diagonal matrix. Any terms i > r vanish because the corresponding singular values are 0. In (4.90), we introduced rank-1 matrices Ai. We summed up the r in- dividual rank-1 matrices to obtain a rank-r matrix A; see (4.91). If the sum does not run over all matrices Ai, i = 1, . . . , r, but only up to an intermediate value k < r, we obtain a rank-k approximationrank-k approximation ̂A(k) := k∑ i=1 σiuiv⊤ i = k∑ i=1 σiAi (4.92) of A with rk( ̂A(k)) = k. Figure 4.12 shows low-rank approximations ̂A(k) of an original image A of Stonehenge. The shape of the rocks be- comes increasingly visible and clearly recognizable in the rank-5 approx- imation. While the original image requires 1, 432 · 1, 910 = 2, 735, 120 numbers, the rank-5 approximation requires us only to store the ﬁve sin- gular values and the ﬁve left- and right-singular vectors (1, 432 and 1, 910- Draft (2019-12-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com. 4.6 Matrix Approximation 131 Figure 4.12 Image reconstruction with the SVD. (a) Original image. (b)–(f) Image reconstruction using the low-rank approximation of the SVD, where the rank-k approximation is given by ̂A(k) = ∑k i=1 σiAi. (a) Original image A. (b) Rank-1 approximation ̂A(1).(c) Rank-2 approximation ̂A(2). (d) Rank-3 approximation ̂A(3).(e) Rank-4 approximation ̂A(4).(f) Rank-5 approximation ̂A(5). dimensional each) for a total of 5 · (1, 432 + 1, 910 + 1) = 16, 715 numbers – just above 0.6% of the original. To measure the difference (error) between A and its rank-k approxima- tion ̂A(k), we need the notion of a norm. In Section 3.1, we already used norms on vectors that measure the length of a vector. By analogy we can also deﬁne norms on matrices. Deﬁnition 4.23 (Spectral Norm of a Matrix). For x ∈ R n\\{0}, the spectral spectral norm norm of a matrix A ∈ Rm×n is deﬁned as ∥A∥2 := max x ∥Ax∥2 ∥x∥2 . (4.93) We introduce the notation of a subscript in the matrix norm (left-hand side), similar to the Euclidean norm for vectors (right-hand side), which has subscript 2. The spectral norm (4.93) determines how long any vector x can at most become when multiplied by A. Theorem 4.24. The spectral norm of A is its largest singular value σ1. We leave the proof of this theorem as an exercise. Eckart-Young theoremTheorem 4.25 (Eckart-Young Theorem (Eckart and Young, 1936)). Con- sider a matrix A ∈ Rm×n of rank r and let B ∈ Rm×n be a matrix of rank k. For any k ⩽ r with ̂A(k) = ∑k i=1 σiuiv⊤ i it holds that ̂A(k) = argminrk(B)=k ∥A − B∥2 , (4.94) ∥ ∥ ∥A − ̂A(k)∥ ∥ ∥ 2 = σk+1 . (4.95) The Eckart-Young theorem states explicitly how much error we intro- duce by approximating A using a rank-k approximation. We can inter- pret the rank-k approximation obtained with the SVD as a projection of c⃝2019 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press. 132 Matrix Decompositions the full-rank matrix A onto a lower-dimensional space of rank-at-most-k matrices. Of all possible projections, the SVD minimizes the error (with respect to the spectral norm) between A and any rank-k approximation. We can retrace some of the steps to understand why (4.95) should hold. We observe that the difference between A − ̂A(k) is a matrix containing the sum of the remaining rank-1 matrices A − ̂A(k) = r∑ i=k+1 σiuiv⊤ i . (4.96) By Theorem 4.24, we immediately obtain σk+1 as the spectral norm of the difference matrix. Let us have a closer look at (4.94). If we assume that there is another matrix B with rk(B) ⩽ k, such that ∥A − B∥2 < ∥ ∥ ∥A − ̂A(k) ∥ ∥ ∥ 2 , (4.97) then there exists an at least (n − k)-dimensional null space Z ⊆ Rn, such that x ∈ Z implies that Bx = 0. Then it follows that ∥Ax∥2 = ∥(A − B)x∥2 , (4.98) and by using a version of the Cauchy-Schwartz inequality (3.17) that en- compasses norms of matrices, we obtain ∥Ax∥2 ⩽ ∥A − B∥2 ∥x∥2 < σk+1 ∥x∥2 . (4.99) However, there exists a (k + 1)-dimensional subspace where ∥Ax∥2 ⩾ σk+1 ∥x∥2, which is spanned by the right-singular vectors vj, j ⩽ k + 1 of A. Adding up dimensions of these two spaces yields a number greater than n, as there must be a nonzero vector in both spaces. This is a contradiction of the rank-nullity theorem (Theorem 2.24) in Section 2.7.3. The Eckart-Young theorem implies that we can use SVD to reduce a rank-r matrix A to a rank-k matrix ̂A in a principled, optimal (in the spectral norm sense) manner. We can interpret the approximation of A by a rank-k matrix as a form of lossy compression. Therefore, the low-rank approximation of a matrix appears in many machine learning applications, e.g., image processing, noise ﬁltering, and regularization of ill-posed prob- lems. Furthermore, it plays a key role in dimensionality reduction and principal component analysis, as we will see in Chapter 10. Example 4.15 (Finding Structure in Movie Ratings and Consumers (continued)) Coming back to our movie-rating example, we can now apply the con- cept of low-rank approximations to approximate the original data matrix. Recall that our ﬁrst singular value captures the notion of science ﬁction theme in movies and science ﬁction lovers. Thus, by using only the ﬁrst Draft (2019-12-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com. 4.6 Matrix Approximation 133 singular value term in a rank-1 decomposition of the movie-rating matrix, we obtain the predicted ratings A1 = u1v⊤ 1 =     −0.6710 −0.7197 −0.0939 −0.1515     [ −0.7367 −0.6515 −0.1811] (4.100a) =     0.4943 0.4372 0.1215 0.5302 0.4689 0.1303 0.0692 0.0612 0.0170 0.1116 0.0987 0.0274     . (4.100b) This ﬁrst rank-1 approximation A1 is insightful: it tells us that Ali and Beatrix like science ﬁction movies, such as Star Wars and Bladerunner (entries have values > 0.4), but fails to capture the ratings of the other movies by Chandra. This is not surprising, as Chandra’s type of movies is not captured by the ﬁrst singular value. The second singular value gives us a better rank-1 approximation for those movie-theme lovers: A2 = u2v⊤ 2 =     0.0236 0.2054 −0.7705 −0.6030     [ 0.0852 0.1762 −0.9807 ] (4.101a) =     −0.0154 0.0042 −0.0174 −0.1338 0.0362 −0.1516 0.5019 −0.1358 0.5686 0.3928 −0.1063 0.445     . (4.101b) In this second rank-1 approximation A2, we capture Chandra’s ratings and movie types well, but not the science ﬁction movies. This leads us to consider the rank-2 approximation ̂A(2), where we combine the ﬁrst two rank-1 approximations ̂A(2) = σ1A1 + σ2A2 =     4.7801 4.2419 1.0244 5.2252 4.7522 −0.0250 0.2493 −0.2743 4.9724 0.7495 0.2756 4.0278     . (4.102) ̂A(2) is similar to the original movie ratings table A =     5 4 1 5 5 0 0 0 5 1 0 4     , (4.103) and this suggests that we can ignore the contribution of A3. We can in- terpret this so that in the data table there is no evidence of a third movie- c⃝2019 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press. 134 Matrix Decompositions Figure 4.13 A functional phylogeny of matrices encountered in machine learning. Real matrices ∃ Pseudo-inverse ∃ SVD Square ∃ Determinant ∃ Trace Nonsquare Defective Singular Non-defective (diagonalizable) Singular Normal Non-normal Symmetric eigenvalues ∈ R Positive deﬁnite Cholesky eigenvalues > 0 Diagonal Identity matrix ∃ Inverse Matrix Regular (invertible) OrthogonalRotation Rn×n Rn×m No basis of eigenvectors Basis of eigenvectors A ⊤A = AA ⊤ A⊤A ̸= AA⊤ Columns are orthogonal eigenvectors A ⊤ A = AA ⊤ = I det ̸= 0 det ̸= 0 det = 0 theme/movie-lovers category. This also means that the entire space of movie-themes/movie-lovers in our example is a two-dimensional space spanned by science ﬁction and French art house movies and lovers. 4.7 Matrix Phylogeny The word “phylogenetic” describes how we capture the relationships among individuals or groups and derived from the Greek words for “tribe” and “source”. In Chapters 2 and 3, we covered the basics of linear algebra and analytic geometry. In this chapter, we looked at fundamental characteristics of ma- trices and linear mappings. Figure 4.13 depicts the phylogenetic tree of relationships between different types of matrices (black arrows indicating “is a subset of”) and the covered operations we can perform on them (in blue). We consider all real matrices A ∈ R n×m. For non-square matrices (where n ̸= m), the SVD always exists, as we saw in this chapter. Focus- ing on square matrices A ∈ R n×n, the determinant informs us whether a square matrix possesses an inverse matrix, i.e., whether it belongs to the class of regular, invertible matrices. If the square n × n matrix possesses n linearly independent eigenvectors, then the matrix is non-defective and an Draft (2019-12-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com. 4.8 Further Reading 135 eigendecomposition exists (Theorem 4.12). We know that repeated eigen- values may result in defective matrices, which cannot be diagonalized. Non-singular and non-defective matrices are not the same. For exam- ple, a rotation matrix will be invertible (determinant is nonzero) but not diagonalizable in the real numbers (eigenvalues are not guaranteed to be real numbers). We dive further into the branch of non-defective square n × n matrices. A is normal if the condition A⊤A = AA⊤ holds. Moreover, if the more restrictive condition holds that A⊤A = AA⊤ = I, then A is called or- thogonal (see Deﬁnition 3.8). The set of orthogonal matrices is a subset of the regular (invertible) matrices and satisﬁes A⊤ = A−1. Normal matrices have a frequently encountered subset, the symmetric matrices S ∈ Rn×n, which satisfy S = S⊤. Symmetric matrices have only real eigenvalues. A subset of the symmetric matrices consists of the pos- itive deﬁnite matrices P that satisfy the condition of x ⊤P x > 0 for all x ∈ R n\\{0}. In this case, a unique Cholesky decomposition exists (Theo- rem 4.18). Positive deﬁnite matrices have only positive eigenvalues and are always invertible (i.e., have a nonzero determinant). Another subset of symmetric matrices consists of the diagonal matrices D. Diagonal matrices are closed under multiplication and addition, but do not necessarily form a group (this is only the case if all diagonal entries are nonzero so that the matrix is invertible). A special diagonal matrix is the identity matrix I. 4.8 Further Reading Most of the content in this chapter establishes underlying mathematics and connects them to methods for studying mappings, many of which are at the heart of machine learning at the level of underpinning software so- lutions and building blocks for almost all machine learning theory. Matrix characterization using determinants, eigenspectra, and eigenspaces pro- vides fundamental features and conditions for categorizing and analyzing matrices. This extends to all forms of representations of data and map- pings involving data, as well as judging the numerical stability of compu- tational operations on such matrices (Press et al., 2007). Determinants are fundamental tools in order to invert matrices and compute eigenvalues “by hand”. However, for almost all but the smallest instances, numerical computation by Gaussian elimination outperforms determinants (Press et al., 2007). Determinants remain nevertheless a powerful theoretical concept, e.g., to gain intuition about the orientation of a basis based on the sign of the determinant. Eigenvectors can be used to perform basis changes to transform data into the coordinates of mean- ingful orthogonal, feature vectors. Similarly, matrix decomposition meth- ods, such as the Cholesky decomposition, reappear often when we com- pute or simulate random events (Rubinstein and Kroese, 2016). Therefore, c⃝2019 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press. 136 Matrix Decompositions the Cholesky decomposition enables us to compute the reparametrization trick where we want to perform continuous differentiation over random variables, e.g., in variational autoencoders (Jimenez Rezende et al., 2014; Kingma and Welling, 2014). Eigendecomposition is fundamental in enabling us to extract mean- ingful and interpretable information that characterizes linear mappings. Therefore, the eigendecomposition underlies a general class of machine learning algorithms called spectral methods that perform eigendecomposi- tion of a positive-deﬁnite kernel. These spectral decomposition methods encompass classical approaches to statistical data analysis, such as the following: principal component analysis Principal component analysis (PCA (Pearson, 1901), see also Chapter 10), in which a low-dimensional subspace, which explains most of the vari- ability in the data, is sought.Fisher discriminant analysis Fisher discriminant analysis, which aims to determine a separating hy- perplane for data classiﬁcation (Mika et al., 1999).multidimensional scaling Multidimensional scaling (MDS) (Carroll and Chang, 1970). The computational efﬁciency of these methods typically comes from ﬁnd- ing the best rank-k approximation to a symmetric, positive semideﬁnite matrix. More contemporary examples of spectral methods have different origins, but each of them requires the computation of the eigenvectors and eigenvalues of a positive-deﬁnite kernel, such as Isomap (TenenbaumIsomap et al., 2000), Laplacian eigenmaps (Belkin and Niyogi, 2003), HessianLaplacian eigenmaps Hessian eigenmaps eigenmaps (Donoho and Grimes, 2003), and spectral clustering (Shi and spectral clustering Malik, 2000). The core computations of these are generally underpinned by low-rank matrix approximation techniques (Belabbas and Wolfe, 2009) as we encountered here via the SVD. The SVD allows us to discover some of the same kind of information as the eigendecomposition. However, the SVD is more generally applicable to non-square matrices and data tables. These matrix factorization meth- ods become relevant whenever we want to identify heterogeneity in data when we want to perform data compression by approximation, e.g., in- stead of storing n×m values just storing (n+m)k values, or when we want to perform data pre-processing, e.g., to decorrelate predictor variables of a design matrix (Ormoneit et al., 2001). The SVD operates on matrices, which we can interpret as rectangular arrays with two indices (rows and columns). The extension of matrix-like structure to higher-dimensional arrays are called tensors. It turns out that the SVD is the special case of a more general family of decompositions that operate on such tensors (Kolda and Bader, 2009). SVD-like operations and low-rank approxima- tions on tensors are, for example, the Tucker decomposition (Tucker, 1966)Tucker decomposition or the CP decomposition (Carroll and Chang, 1970). CP decomposition The SVD low-rank approximation is frequently used in machine learn- ing for computational efﬁciency reasons. This is because it reduces the Draft (2019-12-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com. Exercises 137 amount of memory and operations with nonzero multiplications we need to perform on potentially very large matrices of data (Trefethen and Bau III, 1997). Moreover, low-rank approximations are used to operate on ma- trices that may contain missing values as well as for purposes of lossy compression and dimensionality reduction (Moonen and De Moor, 1995; Markovsky, 2011). Exercises 4.1 Compute the determinant using the Laplace expansion (using the ﬁrst row) and the Sarrus Rule for A =   1 3 5 2 4 6 0 2 4   . 4.2 Compute the following determinant efﬁciently:       2 0 1 2 0 2 −1 0 1 1 0 1 2 1 2 −2 0 2 −1 2 2 0 0 1 1       . 4.3 Compute the eigenspaces of [ 1 0 1 1 ] , [ −2 2 2 1 ] . 4.4 Compute all eigenspaces of A =     0 −1 1 1 −1 1 −2 3 2 −1 0 0 1 −1 1 0     . 4.5 Diagonalizability of a matrix is unrelated to its invertibility. Determine for the following four matrices whether they are diagonalizable and/or invert- ible [ 1 0 0 1 ] , [1 0 0 0 ] , [ 1 1 0 1 ] , [ 0 1 0 0 ] . 4.6 Compute the eigenspaces of the following transformation matrices. Are they diagonalizable? a. A =  2 3 0 1 4 3 0 0 1   b. A =     1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0     c⃝2019 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press. 138 Matrix Decompositions 4.7 Are the following matrices diagonalizable? If yes, determine their diagonal form and a basis with respect to which the transformation matrices are di- agonal. If no, give reasons why they are not diagonalizable. a. A = [ 0 1 −8 4 ] b. A =  1 1 1 1 1 1 1 1 1   c. A =     5 4 2 1 0 1 −1 −1 −1 −1 3 0 1 1 −1 2     d. A =   5 −6 −6 −1 4 2 3 −6 −4   4.8 Find the SVD of the matrix A = [ 3 2 2 2 3 −2 ] . 4.9 Find the singular value decomposition of A = [ 2 2 −1 1 ] . 4.10 Find the best rank-1 approximation of A = [ 3 2 2 2 3 −2 ] . 4.11 Show that for any A ∈ Rm×n the matrices A ⊤A and AA ⊤ possess the same nonzero eigenvalues. 4.12 Show that for x ̸= 0 Theorem 4.24 holds, i.e., show that max x ∥Ax∥2 ∥x∥2 = σ1 , where σ1 is the largest singular value of A ∈ Rm×n. Draft (2019-12-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com. 5 Vector Calculus Many algorithms in machine learning optimize an objective function with respect to a set of desired model parameters that control how well a model explains the data: Finding good parameters can be phrased as an opti- mization problem (see Sections 8.2 and 8.3). Examples include: (i) lin- ear regression (see Chapter 9), where we look at curve-ﬁtting problems and optimize linear weight parameters to maximize the likelihood; (ii) neural-network auto-encoders for dimensionality reduction and data com- pression, where the parameters are the weights and biases of each layer, and where we minimize a reconstruction error by repeated application of the chain rule; and (iii) Gaussian mixture models (see Chapter 11) for modeling data distributions, where we optimize the location and shape parameters of each mixture component to maximize the likelihood of the model. Figure 5.1 illustrates some of these problems, which we typically solve by using optimization algorithms that exploit gradient information (Section 7.1). Figure 5.2 gives an overview of how concepts in this chap- ter are related and how they are connected to other chapters of the book. Central to this chapter is the concept of a function. A function f is a quantity that relates two quantities to each other. In this book, these quantities are typically inputs x ∈ R D and targets (function values) f (x), which we assume are real-valued if not stated otherwise. Here RD is the domain of f , and the function values f (x) are the image/codomain of f . domain image/codomain Figure 5.1 Vector calculus plays a central role in (a) regression (curve ﬁtting) and (b) density estimation, i.e., modeling data distributions. −4 −2 0 2 4 x −4 −2 0 2 4y Training data MLE (a) Regression problem: Find parameters, such that the curve explains the observations (crosses) well. −10 −5 0 5 10 x1 −10 −5 0 5 10x2 (b) Density estimation with a Gaussian mixture model: Find means and covariances, such that the data (dots) can be explained well. 139 This material will be published by Cambridge University Press as Mathematics for Machine Learn- ing by Marc Peter Deisenroth, A. Aldo Faisal, and Cheng Soon Ong. This pre-publication version is free to view and download for personal use only. Not for re-distribution, re-sale or use in deriva- tive works. c⃝by M. P. Deisenroth, A. A. Faisal, and C. S. Ong, 2019. https://mml-book.com. 140 Vector Calculus Figure 5.2 A mind map of the concepts introduced in this chapter, along with when they are used in other parts of the book. Difference quotient Partial derivatives Jacobian Hessian Taylor series Chapter 7 Optimization Chapter 6 Probability Chapter 9 Regression Chapter 10 Dimensionality reduction Chapter 11 Density estimation Chapter 12 Classiﬁcationdeﬁnescollectedinusedin used in used inusedin used in used in used in Section 2.7.3 provides much more detailed discussion in the context of linear functions. We often write f : R D → R (5.1a) x ↦→ f (x) (5.1b) to specify a function, where (5.1a) speciﬁes that f is a mapping from R D to R and (5.1b) speciﬁes the explicit assignment of an input x to a function value f (x). A function f assigns every input x exactly one function value f (x). Example 5.1 Recall the dot product as a special case of an inner product (Section 3.2). In the previous notation, the function f (x) = x⊤x, x ∈ R2, would be speciﬁed as f : R 2 → R (5.2a) x ↦→ x2 1 + x2 2 . (5.2b) In this chapter, we will discuss how to compute gradients of functions, which is often essential to facilitate learning in machine learning models since the gradient points in the direction of steepest ascent. Therefore, Draft (2019-12-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com. 5.1 Differentiation of Univariate Functions 141 Figure 5.3 The average incline of a function f between x0 and x0 + δx is the incline of the secant (blue) through f (x0) and f (x0 + δx) and given by δy/δx. δy δx f (x) x y f (x0) f (x0 + δx) vector calculus is one of the fundamental mathematical tools we need in machine learning. Throughout this book, we assume that functions are differentiable. With some additional technical deﬁnitions, which we do not cover here, many of the approaches presented can be extended to sub-differentials (functions that are continuous but not differentiable at certain points). We will look at an extension to the case of functions with constraints in Chapter 7. 5.1 Differentiation of Univariate Functions In the following, we brieﬂy revisit differentiation of a univariate function, which may be familiar from high school mathematics. We start with the difference quotient of a univariate function y = f (x), x, y ∈ R, which we will subsequently use to deﬁne derivatives. Deﬁnition 5.1 (Difference Quotient). The difference quotient difference quotient δy δx := f (x + δx) − f (x) δx (5.3) computes the slope of the secant line through two points on the graph of f . In Figure 5.3, these are the points with x-coordinates x0 and x0 + δx. The difference quotient can also be considered the average slope of f between x and x + δx if we assume f to be a linear function. In the limit for δx → 0, we obtain the tangent of f at x, if f is differentiable. The tangent is then the derivative of f at x. Deﬁnition 5.2 (Derivative). More formally, for h > 0 the derivative of f derivative at x is deﬁned as the limit df dx := lim h→0 f (x + h) − f (x) h , (5.4) and the secant in Figure 5.3 becomes a tangent. The derivative of f points in the direction of steepest ascent of f . c⃝2019 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press. 142 Vector Calculus Example 5.2 (Derivative of a Polynomial) We want to compute the derivative of f (x) = xn, n ∈ N. We may already know that the answer will be nxn−1, but we want to derive this result using the deﬁnition of the derivative as the limit of the difference quotient. Using the deﬁnition of the derivative in (5.4), we obtain df dx = lim h→0 f (x + h) − f (x) h (5.5a) = lim h→0 (x + h) n − x n h (5.5b) = lim h→0 ∑n i=0 (n i ) xn−ihi − xn h . (5.5c) We see that xn = (n 0) xn−0h 0. By starting the sum at 1, the xn-term cancels, and we obtain df dx = lim h→0 ∑n i=1 ( n i )xn−ih i h (5.6a) = lim h→0 n∑ i=1 (n i ) xn−ih i−1 (5.6b) = lim h→0 (n 1 ) xn−1 + n∑ i=2 (n i ) xn−ih i−1 ︸ ︷︷ ︸ →0 as h→0 (5.6c) = n! 1!(n − 1)! xn−1 = nxn−1 . (5.6d) 5.1.1 Taylor Series The Taylor series is a representation of a function f as an inﬁnite sum of terms. These terms are determined using derivatives of f evaluated at x0. Deﬁnition 5.3 (Taylor Polynomial). The Taylor polynomial of degree n ofTaylor polynomial f : R → R at x0 is deﬁned asWe deﬁne t0 := 1 for all t ∈ R. Tn(x) := n∑ k=0 f (k)(x0) k! (x − x0)k , (5.7) where f (k)(x0) is the kth derivative of f at x0 (which we assume exists) and f (k)(x0) k! are the coefﬁcients of the polynomial. Deﬁnition 5.4 (Taylor Series). For a smooth function f ∈ C∞, f : R → R, the Taylor series of f at x0 is deﬁned asTaylor series Draft (2019-12-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com. 5.1 Differentiation of Univariate Functions 143 T∞(x) = ∞∑ k=0 f (k)(x0) k! (x − x0) k . (5.8) For x0 = 0, we obtain the Maclaurin series as a special instance of the f ∈ C∞ means that f is continuously differentiable inﬁnitely many times. Maclaurin series Taylor series. If f (x) = T∞(x), then f is called analytic. analytic Remark. In general, a Taylor polynomial of degree n is an approximation of a function, which does not need to be a polynomial. The Taylor poly- nomial is similar to f in a neighborhood around x0. However, a Taylor polynomial of degree n is an exact representation of a polynomial f of degree k ⩽ n since all derivatives f (i), i > k vanish. ♦ Example 5.3 (Taylor Polynomial) We consider the polynomial f (x) = x4 (5.9) and seek the Taylor polynomial T6, evaluated at x0 = 1. We start by com- puting the coefﬁcients f (k)(1) for k = 0, . . . , 6: f (1) = 1 (5.10) f ′(1) = 4 (5.11) f ′′(1) = 12 (5.12) f (3)(1) = 24 (5.13) f (4)(1) = 24 (5.14) f (5)(1) = 0 (5.15) f (6)(1) = 0 (5.16) Therefore, the desired Taylor polynomial is T6(x) = 6∑ k=0 f (k)(x0) k! (x − x0)k (5.17a) = 1 + 4(x − 1) + 6(x − 1) 2 + 4(x − 1) 3 + (x − 1) 4 + 0 . (5.17b) Multiplying out and re-arranging yields T6(x) = (1 − 4 + 6 − 4 + 1) + x(4 − 12 + 12 − 4) + x2(6 − 12 + 6) + x3(4 − 4) + x4 (5.18a) = x 4 = f (x) , (5.18b) i.e., we obtain an exact representation of the original function. c⃝2019 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press. 144 Vector Calculus Figure 5.4 Taylor polynomials. The original function f (x) = sin(x) + cos(x) (black, solid) is approximated by Taylor polynomials (dashed) around x0 = 0. Higher-order Taylor polynomials approximate the function f better and more globally. T10 is already similar to f in [−4, 4]. −4 −2 0 2 4 x −2 0 2 4y f T0 T1 T5 T10 Example 5.4 (Taylor Series) Consider the function in Figure 5.4 given by f (x) = sin(x) + cos(x) ∈ C∞ . (5.19) We seek a Taylor series expansion of f at x0 = 0, which is the Maclaurin series expansion of f . We obtain the following derivatives: f (0) = sin(0) + cos(0) = 1 (5.20) f ′(0) = cos(0) − sin(0) = 1 (5.21) f ′′(0) = − sin(0) − cos(0) = −1 (5.22) f (3)(0) = − cos(0) + sin(0) = −1 (5.23) f (4)(0) = sin(0) + cos(0) = f (0) = 1 (5.24) ... We can see a pattern here: The coefﬁcients in our Taylor series are only ±1 (since sin(0) = 0), each of which occurs twice before switching to the other one. Furthermore, f (k+4)(0) = f (k)(0). Therefore, the full Taylor series expansion of f at x0 = 0 is given by T∞(x) = ∞∑ k=0 f (k)(x0) k! (x − x0) k (5.25a) = 1 + x − 1 2! x2 − 1 3! x3 + 1 4! x4 + 1 5! x5 − · · · (5.25b) = 1 − 1 2! x 2 + 1 4! x4 ∓ · · · + x − 1 3! x3 + 1 5! x5 ∓ · · · (5.25c) = ∞∑ k=0(−1) k 1 (2k)! x 2k + ∞∑ k=0(−1) k 1 (2k + 1)! x2k+1 (5.25d) = cos(x) + sin(x) , (5.25e) Draft (2019-12-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com. 5.1 Differentiation of Univariate Functions 145 where we used the power series representations power series representation cos(x) = ∞∑ k=0(−1) k 1 (2k)! x2k , (5.26) sin(x) = ∞∑ k=0(−1) k 1 (2k + 1)! x2k+1 . (5.27) Figure 5.4 shows the corresponding ﬁrst Taylor polynomials Tn for n = 0, 1, 5, 10. Remark. A Taylor series is a special case of a power series f (x) = ∞∑ k=0 ak(x − c)k (5.28) where ak are coefﬁcients and c is a constant, which has the special form in Deﬁnition 5.4. ♦ 5.1.2 Differentiation Rules In the following, we brieﬂy state basic differentiation rules, where we denote the derivative of f by f ′. Product rule: (f (x)g(x)) ′ = f ′(x)g(x) + f (x)g′(x) (5.29) Quotient rule: ( f (x) g(x) )′ = f ′(x)g(x) − f (x)g′(x) (g(x))2 (5.30) Sum rule: (f (x) + g(x)) ′ = f ′(x) + g′(x) (5.31) Chain rule: (g(f (x)) )′ = (g ◦ f ) ′(x) = g′(f (x))f ′(x) (5.32) Here, g ◦ f denotes function composition x ↦→ f (x) ↦→ g(f (x)). Example 5.5 (Chain rule) Let us compute the derivative of the function h(x) = (2x + 1)4 using the chain rule. With h(x) = (2x + 1)4 = g(f (x)) , (5.33) f (x) = 2x + 1 , (5.34) g(f ) = f 4 , (5.35) we obtain the derivatives of f and g as f ′(x) = 2 , (5.36) g′(f ) = 4f 3 , (5.37) c⃝2019 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press. 146 Vector Calculus such that the derivative of h is given as h′(x) = g′(f )f ′(x) = (4f 3) · 2 (5.34) = 4(2x + 1)3 · 2 = 8(2x + 1)3 , (5.38) where we used the chain rule (5.32) and substituted the deﬁnition of f in (5.34) in g′(f ). 5.2 Partial Differentiation and Gradients Differentiation as discussed in Section 5.1 applies to functions f of a scalar variable x ∈ R. In the following, we consider the general case where the function f depends on one or more variables x ∈ R n, e.g., f (x) = f (x1, x2). The generalization of the derivative to functions of sev- eral variables is the gradient. We ﬁnd the gradient of the function f with respect to x by varying one variable at a time and keeping the others constant. The gradient is then the collection of these partial derivatives. Deﬁnition 5.5 (Partial Derivative). For a function f : Rn → R, x ↦→ f (x), x ∈ R n of n variables x1, . . . , xn we deﬁne the partial derivatives aspartial derivative ∂f ∂x1 = lim h→0 f (x1 + h, x2, . . . , xn) − f (x) h ... ∂f ∂xn = lim h→0 f (x1, . . . , xn−1, xn + h) − f (x) h (5.39) and collect them in the row vector ∇xf = gradf = df dx = [ ∂f (x) ∂x1 ∂f (x) ∂x2 · · · ∂f (x) ∂xn ] ∈ R1×n , (5.40) where n is the number of variables and 1 is the dimension of the image/ range/codomain of f . Here, we deﬁned the column vector x = [x1, . . . , xn]⊤ ∈ R n. The row vector in (5.40) is called the gradient of f or the Jacobiangradient Jacobian and is the generalization of the derivative from Section 5.1. Remark. This deﬁnition of the Jacobian is a special case of the general deﬁnition of the Jacobian for vector-valued functions as the collection of partial derivatives. We will get back to this in Section 5.3. ♦ We can use results from scalar differentiation: Each partial derivative is a derivative with respect to a scalar. Example 5.6 (Partial Derivatives Using the Chain Rule) For f (x, y) = (x + 2y3) 2, we obtain the partial derivatives ∂f (x, y) ∂x = 2(x + 2y3) ∂ ∂x (x + 2y3) = 2(x + 2y3) , (5.41) Draft (2019-12-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com. 5.2 Partial Differentiation and Gradients 147 ∂f (x, y) ∂y = 2(x + 2y3) ∂ ∂y (x + 2y3) = 12(x + 2y3)y2 . (5.42) where we used the chain rule (5.32) to compute the partial derivatives. Remark (Gradient as a Row Vector). It is not uncommon in the literature to deﬁne the gradient vector as a column vector, following the conven- tion that vectors are generally column vectors. The reason why we deﬁne the gradient vector as a row vector is twofold: First, we can consistently generalize the gradient to vector-valued functions f : R n → R m (then the gradient becomes a matrix). Second, we can immediately apply the multi-variate chain rule without paying attention to the dimension of the gradient. We will discuss both points in Section 5.3. ♦ Example 5.7 (Gradient) For f (x1, x2) = x2 1x2 + x1x3 2 ∈ R, the partial derivatives (i.e., the deriva- tives of f with respect to x1 and x2) are ∂f (x1, x2) ∂x1 = 2x1x2 + x3 2 (5.43) ∂f (x1, x2) ∂x2 = x 2 1 + 3x1x2 2 (5.44) and the gradient is then df dx = [ ∂f (x1, x2) ∂x1 ∂f (x1, x2) ∂x2 ] = [ 2x1x2 + x3 2 x2 1 + 3x1x 2 2] ∈ R 1×2 . (5.45) 5.2.1 Basic Rules of Partial Differentiation Product rule: (f g)′ = f ′g + f g′, Sum rule: (f + g)′ = f ′ + g′, Chain rule: (g(f ))′ = g′(f )f ′ In the multivariate case, where x ∈ Rn, the basic differentiation rules that we know from school (e.g., sum rule, product rule, chain rule; see also Section 5.1.2) still apply. However, when we compute derivatives with re- spect to vectors x ∈ Rn we need to pay attention: Our gradients now involve vectors and matrices, and matrix multiplication is not commuta- tive (Section 2.2.1), i.e., the order matters. Here are the general product rule, sum rule, and chain rule: Product rule: ∂ ∂x ( f (x)g(x) ) = ∂f ∂x g(x) + f (x) ∂g ∂x (5.46) Sum rule: ∂ ∂x (f (x) + g(x) ) = ∂f ∂x + ∂g ∂x (5.47) c⃝2019 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press. 148 Vector Calculus Chain rule: ∂ ∂x (g ◦ f )(x) = ∂ ∂x (g(f (x)) ) = ∂g ∂f ∂f ∂x (5.48) Let us have a closer look at the chain rule. The chain rule (5.48) resem-This is only an intuition, but not mathematically correct since the partial derivative is not a fraction. bles to some degree the rules for matrix multiplication where we said that neighboring dimensions have to match for matrix multiplication to be de- ﬁned; see Section 2.2.1. If we go from left to right, the chain rule exhibits similar properties: ∂f shows up in the “denominator” of the ﬁrst factor and in the “numerator” of the second factor. If we multiply the factors to- gether, multiplication is deﬁned, i.e., the dimensions of ∂f match, and ∂f “cancels”, such that ∂g/∂x remains. 5.2.2 Chain Rule Consider a function f : R 2 → R of two variables x1, x2. Furthermore, x1(t) and x2(t) are themselves functions of t. To compute the gradient of f with respect to t, we need to apply the chain rule (5.48) for multivariate functions as df dt = [ ∂f ∂x1 ∂f ∂x2 ] [ ∂x1(t) ∂t ∂x2(t) ∂t ] = ∂f ∂x1 ∂x1 ∂t + ∂f ∂x2 ∂x2 ∂t , (5.49) where d denotes the gradient and ∂ partial derivatives. Example 5.8 Consider f (x1, x2) = x2 1 + 2x2, where x1 = sin t and x2 = cos t, then df dt = ∂f ∂x1 ∂x1 ∂t + ∂f ∂x2 ∂x2 ∂t (5.50a) = 2 sin t ∂ sin t ∂t + 2 ∂ cos t ∂t (5.50b) = 2 sin t cos t − 2 sin t = 2 sin t(cos t − 1) (5.50c) is the corresponding derivative of f with respect to t. If f (x1, x2) is a function of x1 and x2, where x1(s, t) and x2(s, t) are themselves functions of two variables s and t, the chain rule yields the partial derivatives ∂f ∂s = ∂f ∂x1 ∂x1 ∂s + ∂f ∂x2 ∂x2 ∂s , (5.51) ∂f ∂t = ∂f ∂x1 ∂x1 ∂t + ∂f ∂x2 ∂x2 ∂t , (5.52) Draft (2019-12-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com. 5.3 Gradients of Vector-Valued Functions 149 and the gradient is obtained by the matrix multiplication df d(s, t) = ∂f ∂x ∂x ∂(s, t) = [ ∂f ∂x1 ∂f ∂x2 ] ︸ ︷︷ ︸ = ∂f ∂x    ∂x1 ∂s ∂x1 ∂t ∂x2 ∂s ∂x2 ∂t    ︸ ︷︷ ︸ = ∂x ∂(s, t) . (5.53) This compact way of writing the chain rule as a matrix multiplication only The chain rule can be written as a matrix multiplication. makes sense if the gradient is deﬁned as a row vector. Otherwise, we will need to start transposing gradients for the matrix dimensions to match. This may still be straightforward as long as the gradient is a vector or a matrix; however, when the gradient becomes a tensor (we will discuss this in the following), the transpose is no longer a triviality. Remark (Verifying the Correctness of a Gradient Implementation). The deﬁnition of the partial derivatives as the limit of the corresponding dif- ference quotient (see (5.39)) can be exploited when numerically checking the correctness of gradients in computer programs: When we compute Gradient checking gradients and implement them, we can use ﬁnite differences to numer- ically test our computation and implementation: We choose the value h to be small (e.g., h = 10 −4) and compare the ﬁnite-difference approxima- tion from (5.39) with our (analytic) implementation of the gradient. If the error is small, our gradient implementation is probably correct. “Small” could mean that √ ∑ i(dhi−dfi)2 ∑ i(dhi+dfi)2 < 10−6, where dhi is the ﬁnite-difference approximation and dfi is the analytic gradient of f with respect to the ith variable xi. ♦ 5.3 Gradients of Vector-Valued Functions Thus far, we discussed partial derivatives and gradients of functions f : R n → R mapping to the real numbers. In the following, we will generalize the concept of the gradient to vector-valued functions (vector ﬁelds) f : R n → Rm, where n ⩾ 1 and m > 1. For a function f : R n → R m and a vector x = [x1, . . . , xn] ⊤ ∈ Rn, the corresponding vector of function values is given as f (x) =    f1(x) ... fm(x)    ∈ R m . (5.54) Writing the vector-valued function in this way allows us to view a vector- valued function f : R n → R m as a vector of functions [f1, . . . , fm] ⊤, fi : R n → R that map onto R. The differentiation rules for every fi are exactly the ones we discussed in Section 5.2. c⃝2019 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press. 150 Vector Calculus Therefore, the partial derivative of a vector-valued function f : Rn → R m with respect to xi ∈ R, i = 1, . . . n, is given as the vector ∂f ∂xi =    ∂f1 ∂xi ... ∂fm ∂xi    =    limh→0 f1(x1,...,xi−1,xi+h,xi+1,...xn)−f1(x) h ... limh→0 fm(x1,...,xi−1,xi+h,xi+1,...xn)−fm(x) h    ∈ Rm . (5.55) From (5.40), we know that the gradient of f with respect to a vector is the row vector of the partial derivatives. In (5.55), every partial derivative ∂f /∂xi is a column vector. Therefore, we obtain the gradient of f : R n → R m with respect to x ∈ Rn by collecting these partial derivatives: df (x) dx = ∂f (x) ∂x1 · · · ∂f (x) ∂xn [ ] (5.56a) = ∂f1(x) ∂x1 · · · ∂f1(x) ∂xn ... ... ∂fm(x) ∂x1 · · · ∂fm(x) ∂xn             ∈ R m×n . (5.56b) Deﬁnition 5.6 (Jacobian). The collection of all ﬁrst-order partial deriva- tives of a vector-valued function f : Rn → Rm is called the Jacobian. TheJacobian Jacobian J is an m × n matrix, which we deﬁne and arrange as follows:The gradient of a function f : Rn → Rm is a matrix of size m × n. J = ∇xf = df (x) dx = [ ∂f (x) ∂x1 · · · ∂f (x) ∂xn ] (5.57) =        ∂f1(x) ∂x1 · · · ∂f1(x) ∂xn ... ... ∂fm(x) ∂x1 · · · ∂fm(x) ∂xn        , (5.58) x =    x1 ... xn    , J(i, j) = ∂fi ∂xj . (5.59) As a special case of (5.58), a function f : Rn → R1, which maps a vector x ∈ Rn onto a scalar (e.g., f (x) = ∑n i=1 xi), possesses a Jacobian that is a row vector (matrix of dimension 1 × n); see (5.40). Remark. In this book, we use the numerator layout of the derivative, i.e.,numerator layout the derivative df /dx of f ∈ Rm with respect to x ∈ Rn is an m × n matrix, where the elements of f deﬁne the rows and the elements of x deﬁne the columns of the corresponding Jacobian; see (5.58). There Draft (2019-12-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com. 5.3 Gradients of Vector-Valued Functions 151 Figure 5.5 The determinant of the Jacobian of f can be used to compute the magniﬁer between the blue and orange area. b1 b2 c1 c2 f (·) exists also the denominator layout, which is the transpose of the numerator denominator layout layout. In this book, we will use the numerator layout. ♦ We will see how the Jacobian is used in the change-of-variable method for probability distributions in Section 6.7. The amount of scaling due to the transformation of a variable is provided by the determinant. In Section 4.1, we saw that the determinant can be used to compute the area of a parallelogram. If we are given two vectors b1 = [1, 0] ⊤, b2 = [0, 1] ⊤ as the sides of the unit square (blue; see Figure 5.5), the area of this square is ∣ ∣ ∣ ∣det ([ 1 0 0 1 ])∣ ∣ ∣ ∣ = 1 . (5.60) If we take a parallelogram with the sides c1 = [−2, 1] ⊤, c2 = [1, 1] ⊤ (orange in Figure 5.5), its area is given as the absolute value of the deter- minant (see Section 4.1) ∣ ∣ ∣ ∣det ([ −2 1 1 1 ])∣ ∣ ∣ ∣ = | − 3| = 3 , (5.61) i.e., the area of this is exactly three times the area of the unit square. We can ﬁnd this scaling factor by ﬁnding a mapping that transforms the unit square into the other square. In linear algebra terms, we effectively perform a variable transformation from (b1, b2) to (c1, c2). In our case, the mapping is linear and the absolute value of the determinant of this mapping gives us exactly the scaling factor we are looking for. We will describe two approaches to identify this mapping. First, we ex- ploit that the mapping is linear so that we can use the tools from Chapter 2 to identify this mapping. Second, we will ﬁnd the mapping using partial derivatives using the tools we have been discussing in this chapter. Approach 1 To get started with the linear algebra approach, we identify both {b1, b2} and {c1, c2} as bases of R2 (see Section 2.6.1 for a recap). What we effectively perform is a change of basis from (b1, b2) to (c1, c2), and we are looking for the transformation matrix that implements the basis change. Using results from Section 2.7.2, we identify the desired basis change matrix as J = [ −2 1 1 1 ] , (5.62) such that J b1 = c1 and J b2 = c2. The absolute value of the determi- c⃝2019 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press. 152 Vector Calculus nant of J , which yields the scaling factor we are looking for, is given as |det(J )| = 3, i.e., the area of the square spanned by (c1, c2) is three times greater than the area spanned by (b1, b2). Approach 2 The linear algebra approach works for linear trans- formations; for nonlinear transformations (which become relevant in Sec- tion 6.7), we follow a more general approach using partial derivatives. For this approach, we consider a function f : R2 → R2 that performs a variable transformation. In our example, f maps the coordinate represen- tation of any vector x ∈ R2 with respect to (b1, b2) onto the coordinate representation y ∈ R2 with respect to (c1, c2). We want to identify the mapping so that we can compute how an area (or volume) changes when it is being transformed by f . For this, we need to ﬁnd out how f (x) changes if we modify x a bit. This question is exactly answered by the Jacobian matrix df dx ∈ R 2×2. Since we can write y1 = −2x1 + x2 (5.63) y2 = x1 + x2 (5.64) we obtain the functional relationship between x and y, which allows us to get the partial derivatives ∂y1 ∂x1 = −2 , ∂y1 ∂x2 = 1 , ∂y2 ∂x1 = 1 , ∂y2 ∂x2 = 1 (5.65) and compose the Jacobian as J =    ∂y1 ∂x1 ∂y1 ∂x2 ∂y2 ∂x1 ∂y2 ∂x2    = [ −2 1 1 1 ] . (5.66) The Jacobian represents the coordinate transformation we are lookingGeometrically, the Jacobian determinant gives the magniﬁcation/ scaling factor when we transform an area or volume. for. It is exact if the coordinate transformation is linear (as in our case), and (5.66) recovers exactly the basis change matrix in (5.62). If the co- ordinate transformation is nonlinear, the Jacobian approximates this non- linear transformation locally with a linear one. The absolute value of the Jacobian determinant |det(J )| is the factor by which areas or volumes are Jacobian determinant scaled when coordinates are transformed. Our case yields |det(J )| = 3. The Jacobian determinant and variable transformations will become relevant in Section 6.7 when we transform random variables and prob- ability distributions. These transformations are extremely relevant in ma-Figure 5.6 Dimensionality of (partial) derivatives. f (x)x ∂f ∂x chine learning in the context of training deep neural networks using the reparametrization trick, also called inﬁnite perturbation analysis. In this chapter, we encountered derivatives of functions. Figure 5.6 sum- marizes the dimensions of those derivatives. If f : R → R the gradient is simply a scalar (top-left entry). For f : R D → R the gradient is a 1 × D row vector (top-right entry). For f : R → R E, the gradient is an E × 1 column vector, and for f : RD → R E the gradient is an E × D matrix. Draft (2019-12-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com. 5.3 Gradients of Vector-Valued Functions 153 Example 5.9 (Gradient of a Vector-Valued Function) We are given f (x) = Ax , f (x) ∈ R M , A ∈ R M ×N , x ∈ RN . To compute the gradient df /dx we ﬁrst determine the dimension of df /dx: Since f : RN → R M , it follows that df /dx ∈ RM ×N . Second, to compute the gradient we determine the partial derivatives of f with respect to every xj: fi(x) = N∑ j=1 Aijxj =⇒ ∂fi ∂xj = Aij (5.67) We collect the partial derivatives in the Jacobian and obtain the gradient df dx =    ∂f1 ∂x1 · · · ∂f1 ∂xN ... ... ∂fM ∂x1 · · · ∂fM ∂xN    =    A11 · · · A1N ... ... AM 1 · · · AM N    = A ∈ RM ×N . (5.68) Example 5.10 (Chain Rule) Consider the function h : R → R, h(t) = (f ◦ g)(t) with f : R 2 → R (5.69) g : R → R 2 (5.70) f (x) = exp(x1x 2 2) , (5.71) x = [ x1 x2 ] = g(t) = [ t cos t t sin t ] (5.72) and compute the gradient of h with respect to t. Since f : R 2 → R and g : R → R2 we note that ∂f ∂x ∈ R 1×2 , ∂g ∂t ∈ R 2×1 . (5.73) The desired gradient is computed by applying the chain rule: dh dt = ∂f ∂x ∂x ∂t = [ ∂f ∂x1 ∂f ∂x2 ]    ∂x1 ∂t ∂x2 ∂t    (5.74a) = [ exp(x1x2 2)x2 2 2 exp(x1x2 2)x1x2] [ cos t − t sin t sin t + t cos t ] (5.74b) = exp(x1x2 2) (x2 2(cos t − t sin t) + 2x1x2(sin t + t cos t) ) , (5.74c) where x1 = t cos t and x2 = t sin t; see (5.72). c⃝2019 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press. 154 Vector Calculus Example 5.11 (Gradient of a Least-Squares Loss in a Linear Model) Let us consider the linear modelWe will discuss this model in much more detail in Chapter 9 in the context of linear regression, where we need derivatives of the least-squares loss L with respect to the parameters θ. y = Φθ , (5.75) where θ ∈ RD is a parameter vector, Φ ∈ RN ×D are input features and y ∈ R N are the corresponding observations. We deﬁne the functions L(e) := ∥e∥ 2 , (5.76) e(θ) := y − Φθ . (5.77) We seek ∂L ∂θ , and we will use the chain rule for this purpose. L is called a least-squares loss function.least-squares loss Before we start our calculation, we determine the dimensionality of the gradient as ∂L ∂θ ∈ R1×D . (5.78) The chain rule allows us to compute the gradient as ∂L ∂θ = ∂L ∂e ∂e ∂θ , (5.79) where the dth element is given bydLdtheta = np.einsum( ’n,nd’, dLde,dedtheta) ∂L ∂θ [1, d] = N∑ n=1 ∂L ∂e [n] ∂e ∂θ [n, d] . (5.80) We know that ∥e∥ 2 = e ⊤e (see Section 3.2) and determine ∂L ∂e = 2e ⊤ ∈ R1×N . (5.81) Furthermore, we obtain ∂e ∂θ = −Φ ∈ R N ×D , (5.82) such that our desired derivative is ∂L ∂θ = −2e⊤Φ (5.77) = − 2(y⊤ − θ⊤Φ ⊤) ︸ ︷︷ ︸ 1×N Φ︸︷︷︸ N ×D ∈ R 1×D . (5.83) Remark. We would have obtained the same result without using the chain rule by immediately looking at the function L2(θ) := ∥y − Φθ∥ 2 = (y − Φθ)⊤(y − Φθ) . (5.84) This approach is still practical for simple functions like L2 but becomes impractical for deep function compositions. ♦ Draft (2019-12-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com. 5.4 Gradients of Matrices 155 Figure 5.7 Visualization of gradient computation of a matrix with respect to a vector. We are interested in computing the gradient of A ∈ R4×2 with respect to a vector x ∈ R3. We know that gradient dA dx ∈ R4×2×3. We follow two equivalent approaches to arrive there: (a) collating partial derivatives into a Jacobian tensor; (b) ﬂattening of the matrix into a vector, computing the Jacobian matrix, re-shaping into a Jacobian tensor. A ∈ R 4×2 x ∈ R 3 ∂A ∂x1 ∈ R 4×2 ∂A ∂x2 ∈ R4×2 ∂A ∂x3 ∈ R 4×2 x1 x2 x3 dA dx ∈ R4×2×3 4 2 3 Partial derivatives: collate (a) Approach 1: We compute the partial derivative ∂A ∂x1 , ∂A ∂x2 , ∂A ∂x3 , each of which is a 4 × 2 matrix, and col- late them in a 4 × 2 × 3 tensor. A ∈ R 4×2 x ∈ R 3 x1 x2 x3 dA dx ∈ R 4×2×3 re-shape re-shapegradient A ∈ R 4×2 ˜A ∈ R 8 d ˜A dx ∈ R 8×3 (b) Approach 2: We re-shape (ﬂatten) A ∈ R4×2 into a vec- tor ˜A ∈ R8. Then, we compute the gradient d ˜A dx ∈ R8×3. We obtain the gradient tensor by re-shaping this gradient as illustrated above. 5.4 Gradients of Matrices We can think of a tensor as a multidimensional array. We will encounter situations where we need to take gradients of matrices with respect to vectors (or other matrices), which results in a multidimen- sional tensor. We can think of this tensor as a multidimensional array that c⃝2019 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press. 156 Vector Calculus collects partial derivatives. For example, if we compute the gradient of an m × n matrix A with respect to a p × q matrix B, the resulting Jacobian would be (m×n)×(p×q), i.e., a four-dimensional tensor J , whose entries are given as Jijkl = ∂Aij/∂Bkl. Since matrices represent linear mappings, we can exploit the fact that there is a vector-space isomorphism (linear, invertible mapping) between the space R m×n of m × n matrices and the space Rmn of mn vectors. Therefore, we can re-shape our matrices into vectors of lengths mn and pq, respectively. The gradient using these mn vectors results in a Jacobian of size mn × pq. Figure 5.7 visualizes both approaches. In practical ap-Matrices can be transformed into vectors by stacking the columns of the matrix (“ﬂattening”). plications, it is often desirable to re-shape the matrix into a vector and continue working with this Jacobian matrix: The chain rule (5.48) boils down to simple matrix multiplication, whereas in the case of a Jacobian tensor, we will need to pay more attention to what dimensions we need to sum out. Example 5.12 (Gradient of Vectors with Respect to Matrices) Let us consider the following example, where f = Ax , f ∈ RM , A ∈ RM ×N , x ∈ R N (5.85) and where we seek the gradient df /dA. Let us start again by determining the dimension of the gradient as df dA ∈ R M ×(M ×N ) . (5.86) By deﬁnition, the gradient is the collection of the partial derivatives: df dA =    ∂f1 ∂A ... ∂fM ∂A    , ∂fi ∂A ∈ R1×(M ×N ) . (5.87) To compute the partial derivatives, it will be helpful to explicitly write out the matrix vector multiplication: fi = N∑ j=1 Aijxj, i = 1, . . . , M , (5.88) and the partial derivatives are then given as ∂fi ∂Aiq = xq . (5.89) This allows us to compute the partial derivatives of fi with respect to a row of A, which is given as ∂fi ∂Ai,: = x ⊤ ∈ R 1×1×N , (5.90) Draft (2019-12-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com. 5.4 Gradients of Matrices 157 ∂fi ∂Ak̸=i,: = 0 ⊤ ∈ R1×1×N (5.91) where we have to pay attention to the correct dimensionality. Since fi maps onto R and each row of A is of size 1 × N , we obtain a 1 × 1 × N - sized tensor as the partial derivative of fi with respect to a row of A. We stack the partial derivatives (5.91) and get the desired gradient in (5.87) via ∂fi ∂A =             0⊤ ... 0⊤ x ⊤ 0⊤ ... 0⊤             ∈ R1×(M ×N ) . (5.92) Example 5.13 (Gradient of Matrices with Respect to Matrices) Consider a matrix R ∈ RM ×N and f : RM ×N → RN ×N with f (R) = R⊤R =: K ∈ RN ×N , (5.93) where we seek the gradient dK/dR. To solve this hard problem, let us ﬁrst write down what we already know: The gradient has the dimensions dK dR ∈ R(N ×N )×(M ×N ) , (5.94) which is a tensor. Moreover, dKpq dR ∈ R 1×M ×N (5.95) for p, q = 1, . . . , N , where Kpq is the (p, q)th entry of K = f (R). De- noting the ith column of R by ri, every entry of K is given by the dot product of two columns of R, i.e., Kpq = r⊤ p rq = M∑ m=1 RmpRmq . (5.96) When we now compute the partial derivative ∂Kpq ∂Rij we obtain ∂Kpq ∂Rij = M∑ m=1 ∂ ∂Rij RmpRmq = ∂pqij , (5.97) c⃝2019 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press. 158 Vector Calculus ∂pqij =    Riq if j = p, p ̸= q Rip if j = q, p ̸= q 2Riq if j = p, p = q 0 otherwise . (5.98) From (5.94), we know that the desired gradient has the dimension (N × N ) × (M × N ), and every single entry of this tensor is given by ∂pqij in (5.98), where p, q, j = 1, . . . , N and i = 1, . . . , M . 5.5 Useful Identities for Computing Gradients In the following, we list some useful gradients that are frequently required in a machine learning context (Petersen and Pedersen, 2012). Here, we use tr(·) as the trace (see Deﬁnition 4.4), det(·) as the determinant (see Section 4.1) and f (X) −1 as the inverse of f (X), assuming it exists. ∂ ∂X f (X) ⊤ = ( ∂f (X) ∂X )⊤ (5.99) ∂ ∂X tr(f (X)) = tr ( ∂f (X) ∂X ) (5.100) ∂ ∂X det(f (X)) = det(f (X))tr ( f (X) −1 ∂f (X) ∂X ) (5.101) ∂ ∂X f (X) −1 = −f (X)−1 ∂f (X) ∂X f (X)−1 (5.102) ∂a ⊤X −1b ∂X = −(X −1)⊤ab⊤(X −1)⊤ (5.103) ∂x ⊤a ∂x = a ⊤ (5.104) ∂a ⊤x ∂x = a ⊤ (5.105) ∂a ⊤Xb ∂X = ab⊤ (5.106) ∂x ⊤Bx ∂x = x⊤(B + B⊤) (5.107) ∂ ∂s (x − As)⊤W (x − As) = −2(x − As) ⊤W A for symmetric W (5.108) Remark. In this book, we only cover traces and transposes of matrices. However, we have seen that derivatives can be higher-dimensional ten- sors, in which case the usual trace and transpose are not deﬁned. In these cases, the trace of a D ×D ×E ×F tensor would be an E ×F -dimensional matrix. This is a special case of a tensor contraction. Similarly, when we Draft (2019-12-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com. 5.6 Backpropagation and Automatic Differentiation 159 “transpose” a tensor, we mean swapping the ﬁrst two dimensions. Specif- ically, in (5.99) through (5.102), we require tensor-related computations when we work with multivariate functions f (·) and compute derivatives with respect to matrices (and choose not to vectorize them as discussed in Section 5.4). ♦ 5.6 Backpropagation and Automatic Differentiation A good discussion about backpropagation and the chain rule is available at a blog by Tim Viera at https://tinyurl. com/ycfm2yrw. In many machine learning applications, we ﬁnd good model parameters by performing gradient descent (Section 7.1), which relies on the fact that we can compute the gradient of a learning objective with respect to the parameters of the model. For a given objective function, we can obtain the gradient with respect to the model parameters using calculus and applying the chain rule; see Section 5.2.2. We already had a taste in Section 5.3 when we looked at the gradient of a squared loss with respect to the parameters of a linear regression model. Consider the function f (x) = √x2 + exp(x2) + cos (x2 + exp(x2) ) . (5.109) By application of the chain rule, and noting that differentiation is linear, we compute the gradient df dx = 2x + 2x exp(x2) 2√x2 + exp(x2) − sin ( x2 + exp(x2)) ( 2x + 2x exp(x2)) = 2x ( 1 2√x2 + exp(x2) − sin ( x2 + exp(x2)) ) ( 1 + exp(x2)) . (5.110) Writing out the gradient in this explicit way is often impractical since it often results in a very lengthy expression for a derivative. In practice, it means that, if we are not careful, the implementation of the gradient could be signiﬁcantly more expensive than computing the function, which imposes unnecessary overhead. For training deep neural network mod- els, the backpropagation algorithm (Kelley, 1960; Bryson, 1961; Dreyfus, backpropagation 1962; Rumelhart et al., 1986) is an efﬁcient way to compute the gradient of an error function with respect to the parameters of the model. 5.6.1 Gradients in a Deep Network An area where the chain rule is used to an extreme is deep learning, where the function value y is computed as a many-level function composition y = (fK ◦ fK−1 ◦ · · · ◦ f1)(x) = fK(fK−1(· · · (f1(x)) · · · )) , (5.111) where x are the inputs (e.g., images), y are the observations (e.g., class labels), and every function fi, i = 1, . . . , K, possesses its own parameters. c⃝2019 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press. 160 Vector Calculus Figure 5.8 Forward pass in a multi-layer neural network to compute the loss L as a function of the inputs x and the parameters Ai, bi. x f K A0, b0 AK−1, bK−1 Lf K−1 AK−2, bK−2 f 1 A1, b1 In neural networks with multiple layers, we have functions fi(xi−1) =We discuss the case, where the activation functions are identical in each layer to unclutter notation. σ(Ai−1xi−1 + bi−1) in the ith layer. Here xi−1 is the output of layer i − 1 and σ an activation function, such as the logistic sigmoid 1 1+e−x , tanh or a rectiﬁed linear unit (ReLU). In order to train these models, we require the gradient of a loss function L with respect to all model parameters Aj, bj for j = 1, . . . , K. This also requires us to compute the gradient of L with respect to the inputs of each layer. For example, if we have inputs x and observations y and a network structure deﬁned by f 0 := x (5.112) f i := σi(Ai−1f i−1 + bi−1) , i = 1, . . . , K , (5.113) see also Figure 5.8 for a visualization, we may be interested in ﬁnding Aj, bj for j = 0, . . . , K − 1, such that the squared loss L(θ) = ∥y − f K(θ, x)∥2 (5.114) is minimized, where θ = {A0, b0, . . . , AK−1, bK−1}. To obtain the gradients with respect to the parameter set θ, we require the partial derivatives of L with respect to the parameters θj = {Aj, bj} of each layer j = 0, . . . , K − 1. The chain rule allows us to determine the partial derivatives asA more in-depth discussion about gradients of neural networks can be found in Justin Domke’s lecture notes https://tinyurl. com/yalcxgtv. ∂L ∂θK−1 = ∂L ∂f K ∂f K ∂θK−1 (5.115) ∂L ∂θK−2 = ∂L ∂f K ∂f K ∂f K−1 ∂f K−1 ∂θK−2 (5.116) ∂L ∂θK−3 = ∂L ∂f K ∂f K ∂f K−1 ∂f K−1 ∂f K−2 ∂f K−2 ∂θK−3 (5.117) ∂L ∂θi = ∂L ∂f K ∂f K ∂f K−1 · · · ∂f i+2 ∂f i+1 ∂f i+1 ∂θi (5.118) The orange terms are partial derivatives of the output of a layer with respect to its inputs, whereas the blue terms are partial derivatives of the output of a layer with respect to its parameters. Assuming, we have already computed the partial derivatives ∂L/∂θi+1, then most of the com- putation can be reused to compute ∂L/∂θi. The additional terms that we Draft (2019-12-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com. 5.6 Backpropagation and Automatic Differentiation 161 Figure 5.9 Backward pass in a multi-layer neural network to compute the gradients of the loss function. x f K A0, b0 AK−1, bK−1 Lf K−1 AK−2, bK−2 f 1 A1, b1 Figure 5.10 Simple graph illustrating the ﬂow of data from x to y via some intermediate variables a, b. x a b y need to compute are indicated by the boxes. Figure 5.9 visualizes that the gradients are passed backward through the network. 5.6.2 Automatic Differentiation It turns out that backpropagation is a special case of a general technique in numerical analysis called automatic differentiation. We can think of au- automatic differentiationtomatic differentation as a set of techniques to numerically (in contrast to symbolically) evaluate the exact (up to machine precision) gradient of a function by working with intermediate variables and applying the chain rule. Automatic differentiation applies a series of elementary arithmetic Automatic differentiation is different from symbolic differentiation and numerical approximations of the gradient, e.g., by using ﬁnite differences. operations, e.g., addition and multiplication and elementary functions, e.g., sin, cos, exp, log. By applying the chain rule to these operations, the gradient of quite complicated functions can be computed automatically. Automatic differentiation applies to general computer programs and has forward and reverse modes. Baydin et al. (2018) give a great overview of automatic differentiation in machine learning. Figure 5.10 shows a simple graph representing the data ﬂow from in- puts x to outputs y via some intermediate variables a, b. If we were to compute the derivative dy/dx, we would apply the chain rule and obtain dy dx = dy db db da da dx . (5.119) Intuitively, the forward and reverse mode differ in the order of multipli- In the general case, we work with Jacobians, which can be vectors, matrices, or tensors. cation. Due to the associativity of matrix multiplication, we can choose between dy dx = ( dy db db da ) da dx , (5.120) dy dx = dy db ( db da da dx ) . (5.121) Equation (5.120) would be the reverse mode because gradients are prop- reverse mode agated backward through the graph, i.e., reverse to the data ﬂow. Equa- tion (5.121) would be the forward mode, where the gradients ﬂow with forward mode the data from left to right through the graph. c⃝2019 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press. 162 Vector Calculus In the following, we will focus on reverse mode automatic differentia- tion, which is backpropagation. In the context of neural networks, where the input dimensionality is often much higher than the dimensionality of the labels, the reverse mode is computationally signiﬁcantly cheaper than the forward mode. Let us start with an instructive example. Example 5.14 Consider the function f (x) = √x2 + exp(x2) + cos ( x2 + exp(x2) ) (5.122) from (5.109). If we were to implement a function f on a computer, we would be able to save some computation by using intermediate variables:intermediate variables a = x2 , (5.123) b = exp(a) , (5.124) c = a + b , (5.125) d = √c , (5.126) e = cos(c) , (5.127) f = d + e . (5.128) Figure 5.11 Computation graph with inputs x, function values f , and intermediate variables a, b, c, d, e. x (·) 2 a exp(·) b + c √· cos(·) d e + f This is the same kind of thinking process that occurs when applying the chain rule. Note that the preceding set of equations requires fewer operations than a direct implementation of the function f (x) as deﬁned in (5.109). The corresponding computation graph in Figure 5.11 shows the ﬂow of data and computations required to obtain the function value f . The set of equations that include intermediate variables can be thought of as a computation graph, a representation that is widely used in imple- mentations of neural network software libraries. We can directly compute the derivatives of the intermediate variables with respect to their corre- sponding inputs by recalling the deﬁnition of the derivative of elementary functions. We obtain the following: ∂a ∂x = 2x (5.129) ∂b ∂a = exp(a) (5.130) Draft (2019-12-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com. 5.6 Backpropagation and Automatic Differentiation 163 ∂c ∂a = 1 = ∂c ∂b (5.131) ∂d ∂c = 1 2√c (5.132) ∂e ∂c = − sin(c) (5.133) ∂f ∂d = 1 = ∂f ∂e . (5.134) By looking at the computation graph in Figure 5.11, we can compute ∂f /∂x by working backward from the output and obtain ∂f ∂c = ∂f ∂d ∂d ∂c + ∂f ∂e ∂e ∂c (5.135) ∂f ∂b = ∂f ∂c ∂c ∂b (5.136) ∂f ∂a = ∂f ∂b ∂b ∂a + ∂f ∂c ∂c ∂a (5.137) ∂f ∂x = ∂f ∂a ∂a ∂x . (5.138) Note that we implicitly applied the chain rule to obtain ∂f /∂x. By substi- tuting the results of the derivatives of the elementary functions, we get ∂f ∂c = 1 · 1 2√c + 1 · (− sin(c)) (5.139) ∂f ∂b = ∂f ∂c · 1 (5.140) ∂f ∂a = ∂f ∂b exp(a) + ∂f ∂c · 1 (5.141) ∂f ∂x = ∂f ∂a · 2x . (5.142) By thinking of each of the derivatives above as a variable, we observe that the computation required for calculating the derivative is of similar complexity as the computation of the function itself. This is quite counter- intuitive since the mathematical expression for the derivative ∂f ∂x (5.110) is signiﬁcantly more complicated than the mathematical expression of the function f (x) in (5.109). Automatic differentiation is a formalization of Example 5.14. Let x1, . . . , xd be the input variables to the function, xd+1, . . . , xD−1 be the intermediate variables, and xD the output variable. Then the computation graph can be expressed as follows: For i = d + 1, . . . , D : xi = gi(xPa(xi)) , (5.143) c⃝2019 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press. 164 Vector Calculus where the gi(·) are elementary functions and xPa(xi) are the parent nodes of the variable xi in the graph. Given a function deﬁned in this way, we can use the chain rule to compute the derivative of the function in a step- by-step fashion. Recall that by deﬁnition f = xD and hence ∂f ∂xD = 1 . (5.144) For other variables xi, we apply the chain rule ∂f ∂xi = ∑ xj :xi∈Pa(xj ) ∂f ∂xj ∂xj ∂xi = ∑ xj :xi∈Pa(xj ) ∂f ∂xj ∂gj ∂xi , (5.145) where Pa(xj) is the set of parent nodes of xj in the computation graph. Equation (5.143) is the forward propagation of a function, whereas (5.145)Auto-differentiation in reverse mode requires a parse tree. is the backpropagation of the gradient through the computation graph. For neural network training, we backpropagate the error of the prediction with respect to the label. The automatic differentiation approach above works whenever we have a function that can be expressed as a computation graph, where the ele- mentary functions are differentiable. In fact, the function may not even be a mathematical function but a computer program. However, not all com- puter programs can be automatically differentiated, e.g., if we cannot ﬁnd differential elementary functions. Programming structures, such as for loops and if statements, require more care as well. 5.7 Higher-Order Derivatives So far, we have discussed gradients, i.e., ﬁrst-order derivatives. Some- times, we are interested in derivatives of higher order, e.g., when we want to use Newton’s Method for optimization, which requires second-order derivatives (Nocedal and Wright, 2006). In Section 5.1.1, we discussed the Taylor series to approximate functions using polynomials. In the mul- tivariate case, we can do exactly the same. In the following, we will do exactly this. But let us start with some notation. Consider a function f : R2 → R of two variables x, y. We use the following notation for higher-order partial derivatives (and for gradients): ∂2f ∂x2 is the second partial derivative of f with respect to x. ∂nf ∂xn is the nth partial derivative of f with respect to x. ∂2f ∂y∂x = ∂ ∂y ( ∂f ∂x ) is the partial derivative obtained by ﬁrst partial differ- entiating with respect to x and then with respect to y. ∂2f ∂x∂y is the partial derivative obtained by ﬁrst partial differentiating by y and then x. The Hessian is the collection of all second-order partial derivatives.Hessian Draft (2019-12-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com. 5.8 Linearization and Multivariate Taylor Series 165 Figure 5.12 Linear approximation of a function. The original function f is linearized at x0 = −2 using a ﬁrst-order Taylor series expansion. −4 −2 0 2 4 x −2 −1 0 1f(x) f (x) f (x0) f (x0) + f ′(x0)(x − x0) If f (x, y) is a twice (continuously) differentiable function, then ∂2f ∂x∂y = ∂2f ∂y∂x , (5.146) i.e., the order of differentiation does not matter, and the corresponding Hessian matrix Hessian matrix H =     ∂2f ∂x2 ∂2f ∂x∂y ∂2f ∂x∂y ∂2f ∂y2     (5.147) is symmetric. The Hessian is denoted as ∇2 x,yf (x, y). Generally, for x ∈ Rn and f : Rn → R, the Hessian is an n × n matrix. The Hessian measures the curvature of the function locally around (x, y). Remark (Hessian of a Vector Field). If f : Rn → R m is a vector ﬁeld, the Hessian is an (m × n × n)-tensor. ♦ 5.8 Linearization and Multivariate Taylor Series The gradient ∇f of a function f is often used for a locally linear approxi- mation of f around x0: f (x) ≈ f (x0) + (∇xf )(x0)(x − x0) . (5.148) Here (∇xf )(x0) is the gradient of f with respect to x, evaluated at x0. Figure 5.12 illustrates the linear approximation of a function f at an input x0. The original function is approximated by a straight line. This approx- imation is locally accurate, but the farther we move away from x0 the worse the approximation gets. Equation (5.148) is a special case of a mul- tivariate Taylor series expansion of f at x0, where we consider only the ﬁrst two terms. We discuss the more general case in the following, which will allow for better approximations. c⃝2019 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press. 166 Vector Calculus Figure 5.13 Visualizing outer products. Outer products of vectors increase the dimensionality of the array by 1 per term. (a) The outer product of two vectors results in a matrix; (b) the outer product of three vectors yields a third-order tensor. (a) Given a vector δ ∈ R4, we obtain the outer product δ2 := δ ⊗ δ = δδ⊤ ∈ R4×4 as a matrix. (b) An outer product δ3 := δ ⊗ δ ⊗ δ ∈ R4×4×4 results in a third-order tensor (“three- dimensional matrix”), i.e., an array with three indexes. Deﬁnition 5.7 (Multivariate Taylor Series). We consider a function f : R D → R (5.149) x ↦→ f (x) , x ∈ R D , (5.150) that is smooth at x0. When we deﬁne the difference vector δ := x − x0, the multivariate Taylor series of f at (x0) is deﬁned asmultivariate Taylor series f (x) = ∞∑ k=0 Dk xf (x0) k! δk , (5.151) where Dk xf (x0) is the k-th (total) derivative of f with respect to x, eval- uated at x0. Deﬁnition 5.8 (Taylor Polynomial). The Taylor polynomial of degree n ofTaylor polynomial f at x0 contains the ﬁrst n + 1 components of the series in (5.151) and is deﬁned as Tn(x) = n∑ k=0 Dk xf (x0) k! δk . (5.152) In (5.151) and (5.152), we used the slightly sloppy notation of δk, which is not deﬁned for vectors x ∈ R D, D > 1, and k > 1. Note that both Dk xf and δk are k-th order tensors, i.e., k-dimensional arrays. TheA vector can be implemented as a one-dimensional array, a matrix as a two-dimensional array. kth-order tensor δk ∈ R k times ︷ ︸︸ ︷ D×D×...×D is obtained as a k-fold outer product, denoted by ⊗, of the vector δ ∈ R D. For example, δ2 := δ ⊗ δ = δδ⊤ , δ2[i, j] = δ[i]δ[j] (5.153) Draft (2019-12-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com. 5.8 Linearization and Multivariate Taylor Series 167 δ3 := δ ⊗ δ ⊗ δ , δ3[i, j, k] = δ[i]δ[j]δ[k] . (5.154) Figure 5.13 visualizes two such outer products. In general, we obtain the terms Dk xf (x0)δk = D∑ i1=1 · · · D∑ ik=1 Dk xf (x0)[i1, . . . , ik]δ[i1] · · · δ[ik] (5.155) in the Taylor series, where Dk xf (x0)δk contains k-th order polynomials. Now that we deﬁned the Taylor series for vector ﬁelds, let us explicitly write down the ﬁrst terms Dk xf (x0)δk of the Taylor series expansion for k = 0, . . . , 3 and δ := x − x0: np.einsum( ’i,i’,Df1,d) np.einsum( ’ij,i,j’, Df2,d,d) np.einsum( ’ijk,i,j,k’, Df3,d,d,d) k = 0 : D0 xf (x0)δ0 = f (x0) ∈ R (5.156) k = 1 : D1 xf (x0)δ1 = ∇xf (x0) ︸ ︷︷ ︸ 1×D δ︸︷︷︸ D×1 = D∑ i=1 ∇xf (x0)[i]δ[i] ∈ R (5.157) k = 2 : D2 xf (x0)δ2 = tr( H(x0) ︸ ︷︷ ︸ D×D δ︸︷︷︸ D×1 δ⊤ ︸︷︷︸ 1×D ) = δ⊤H(x0)δ (5.158) = D∑ i=1 D∑ j=1 H[i, j]δ[i]δ[j] ∈ R (5.159) k = 3 : D3 xf (x0)δ3 = D∑ i=1 D∑ j=1 D∑ k=1 D3 xf (x0)[i, j, k]δ[i]δ[j]δ[k] ∈ R (5.160) Here, H(x0) is the Hessian of f evaluated at x0. Example 5.15 (Taylor Series Expansion of a Function with Two Vari- ables) Consider the function f (x, y) = x2 + 2xy + y3 . (5.161) We want to compute the Taylor series expansion of f at (x0, y0) = (1, 2). Before we start, let us discuss what to expect: The function in (5.161) is a polynomial of degree 3. We are looking for a Taylor series expansion, which itself is a linear combination of polynomials. Therefore, we do not expect the Taylor series expansion to contain terms of fourth or higher order to express a third-order polynomial. This means that it should be sufﬁcient to determine the ﬁrst four terms of (5.151) for an exact alterna- tive representation of (5.161). To determine the Taylor series expansion, we start with the constant term and the ﬁrst-order derivatives, which are given by f (1, 2) = 13 (5.162) c⃝2019 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press. 168 Vector Calculus ∂f ∂x = 2x + 2y =⇒ ∂f ∂x (1, 2) = 6 (5.163) ∂f ∂y = 2x + 3y2 =⇒ ∂f ∂y (1, 2) = 14 . (5.164) Therefore, we obtain D1 x,yf (1, 2) = ∇x,yf (1, 2) = [ ∂f ∂x (1, 2) ∂f ∂y (1, 2) ] = [ 6 14] ∈ R 1×2 (5.165) such that D1 x,yf (1, 2) 1! δ = [ 6 14] [ x − 1 y − 2 ] = 6(x − 1) + 14(y − 2) . (5.166) Note that D1 x,yf (1, 2)δ contains only linear terms, i.e., ﬁrst-order polyno- mials. The second-order partial derivatives are given by ∂2f ∂x2 = 2 =⇒ ∂2f ∂x2 (1, 2) = 2 (5.167) ∂2f ∂y2 = 6y =⇒ ∂2f ∂y2 (1, 2) = 12 (5.168) ∂2f ∂y∂x = 2 =⇒ ∂2f ∂y∂x (1, 2) = 2 (5.169) ∂2f ∂x∂y = 2 =⇒ ∂2f ∂x∂y (1, 2) = 2 . (5.170) When we collect the second-order partial derivatives, we obtain the Hes- sian H = [ ∂2f ∂x2 ∂2f ∂x∂y ∂2f ∂y∂x ∂2f ∂y2 ] = [ 2 2 2 6y ] , (5.171) such that H(1, 2) = [ 2 2 2 12 ] ∈ R2×2 . (5.172) Therefore, the next term of the Taylor-series expansion is given by D2 x,yf (1, 2) 2! δ2 = 1 2 δ⊤H(1, 2)δ (5.173a) = 1 2 [ x − 1 y − 2] [ 2 2 2 12 ] [ x − 1 y − 2 ] (5.173b) = (x − 1) 2 + 2(x − 1)(y − 2) + 6(y − 2) 2 . (5.173c) Here, D2 x,yf (1, 2)δ2 contains only quadratic terms, i.e., second-order poly- nomials. Draft (2019-12-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com. 5.8 Linearization and Multivariate Taylor Series 169 The third-order derivatives are obtained as D3 x,yf = [ ∂H ∂x ∂H ∂y ] ∈ R2×2×2 , (5.174) D3 x,yf [:, :, 1] = ∂H ∂x = [ ∂3f ∂x3 ∂3f ∂x2∂y ∂3f ∂x∂y∂x ∂3f ∂x∂y2 ] , (5.175) D3 x,yf [:, :, 2] = ∂H ∂y = [ ∂3f ∂y∂x2 ∂3f ∂y∂x∂y ∂3f ∂y2∂x ∂3f ∂y3 ] . (5.176) Since most second-order partial derivatives in the Hessian in (5.171) are constant, the only nonzero third-order partial derivative is ∂3f ∂y3 = 6 =⇒ ∂3f ∂y3 (1, 2) = 6 . (5.177) Higher-order derivatives and the mixed derivatives of degree 3 (e.g., ∂f 3 ∂x2∂y ) vanish, such that D3 x,yf [:, :, 1] = [ 0 0 0 0 ] , D3 x,yf [:, :, 2] = [ 0 0 0 6 ] (5.178) and D3 x,yf (1, 2) 3! δ3 = (y − 2) 3 , (5.179) which collects all cubic terms of the Taylor series. Overall, the (exact) Taylor series expansion of f at (x0, y0) = (1, 2) is f (x) = f (1, 2) + D1 x,yf (1, 2)δ + D2 x,yf (1, 2) 2! δ2 + D3 x,yf (1, 2) 3! δ3 (5.180a) = f (1, 2) + ∂f (1, 2) ∂x (x − 1) + ∂f (1, 2) ∂y (y − 2) + 1 2! ( ∂2f (1, 2) ∂x2 (x − 1) 2 + ∂2f (1, 2) ∂y2 (y − 2) 2 + 2 ∂2f (1, 2) ∂x∂y (x − 1)(y − 2) ) + 1 6 ∂3f (1, 2) ∂y3 (y − 2) 3 (5.180b) = 13 + 6(x − 1) + 14(y − 2) + (x − 1) 2 + 6(y − 2) 2 + 2(x − 1)(y − 2) + (y − 2) 3 . (5.180c) In this case, we obtained an exact Taylor series expansion of the polyno- mial in (5.161), i.e., the polynomial in (5.180c) is identical to the original polynomial in (5.161). In this particular example, this result is not sur- prising since the original function was a third-order polynomial, which we expressed through a linear combination of constant terms, ﬁrst-order, second-order, and third-order polynomials in (5.180c). c⃝2019 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press. 170 Vector Calculus 5.9 Further Reading Further details of matrix differentials, along with a short review of the required linear algebra, can be found in Magnus and Neudecker (2007). Automatic differentiation has had a long history, and we refer to Griewank and Walther (2003), Griewank and Walther (2008), and Elliott (2009) and the references therein. In machine learning (and other disciplines), we often need to compute expectations, i.e., we need to solve integrals of the form Ex[f (x)] = ∫ f (x)p(x)dx . (5.181) Even if p(x) is in a convenient form (e.g., Gaussian), this integral gen- erally cannot be solved analytically. The Taylor series expansion of f is one way of ﬁnding an approximate solution: Assuming p(x) = N (µ, Σ ) is Gaussian, then the ﬁrst-order Taylor series expansion around µ locally linearizes the nonlinear function f . For linear functions, we can compute the mean (and the covariance) exactly if p(x) is Gaussian distributed (see Section 6.5). This property is heavily exploited by the extended Kalmanextended Kalman ﬁlter ﬁlter (Maybeck, 1979) for online state estimation in nonlinear dynami- cal systems (also called “state-space models”). Other deterministic ways to approximate the integral in (5.181) are the unscented transform (Julierunscented transform and Uhlmann, 1997), which does not require any gradients, or the LaplaceLaplace approximation approximation (MacKay, 2003; Bishop, 2006; Murphy, 2012), which uses a second-order Taylor series expansion (requiring the Hessian) for a local Gaussian approximation of p(x) around its mode. Exercises 5.1 Compute the derivative f ′(x) for f (x) = log(x 4) sin(x3) . 5.2 Compute the derivative f ′(x) of the logistic sigmoid f (x) = 1 1 + exp(−x) . 5.3 Compute the derivative f ′(x) of the function f (x) = exp(− 1 2σ2 (x − µ)2) , where µ, σ ∈ R are constants. 5.4 Compute the Taylor polynomials Tn, n = 0, . . . , 5 of f (x) = sin(x) + cos(x) at x0 = 0. 5.5 Consider the following functions: f1(x) = sin(x1) cos(x2) , x ∈ R2 f2(x, y) = x⊤y , x, y ∈ Rn f3(x) = xx⊤ , x ∈ Rn Draft (2019-12-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com. Exercises 171 a. What are the dimensions of ∂fi ∂x ? b. Compute the Jacobians. 5.6 Differentiate f with respect to t and g with respect to X, where f (t) = sin(log(t ⊤t)) , t ∈ RD g(X) = tr(AXB) , A ∈ RD×E, X ∈ RE×F , B ∈ RF ×D , where tr denotes the trace. 5.7 Compute the derivatives df /dx of the following functions by using the chain rule. Provide the dimensions of every single partial derivative. Describe your steps in detail. a. f (z) = log(1 + z) , z = x⊤x , x ∈ RD b. f (z) = sin(z) , z = Ax + b , A ∈ RE×D, x ∈ RD, b ∈ RE where sin(·) is applied to every element of z. 5.8 Compute the derivatives df /dx of the following functions. Describe your steps in detail. a. Use the chain rule. Provide the dimensions of every single partial deriva- tive. f (z) = exp(− 1 2 z) z = g(y) = y⊤S−1y y = h(x) = x − µ where x, µ ∈ RD, S ∈ RD×D. b. f (x) = tr(xx⊤ + σ2I) , x ∈ RD Here tr(A) is the trace of A, i.e., the sum of the diagonal elements Aii. Hint: Explicitly write out the outer product. c. Use the chain rule. Provide the dimensions of every single partial deriva- tive. You do not need to compute the product of the partial derivatives explicitly. f = tanh(z) ∈ RM z = Ax + b, x ∈ RN , A ∈ RM ×N , b ∈ RM . Here, tanh is applied to every component of z. 5.9 We deﬁne g(z, ν) := log p(x, z) − log q(z, ν) z := t(ϵ, ν) for differentiable functions p, q, t. By using the chain rule, compute the gra- dient d dν g(z, ν) . c⃝2019 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press. 6 Probability and Distributions Probability, loosely speaking, concerns the study of uncertainty. Probabil- ity can be thought of as the fraction of times an event occurs, or as a degree of belief about an event. We then would like to use this probability to mea- sure the chance of something occurring in an experiment. As mentioned in Chapter 1, we often quantify uncertainty in the data, uncertainty in the machine learning model, and uncertainty in the predictions produced by the model. Quantifying uncertainty requires the idea of a random variable,random variable which is a function that maps outcomes of random experiments to a set of properties that we are interested in. Associated with the random variable is a function that measures the probability that a particular outcome (or set of outcomes) will occur; this is called the probability distribution.probability distribution Probability distributions are used as a building block for other con- cepts, such as probabilistic modeling (Section 8.4), graphical models (Sec- tion 8.5), and model selection (Section 8.6). In the next section, we present the three concepts that deﬁne a probability space (the sample space, the events, and the probability of an event) and how they are related to a fourth concept called the random variable. The presentation is deliber- ately slightly hand wavy since a rigorous presentation may occlude the intuition behind the concepts. An outline of the concepts presented in this chapter are shown in Figure 6.1. 6.1 Construction of a Probability Space The theory of probability aims at deﬁning a mathematical structure to describe random outcomes of experiments. For example, when tossing a single coin, we cannot determine the outcome, but by doing a large num- ber of coin tosses, we can observe a regularity in the average outcome. Using this mathematical structure of probability, the goal is to perform automated reasoning, and in this sense, probability generalizes logical reasoning (Jaynes, 2003). 6.1.1 Philosophical Issues When constructing automated reasoning systems, classical Boolean logic does not allow us to express certain forms of plausible reasoning. Consider 172 This material will be published by Cambridge University Press as Mathematics for Machine Learn- ing by Marc Peter Deisenroth, A. Aldo Faisal, and Cheng Soon Ong. This pre-publication version is free to view and download for personal use only. Not for re-distribution, re-sale or use in deriva- tive works. c⃝by M. P. Deisenroth, A. A. Faisal, and C. S. Ong, 2019. https://mml-book.com. 6.1 Construction of a Probability Space 173 Figure 6.1 A mind map of the concepts related to random variables and probability distributions, as described in this chapter. Random variable & distribution Sum ruleProduct rule Bayes’ Theorem Summary statistics Mean Variance Transformations Independence Inner product Gaussian Bernoulli Beta Sufﬁcient statistics Exponential family Chapter 9 Regression Chapter 10 Dimensionality reduction Chapter 11 Density estimation PropertySimilarity Example ExampleConjugatePropertyFinite the following scenario: We observe that A is false. We ﬁnd B becomes less plausible, although no conclusion can be drawn from classical logic. We observe that B is true. It seems A becomes more plausible. We use this form of reasoning daily. We are waiting for a friend, and consider three possibilities: H1, she is on time; H2, she has been delayed by trafﬁc; and H3, she has been abducted by aliens. When we observe our friend is late, we must logically rule out H1. We also tend to consider H2 to be more likely, though we are not logically required to do so. Finally, we may consider H3 to be possible, but we continue to consider it quite unlikely. How do we conclude H2 is the most plausible answer? Seen in this way, “For plausible reasoning it is necessary to extend the discrete true and false values of truth to continuous plausibilities” (Jaynes, 2003). probability theory can be considered a generalization of Boolean logic. In the context of machine learning, it is often applied in this way to formalize the design of automated reasoning systems. Further arguments about how probability theory is the foundation of reasoning systems can be found in Pearl (1988). The philosophical basis of probability and how it should be somehow related to what we think should be true (in the logical sense) was studied by Cox (Jaynes, 2003). Another way to think about it is that if we are precise about our common sense we end up constructing probabilities. E. T. Jaynes (1922–1998) identiﬁed three mathematical criteria, which must apply to all plausibilities: 1. The degrees of plausibility are represented by real numbers. 2. These numbers must be based on the rules of common sense. c⃝2019 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press. 174 Probability and Distributions 3. The resulting reasoning must be consistent, with the three following meanings of the word “consistent”: (a) Consistency or non-contradiction: When the same result can be reached through different means, the same plausibility value must be found in all cases. (b) Honesty: All available data must be taken into account. (c) Reproducibility: If our state of knowledge about two problems are the same, then we must assign the same degree of plausibility to both of them. The Cox–Jaynes theorem proves these plausibilities to be sufﬁcient to deﬁne the universal mathematical rules that apply to plausibility p, up to transformation by an arbitrary monotonic function. Crucially, these rules are the rules of probability. Remark. In machine learning and statistics, there are two major interpre- tations of probability: the Bayesian and frequentist interpretations (Bishop, 2006; Efron and Hastie, 2016). The Bayesian interpretation uses probabil- ity to specify the degree of uncertainty that the user has about an event. It is sometimes referred to as “subjective probability” or “degree of belief”. The frequentist interpretation considers the relative frequencies of events of interest to the total number of events that occurred. The probability of an event is deﬁned as the relative frequency of the event in the limit when one has inﬁnite data. ♦ Some machine learning texts on probabilistic models use lazy notation and jargon, which is confusing. This text is no exception. Multiple distinct concepts are all referred to as “probability distribution”, and the reader has to often disentangle the meaning from the context. One trick to help make sense of probability distributions is to check whether we are trying to model something categorical (a discrete random variable) or some- thing continuous (a continuous random variable). The kinds of questions we tackle in machine learning are closely related to whether we are con- sidering categorical or continuous models. 6.1.2 Probability and Random Variables There are three distinct ideas that are often confused when discussing probabilities. First is the idea of a probability space, which allows us to quantify the idea of a probability. However, we mostly do not work directly with this basic probability space. Instead, we work with random variables (the second idea), which transfers the probability to a more convenient (often numerical) space. The third idea is the idea of a distribution or law associated with a random variable. We will introduce the ﬁrst two ideas in this section and expand on the third idea in Section 6.2. Modern probability is based on a set of axioms proposed by Kolmogorov Draft (2019-12-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com. 6.1 Construction of a Probability Space 175 (Grinstead and Snell, 1997; Jaynes, 2003) that introduce the three con- cepts of sample space, event space, and probability measure. The prob- ability space models a real-world process (referred to as an experiment) with random outcomes. The sample space Ω The sample space is the set of all possible outcomes of the experiment, sample space usually denoted by Ω. For example, two successive coin tosses have a sample space of {hh, tt, ht, th}, where “h” denotes “heads” and “t” denotes “tails”. The event space A The event space is the space of potential results of the experiment. A event space subset A of the sample space Ω is in the event space A if at the end of the experiment we can observe whether a particular outcome ω ∈ Ω is in A. The event space A is obtained by considering the collection of subsets of Ω, and for discrete probability distributions (Section 6.2.1) A is often the power set of Ω. The probability P With each event A ∈ A, we associate a number P (A) that measures the probability or degree of belief that the event will occur. P (A) is called the probability of A. probability The probability of a single event must lie in the interval [0, 1], and the total probability over all outcomes in the sample space Ω must be 1, i.e., P (Ω) = 1. Given a probability space (Ω, A, P ), we want to use it to model some real-world phenomenon. In machine learning, we often avoid explic- itly referring to the probability space, but instead refer to probabilities on quantities of interest, which we denote by T . In this book, we refer to T as the target space and refer to elements of T as states. We introduce a target space function X : Ω → T that takes an element of Ω (an outcome) and returns a particular quantity of interest x, a value in T . This association/mapping from Ω to T is called a random variable. For example, in the case of tossing random variable two coins and counting the number of heads, a random variable X maps to the three possible outcomes: X(hh) = 2, X(ht) = 1, X(th) = 1, and X(tt) = 0. In this particular case, T = {0, 1, 2}, and it is the probabilities on elements of T that we are interested in. For a ﬁnite sample space Ω and The name “random variable” is a great source of misunderstanding as it is neither random nor is it a variable. It is a function. ﬁnite T , the function corresponding to a random variable is essentially a lookup table. For any subset S ⊆ T , we associate PX(S) ∈ [0, 1] (the probability) to a particular event occurring corresponding to the random variable X. Example 6.1 provides a concrete illustration of the terminol- ogy. Remark. The aforementioned sample space Ω unfortunately is referred to by different names in different books. Another common name for Ω is “state space” (Jacod and Protter, 2004), but state space is sometimes reserved for referring to states in a dynamical system (Hasselblatt and c⃝2019 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press. 176 Probability and Distributions Katok, 2003). Other names sometimes used to describe Ω are: “sample description space”, “possibility space,” and “event space”. ♦ Example 6.1 We assume that the reader is already familiar with computing probabil-This toy example is essentially a biased coin ﬂip example. ities of intersections and unions of sets of events. A gentler introduction to probability with many examples can be found in chapter 2 of Walpole et al. (2011). Consider a statistical experiment where we model a funfair game con- sisting of drawing two coins from a bag (with replacement). There are coins from USA (denoted as $) and UK (denoted as £) in the bag, and since we draw two coins from the bag, there are four outcomes in total. The state space or sample space Ω of this experiment is then ($, $), ($, £), (£, $), (£, £). Let us assume that the composition of the bag of coins is such that a draw returns at random a $ with probability 0.3. The event we are interested in is the total number of times the repeated draw returns $. Let us deﬁne a random variable X that maps the sample space Ω to T , which denotes the number of times we draw $ out of the bag. We can see from the preceding sample space we can get zero $, one $, or two $s, and therefore T = {0, 1, 2}. The random variable X (a function or lookup table) can be represented as a table like the following: X(($, $)) = 2 (6.1) X(($, £)) = 1 (6.2) X((£, $)) = 1 (6.3) X((£, £)) = 0 . (6.4) Since we return the ﬁrst coin we draw before drawing the second, this implies that the two draws are independent of each other, which we will discuss in Section 6.4.5. Note that there are two experimental outcomes, which map to the same event, where only one of the draws returns $. Therefore, the probability mass function (Section 6.2.1) of X is given by P (X = 2) = P (($, $)) = P ($) · P ($) = 0.3 · 0.3 = 0.09 (6.5) P (X = 1) = P (($, £) ∪ (£, $)) = P (($, £)) + P ((£, $)) = 0.3 · (1 − 0.3) + (1 − 0.3) · 0.3 = 0.42 (6.6) P (X = 0) = P ((£, £)) = P (£) · P (£) = (1 − 0.3) · (1 − 0.3) = 0.49 . (6.7) Draft (2019-12-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com. 6.1 Construction of a Probability Space 177 In the calculation, we equated two different concepts, the probability of the output of X and the probability of the samples in Ω. For example, in (6.7) we say P (X = 0) = P ((£, £)). Consider the random variable X : Ω → T and a subset S ⊆ T (for example, a single element of T , such as the outcome that one head is obtained when tossing two coins). Let X −1(S) be the pre-image of S by X, i.e., the set of elements of Ω that map to S under X; {ω ∈ Ω : X(ω) ∈ S}. One way to understand the transformation of probability from events in Ω via the random variable X is to associate it with the probability of the pre-image of S (Jacod and Protter, 2004). For S ⊆ T , we have the notation PX(S) = P (X ∈ S) = P (X −1(S)) = P ({ω ∈ Ω : X(ω) ∈ S}) . (6.8) The left-hand side of (6.8) is the probability of the set of possible outcomes (e.g., number of $ = 1) that we are interested in. Via the random variable X, which maps states to outcomes, we see in the right-hand side of (6.8) that this is the probability of the set of states (in Ω) that have the property (e.g., $£, £$). We say that a random variable X is distributed according to a particular probability distribution PX, which deﬁnes the probability mapping between the event and the probability of the outcome of the random variable. In other words, the function PX or equivalently P ◦ X −1 is the law or distribution of random variable X. law distribution Remark. The target space, that is, the range T of the random variable X, is used to indicate the kind of probability space, i.e., a T random variable. When T is ﬁnite or countably inﬁnite, this is called a discrete random variable (Section 6.2.1). For continuous random variables (Section 6.2.2), we only consider T = R or T = R D. ♦ 6.1.3 Statistics Probability theory and statistics are often presented together, but they con- cern different aspects of uncertainty. One way of contrasting them is by the kinds of problems that are considered. Using probability, we can consider a model of some process, where the underlying uncertainty is captured by random variables, and we use the rules of probability to derive what happens. In statistics, we observe that something has happened and try to ﬁgure out the underlying process that explains the observations. In this sense, machine learning is close to statistics in its goals to construct a model that adequately represents the process that generated the data. We can use the rules of probability to obtain a “best-ﬁtting” model for some data. Another aspect of machine learning systems is that we are interested in generalization error (see Chapter 8). This means that we are actually interested in the performance of our system on instances that we will observe in future, which are not identical to the instances that we have c⃝2019 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press. 178 Probability and Distributions seen so far. This analysis of future performance relies on probability and statistics, most of which is beyond what will be presented in this chapter. The interested reader is encouraged to look at the books by Boucheron et al. (2013) and Shalev-Shwartz and Ben-David (2014). We will see more about statistics in Chapter 8. 6.2 Discrete and Continuous Probabilities Let us focus our attention on ways to describe the probability of an event as introduced in Section 6.1. Depending on whether the target space is dis- crete or continuous, the natural way to refer to distributions is different. When the target space T is discrete, we can specify the probability that a random variable X takes a particular value x ∈ T , denoted as P (X = x). The expression P (X = x) for a discrete random variable X is known as the probability mass function. When the target space T is continuous, e.g.,probability mass function the real line R, it is more natural to specify the probability that a random variable X is in an interval, denoted by P (a ⩽ X ⩽ b) for a < b. By con- vention, we specify the probability that a random variable X is less than a particular value x, denoted by P (X ⩽ x). The expression P (X ⩽ x) for a continuous random variable X is known as the cumulative distributioncumulative distribution function function. We will discuss continuous random variables in Section 6.2.2. We will revisit the nomenclature and contrast discrete and continuous random variables in Section 6.2.3. Remark. We will use the phrase univariate distribution to refer to distribu-univariate tions of a single random variable (whose states are denoted by non-bold x). We will refer to distributions of more than one random variable as multivariate distributions, and will usually consider a vector of randommultivariate variables (whose states are denoted by bold x). ♦ 6.2.1 Discrete Probabilities When the target space is discrete, we can imagine the probability distri- bution of multiple random variables as ﬁlling out a (multidimensional) array of numbers. Figure 6.2 shows an example. The target space of the joint probability is the Cartesian product of the target spaces of each of the random variables. We deﬁne the joint probability as the entry of bothjoint probability values jointly P (X = xi, Y = yj) = nij N , (6.9) where nij is the number of events with state xi and yj and N the total number of events. The joint probability is the probability of the intersec- tion of both events, that is, P (X = xi, Y = yj) = P (X = xi ∩ Y = yj). Figure 6.2 illustrates the probability mass function (pmf) of a discrete prob-probability mass function ability distribution. For two random variables X and Y , the probability Draft (2019-12-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com. 6.2 Discrete and Continuous Probabilities 179 Figure 6.2 Visualization of a discrete bivariate probability mass function, with random variables X and Y . This diagram is adapted from Bishop (2006). X x1 x2 x3 x4 x5 Y y3 y2 y1 nij }rj ci︷ ︸︸︷ that X = x and Y = y is (lazily) written as p(x, y) and is called the joint probability. One can think of a probability as a function that takes state x and y and returns a real number, which is the reason we write p(x, y). The marginal probability that X takes the value x irrespective of the value marginal probability of random variable Y is (lazily) written as p(x). We write X ∼ p(x) to denote that the random variable X is distributed according to p(x). If we consider only the instances where X = x, then the fraction of instances (the conditional probability) for which Y = y is written (lazily) as p(y | x). conditional probability Example 6.2 Consider two random variables X and Y , where X has ﬁve possible states and Y has three possible states, as shown in Figure 6.2. We denote by nij the number of events with state X = xi and Y = yj, and denote by N the total number of events. The value ci is the sum of the individual frequencies for the ith column, that is, ci = ∑3 j=1 nij. Similarly, the value rj is the row sum, that is, rj = ∑5 i=1 nij. Using these deﬁnitions, we can compactly express the distribution of X and Y . The probability distribution of each random variable, the marginal probability, can be seen as the sum over a row or column P (X = xi) = ci N = ∑3 j=1 nij N (6.10) and P (Y = yj) = rj N = ∑5 i=1 nij N , (6.11) where ci and rj are the ith column and jth row of the probability table, respectively. By convention, for discrete random variables with a ﬁnite number of events, we assume that probabilties sum up to one, that is, 5∑ i=1 P (X = xi) = 1 and 3∑ j=1 P (Y = yj) = 1 . (6.12) The conditional probability is the fraction of a row or column in a par- c⃝2019 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press. 180 Probability and Distributions ticular cell. For example, the conditional probability of Y given X is P (Y = yj | X = xi) = nij ci , (6.13) and the conditional probability of X given Y is P (X = xi | Y = yj) = nij rj . (6.14) In machine learning, we use discrete probability distributions to model categorical variables, i.e., variables that take a ﬁnite set of unordered val-categorical variable ues. They could be categorical features, such as the degree taken at uni- versity when used for predicting the salary of a person, or categorical la- bels, such as letters of the alphabet when doing handwriting recognition. Discrete distributions are also often used to construct probabilistic models that combine a ﬁnite number of continuous distributions (Chapter 11). 6.2.2 Continuous Probabilities We consider real-valued random variables in this section, i.e., we consider target spaces that are intervals of the real line R. In this book, we pretend that we can perform operations on real random variables as if we have dis- crete probability spaces with ﬁnite states. However, this simpliﬁcation is not precise for two situations: when we repeat something inﬁnitely often, and when we want to draw a point from an interval. The ﬁrst situation arises when we discuss generalization errors in machine learning (Chap- ter 8). The second situation arises when we want to discuss continuous distributions, such as the Gaussian (Section 6.5). For our purposes, the lack of precision allows for a briefer introduction to probability. Remark. In continuous spaces, there are two additional technicalities, which are counterintuitive. First, the set of all subsets (used to deﬁne the event space A in Section 6.1) is not well behaved enough. A needs to be restricted to behave well under set complements, set intersections, and set unions. Second, the size of a set (which in discrete spaces can be obtained by counting the elements) turns out to be tricky. The size of a set is called its measure. For example, the cardinality of discrete sets, themeasure length of an interval in R, and the volume of a region in Rd are all mea- sures. Sets that behave well under set operations and additionally have a topology are called a Borel σ-algebra. Betancourt details a careful con-Borel σ-algebra struction of probability spaces from set theory without being bogged down in technicalities; see https://tinyurl.com/yb3t6mfd. For a more pre- cise construction, we refer to Billingsley (1995) and Jacod and Protter (2004). In this book, we consider real-valued random variables with their cor- Draft (2019-12-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com. 6.2 Discrete and Continuous Probabilities 181 responding Borel σ-algebra. We consider random variables with values in R D to be a vector of real-valued random variables. ♦ Deﬁnition 6.1 (Probability Density Function). A function f : R D → R is called a probability density function (pdf ) if probability density function pdf1. ∀x ∈ R D : f (x) ⩾ 0 2. Its integral exists and ∫ RD f (x)dx = 1 . (6.15) For probability mass functions (pmf) of discrete random variables, the integral in (6.15) is replaced with a sum (6.12). Observe that the probability density function is any function f that is non-negative and integrates to one. We associate a random variable X with this function f by P (a ⩽ X ⩽ b) = ∫ b a f (x)dx , (6.16) where a, b ∈ R and x ∈ R are outcomes of the continuous random vari- able X. States x ∈ R D are deﬁned analogously by considering a vector of x ∈ R. This association (6.16) is called the law or distribution of the law random variable X. P (X = x) is a set of measure zero.Remark. In contrast to discrete random variables, the probability of a con- tinuous random variable X taking a particular value P (X = x) is zero. This is like trying to specify an interval in (6.16) where a = b. ♦ Deﬁnition 6.2 (Cumulative Distribution Function). A cumulative distribu- cumulative distribution functiontion function (cdf) of a multivariate real-valued random variable X with states x ∈ R D is given by FX(x) = P (X1 ⩽ x1, . . . , XD ⩽ xD) , (6.17) where X = [X1, . . . , XD] ⊤, x = [x1, . . . , xD] ⊤, and the right-hand side represents the probability that random variable Xi takes the value smaller than or equal to xi. There are cdfs, which do not have corresponding pdfs. The cdf can be expressed also as the integral of the probability density function f (x) so that FX(x) = ∫ x1 −∞ · · · ∫ xD −∞ f (z1, . . . , zD)dz1 · · · dzD . (6.18) Remark. We reiterate that there are in fact two distinct concepts when talking about distributions. First is the idea of a pdf (denoted by f (x)), which is a nonnegative function that sums to one. Second is the law of a random variable X, that is, the association of a random variable X with the pdf f (x). ♦ c⃝2019 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press. 182 Probability and Distributions Figure 6.3 Examples of (a) discrete and (b) continuous uniform distributions. See Example 6.3 for details of the distributions. −1 0 1 2 z 0.0 0.5 1.0 1.5 2.0P(Z=z) (a) Discrete distribution −1 0 1 2 x 0.0 0.5 1.0 1.5 2.0p(x) (b) Continuous distribution For most of this book, we will not use the notation f (x) and FX(x) as we mostly do not need to distinguish between the pdf and cdf. However, we will need to be careful about pdfs and cdfs in Section 6.7. 6.2.3 Contrasting Discrete and Continuous Distributions Recall from Section 6.1.2 that probabilities are positive and the total prob- ability sums up to one. For discrete random variables (see (6.12)), this implies that the probability of each state must lie in the interval [0, 1]. However, for continuous random variables the normalization (see (6.15)) does not imply that the value of the density is less than or equal to 1 for all values. We illustrate this in Figure 6.3 using the uniform distributionuniform distribution for both discrete and continuous random variables. Example 6.3 We consider two examples of the uniform distribution, where each state is equally likely to occur. This example illustrates some differences between discrete and continuous probability distributions. Let Z be a discrete uniform random variable with three states {z = −1.1, z = 0.3, z = 1.5}. The probability mass function can be representedThe actual values of these states are not meaningful here, and we deliberately chose numbers to drive home the point that we do not want to use (and should ignore) the ordering of the states. as a table of probability values: z P (Z = z) −1.1 1 3 0.3 1 3 1.5 1 3 Alternatively, we can think of this as a graph (Figure 6.3(a)), where we use the fact that the states can be located on the x-axis, and the y-axis represents the probability of a particular state. The y-axis in Figure 6.3(a) is deliberately extended so that is it the same as in Figure 6.3(b). Let X be a continuous random variable taking values in the range 0.9 ⩽ X ⩽ 1.6, as represented by Figure 6.3(b). Observe that the height of the Draft (2019-12-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com. 6.3 Sum Rule, Product Rule, and Bayes’ Theorem 183 Table 6.1 Nomenclature for probability distributions. Type “Point probability” “Interval probability” Discrete P (X = x) Not applicable Probability mass function Continuous p(x) P (X ⩽ x) Probability density function Cumulative distribution function density can be greater than 1. However, it needs to hold that ∫ 1.6 0.9 p(x)dx = 1 . (6.19) Remark. There is an additional subtlety with regards to discrete prob- ability distributions. The states z1, . . . , zd do not in principle have any structure, i.e., there is usually no way to compare them, for example z1 = red, z2 = green, z3 = blue. However, in many machine learning applications discrete states take numerical values, e.g., z1 = −1.1, z2 = 0.3, z3 = 1.5, where we could say z1 < z2 < z3. Discrete states that as- sume numerical values are particularly useful because we often consider expected values (Section 6.4.1) of random variables. ♦ Unfortunately, machine learning literature uses notation and nomen- clature that hides the distinction between the sample space Ω, the target space T , and the random variable X. For a value x of the set of possible outcomes of the random variable X, i.e., x ∈ T , p(x) denotes the prob- We think of the outcome x as the argument that results in the probability p(x). ability that random variable X has the outcome x. For discrete random variables, this is written as P (X = x), which is known as the probabil- ity mass function. The pmf is often referred to as the “distribution”. For continuous variables, p(x) is called the probability density function (often referred to as a density). To muddy things even further, the cumulative distribution function P (X ⩽ x) is often also referred to as the “distribu- tion”. In this chapter, we will use the notation X to refer to both univariate and multivariate random variables, and denote the states by x and x re- spectively. We summarize the nomenclature in Table 6.1. Remark. We will be using the expression “probability distribution” not only for discrete probability mass functions but also for continuous proba- bility density functions, although this is technically incorrect. In line with most machine learning literature, we also rely on context to distinguish the different uses of the phrase probability distribution. ♦ 6.3 Sum Rule, Product Rule, and Bayes’ Theorem We think of probability theory as an extension to logical reasoning. As we discussed in Section 6.1.1, the rules of probability presented here follow c⃝2019 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press. 184 Probability and Distributions naturally from fulﬁlling the desiderata (Jaynes, 2003, chapter 2). Prob- abilistic modeling (Section 8.4) provides a principled foundation for de- signing machine learning methods. Once we have deﬁned probability dis- tributions (Section 6.2) corresponding to the uncertainties of the data and our problem, it turns out that there are only two fundamental rules, the sum rule and the product rule. Recall from (6.9) that p(x, y) is the joint distribution of the two ran- dom variables x, y. The distributions p(x) and p(y) are the correspond- ing marginal distributions, and p(y | x) is the conditional distribution of y given x. Given the deﬁnitions of the marginal and conditional probability for discrete and continuous random variables in Section 6.2, we can now present the two fundamental rules in probability theory.These two rules arise naturally (Jaynes, 2003) from the requirements we discussed in Section 6.1.1. The ﬁrst rule, the sum rule, states that sum rule p(x) =    ∑ y∈Y p(x, y) if y is discrete ∫ Y p(x, y)dy if y is continuous , (6.20) where Y are the states of the target space of random variable Y . This means that we sum out (or integrate out) the set of states y of the random variable Y . The sum rule is also known as the marginalization property.marginalization property The sum rule relates the joint distribution to a marginal distribution. In general, when the joint distribution contains more than two random vari- ables, the sum rule can be applied to any subset of the random variables, resulting in a marginal distribution of potentially more than one random variable. More concretely, if x = [x1, . . . , xD] ⊤, we obtain the marginal p(xi) = ∫ p(x1, . . . , xD)dx\\i (6.21) by repeated application of the sum rule where we integrate/sum out all random variables except xi, which is indicated by \\i, which reads “all except i.” Remark. Many of the computational challenges of probabilistic modeling are due to the application of the sum rule. When there are many variables or discrete variables with many states, the sum rule boils down to per- forming a high-dimensional sum or integral. Performing high-dimensional sums or integrals is generally computationally hard, in the sense that there is no known polynomial-time algorithm to calculate them exactly. ♦ The second rule, known as the product rule, relates the joint distributionproduct rule to the conditional distribution via p(x, y) = p(y | x)p(x) . (6.22) The product rule can be interpreted as the fact that every joint distribu- tion of two random variables can be factorized (written as a product) Draft (2019-12-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com. 6.3 Sum Rule, Product Rule, and Bayes’ Theorem 185 of two other distributions. The two factors are the marginal distribu- tion of the ﬁrst random variable p(x), and the conditional distribution of the second random variable given the ﬁrst p(y | x). Since the ordering of random variables is arbitrary in p(x, y), the product rule also implies p(x, y) = p(x | y)p(y). To be precise, (6.22) is expressed in terms of the probability mass functions for discrete random variables. For continuous random variables, the product rule is expressed in terms of the probability density functions (Section 6.2.3). In machine learning and Bayesian statistics, we are often interested in making inferences of unobserved (latent) random variables given that we have observed other random variables. Let us assume we have some prior knowledge p(x) about an unobserved random variable x and some rela- tionship p(y | x) between x and a second random variable y, which we can observe. If we observe y, we can use Bayes’ theorem to draw some conclusions about x given the observed values of y. Bayes’ theorem (also Bayes’ theorem Bayes’ rule or Bayes’ law) Bayes’ rule Bayes’ law p(x | y) ︸ ︷︷ ︸ posterior = likelihood ︷ ︸︸ ︷ p(y | x) prior ︷ ︸︸ ︷ p(x) p(y) ︸︷︷︸ evidence (6.23) is a direct consequence of the product rule in (6.22) since p(x, y) = p(x | y)p(y) (6.24) and p(x, y) = p(y | x)p(x) (6.25) so that p(x | y)p(y) = p(y | x)p(x) ⇐⇒ p(x | y) = p(y | x)p(x) p(y) . (6.26) In (6.23), p(x) is the prior, which encapsulates our subjective prior prior knowledge of the unobserved (latent) variable x before observing any data. We can choose any prior that makes sense to us, but it is critical to ensure that the prior has a nonzero pdf (or pmf) on all plausible x, even if they are very rare. The likelihood p(y | x) describes how x and y are related, and in the likelihood The likelihood is sometimes also called the “measurement model”. case of discrete probability distributions, it is the probability of the data y if we were to know the latent variable x. Note that the likelihood is not a distribution in x, but only in y. We call p(y | x) either the “likelihood of x (given y)” or the “probability of y given x” but never the likelihood of y (MacKay, 2003). The posterior p(x | y) is the quantity of interest in Bayesian statistics posterior because it expresses exactly what we are interested in, i.e., what we know about x after having observed y. c⃝2019 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press. 186 Probability and Distributions The quantity p(y) := ∫ p(y | x)p(x)dx = EX[p(y | x)] (6.27) is the marginal likelihood/evidence. The right-hand side of (6.27) uses themarginal likelihood evidence expectation operator which we deﬁne in Section 6.4.1. By deﬁnition, the marginal likelihood integrates the numerator of (6.23) with respect to the latent variable x. Therefore, the marginal likelihood is independent of x, and it ensures that the posterior p(x | y) is normalized. The marginal likelihood can also be interpreted as the expected likelihood where we take the expectation with respect to the prior p(x). Beyond normalization of the posterior, the marginal likelihood also plays an important role in Bayesian model selection, as we will discuss in Section 8.6. Due to the integration in (8.44), the evidence is often hard to compute.Bayes’ theorem is also called the “probabilistic inverse.” Bayes’ theorem (6.23) allows us to invert the relationship between x and y given by the likelihood. Therefore, Bayes’ theorem is sometimes called the probabilistic inverse. We will discuss Bayes’ theorem further inprobabilistic inverse Section 8.4. Remark. In Bayesian statistics, the posterior distribution is the quantity of interest as it encapsulates all available information from the prior and the data. Instead of carrying the posterior around, it is possible to focus on some statistic of the posterior, such as the maximum of the posterior, which we will discuss in Section 8.3. However, focusing on some statistic of the posterior leads to loss of information. If we think in a bigger con- text, then the posterior can be used within a decision-making system, and having the full posterior can be extremely useful and lead to decisions that are robust to disturbances. For example, in the context of model-based re- inforcement learning, Deisenroth et al. (2015) show that using the full posterior distribution of plausible transition functions leads to very fast (data/sample efﬁcient) learning, whereas focusing on the maximum of the posterior leads to consistent failures. Therefore, having the full pos- terior can be very useful for a downstream task. In Chapter 9, we will continue this discussion in the context of linear regression. ♦ 6.4 Summary Statistics and Independence We are often interested in summarizing sets of random variables and com- paring pairs of random variables. A statistic of a random variable is a de- terministic function of that random variable. The summary statistics of a distribution provide one useful view of how a random variable behaves, and as the name suggests, provide numbers that summarize and charac- terize the distribution. We describe the mean and the variance, two well- known summary statistics. Then we discuss two ways to compare a pair of random variables: ﬁrst, how to say that two random variables are inde- pendent; and second, how to compute an inner product between them. Draft (2019-12-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com. 6.4 Summary Statistics and Independence 187 6.4.1 Means and Covariances Mean and (co)variance are often useful to describe properties of probabil- ity distributions (expected values and spread). We will see in Section 6.6 that there is a useful family of distributions (called the exponential fam- ily), where the statistics of the random variable capture all possible infor- mation. The concept of the expected value is central to machine learning, and the foundational concepts of probability itself can be derived from the expected value (Whittle, 2000). Deﬁnition 6.3 (Expected Value). The expected value of a function g : R → expected value R of a univariate continuous random variable X ∼ p(x) is given by EX[g(x)] = ∫ X g(x)p(x)dx . (6.28) Correspondingly, the expected value of a function g of a discrete random variable X ∼ p(x) is given by EX[g(x)] = ∑ x∈X g(x)p(x) , (6.29) where X is the set of possible outcomes (the target space) of the random variable X. In this section, we consider discrete random variables to have numerical outcomes. This can be seen by observing that the function g takes real numbers as inputs. The expected value of a function of a random variable is sometimes referred to as the law of the unconscious statistician (Casella and Berger, 2002, Section 2.2). Remark. We consider multivariate random variables X as a ﬁnite vector of univariate random variables [X1, . . . , XD]⊤. For multivariate random variables, we deﬁne the expected value element wise EX[g(x)] =    EX1[g(x1)] ... EXD [g(xD)]    ∈ R D , (6.30) where the subscript EXd indicates that we are taking the expected value with respect to the dth element of the vector x. ♦ Deﬁnition 6.3 deﬁnes the meaning of the notation EX as the operator indicating that we should take the integral with respect to the probabil- ity density (for continuous distributions) or the sum over all states (for discrete distributions). The deﬁnition of the mean (Deﬁnition 6.4), is a special case of the expected value, obtained by choosing g to be the iden- tity function. Deﬁnition 6.4 (Mean). The mean of a random variable X with states mean c⃝2019 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press. 188 Probability and Distributions x ∈ R D is an average and is deﬁned as EX[x] =    EX1[x1] ... EXD [xD]    ∈ RD , (6.31) where Exd[xd] :=    ∫ X xdp(xd)dxd if X is a continuous random variable ∑ xi∈X xip(xd = xi) if X is a discrete random variable (6.32) for d = 1, . . . , D, where the subscript d indicates the corresponding di- mension of x. The integral and sum are over the states X of the target space of the random variable X. In one dimension, there are two other intuitive notions of “average”, which are the median and the mode. The median is the “middle” value ifmedian we sort the values, i.e., 50% of the values are greater than the median and 50% are smaller than the median. This idea can be generalized to contin- uous values by considering the value where the cdf (Deﬁnition 6.2) is 0.5. For distributions, which are asymmetric or have long tails, the median provides an estimate of a typical value that is closer to human intuition than the mean value. Furthermore, the median is more robust to outliers than the mean. The generalization of the median to higher dimensions is non-trivial as there is no obvious way to “sort” in more than one dimen- sion (Hallin et al., 2010; Kong and Mizera, 2012). The mode is the mostmode frequently occurring value. For a discrete random variable, the mode is deﬁned as the value of x having the highest frequency of occurrence. For a continuous random variable, the mode is deﬁned as a peak in the density p(x). A particular density p(x) may have more than one mode, and fur- thermore there may be a very large number of modes in high-dimensional distributions. Therefore, ﬁnding all the modes of a distribution can be computationally challenging. Example 6.4 Consider the two-dimensional distribution illustrated in Figure 6.4: p(x) = 0.4 N (x ∣ ∣ ∣ ∣ [ 10 2 ] , [ 1 0 0 1 ] ) + 0.6 N ( x ∣ ∣ ∣ ∣ [ 0 0 ] , [ 8.4 2.0 2.0 1.7 ] ) . (6.33) We will deﬁne the Gaussian distribution N (µ, σ2) in Section 6.5. Also shown is its corresponding marginal distribution in each dimension. Ob- serve that the distribution is bimodal (has two modes), but one of the Draft (2019-12-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com. 6.4 Summary Statistics and Independence 189 marginal distributions is unimodal (has one mode). The horizontal bi- modal univariate distribution illustrates that the mean and median can be different from each other. While it is tempting to deﬁne the two- dimensional median to be the concatenation of the medians in each di- mension, the fact that we cannot deﬁne an ordering of two-dimensional points makes it difﬁcult. When we say “cannot deﬁne an ordering”, we mean that there is more than one way to deﬁne the relation < so that[ 3 0 ] < [ 2 3 ] . Figure 6.4 Illustration of the mean, mode, and median for a two-dimensional dataset, as well as its marginal densities. Mean Modes Median Remark. The expected value (Deﬁnition 6.3) is a linear operator. For ex- ample, given a real-valued function f (x) = ag(x) + bh(x) where a, b ∈ R and x ∈ RD, we obtain EX[f (x)] = ∫ f (x)p(x)dx (6.34a) = ∫ [ag(x) + bh(x)]p(x)dx (6.34b) = a ∫ g(x)p(x)dx + b ∫ h(x)p(x)dx (6.34c) = aEX[g(x)] + bEX[h(x)] . (6.34d) ♦ For two random variables, we may wish to characterize their correspon- c⃝2019 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press. 190 Probability and Distributions dence to each other. The covariance intuitively represents the notion of how dependent random variables are to one another. Deﬁnition 6.5 (Covariance (Univariate)). The covariance between twocovariance univariate random variables X, Y ∈ R is given by the expected product of their deviations from their respective means, i.e., CovX,Y [x, y] := EX,Y [ (x − EX[x])(y − EY [y]) ] . (6.35) Terminology: The covariance of multivariate random variables Cov[x, y] is sometimes referred to as cross-covariance, with covariance referring to Cov[x, x]. Remark. When the random variable associated with the expectation or covariance is clear by its arguments, the subscript is often suppressed (for example, EX[x] is often written as E[x]). ♦ By using the linearity of expectations, the expression in Deﬁnition 6.5 can be rewritten as the expected value of the product minus the product of the expected values, i.e., Cov[x, y] = E[xy] − E[x]E[y] . (6.36) The covariance of a variable with itself Cov[x, x] is called the variance andvariance is denoted by VX[x]. The square root of the variance is called the standardstandard deviation deviation and is often denoted by σ(x). The notion of covariance can be generalized to multivariate random variables. Deﬁnition 6.6 (Covariance (Multivariate)). If we consider two multivari- ate random variables X and Y with states x ∈ R D and y ∈ RE respec- tively, the covariance between X and Y is deﬁned ascovariance Cov[x, y] = E[xy⊤] − E[x]E[y] ⊤ = Cov[y, x] ⊤ ∈ RD×E . (6.37) Deﬁnition 6.6 can be applied with the same multivariate random vari- able in both arguments, which results in a useful concept that intuitively captures the “spread” of a random variable. For a multivariate random variable, the variance describes the relation between individual dimen- sions of the random variable. Deﬁnition 6.7 (Variance). The variance of a random variable X withvariance states x ∈ R D and a mean vector µ ∈ R D is deﬁned as VX[x] = CovX[x, x] (6.38a) = EX[(x − µ)(x − µ) ⊤] = EX[xx⊤] − EX[x]EX[x] ⊤ (6.38b) =      Cov[x1, x1] Cov[x1, x2] . . . Cov[x1, xD] Cov[x2, x1] Cov[x2, x2] . . . Cov[x2, xD] ... ... . . . ... Cov[xD, x1] . . . . . . Cov[xD, xD]      . (6.38c) The D × D matrix in (6.38c) is called the covariance matrix of the mul-covariance matrix tivariate random variable X. The covariance matrix is symmetric and pos- itive semideﬁnite and tells us something about the spread of the data. On its diagonal, the covariance matrix contains the variances of the marginalsmarginal Draft (2019-12-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com. 6.4 Summary Statistics and Independence 191 Figure 6.5 Two-dimensional datasets with identical means and variances along each axis (colored lines) but with different covariances. −5 0 5 x −2 0 2 4 6y (a) x and y are negatively correlated. −5 0 5 x −2 0 2 4 6y (b) x and y are positively correlated. p(xi) = ∫ p(x1, . . . , xD)dx\\i , (6.39) where “\\i” denotes “all variables but i”. The off-diagonal entries are the cross-covariance terms Cov[xi, xj] for i, j = 1, . . . , D, i ̸= j. cross-covariance Remark. In this book, we generally assume that covariance matrices are positive deﬁnite to enable better intuition. We therefore do not discuss corner cases that result in positive semideﬁnite (low-rank) covariance ma- trices. ♦ When we want to compare the covariances between different pairs of random variables, it turns out that the variance of each random variable affects the value of the covariance. The normalized version of covariance is called the correlation. Deﬁnition 6.8 (Correlation). The correlation between two random vari- correlation ables X, Y is given by corr[x, y] = Cov[x, y] √V[x]V[y] ∈ [−1, 1] . (6.40) The correlation matrix is the covariance matrix of standardized random variables, x/σ(x). In other words, each random variable is divided by its standard deviation (the square root of the variance) in the correlation matrix. The covariance (and correlation) indicate how two random variables are related; see Figure 6.5. Positive correlation corr[x, y] means that when x grows, then y is also expected to grow. Negative correlation means that as x increases, then y decreases. 6.4.2 Empirical Means and Covariances The deﬁnitions in Section 6.4.1 are often also called the population mean population mean and covarianceand covariance, as it refers to the true statistics for the population. In ma- chine learning, we need to learn from empirical observations of data. Con- sider a random variable X. There are two conceptual steps to go from c⃝2019 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press. 192 Probability and Distributions population statistics to the realization of empirical statistics. First, we use the fact that we have a ﬁnite dataset (of size N ) to construct an empirical statistic that is a function of a ﬁnite number of identical random variables, X1, . . . , XN . Second, we observe the data, that is, we look at the realiza- tion x1, . . . , xN of each of the random variables and apply the empirical statistic. Speciﬁcally, for the mean (Deﬁnition 6.4), given a particular dataset we can obtain an estimate of the mean, which is called the empirical mean orempirical mean sample mean. The same holds for the empirical covariance.sample mean Deﬁnition 6.9 (Empirical Mean and Covariance). The empirical mean vec-empirical mean tor is the arithmetic average of the observations for each variable, and it is deﬁned as ¯x := 1 N N∑ n=1 xn , (6.41) where xn ∈ RD. Similar to the empirical mean, the empirical covariance matrix is a D×Dempirical covariance matrix Σ := 1 N N∑ n=1(xn − ¯x)(xn − ¯x)⊤. (6.42) Throughout the book, we use the empirical covariance, which is a biased estimate. The unbiased (sometimes called corrected) covariance has the factor N − 1 in the denominator instead of N . To compute the statistics for a particular dataset, we would use the realizations (observations) x1, . . . , xN and use (6.41) and (6.42). Em- pirical covariance matrices are symmetric, positive semideﬁnite (see Sec- tion 3.2.3). 6.4.3 Three Expressions for the Variance We now focus on a single random variable X and use the preceding em- pirical formulas to derive three possible expressions for the variance. The The derivations are exercises at the end of this chapter. following derivation is the same for the population variance, except that we need to take care of integrals. The standard deﬁnition of variance, cor- responding to the deﬁnition of covariance (Deﬁnition 6.5), is the expec- tation of the squared deviation of a random variable X from its expected value µ, i.e., VX[x] := EX[(x − µ) 2] . (6.43) The expectation in (6.43) and the mean µ = EX(x) are computed us- ing (6.32), depending on whether X is a discrete or continuous random variable. The variance as expressed in (6.43) is the mean of a new random variable Z := (X − µ)2. When estimating the variance in (6.43) empirically, we need to resort to a two-pass algorithm: one pass through the data to calculate the mean µ using (6.41), and then a second pass using this estimate ˆµ calculate the Draft (2019-12-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com. 6.4 Summary Statistics and Independence 193 variance. It turns out that we can avoid two passes by rearranging the terms. The formula in (6.43) can be converted to the so-called raw-score raw-score formula for varianceformula for variance: VX[x] = EX[x2] − (EX[x]) 2 . (6.44) The expression in (6.44) can be remembered as “the mean of the square minus the square of the mean”. It can be calculated empirically in one pass through data since we can accumulate xi (to calculate the mean) and x2 i simultaneously, where xi is the ith observation. Unfortunately, if imple- If the two terms in (6.44) are huge and approximately equal, we may suffer from an unnecessary loss of numerical precision in ﬂoating-point arithmetic. mented in this way, it can be numerically unstable. The raw-score version of the variance can be useful in machine learning, e.g., when deriving the bias–variance decomposition (Bishop, 2006). A third way to understand the variance is that it is a sum of pairwise dif- ferences between all pairs of observations. Consider a sample x1, . . . , xN of realizations of random variable X, and we compute the squared differ- ence between pairs of xi and xj. By expanding the square, we can show that the sum of N 2 pairwise differences is the empirical variance of the observations: 1 N 2 N∑ i,j=1(xi − xj) 2 = 2   1 N N∑ i=1 x2 i − ( 1 N N∑ i=1 xi )2  . (6.45) We see that (6.45) is twice the raw-score expression (6.44). This means that we can express the sum of pairwise distances (of which there are N 2 of them) as a sum of deviations from the mean (of which there are N ). Ge- ometrically, this means that there is an equivalence between the pairwise distances and the distances from the center of the set of points. From a computational perspective, this means that by computing the mean (N terms in the summation), and then computing the variance (again N terms in the summation), we can obtain an expression (left-hand side of (6.45)) that has N 2 terms. 6.4.4 Sums and Transformations of Random Variables We may want to model a phenomenon that cannot be well explained by textbook distributions (we introduce some in Sections 6.5 and 6.6), and hence may perform simple manipulations of random variables (such as adding two random variables). Consider two random variables X, Y with states x, y ∈ R D. Then: E[x + y] = E[x] + E[y] (6.46) E[x − y] = E[x] − E[y] (6.47) V[x + y] = V[x] + V[y] + Cov[x, y] + Cov[y, x] (6.48) V[x − y] = V[x] + V[y] − Cov[x, y] − Cov[y, x] . (6.49) c⃝2019 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press. 194 Probability and Distributions Mean and (co)variance exhibit some useful properties when it comes to afﬁne transformation of random variables. Consider a random variable X with mean µ and covariance matrix Σ and a (deterministic) afﬁne transformation y = Ax + b of x. Then y is itself a random variable whose mean vector and covariance matrix are given by EY [y] = EX[Ax + b] = AEX[x] + b = Aµ + b , (6.50) VY [y] = VX[Ax + b] = VX[Ax] = AVX[x]A⊤ = AΣA⊤ , (6.51) respectively. Furthermore,This can be shown directly by using the deﬁnition of the mean and covariance. Cov[x, y] = E[x(Ax + b) ⊤] − E[x]E[Ax + b]⊤ (6.52a) = E[x]b⊤ + E[xx ⊤]A ⊤ − µb ⊤ − µµ ⊤A ⊤ (6.52b) = µb ⊤ − µb ⊤ + (E[xx⊤] − µµ ⊤)A⊤ (6.52c) (6.38b) = ΣA⊤ , (6.52d) where Σ = E[xx⊤] − µµ ⊤ is the covariance of X. 6.4.5 Statistical Independence Deﬁnition 6.10 (Independence). Two random variables X, Y are statis-statistical independence tically independent if and only if p(x, y) = p(x)p(y) . (6.53) Intuitively, two random variables X and Y are independent if the value of y (once known) does not add any additional information about x (and vice versa). If X, Y are (statistically) independent, then p(y | x) = p(y) p(x | y) = p(x) VX,Y [x + y] = VX[x] + VY [y] CovX,Y [x, y] = 0 The last point may not hold in converse, i.e., two random variables can have covariance zero but are not statistically independent. To understand why, recall that covariance measures only linear dependence. Therefore, random variables that are nonlinearly dependent could have covariance zero. Example 6.5 Consider a random variable X with zero mean (EX[x] = 0) and also EX[x3] = 0. Let y = x2 (hence, Y is dependent on X) and consider the covariance (6.36) between X and Y . But this gives Cov[x, y] = E[xy] − E[x]E[y] = E[x3] = 0 . (6.54) Draft (2019-12-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com. 6.4 Summary Statistics and Independence 195 In machine learning, we often consider problems that can be mod- eled as independent and identically distributed (i.i.d.) random variables, independent and identically distributed i.i.d. X1, . . . , XN . For more than two random variables, the word “indepen- dent” (Deﬁnition 6.10) usually refers to mutually independent random variables, where all subsets are independent (see Pollard (2002, chap- ter 4) and Jacod and Protter (2004, chapter 3)). The phrase “identically distributed” means that all the random variables are from the same distri- bution. Another concept that is important in machine learning is conditional independence. Deﬁnition 6.11 (Conditional Independence). Two random variables X and Y are conditionally independent given Z if and only if conditionally independent p(x, y | z) = p(x | z)p(y | z) for all z ∈ Z , (6.55) where Z is the set of states of random variable Z. We write X ⊥⊥ Y | Z to denote that X is conditionally independent of Y given Z. Deﬁnition 6.11 requires that the relation in (6.55) must hold true for every value of z. The interpretation of (6.55) can be understood as “given knowledge about z, the distribution of x and y factorizes”. Independence can be cast as a special case of conditional independence if we write X ⊥⊥ Y | ∅. By using the product rule of probability (6.22), we can expand the left-hand side of (6.55) to obtain p(x, y | z) = p(x | y, z)p(y | z) . (6.56) By comparing the right-hand side of (6.55) with (6.56), we see that p(y | z) appears in both of them so that p(x | y, z) = p(x | z) . (6.57) Equation (6.57) provides an alternative deﬁnition of conditional indepen- dence, i.e., X ⊥⊥ Y | Z. This alternative presentation provides the inter- pretation “given that we know z, knowledge about y does not change our knowledge of x”. 6.4.6 Inner Products of Random Variables Recall the deﬁnition of inner products from Section 3.2. We can deﬁne an Inner products between multivariate random variables can be treated in a similar fashion inner product between random variables, which we brieﬂy describe in this section. If we have two uncorrelated random variables X, Y , then V[x + y] = V[x] + V[y] . (6.58) Since variances are measured in squared units, this looks very much like the Pythagorean theorem for right triangles c 2 = a2 + b 2. In the following, we see whether we can ﬁnd a geometric interpreta- tion of the variance relation of uncorrelated random variables in (6.58). c⃝2019 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press. 196 Probability and Distributions Figure 6.6 Geometry of random variables. If random variables X and Y are uncorrelated, they are orthogonal vectors in a corresponding vector space, and the Pythagorean theorem applies. √var[y] √var[x] √var[x + y] = √var[x] + var[y]acb Random variables can be considered vectors in a vector space, and we can deﬁne inner products to obtain geometric properties of random vari- ables (Eaton, 2007). If we deﬁne ⟨X, Y ⟩ := Cov[x, y] (6.59) for zero mean random variables X and Y , we obtain an inner product. We see that the covariance is symmetric, positive deﬁnite, and linear in eitherCov[x, x] = 0 ⇐⇒ x = 0 argument. The length of a random variable is Cov[αx + z, y] = α Cov[x, y] + Cov[z, y] for α ∈ R. ∥X∥ = √Cov[x, x] = √V[x] = σ[x] , (6.60) i.e., its standard deviation. The “longer” the random variable, the more uncertain it is; and a random variable with length 0 is deterministic. If we look at the angle θ between two random variables X, Y , we get cos θ = ⟨X, Y ⟩ ∥X∥ ∥Y ∥ = Cov[x, y] √V[x]V[y] , (6.61) which is the correlation (Deﬁnition 6.8) between the two random vari- ables. This means that we can think of correlation as the cosine of the angle between two random variables when we consider them geometri- cally. We know from Deﬁnition 3.7 that X ⊥ Y ⇐⇒ ⟨X, Y ⟩ = 0. In our case, this means that X and Y are orthogonal if and only if Cov[x, y] = 0, i.e., they are uncorrelated. Figure 6.6 illustrates this relationship. Remark. While it is tempting to use the Euclidean distance (constructed Draft (2019-12-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com. 6.5 Gaussian Distribution 197 Figure 6.7 Gaussian distribution of two random variables x1 and x2. x1 −1 0 1 x 2 −5.0 −2.5 0.0 2.5 5.0 7.5p(x1,x2) 0.00 0.05 0.10 0.15 0.20 from the preceding deﬁnition of inner products) to compare probability distributions, it is unfortunately not the best way to obtain distances be- tween distributions. Recall that the probability mass (or density) is posi- tive and needs to add up to 1. These constraints mean that distributions live on something called a statistical manifold. The study of this space of probability distributions is called information geometry. Computing dis- tances between distributions are often done using Kullback-Leibler diver- gence, which is a generalization of distances that account for properties of the statistical manifold. Just like the Euclidean distance is a special case of a metric (Section 3.3), the Kullback-Leibler divergence is a special case of two more general classes of divergences called Bregman divergences and f -divergences. The study of divergences is beyond the scope of this book, and we refer for more details to the recent book by Amari (2016), one of the founders of the ﬁeld of information geometry. ♦ 6.5 Gaussian Distribution The Gaussian distribution is the most well-studied probability distribution for continuous-valued random variables. It is also referred to as the normal normal distribution distribution. Its importance originates from the fact that it has many com- The Gaussian distribution arises naturally when we consider sums of independent and identically distributed random variables. This is known as the central limit theorem (Grinstead and Snell, 1997). putationally convenient properties, which we will be discussing in the fol- lowing. In particular, we will use it to deﬁne the likelihood and prior for linear regression (Chapter 9), and consider a mixture of Gaussians for density estimation (Chapter 11). There are many other areas of machine learning that also beneﬁt from using a Gaussian distribution, for example Gaussian processes, variational inference, and reinforcement learning. It is also widely used in other ap- plication areas such as signal processing (e.g., Kalman ﬁlter), control (e.g., linear quadratic regulator), and statistics (e.g., hypothesis testing). c⃝2019 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press. 198 Probability and Distributions Figure 6.8 Gaussian distributions overlaid with 100 samples. (a) One- dimensional case; (b) two-dimensional case. −5.0 −2.5 0.0 2.5 5.0 7.5 x 0.00 0.05 0.10 0.15 0.20 p(x) Mean Sample 2σ (a) Univariate (one-dimensional) Gaussian; The red cross shows the mean and the red line shows the extent of the variance. −1 0 1 x1 −4 −2 0 2 4 6 8x2 Mean Sample (b) Multivariate (two-dimensional) Gaus- sian, viewed from top. The red cross shows the mean and the colored lines show the con- tour lines of the density. For a univariate random variable, the Gaussian distribution has a den- sity that is given by p(x | µ, σ2) = 1 √2πσ2 exp ( − (x − µ) 2 2σ2 ) . (6.62) The multivariate Gaussian distribution is fully characterized by a meanmultivariate Gaussian distribution mean vector vector µ and a covariance matrix Σ and deﬁned as covariance matrix p(x | µ, Σ) = (2π)− D 2 |Σ| − 1 2 exp ( − 1 2 (x − µ) ⊤Σ −1(x − µ) ) , (6.63) where x ∈ R D. We write p(x) = N (x | µ, Σ ) or X ∼ N ( µ, Σ ). Fig-Also known as a multivariate normal distribution. ure 6.7 shows a bivariate Gaussian (mesh), with the corresponding con- tour plot. Figure 6.8 shows a univariate Gaussian and a bivariate Gaussian with corresponding samples. The special case of the Gaussian with zero mean and identity covariance, that is, µ = 0 and Σ = I, is referred to as the standard normal distribution.standard normal distribution Gaussians are widely used in statistical estimation and machine learn- ing as they have closed-form expressions for marginal and conditional dis- tributions. In Chapter 9, we use these closed-form expressions extensively for linear regression. A major advantage of modeling with Gaussian ran- dom variables is that variable transformations (Section 6.7) are often not needed. Since the Gaussian distribution is fully speciﬁed by its mean and covariance, we often can obtain the transformed distribution by applying the transformation to the mean and covariance of the random variable. 6.5.1 Marginals and Conditionals of Gaussians are Gaussians In the following, we present marginalization and conditioning in the gen- eral case of multivariate random variables. If this is confusing at ﬁrst read- ing, the reader is advised to consider two univariate random variables in- stead. Let X and Y be two multivariate random variables, that may have Draft (2019-12-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com. 6.5 Gaussian Distribution 199 different dimensions. To consider the effect of applying the sum rule of probability and the effect of conditioning, we explicitly write the Gaus- sian distribution in terms of the concatenated states [x, y]⊤, p(x, y) = N ([ µx µy ] , [ Σxx Σxy Σyx Σyy ]) . (6.64) where Σxx = Cov[x, x] and Σyy = Cov[y, y] are the marginal covari- ance matrices of x and y, respectively, and Σxy = Cov[x, y] is the cross- covariance matrix between x and y. The conditional distribution p(x | y) is also Gaussian (illustrated in Fig- ure 6.9(c)) and given by (derived in Section 2.3 of Bishop, 2006) p(x | y) = N (µx | y, Σx | y) (6.65) µx | y = µx + ΣxyΣ −1 yy (y − µy) (6.66) Σx | y = Σxx − ΣxyΣ −1 yy Σyx . (6.67) Note that in the computation of the mean in (6.66), the y-value is an observation and no longer random. Remark. The conditional Gaussian distribution shows up in many places, where we are interested in posterior distributions: The Kalman ﬁlter (Kalman, 1960), one of the most central algorithms for state estimation in signal processing, does nothing but computing Gaussian conditionals of joint distributions (Deisenroth and Ohlsson, 2011; S¨arkk¨a, 2013). Gaussian processes (Rasmussen and Williams, 2006), which are a prac- tical implementation of a distribution over functions. In a Gaussian pro- cess, we make assumptions of joint Gaussianity of random variables. By (Gaussian) conditioning on observed data, we can determine a poste- rior distribution over functions. Latent linear Gaussian models (Roweis and Ghahramani, 1999; Mur- phy, 2012), which include probabilistic principal component analysis (PPCA) (Tipping and Bishop, 1999). We will look at PPCA in more de- tail in Section 10.7. ♦ The marginal distribution p(x) of a joint Gaussian distribution p(x, y) (see (6.64)) is itself Gaussian and computed by applying the sum rule (6.20) and given by p(x) = ∫ p(x, y)dy = N ( x | µx, Σxx) . (6.68) The corresponding result holds for p(y), which is obtained by marginaliz- ing with respect to x. Intuitively, looking at the joint distribution in (6.64), we ignore (i.e., integrate out) everything we are not interested in. This is illustrated in Figure 6.9(b). c⃝2019 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press. 200 Probability and Distributions Example 6.6 Figure 6.9 (a) Bivariate Gaussian; (b) marginal of a joint Gaussian distribution is Gaussian; (c) the conditional distribution of a Gaussian is also Gaussian. −1 0 1 x1 −4 −2 0 2 4 6 8x2 x2 = −1 (a) Bivariate Gaussian. −1.5 −1.0 −0.5 0.0 0.5 1.0 1.5 x1 0.0 0.2 0.4 0.6 p(x1) Mean 2σ (b) Marginal distribution. −1.5 −1.0 −0.5 0.0 0.5 1.0 1.5 x1 0.0 0.2 0.4 0.6 0.8 1.0 1.2 p(x1|x2 = −1) Mean 2σ (c) Conditional distribution. Consider the bivariate Gaussian distribution (illustrated in Figure 6.9): p(x1, x2) = N ([0 2 ] , [ 0.3 −1 −1 5 ]) . (6.69) We can compute the parameters of the univariate Gaussian, conditioned on x2 = −1, by applying (6.66) and (6.67) to obtain the mean and vari- ance respectively. Numerically, this is µx1 | x2=−1 = 0 + (−1) · 0.2 · (−1 − 2) = 0.6 (6.70) and σ2 x1 | x2=−1 = 0.3 − (−1) · 0.2 · (−1) = 0.1 . (6.71) Therefore, the conditional Gaussian is given by p(x1 | x2 = −1) = N (0.6, 0.1 ) . (6.72) The marginal distribution p(x1), in contrast, can be obtained by apply- ing (6.68), which is essentially using the mean and variance of the random variable x1, giving us p(x1) = N (0, 0.3 ) . (6.73) Draft (2019-12-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com. 6.5 Gaussian Distribution 201 6.5.2 Product of Gaussian Densities For linear regression (Chapter 9), we need to compute a Gaussian likeli- hood. Furthermore, we may wish to assume a Gaussian prior (Section 9.3). We apply Bayes’ Theorem to compute the posterior, which results in a mul- tiplication of the likelihood and the prior, that is, the multiplication of two Gaussian densities. The product of two Gaussians N (x | a, A)N ( x | b, B) The derivation is an exercise at the end of this chapter. is a Gaussian distribution scaled by a c ∈ R, given by c N (x | c, C) with C = (A−1 + B−1)−1 (6.74) c = C(A −1a + B−1b) (6.75) c = (2π) − D 2 |A + B| − 1 2 exp ( − 1 2 (a − b) ⊤(A + B) −1(a − b)) . (6.76) The scaling constant c itself can be written in the form of a Gaussian density either in a or in b with an “inﬂated” covariance matrix A + B, i.e., c = N (a | b, A + B) = N (b | a, A + B). Remark. For notation convenience, we will sometimes use N ( x | m, S) to describe the functional form of a Gaussian density even if x is not a random variable. We have just done this in the preceding demonstration when we wrote c = N (a | b, A + B) = N (b | a, A + B) . (6.77) Here, neither a nor b are random variables. However, writing c in this way is more compact than (6.76). ♦ 6.5.3 Sums and Linear Transformations If X, Y are independent Gaussian random variables (i.e., the joint distri- bution is given as p(x, y) = p(x)p(y)) with p(x) = N (x | µx, Σx) and p(y) = N (y | µy, Σy) , then x + y is also Gaussian distributed and given by p(x + y) = N (µx + µy, Σx + Σy) . (6.78) Knowing that p(x + y) is Gaussian, the mean and covariance matrix can be determined immediately using the results from (6.46) through (6.49). This property will be important when we consider i.i.d. Gaussian noise acting on random variables, as is the case for linear regression (Chap- ter 9). Example 6.7 Since expectations are linear operations, we can obtain the weighted sum of independent Gaussian random variables p(ax + by) = N ( aµx + bµy, a2Σx + b 2Σy) . (6.79) c⃝2019 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press. 202 Probability and Distributions Remark. A case that will be useful in Chapter 11 is the weighted sum of Gaussian densities. This is different from the weighted sum of Gaussian random variables. ♦ In Theorem 6.12, the random variable x is from a density that is a mixture of two densities p1(x) and p2(x), weighted by α. The theorem can be generalized to the multivariate random variable case, since linearity of expectations holds also for multivariate random variables. However, the idea of a squared random variable needs to be replaced by xx ⊤. Theorem 6.12. Consider a mixture of two univariate Gaussian densities p(x) = αp1(x) + (1 − α)p2(x) , (6.80) where the scalar 0 < α < 1 is the mixture weight, and p1(x) and p2(x) are univariate Gaussian densities (Equation (6.62)) with different parameters, i.e., (µ1, σ2 1) ̸= (µ2, σ2 2). Then the mean of the mixture density p(x) is given by the weighted sum of the means of each random variable: E[x] = αµ1 + (1 − α)µ2 . (6.81) The variance of the mixture density p(x) is given by V[x] = [ ασ2 1 + (1 − α)σ2 2]+([ αµ 2 1 + (1 − α)µ2 2] − [αµ1 + (1 − α)µ2] 2) . (6.82) Proof The mean of the mixture density p(x) is given by the weighted sum of the means of each random variable. We apply the deﬁnition of the mean (Deﬁnition 6.4), and plug in our mixture (6.80), which yields E[x] = ∫ ∞ −∞ xp(x)dx (6.83a) = ∫ ∞ −∞ αxp1(x) + (1 − α)xp2(x)dx (6.83b) = α ∫ ∞ −∞ xp1(x)dx + (1 − α) ∫ ∞ −∞ xp2(x)dx (6.83c) = αµ1 + (1 − α)µ2 . (6.83d) To compute the variance, we can use the raw-score version of the vari- ance from (6.44), which requires an expression of the expectation of the squared random variable. Here we use the deﬁnition of an expectation of a function (the square) of a random variable (Deﬁnition 6.3), E[x2] = ∫ ∞ −∞ x 2p(x)dx (6.84a) = ∫ ∞ −∞ αx 2p1(x) + (1 − α)x2p2(x)dx (6.84b) Draft (2019-12-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com. 6.5 Gaussian Distribution 203 = α ∫ ∞ −∞ x2p1(x)dx + (1 − α) ∫ ∞ −∞ x2p2(x)dx (6.84c) = α(µ2 1 + σ2 1) + (1 − α)(µ2 2 + σ2 2) , (6.84d) where in the last equality, we again used the raw-score version of the variance (6.44) giving σ2 = E[x2] − µ2. This is rearranged such that the expectation of a squared random variable is the sum of the squared mean and the variance. Therefore, the variance is given by subtracting (6.83d) from (6.84d), V[x] = E[x2] − (E[x]) 2 (6.85a) = α(µ2 1 + σ2 1) + (1 − α)(µ2 2 + σ2 2) − (αµ1 + (1 − α)µ2) 2 (6.85b) = [ ασ2 1 + (1 − α)σ2 2] + ([ αµ 2 1 + (1 − α)µ2 2] − [αµ1 + (1 − α)µ2] 2) . (6.85c) Remark. The preceding derivation holds for any density, but since the Gaussian is fully determined by the mean and variance, the mixture den- sity can be determined in closed form. ♦ For a mixture density, the individual components can be considered to be conditional distributions (conditioned on the component identity). Equation (6.85c) is an example of the conditional variance formula, also known as the law of total variance, which generally states that for two ran- law of total variance dom variables X and Y it holds that VX[x] = EY [VX[x|y]]+VY [EX[x|y]], i.e., the (total) variance of X is the expected conditional variance plus the variance of a conditional mean. We consider in Example 6.17 a bivariate standard Gaussian random variable X and performed a linear transformation Ax on it. The outcome is a Gaussian random variable with mean zero and covariance AA⊤. Ob- serve that adding a constant vector will change the mean of the distribu- tion, without affecting its variance, that is, the random variable x + µ is Gaussian with mean µ and identity covariance. Hence, any linear/afﬁne transformation of a Gaussian random variable is Gaussian distributed. Any linear/afﬁne transformation of a Gaussian random variable is also Gaussian distributed. Consider a Gaussian distributed random variable X ∼ N ( µ, Σ ) . For a given matrix A of appropriate shape, let Y be a random variable such that y = Ax is a transformed version of x. We can compute the mean of y by exploiting that the expectation is a linear operator (6.50) as follows: E[y] = E[Ax] = AE[x] = Aµ . (6.86) Similarly the variance of y can be found by using (6.51): V[y] = V[Ax] = AV[x]A⊤ = AΣA ⊤ . (6.87) This means that the random variable y is distributed according to p(y) = N ( y | Aµ, AΣA⊤) . (6.88) c⃝2019 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press. 204 Probability and Distributions Let us now consider the reverse transformation: when we know that a random variable has a mean that is a linear transformation of another random variable. For a given full rank matrix A ∈ R M ×N , where M ⩾ N , let y ∈ R M be a Gaussian random variable with mean Ax, i.e., p(y) = N (y | Ax, Σ ) . (6.89) What is the corresponding probability distribution p(x)? If A is invert- ible, then we can write x = A−1y and apply the transformation in the previous paragraph. However, in general A is not invertible, and we use an approach similar to that of the pseudo-inverse (3.57). That is, we pre- multiply both sides with A⊤ and then invert A⊤A, which is symmetric and positive deﬁnite, giving us the relation y = Ax ⇐⇒ (A⊤A)−1A⊤y = x . (6.90) Hence, x is a linear transformation of y, and we obtain p(x) = N (x | (A⊤A) −1A⊤y, (A⊤A) −1A⊤ΣA(A⊤A) −1) . (6.91) 6.5.4 Sampling from Multivariate Gaussian Distributions We will not explain the subtleties of random sampling on a computer, and the interested reader is referred to Gentle (2004). In the case of a mul- tivariate Gaussian, this process consists of three stages: ﬁrst, we need a source of pseudo-random numbers that provide a uniform sample in the interval [0,1]; second, we use a non-linear transformation such as the Box-M¨uller transform (Devroye, 1986) to obtain a sample from a univari- ate Gaussian; and third, we collate a vector of these samples to obtain a sample from a multivariate standard normal N ( 0, I) . For a general multivariate Gaussian, that is, where the mean is non zero and the covariance is not the identity matrix, we use the proper- ties of linear transformations of a Gaussian random variable. Assume we are interested in generating samples xi, i = 1, . . . , n, from a multivariate Gaussian distribution with mean µ and covariance matrix Σ. We wouldTo compute the Cholesky factorization of a matrix, it is required that the matrix is symmetric and positive deﬁnite (Section 3.2.3). Covariance matrices possess this property. like to construct the sample from a sampler that provides samples from the multivariate standard normal N ( 0, I). To obtain samples from a multivariate normal N ( µ, Σ ) , we can use the properties of a linear transformation of a Gaussian random variable: If x ∼ N (0, I), then y = Ax + µ, where AA⊤ = Σ is Gaussian dis- tributed with mean µ and covariance matrix Σ. One convenient choice of A is to use the Cholesky decomposition (Section 4.3) of the covariance matrix Σ = AA⊤. The Cholesky decomposition has the beneﬁt that A is triangular, leading to efﬁcient computation. Draft (2019-12-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com. 6.6 Conjugacy and the Exponential Family 205 6.6 Conjugacy and the Exponential Family Many of the probability distributions “with names” that we ﬁnd in statis- tics textbooks were discovered to model particular types of phenomena. For example, we have seen the Gaussian distribution in Section 6.5. The distributions are also related to each other in complex ways (Leemis and McQueston, 2008). For a beginner in the ﬁeld, it can be overwhelming to ﬁgure out which distribution to use. In addition, many of these distribu- tions were discovered at a time that statistics and computation were done “Computers” used to be a job description.by pencil and paper. It is natural to ask what are meaningful concepts in the computing age (Efron and Hastie, 2016). In the previous section, we saw that many of the operations required for inference can be conve- niently calculated when the distribution is Gaussian. It is worth recalling at this point the desiderata for manipulating probability distributions in the machine learning context: 1. There is some “closure property” when applying the rules of probability, e.g., Bayes’ theorem. By closure, we mean that applying a particular operation returns an object of the same type. 2. As we collect more data, we do not need more parameters to describe the distribution. 3. Since we are interested in learning from data, we want parameter es- timation to behave nicely. It turns out that the class of distributions called the exponential family exponential family provides the right balance of generality while retaining favorable compu- tation and inference properties. Before we introduce the exponential fam- ily, let us see three more members of “named” probability distributions, the Bernoulli (Example 6.8), Binomial (Example 6.9), and Beta (Exam- ple 6.10) distributions. Example 6.8 The Bernoulli distribution is a distribution for a single binary random Bernoulli distributionvariable X with state x ∈ {0, 1}. It is governed by a single continuous pa- rameter µ ∈ [0, 1] that represents the probability of X = 1. The Bernoulli distribution Ber(µ) is deﬁned as p(x | µ) = µx(1 − µ)1−x , x ∈ {0, 1} , (6.92) E[x] = µ , (6.93) V[x] = µ(1 − µ) , (6.94) where E[x] and V[x] are the mean and variance of the binary random variable X. An example where the Bernoulli distribution can be used is when we are interested in modeling the probability of “heads” when ﬂipping a coin. c⃝2019 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press. 206 Probability and Distributions Figure 6.10 Examples of the Binomial distribution for µ ∈ {0.1, 0.4, 0.75} and N = 15. 0.0 2.5 5.0 7.5 10.0 12.5 15.0 Number m of observations x = 1 in N = 15 experiments 0.0 0.1 0.2 0.3p(m) µ = 0.1 µ = 0.4 µ = 0.75 Remark. The rewriting above of the Bernoulli distribution, where we use Boolean variables as numerical 0 or 1 and express them in the exponents, is a trick that is often used in machine learning textbooks. Another oc- curence of this is when expressing the Multinomial distribution. ♦ Example 6.9 (Binomial Distribution) The Binomial distribution is a generalization of the Bernoulli distributionBinomial distribution to a distribution over integers (illustrated in Figure 6.10). In particular, the Binomial can be used to describe the probability of observing m oc- currences of X = 1 in a set of N samples from a Bernoulli distribution where p(X = 1) = µ ∈ [0, 1]. The Binomial distribution Bin(N, µ) is deﬁned as p(m | N, µ) = ( N m ) µm(1 − µ) N −m , (6.95) E[m] = N µ , (6.96) V[m] = N µ(1 − µ) , (6.97) where E[m] and V[m] are the mean and variance of m, respectively. An example where the Binomial could be used is if we want to describe the probability of observing m “heads” in N coin-ﬂip experiments if the probability for observing head in a single experiment is µ. Example 6.10 (Beta Distribution) We may wish to model a continuous random variable on a ﬁnite interval. The Beta distribution is a distribution over a continuous random variableBeta distribution µ ∈ [0, 1], which is often used to represent the probability for some binary event (e.g., the parameter governing the Bernoulli distribution). The Beta Draft (2019-12-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com. 6.6 Conjugacy and the Exponential Family 207 distribution Beta(α, β) (illustrated in Figure 6.11) itself is governed by two parameters α > 0, β > 0 and is deﬁned as p(µ | α, β) = Γ(α + β) Γ(α)Γ(β) µα−1(1 − µ) β−1 (6.98) E[µ] = α α + β , V[µ] = αβ (α + β)2(α + β + 1) (6.99) where Γ(·) is the Gamma function deﬁned as Γ(t) := ∫ ∞ 0 xt−1 exp(−x)dx, t > 0 . (6.100) Γ(t + 1) = tΓ(t) . (6.101) Note that the fraction of Gamma functions in (6.98) normalizes the Beta distribution. Figure 6.11 Examples of the Beta distribution for different values of α and β. 0.0 0.2 0.4 0.6 0.8 1.0 µ 0 2 4 6 8 10p(µ|α,β) α = 0.5 = β α = 1 = β α = 2, β = 0.3 α = 4, β = 10 α = 5, β = 1 Intuitively, α moves probability mass toward 1, whereas β moves prob- ability mass toward 0. There are some special cases (Murphy, 2012): For α = 1 = β, we obtain the uniform distribution U[0, 1]. For α, β < 1, we get a bimodal distribution with spikes at 0 and 1. For α, β > 1, the distribution is unimodal. For α, β > 1 and α = β, the distribution is unimodal, symmetric, and centered in the interval [0, 1], i.e., the mode/mean is at 1 2 . Remark. There is a whole zoo of distributions with names, and they are related in different ways to each other (Leemis and McQueston, 2008). It is worth keeping in mind that each named distribution is created for a particular reason, but may have other applications. Knowing the reason behind the creation of a particular distribution often allows insight into how to best use it. We introduced preceding three distributions to be able c⃝2019 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press. 208 Probability and Distributions to illustrate the concepts of conjugacy (Section 6.6.1) and exponential families (Section 6.6.3). ♦ 6.6.1 Conjugacy According to Bayes’ theorem (6.23), the posterior is proportional to the product of the prior and the likelihood. The speciﬁcation of the prior can be tricky for two reasons: First, the prior should encapsulate our knowl- edge about the problem before we see any data. This is often difﬁcult to describe. Second, it is often not possible to compute the posterior distribu- tion analytically. However, there are some priors that are computationally convenient: conjugate priors.conjugate prior Deﬁnition 6.13 (Conjugate Prior). A prior is conjugate for the likelihoodconjugate function if the posterior is of the same form/type as the prior. Conjugacy is particularly convenient because we can algebraically cal- culate our posterior distribution by updating the parameters of the prior distribution. Remark. When considering the geometry of probability distributions, con- jugate priors retain the same distance structure as the likelihood (Agarwal and Daum´e III, 2010). ♦ To introduce a concrete example of conjugate priors, we describe in Ex- ample 6.11 the Binomial distribution (deﬁned on discrete random vari- ables) and the Beta distribution (deﬁned on continuous random vari- ables). Example 6.11 (Beta-Binomial Conjugacy) Consider a Binomial random variable x ∼ Bin(N, µ) where p(x | N, µ) = (N x ) µx(1 − µ)N −x , x = 0, 1, . . . , N , (6.102) is the probability of ﬁnding x times the outcome “heads” in N coin ﬂips, where µ is the probability of a “head”. We place a Beta prior on the pa- rameter µ, that is, µ ∼ Beta(α, β), where p(µ | α, β) = Γ(α + β) Γ(α)Γ(β) µα−1(1 − µ)β−1 . (6.103) If we now observe some outcome x = h, that is, we see h heads in N coin ﬂips, we compute the posterior distribution on µ as p(µ | x = h, N, α, β) ∝ p(x | N, µ)p(µ | α, β) (6.104a) ∝ µh(1 − µ)(N −h)µα−1(1 − µ) β−1 (6.104b) = µh+α−1(1 − µ) (N −h)+β−1 (6.104c) Draft (2019-12-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com. 6.6 Conjugacy and the Exponential Family 209 Table 6.2 Examples of conjugate priors for common likelihood functions. Likelihood Conjugate prior Posterior Bernoulli Beta Beta Binomial Beta Beta Gaussian Gaussian/inverse Gamma Gaussian/inverse Gamma Gaussian Gaussian/inverse Wishart Gaussian/inverse Wishart Multinomial Dirichlet Dirichlet ∝ Beta(h + α, N − h + β) , (6.104d) i.e., the posterior distribution is a Beta distribution as the prior, i.e., the Beta prior is conjugate for the parameter µ in the Binomial likelihood function. In the following example, we will derive a result that is similar to the Beta-Binomial conjugacy result. Here we will show that the Beta distribu- tion is a conjugate prior for the Bernoulli distribution. Example 6.12 (Beta-Bernoulli Conjugacy) Let x ∈ {0, 1} be distributed according to the Bernoulli distribution with parameter θ ∈ [0, 1], that is, p(x = 1 | θ) = θ. This can also be expressed as p(x | θ) = θx(1 − θ) 1−x. Let θ be distributed according to a Beta distri- bution with parameters α, β, that is, p(θ | α, β) ∝ θα−1(1 − θ) β−1. Multiplying the Beta and the Bernoulli distributions, we get p(θ | x, α, β) = p(x | θ)p(θ | α, β) (6.105a) ∝ θx(1 − θ) 1−xθα−1(1 − θ)β−1 (6.105b) = θα+x−1(1 − θ) β+(1−x)−1 (6.105c) ∝ p(θ | α + x, β + (1 − x)) . (6.105d) The last line is the Beta distribution with parameters (α + x, β + (1 − x)). Table 6.2 lists examples for conjugate priors for the parameters of some standard likelihoods used in probabilistic modeling. Distributions such as The Gamma prior is conjugate for the precision (inverse variance) in the univariate Gaussian likelihood, and the Wishart prior is conjugate for the precision matrix (inverse covariance matrix) in the multivariate Gaussian likelihood. Multinomial, inverse Gamma, inverse Wishart, and Dirichlet can be found in any statistical text, and are described in Bishop (2006), for example. The Beta distribution is the conjugate prior for the parameter µ in both the Binomial and the Bernoulli likelihood. For a Gaussian likelihood func- tion, we can place a conjugate Gaussian prior on the mean. The reason why the Gaussian likelihood appears twice in the table is that we need to distinguish the univariate from the multivariate case. In the univariate (scalar) case, the inverse Gamma is the conjugate prior for the variance. In the multivariate case, we use a conjugate inverse Wishart distribution as a prior on the covariance matrix. The Dirichlet distribution is the conju- c⃝2019 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press. 210 Probability and Distributions gate prior for the multinomial likelihood function. For further details, we refer to Bishop (2006). 6.6.2 Sufﬁcient Statistics Recall that a statistic of a random variable is a deterministic function of that random variable. For example, if x = [x1, . . . , xN ] ⊤ is a vector of univariate Gaussian random variables, that is, xn ∼ N ( µ, σ2) , then the sample mean ˆµ = 1 N (x1 + · · · + xN ) is a statistic. Sir Ronald Fisher dis- covered the notion of sufﬁcient statistics: the idea that there are statisticssufﬁcient statistics that will contain all available information that can be inferred from data corresponding to the distribution under consideration. In other words, suf- ﬁcient statistics carry all the information needed to make inference about the population, that is, they are the statistics that are sufﬁcient to repre- sent the distribution. For a set of distributions parametrized by θ, let X be a random variable with distribution p(x | θ0) given an unknown θ0. A vector φ(x) of statistics is called sufﬁcient statistics for θ0 if they contain all possible informa- tion about θ0. To be more formal about “contain all possible information”, this means that the probability of x given θ can be factored into a part that does not depend on θ, and a part that depends on θ only via φ(x). The Fisher-Neyman factorization theorem formalizes this notion, which we state in Theorem 6.14 without proof. Theorem 6.14 (Fisher-Neyman). [Theorem 6.5 in Lehmann and Casella (1998)] Let X have probability density function p(x | θ). Then the statisticsFisher-Neyman theorem φ(x) are sufﬁcient for θ if and only if p(x | θ) can be written in the form p(x | θ) = h(x)gθ(φ(x)) , (6.106) where h(x) is a distribution independent of θ and gθ captures all the depen- dence on θ via sufﬁcient statistics φ(x). If p(x | θ) does not depend on θ, then φ(x) is trivially a sufﬁcient statistic for any function φ. The more interesting case is that p(x | θ) is dependent only on φ(x) and not x itself. In this case, φ(x) is a sufﬁcient statistic for θ. In machine learning, we consider a ﬁnite number of samples from a distribution. One could imagine that for simple distributions (such as the Bernoulli in Example 6.8) we only need a small number of samples to estimate the parameters of the distributions. We could also consider the opposite problem: If we have a set of data (a sample from an unknown distribution), which distribution gives the best ﬁt? A natural question to ask is, as we observe more data, do we need more parameters θ to de- scribe the distribution? It turns out that the answer is yes in general, and this is studied in non-parametric statistics (Wasserman, 2007). A converse question is to consider which class of distributions have ﬁnite-dimensional Draft (2019-12-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com. 6.6 Conjugacy and the Exponential Family 211 sufﬁcient statistics, that is the number of parameters needed to describe them does not increase arbitrarily. The answer is exponential family dis- tributions, described in the following section. 6.6.3 Exponential Family There are three possible levels of abstraction we can have when con- sidering distributions (of discrete or continuous random variables). At level one (the most concrete end of the spectrum), we have a particu- lar named distribution with ﬁxed parameters, for example a univariate Gaussian N (0, 1 ) with zero mean and unit variance. In machine learning, we often use the second level of abstraction, that is, we ﬁx the paramet- ric form (the univariate Gaussian) and infer the parameters from data. For example, we assume a univariate Gaussian N ( µ, σ2) with unknown mean µ and unknown variance σ2, and use a maximum likelihood ﬁt to deter- mine the best parameters (µ, σ2). We will see an example of this when considering linear regression in Chapter 9. A third level of abstraction is to consider families of distributions, and in this book, we consider the ex- ponential family. The univariate Gaussian is an example of a member of the exponential family. Many of the widely used statistical models, includ- ing all the “named” models in Table 6.2, are members of the exponential family. They can all be uniﬁed into one concept (Brown, 1986). Remark. A brief historical anecdote: Like many concepts in mathemat- ics and science, exponential families were independently discovered at the same time by different researchers. In the years 1935–1936, Edwin Pitman in Tasmania, Georges Darmois in Paris, and Bernard Koopman in New York independently showed that the exponential families are the only families that enjoy ﬁnite-dimensional sufﬁcient statistics under repeated independent sampling (Lehmann and Casella, 1998). ♦ An exponential family is a family of probability distributions, parame- exponential family terized by θ ∈ RD, of the form p(x | θ) = h(x) exp (⟨θ, φ(x)⟩ − A(θ)) , (6.107) where φ(x) is the vector of sufﬁcient statistics. In general, any inner prod- uct (Section 3.2) can be used in (6.107), and for concreteness we will use the standard dot product here (⟨θ, φ(x)⟩ = θ⊤φ(x)). Note that the form of the exponential family is essentially a particular expression of gθ(φ(x)) in the Fisher-Neyman theorem (Theorem 6.14). The factor h(x) can be absorbed into the dot product term by adding another entry (log h(x)) to the vector of sufﬁcient statistics φ(x), and constraining the corresponding parameter θ0 = 1. The term A(θ) is the normalization constant that ensures that the distribution sums up or inte- grates to one and is called the log-partition function. A good intuitive no- log-partition functiontion of exponential families can be obtained by ignoring these two terms c⃝2019 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press. 212 Probability and Distributions and considering exponential families as distributions of the form p(x | θ) ∝ exp ( θ⊤φ(x) ) . (6.108) For this form of parametrization, the parameters θ are called the naturalnatural parameters parameters. At ﬁrst glance, it seems that exponential families are a mun- dane transformation by adding the exponential function to the result of a dot product. However, there are many implications that allow for conve- nient modeling and efﬁcient computation based on the fact that we can capture information about data in φ(x). Example 6.13 (Gaussian as Exponential Family) Consider the univariate Gaussian distribution N (µ, σ2). Let φ(x) = [ x x2 ] . Then by using the deﬁnition of the exponential family, p(x | θ) ∝ exp(θ1x + θ2x2) . (6.109) Setting θ = [ µ σ2 , − 1 2σ2 ]⊤ (6.110) and substituting into (6.109), we obtain p(x | θ) ∝ exp ( µx σ2 − x2 2σ2 ) ∝ exp ( − 1 2σ2 (x − µ) 2) . (6.111) Therefore, the univariate Gaussian distribution is a member of the expo- nential family with sufﬁcient statistic φ(x) = [ x x2 ] , and natural parame- ters given by θ in (6.110). Example 6.14 (Bernoulli as Exponential Family) Recall the Bernoulli distribution from Example 6.8 p(x | µ) = µx(1 − µ)1−x , x ∈ {0, 1}. (6.112) This can be written in exponential family form p(x | µ) = exp [ log (µx(1 − µ) 1−x)] (6.113a) = exp [x log µ + (1 − x) log(1 − µ)] (6.113b) = exp [x log µ − x log(1 − µ) + log(1 − µ)] (6.113c) = exp [ x log µ 1−µ + log(1 − µ)] . (6.113d) The last line (6.113d) can be identiﬁed as being in exponential family form (6.107) by observing that h(x) = 1 (6.114) Draft (2019-12-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com. 6.6 Conjugacy and the Exponential Family 213 θ = log µ 1−µ (6.115) φ(x) = x (6.116) A(θ) = − log(1 − µ) = log(1 + exp(θ)). (6.117) The relationship between θ and µ is invertible so that µ = 1 1 + exp(−θ) . (6.118) The relation (6.118) is used to obtain the right equality of (6.117). Remark. The relationship between the original Bernoulli parameter µ and the natural parameter θ is known as the sigmoid or logistic function. Ob- sigmoid serve that µ ∈ (0, 1) but θ ∈ R, and therefore the sigmoid function squeezes a real value into the range (0, 1). This property is useful in ma- chine learning, for example it is used in logistic regression (Bishop, 2006, section 4.3.2), as well as as a nonlinear activation functions in neural net- works (Goodfellow et al., 2016, chapter 6). ♦ It is often not obvious how to ﬁnd the parametric form of the conjugate distribution of a particular distribution (for example, those in Table 6.2). Exponential families provide a convenient way to ﬁnd conjugate pairs of distributions. Consider the random variable X is a member of the expo- nential family (6.107): p(x | θ) = h(x) exp (⟨θ, φ(x)⟩ − A(θ)) . (6.119) Every member of the exponential family has a conjugate prior (Brown, 1986) p(θ | γ) = hc(θ) exp (〈[ γ1 γ2 ] , [ θ −A(θ) ]〉 − Ac(γ)) , (6.120) where γ = [ γ1 γ2 ] has dimension dim(θ) + 1. The sufﬁcient statistics of the conjugate prior are [ θ −A(θ) ] . By using the knowledge of the general form of conjugate priors for exponential families, we can derive functional forms of conjugate priors corresponding to particular distributions. Example 6.15 Recall the exponential family form of the Bernoulli distribution (6.113d) p(x | µ) = exp [ x log µ 1 − µ + log(1 − µ)] . (6.121) c⃝2019 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press. 214 Probability and Distributions The canonical conjugate prior has the form p(µ | α, β) = µ 1 − µ exp [ α log µ 1 − µ + (β + α) log(1 − µ) − Ac(γ)] , (6.122) where we deﬁned γ := [α, β + α] ⊤ and hc(µ) := µ/(1 − µ). Equa- tion (6.122) then simpliﬁes to p(µ | α, β) = exp [(α − 1) log µ + (β − 1) log(1 − µ) − Ac(α, β)] . (6.123) Putting this in non-exponential family form yields p(µ | α, β) ∝ µα−1(1 − µ)β−1 , (6.124) which we identify as the Beta distribution (6.98). In example 6.12, we assumed that the Beta distribution is the conjugate prior of the Bernoulli distribution and showed that it was indeed the conjugate prior. In this example, we derived the form of the Beta distribution by looking at the canonical conjugate prior of the Bernoulli distribution in exponential fam- ily form. As mentioned in the previous section, the main motivation for expo- nential families is that they have ﬁnite-dimensional sufﬁcient statistics. Additionally, conjugate distributions are easy to write down, and the con- jugate distributions also come from an exponential family. From an infer- ence perspective, maximum likelihood estimation behaves nicely because empirical estimates of sufﬁcient statistics are optimal estimates of the pop- ulation values of sufﬁcient statistics (recall the mean and covariance of a Gaussian). From an optimization perspective, the log-likelihood function is concave, allowing for efﬁcient optimization approaches to be applied (Chapter 7). 6.7 Change of Variables/Inverse Transform It may seem that there are very many known distributions, but in reality the set of distributions for which we have names is quite limited. There- fore, it is often useful to understand how transformed random variables are distributed. For example, assuming that X is a random variable dis- tributed according to the univariate normal distribution N ( 0, 1) , what is the distribution of X 2? Another example, which is quite common in ma- chine learning, is, given that X1 and X2 are univariate standard normal, what is the distribution of 1 2 (X1 + X2)? One option to work out the distribution of 1 2 (X1 + X2) is to calculate the mean and variance of X1 and X2 and then combine them. As we saw in Section 6.4.4, we can calculate the mean and variance of resulting ran- dom variables when we consider afﬁne transformations of random vari- Draft (2019-12-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com. 6.7 Change of Variables/Inverse Transform 215 ables. However, we may not be able to obtain the functional form of the distribution under transformations. Furthermore, we may be interested in nonlinear transformations of random variables for which closed-form expressions are not readily available. Remark (Notation). In this section, we will be explicit about random vari- ables and the values they take. Hence, recall that we use capital letters X, Y to denote random variables and small letters x, y to denote the val- ues in the target space T that the random variables take. We will explicitly write pmfs of discrete random variables X as P (X = x). For continuous random variables X (Section 6.2.2), the pdf is written as f (x) and the cdf is written as FX(x). ♦ We will look at two approaches for obtaining distributions of transfor- mations of random variables: a direct approach using the deﬁnition of a cumulative distribution function and a change-of-variable approach that uses the chain rule of calculus (Section 5.2.2). The change-of-variable ap- Moment generating functions can also be used to study transformations of random variables (Casella and Berger, 2002, chapter 2). proach is widely used because it provides a “recipe” for attempting to compute the resulting distribution due to a transformation. We will ex- plain the techniques for univariate random variables, and will only brieﬂy provide the results for the general case of multivariate random variables. Transformations of discrete random variables can be understood di- rectly. Suppose that there is a discrete random variable X with pmf P (X = x) (Section 6.2.1), and an invertible function U (x). Consider the trans- formed random variable Y := U (X), with pmf P (Y = y). Then P (Y = y) = P (U (X) = y) transformation of interest (6.125a) = P (X = U −1(y)) inverse (6.125b) where we can observe that x = U −1(y). Therefore, for discrete random variables, transformations directly change the individual events (with the probabilities appropriately transformed). 6.7.1 Distribution Function Technique The distribution function technique goes back to ﬁrst principles, and uses the deﬁnition of a cdf FX(x) = P (X ⩽ x) and the fact that its differential is the pdf f (x) (Wasserman, 2004, chapter 2). For a random variable X and a function U , we ﬁnd the pdf of the random variable Y := U (X) by 1. Finding the cdf: FY (y) = P (Y ⩽ y) (6.126) 2. Differentiating the cdf FY (y) to get the pdf f (y). f (y) = d dy FY (y) . (6.127) c⃝2019 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press. 216 Probability and Distributions We also need to keep in mind that the domain of the random variable may have changed due to the transformation by U . Example 6.16 Let X be a continuous random variable with probability density function on 0 ⩽ x ⩽ 1 f (x) = 3x2 . (6.128) We are interested in ﬁnding the pdf of Y = X 2. The function f is an increasing function of x, and therefore the resulting value of y lies in the interval [0, 1]. We obtain FY (y) = P (Y ⩽ y) deﬁnition of cdf (6.129a) = P (X 2 ⩽ y) transformation of interest (6.129b) = P (X ⩽ y 1 2 ) inverse (6.129c) = FX(y 1 2 ) deﬁnition of cdf (6.129d) = ∫ y 1 2 0 3t 2dt cdf as a deﬁnite integral (6.129e) = [ t 3]t=y 1 2 t=0 result of integration (6.129f) = y 3 2 , 0 ⩽ y ⩽ 1 . (6.129g) Therefore, the cdf of Y is FY (y) = y 3 2 (6.130) for 0 ⩽ y ⩽ 1. To obtain the pdf, we differentiate the cdf f (y) = d dy FY (y) = 3 2 y 1 2 (6.131) for 0 ⩽ y ⩽ 1. In Example 6.16, we considered a strictly monotonically increasing func- tion f (x) = 3x2. This means that we could compute an inverse function.Functions that have inverses are called bijective functions (Section 2.7). In general, we require that the function of interest y = U (x) has an in- verse x = U −1(y). A useful result can be obtained by considering the cu- mulative distribution function FX(x) of a random variable X, and using it as the transformation U (x). This leads to the following theorem. Theorem 6.15. [Theorem 2.1.10 in Casella and Berger (2002)] Let X be a continuous random variable with a strictly monotonic cumulative distribu- tion function FX(x). Then the random variable Y deﬁned as Y := FX(x) (6.132) has a uniform distribution. Draft (2019-12-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com. 6.7 Change of Variables/Inverse Transform 217 Theorem 6.15 is known as the probability integral transform, and it is probability integral transformused to derive algorithms for sampling from distributions by transforming the result of sampling from a uniform random variable (Bishop, 2006). The algorithm works by ﬁrst generating a sample from a uniform distribu- tion, then transforming it by the inverse cdf (assuming this is available) to obtain a sample from the desired distribution. The probability integral transform is also used for hypothesis testing whether a sample comes from a particular distribution (Lehmann and Romano, 2005). The idea that the output of a cdf gives a uniform distribution also forms the basis of copu- las (Nelsen, 2006). 6.7.2 Change of Variables The distribution function technique in Section 6.7.1 is derived from ﬁrst principles, based on the deﬁnitions of cdfs and using properties of in- verses, differentiation, and integration. This argument from ﬁrst principles relies on two facts: 1. We can transform the cdf of Y into an expression that is a cdf of X. 2. We can differentiate the cdf to obtain the pdf. Let us break down the reasoning step by step, with the goal of understand- ing the more general change-of-variables approach in Theorem 6.16. Change of variables in probability relies on the change-of-variables method in calculus (Tandra, 2014). Remark. The name “change of variables” comes from the idea of chang- ing the variable of integration when faced with a difﬁcult integral. For univariate functions, we use the substitution rule of integration, ∫ f (g(x))g′(x)dx = ∫ f (u)du , where u = g(x) . (6.133) The derivation of this rule is based on the chain rule of calculus (5.32) and by applying twice the fundamental theorem of calculus. The fundamental theorem of calculus formalizes the fact that integration and differentiation are somehow “inverses” of each other. An intuitive understanding of the rule can be obtained by thinking (loosely) about small changes (differen- tials) to the equation u = g(x), that is by considering ∆u = g′(x)∆x as a differential of u = g(x). By subsituting u = g(x), the argument inside the integral on the right-hand side of (6.133) becomes f (g(x)). By pretending that the term du can be approximated by du ≈ ∆u = g′(x)∆x, and that dx ≈ ∆x, we obtain (6.133). ♦ Consider a univariate random variable X, and an invertible function U , which gives us another random variable Y = U (X). We assume that random variable X has states x ∈ [a, b]. By the deﬁnition of the cdf, we have FY (y) = P (Y ⩽ y) . (6.134) c⃝2019 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press. 218 Probability and Distributions We are interested in a function U of the random variable P (Y ⩽ y) = P (U (X) ⩽ y) , (6.135) where we assume that the function U is invertible. An invertible function on an interval is either strictly increasing or strictly decreasing. In the case that U is strictly increasing, then its inverse U −1 is also strictly increasing. By applying the inverse U −1 to the arguments of P (U (X) ⩽ y), we obtain P (U (X) ⩽ y) = P (U −1(U (X)) ⩽ U −1(y)) = P (X ⩽ U −1(y)) . (6.136) The right-most term in (6.136) is an expression of the cdf of X. Recall the deﬁnition of the cdf in terms of the pdf P (X ⩽ U −1(y)) = ∫ U −1(y) a f (x)dx . (6.137) Now we have an expression of the cdf of Y in terms of x: FY (y) = ∫ U −1(y) a f (x)dx . (6.138) To obtain the pdf, we differentiate (6.138) with respect to y: f (y) = d dy Fy(y) = d dy ∫ U −1(y) a f (x)dx . (6.139) Note that the integral on the right-hand side is with respect to x, but we need an integral with respect to y because we are differentiating with respect to y. In particular, we use (6.133) to get the substitution ∫ f (U −1(y))U −1′(y)dy = ∫ f (x)dx where x = U −1(y) . (6.140) Using (6.140) on the right-hand side of (6.139) gives us f (y) = d dy ∫ U −1(y) a fx(U −1(y))U −1′(y)dy . (6.141) We then recall that differentiation is a linear operator and we use the subscript x to remind ourselves that fx(U −1(y)) is a function of x and not y. Invoking the fundamental theorem of calculus again gives us f (y) = fx(U −1(y)) · ( d dy U −1(y) ) . (6.142) Recall that we assumed that U is a strictly increasing function. For decreas- ing functions, it turns out that we have a negative sign when we follow the same derivation. We introduce the absolute value of the differential to have the same expression for both increasing and decreasing U : f (y) = fx(U −1(y)) · ∣ ∣ ∣ ∣ d dy U −1(y) ∣ ∣ ∣ ∣ . (6.143) Draft (2019-12-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com. 6.7 Change of Variables/Inverse Transform 219 This is called the change-of-variable technique. The term ∣ ∣ ∣ d dy U −1(y)∣ ∣ ∣ in change-of-variable technique(6.143) measures how much a unit volume changes when applying U (see also the deﬁnition of the Jacobian in Section 5.3). Remark. In comparison to the discrete case in (6.125b), we have an addi- tional factor ∣ ∣ ∣ d dy U −1(y) ∣ ∣ ∣. The continuous case requires more care because P (Y = y) = 0 for all y. The probability density function f (y) does not have a description as a probability of an event involving y. ♦ So far in this section, we have been studying univariate change of vari- ables. The case for multivariate random variables is analogous, but com- plicated by fact that the absolute value cannot be used for multivariate functions. Instead, we use the determinant of the Jacobian matrix. Recall from (5.58) that the Jacobian is a matrix of partial derivatives, and that the existence of a nonzero determinant shows that we can invert the Ja- cobian. Recall the discussion in Section 4.1 that the determinant arises because our differentials (cubes of volume) are transformed into paral- lelepipeds by the Jacobian. Let us summarize preceding the discussion in the following theorem, which gives us a recipe for multivariate change of variables. Theorem 6.16. [Theorem 17.2 in Billingsley (1995)] Let f (x) be the value of the probability density of the multivariate continuous random variable X. If the vector-valued function y = U (x) is differentiable and invertible for all values within the domain of x, then for corresponding values of y, the probability density of Y = U (X) is given by f (y) = fx(U −1(y)) · ∣ ∣ ∣ ∣det ( ∂ ∂y U −1(y) )∣ ∣ ∣ ∣ . (6.144) The theorem looks intimidating at ﬁrst glance, but the key point is that a change of variable of a multivariate random variable follows the pro- cedure of the univariate change of variable. First we need to work out the inverse transform, and substitute that into the density of x. Then we calculate the determinant of the Jacobian and multiply the result. The following example illustrates the case of a bivariate random variable. Example 6.17 Consider a bivariate random variable X with states x = [ x1 x2 ] and proba- bility density function f ([ x1 x2 ]) = 1 2π exp ( − 1 2 [ x1 x2 ]⊤ [ x1 x2 ]) . (6.145) We use the change-of-variable technique from Theorem 6.16 to derive the c⃝2019 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press. 220 Probability and Distributions effect of a linear transformation (Section 2.7) of the random variable. Consider a matrix A ∈ R 2×2 deﬁned as A = [ a b c d ] . (6.146) We are interested in ﬁnding the probability density function of the trans- formed bivariate random variable Y with states y = Ax. Recall that for change of variables we require the inverse transformation of x as a function of y. Since we consider linear transformations, the inverse transformation is given by the matrix inverse (see Section 2.2.2). For 2 × 2 matrices, we can explicitly write out the formula, given by [ x1 x2 ] = A−1 [ y1 y2 ] = 1 ad − bc [ d −b −c a ] [ y1 y2 ] . (6.147) Observe that ad − bc is the determinant (Section 4.1) of A. The corre- sponding probability density function is given by f (x) = f (A−1y) = 1 2π exp ( − 1 2 y⊤A−⊤A−1y) . (6.148) The partial derivative of a matrix times a vector with respect to the vector is the matrix itself (Section 5.5), and therefore ∂ ∂y A−1y = A −1 . (6.149) Recall from Section 4.1 that the determinant of the inverse is the inverse of the determinant so that the determinant of the Jacobian matrix is det ( ∂ ∂y A −1y) = 1 ad − bc . (6.150) We are now able to apply the change-of-variable formula from Theo- rem 6.16 by multiplying (6.148) with (6.150), which yields f (y) = f (x) ∣ ∣ ∣ ∣det ( ∂ ∂y A−1y)∣ ∣ ∣ ∣ (6.151a) = 1 2π exp (− 1 2 y⊤A−⊤A−1y) |ad − bc| −1. (6.151b) While Example 6.17 is based on a bivariate random variable, which al- lows us to easily compute the matrix inverse, the preceding relation holds for higher dimensions. Remark. We saw in Section 6.5 that the density f (x) in (6.148) is actually the standard Gaussian distribution, and the transformed density f (y) is a bivariate Gaussian with covariance Σ = AA⊤. ♦ We will use the ideas in this chapter to describe probabilistic modeling Draft (2019-12-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com. 6.8 Further Reading 221 in Section 8.4, as well as introduce a graphical language in Section 8.5. We will see direct machine learning applications of these ideas in Chapters 9 and 11. 6.8 Further Reading This chapter is rather terse at times. Grinstead and Snell (1997) and Walpole et al. (2011) provide more relaxed presentations that are suit- able for self-study. Readers interested in more philosophical aspects of probability should consider Hacking (2001), whereas an approach that is more related to software engineering is presented by Downey (2014). An overview of exponential families can be found in Barndorff-Nielsen (2014). We will see more about how to use probability distributions to model machine learning tasks in Chapter 8. Ironically, the recent surge in interest in neural networks has resulted in a broader appreciation of probabilistic models. For example, the idea of normalizing ﬂows (Jimenez Rezende and Mohamed, 2015) relies on change of variables for transform- ing random variables. An overview of methods for variational inference as applied to neural networks is described in chapters 16 to 20 of the book by Goodfellow et al. (2016). We side stepped a large part of the difﬁculty in continuous random vari- ables by avoiding measure theoretic questions (Billingsley, 1995; Pollard, 2002), and by assuming without construction that we have real numbers, and ways of deﬁning sets on real numbers as well as their appropriate fre- quency of occurrence. These details do matter, for example, in the speciﬁ- cation of conditional probability p(y | x) for continuous random variables x, y (Proschan and Presnell, 1998). The lazy notation hides the fact that we want to specify that X = x (which is a set of measure zero). Fur- thermore, we are interested in the probability density function of y. A more precise notation would have to say Ey[f (y) | σ(x)], where we take the expectation over y of a test function f conditioned on the σ-algebra of x. A more technical audience interested in the details of probability the- ory have many options (Jaynes, 2003; MacKay, 2003; Jacod and Protter, 2004; Grimmett and Welsh, 2014), including some very technical discus- sions (Shiryayev, 1984; Lehmann and Casella, 1998; Dudley, 2002; Bickel and Doksum, 2006; C¸inlar, 2011). An alternative way to approach proba- bility is to start with the concept of expectation, and “work backward” to derive the necessary properties of a probability space (Whittle, 2000). As machine learning allows us to model more intricate distributions on ever more complex types of data, a developer of probabilistic machine learn- ing models would have to understand these more technical aspects. Ma- chine learning texts with a probabilistic modeling focus include the books by MacKay (2003); Bishop (2006); Rasmussen and Williams (2006); Bar- ber (2012); Murphy (2012). c⃝2019 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press. 222 Probability and Distributions Exercises 6.1 Consider the following bivariate distribution p(x, y) of two discrete random variables X and Y . X x1 x2 x3 x4 x5 Y y3 y2 y1 0.01 0.02 0.03 0.1 0.1 0.05 0.1 0.05 0.07 0.2 0.1 0.05 0.03 0.05 0.04 Compute: a. The marginal distributions p(x) and p(y). b. The conditional distributions p(x|Y = y1) and p(y|X = x3). 6.2 Consider a mixture of two Gaussian distributions (illustrated in Figure 6.4), 0.4 N ([ 10 2 ] , [ 1 0 0 1 ]) + 0.6 N ([ 0 0 ] , [ 8.4 2.0 2.0 1.7 ]) . a. Compute the marginal distributions for each dimension. b. Compute the mean, mode and median for each marginal distribution. c. Compute the mean and mode for the two-dimensional distribution. 6.3 You have written a computer program that sometimes compiles and some- times not (code does not change). You decide to model the apparent stochas- ticity (success vs. no success) x of the compiler using a Bernoulli distribution with parameter µ: p(x | µ) = µx(1 − µ) 1−x , x ∈ {0, 1} . Choose a conjugate prior for the Bernoulli likelihood and compute the pos- terior distribution p(µ | x1, . . . , xN ). 6.4 There are two bags. The ﬁrst bag contains four mangos and two apples; the second bag contains four mangos and four apples. We also have a biased coin, which shows “heads” with probability 0.6 and “tails” with probability 0.4. If the coin shows “heads”. we pick a fruit at random from bag 1; otherwise we pick a fruit at random from bag 2. Your friend ﬂips the coin (you cannot see the result), picks a fruit at random from the corresponding bag, and presents you a mango. What is the probability that the mango was picked from bag 2? Hint: Use Bayes’ theorem. 6.5 Consider the time-series model xt+1 = Axt + w , w ∼ N (0, Q) yt = Cxt + v , v ∼ N (0, R) , where w, v are i.i.d. Gaussian noise variables. Further, assume that p(x0) = N (µ0, Σ0). Draft (2019-12-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com. Exercises 223 a. What is the form of p(x0, x1, . . . , xT )? Justify your answer (you do not have to explicitly compute the joint distribution). b. Assume that p(xt | y1, . . . , yt) = N (µt, Σt). 1. Compute p(xt+1 | y1, . . . , yt). 2. Compute p(xt+1, yt+1 | y1, . . . , yt). 3. At time t+1, we observe the value yt+1 = ˆy. Compute the conditional distribution p(xt+1 | y1, . . . , yt+1). 6.6 Prove the relationship in (6.44), which relates the standard deﬁnition of the variance to the raw-score expression for the variance. 6.7 Prove the relationship in (6.45), which relates the pairwise difference be- tween examples in a dataset with the raw-score expression for the variance. 6.8 Express the Bernoulli distribution in the natural parameter form of the ex- ponential family, see (6.107). 6.9 Express the Binomial distribution as an exponential family distribution. Also express the Beta distribution is an exponential family distribution. Show that the product of the Beta and the Binomial distribution is also a member of the exponential family. 6.10 Derive the relationship in Section 6.5.2 in two ways: a. By completing the square b. By expressing the Gaussian in its exponential family form The product of two Gaussians N (x | a, A )N (x | b, B) is an unnormalized Gaussian distribution c N (x | c, C) with C = (A −1 + B−1)−1 c = C(A −1a + B−1b) c = (2π)− D 2 | A + B | − 1 2 exp ( − 1 2 (a − b)⊤(A + B)−1(a − b)) . Note that the normalizing constant c itself can be considered a (normalized) Gaussian distribution either in a or in b with an “inﬂated” covariance matrix A + B, i.e., c = N (a | b, A + B) = N (b | a, A + B). 6.11 Iterated Expectations. Consider two random variables x, y with joint distribution p(x, y). Show that EX [x] = EY [EX [x | y]] . Here, EX [x | y] denotes the expected value of x under the conditional distri- bution p(x | y). 6.12 Manipulation of Gaussian Random Variables. Consider a Gaussian random variable x ∼ N (x | µx, Σx), where x ∈ RD. Furthermore, we have y = Ax + b + w , where y ∈ RE, A ∈ RE×D, b ∈ RE, and w ∼ N (w | 0, Q) is indepen- dent Gaussian noise. “Independent” implies that x and w are independent random variables and that Q is diagonal. a. Write down the likelihood p(y | x). b. The distribution p(y) = ∫ p(y | x)p(x)dx is Gaussian. Compute the mean µy and the covariance Σy. Derive your result in detail. c⃝2019 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press. 224 Probability and Distributions c. The random variable y is being transformed according to the measure- ment mapping z = Cy + v , where z ∈ RF , C ∈ RF ×E, and v ∼ N (v | 0, R) is independent Gaus- sian (measurement) noise. Write down p(z | y). Compute p(z), i.e., the mean µz and the covariance Σz. Derive your result in detail. d. Now, a value ˆy is measured. Compute the posterior distribution p(x | ˆy). Hint for solution: This posterior is also Gaussian, i.e., we need to de- termine only its mean and covariance matrix. Start by explicitly com- puting the joint Gaussian p(x, y). This also requires us to compute the cross-covariances Covx,y[x, y] and Covy,x[y, x]. Then apply the rules for Gaussian conditioning. 6.13 Probability Integral Transformation Given a continuous random variable x, with cdf Fx(x), show that the ran- dom variable y = Fx(x) is uniformly distributed. Draft (2019-12-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com. 7 Continuous Optimization Since machine learning algorithms are implemented on a computer, the mathematical formulations are expressed as numerical optimization meth- ods. This chapter describes the basic numerical methods for training ma- chine learning models. Training a machine learning model often boils down to ﬁnding a good set of parameters. The notion of “good” is de- termined by the objective function or the probabilistic model, which we will see examples of in the second part of this book. Given an objective function, ﬁnding the best value is done using optimization algorithms. Since we consider data and models in RD, the optimization problems we face are continuous optimization problems, as opposed to combinatorial optimization problems for discrete variables. This chapter covers two main branches of continuous optimization (Fig- ure 7.1): unconstrained and constrained optimization. We will assume in this chapter that our objective function is differentiable (see Chapter 5), hence we have access to a gradient at each location in the space to help us ﬁnd the optimum value. By convention, most objective functions in ma- chine learning are intended to be minimized, that is, the best value is the minimum value. Intuitively ﬁnding the best value is like ﬁnding the val- leys of the objective function, and the gradients point us uphill. The idea is to move downhill (opposite to the gradient) and hope to ﬁnd the deepest point. For unconstrained optimization, this is the only concept we need, but there are several design choices, which we discuss in Section 7.1. For constrained optimization, we need to introduce other concepts to man- age the constraints (Section 7.2). We will also introduce a special class of problems (convex optimization problems in Section 7.3) where we can make statements about reaching the global optimum. Consider the function in Figure 7.2. The function has a global minimum global minimum around x = −4.5, with a function value of approximately −47. Since the function is “smooth,” the gradients can be used to help ﬁnd the min- imum by indicating whether we should take a step to the right or left. This assumes that we are in the correct bowl, as there exists another local local minimum minimum around x = 0.7. Recall that we can solve for all the stationary points of a function by calculating its derivative and setting it to zero. For Stationary points are the real roots of the derivative, that is, points that have zero gradient. ℓ(x) = x4 + 7x3 + 5x2 − 17x + 3 , (7.1) we obtain the corresponding gradient as dℓ(x) dx = 4x3 + 21x2 + 10x − 17 . (7.2) 225 This material will be published by Cambridge University Press as Mathematics for Machine Learn- ing by Marc Peter Deisenroth, A. Aldo Faisal, and Cheng Soon Ong. This pre-publication version is free to view and download for personal use only. Not for re-distribution, re-sale or use in deriva- tive works. c⃝by M. P. Deisenroth, A. A. Faisal, and C. S. Ong, 2019. https://mml-book.com. 226 Continuous Optimization Figure 7.1 A mind map of the concepts related to optimization, as presented in this chapter. There are two main ideas: gradient descent and convex optimization. Continuous optimization Unconstrained optimization Constrained optimization Gradient descent Stepsize Momentum Stochastic gradient descent Lagrange multipliers Convex optimization & duality Convex Convex conjugate Linear programming Quadratic programming Chapter 10 Dimension reduc. Chapter 11 Density estimation Chapter 12 Classiﬁcation Since this is a cubic equation, it has in general three solutions when set to zero. In the example, two of them are minimums and one is a maximum (around x = −1.4). To check whether a stationary point is a minimum or maximum, we need to take the derivative a second time and check whether the second derivative is positive or negative at the stationary point. In our case, the second derivative is d 2ℓ(x) dx2 = 12x2 + 42x + 10 . (7.3) By substituting our visually estimated values of x = −4.5, −1.4, 0.7, we will observe that as expected the middle point is a maximum ( d 2ℓ(x) dx2 < 0 ) and the other two stationary points are minimums. Note that we have avoided analytically solving for values of x in the previous discussion, although for low-order polynomials such as the pre- ceding we could do so. In general, we are unable to ﬁnd analytic solu- tions, and hence we need to start at some value, say x0 = −6, and follow the negative gradient. The negative gradient indicates that we should go Draft (2019-12-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com. 7.1 Optimization Using Gradient Descent 227 Figure 7.2 Example objective function. Negative gradients are indicated by arrows, and the global minimum is indicated by the dashed blue line. −6 −5 −4 −3 −2 −1 0 1 2 Value of parameter −60 −40 −20 0 20 40 60Objective x4 + 7x3 + 5x2 − 17x + 3 right, but not how far (this is called the step-size). Furthermore, if we According to the Abel–Rufﬁni theorem, there is in general no algebraic solution for polynomials of degree 5 or more (Abel, 1826). had started at the right side (e.g., x0 = 0) the negative gradient would have led us to the wrong minimum. Figure 7.2 illustrates the fact that for x > −1, the negative gradient points toward the minimum on the right of the ﬁgure, which has a larger objective value. In Section 7.3, we will learn about a class of functions, called convex functions, that do not exhibit this tricky dependency on the starting point of the optimization algorithm. For convex functions, all local minimums are global minimum. It turns out that many machine learning objective For convex functions all local minima are global minimum. functions are designed such that they are convex, and we will see an ex- ample in Chapter 12. The discussion in this chapter so far was about a one-dimensional func- tion, where we are able to visualize the ideas of gradients, descent direc- tions, and optimal values. In the rest of this chapter we develop the same ideas in high dimensions. Unfortunately, we can only visualize the con- cepts in one dimension, but some concepts do not generalize directly to higher dimensions, therefore some care needs to be taken when reading. 7.1 Optimization Using Gradient Descent We now consider the problem of solving for the minimum of a real-valued function min x f (x) , (7.4) c⃝2019 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press. 228 Continuous Optimization where f : Rd → R is an objective function that captures the machine learning problem at hand. We assume that our function f is differentiable, and we are unable to analytically ﬁnd a solution in closed form. Gradient descent is a ﬁrst-order optimization algorithm. To ﬁnd a local minimum of a function using gradient descent, one takes steps propor- tional to the negative of the gradient of the function at the current point. Recall from Section 5.1 that the gradient points in the direction of theWe use the convention of row vectors for gradients. steepest ascent. Another useful intuition is to consider the set of lines where the function is at a certain value (f (x) = c for some value c ∈ R), which are known as the contour lines. The gradient points in a direction that is orthogonal to the contour lines of the function we wish to optimize. Let us consider multivariate functions. Imagine a surface (described by the function f (x)) with a ball starting at a particular location x0. When the ball is released, it will move downhill in the direction of steepest de- scent. Gradient descent exploits the fact that f (x0) decreases fastest if one moves from x0 in the direction of the negative gradient −((∇f )(x0)) ⊤ of f at x0. We assume in this book that the functions are differentiable, and refer the reader to more general settings in Section 7.4. Then, if x1 = x0 − γ((∇f )(x0)) ⊤ (7.5) for a small step-size γ ⩾ 0, then f (x1) ⩽ f (x0). Note that we use the transpose for the gradient since otherwise the dimensions will not work out. This observation allows us to deﬁne a simple gradient descent algo- rithm: If we want to ﬁnd a local optimum f (x∗) of a function f : R n → R, x ↦→ f (x), we start with an initial guess x0 of the parameters we wish to optimize and then iterate according to xi+1 = xi − γi((∇f )(xi)) ⊤ . (7.6) For suitable step-size γi, the sequence f (x0) ⩾ f (x1) ⩾ . . . converges to a local minimum. Example 7.1 Consider a quadratic function in two dimensions f ([ x1 x2 ]) = 1 2 [ x1 x2 ]⊤ [ 2 1 1 20 ] [ x1 x2 ] − [ 5 3 ]⊤ [ x1 x2 ] (7.7) with gradient ∇f ([ x1 x2 ]) = [ x1 x2 ]⊤ [ 2 1 1 20 ] − [ 5 3 ]⊤ . (7.8) Starting at the initial location x0 = [−3, −1] ⊤, we iteratively apply (7.6) to obtain a sequence of estimates that converge to the minimum value Draft (2019-12-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com. 7.1 Optimization Using Gradient Descent 229 Figure 7.3 Gradient descent on a two-dimensional quadratic surface (shown as a heatmap). See Example 7.1 for a description. −4 −2 0 2 4 x1 −2 −1 0 1 2x2 0.0 10.0 20.0 30.0 40.0 40.0 50.0 50.0 60.0 70.0 80.0 −15 0 15 30 45 60 75 90 (illustrated in Figure 7.3). We can see (both from the ﬁgure and by plug- ging x0 into (7.8) with γ = 0.085) that the negative gradient at x0 points north and east, leading to x1 = [−1.98, 1.21] ⊤. Repeating that argument gives us x2 = [−1.32, −0.42]⊤, and so on. Remark. Gradient descent can be relatively slow close to the minimum: Its asymptotic rate of convergence is inferior to many other methods. Us- ing the ball rolling down the hill analogy, when the surface is a long, thin valley, the problem is poorly conditioned (Trefethen and Bau III, 1997). For poorly conditioned convex problems, gradient descent increasingly “zigzags” as the gradients point nearly orthogonally to the shortest di- rection to a minimum point; see Figure 7.3. ♦ 7.1.1 Step-size As mentioned earlier, choosing a good step-size is important in gradient descent. If the step-size is too small, gradient descent can be slow. If the The step-size is also called the learning rate. step-size is chosen too large, gradient descent can overshoot, fail to con- verge, or even diverge. We will discuss the use of momentum in the next section. It is a method that smoothes out erratic behavior of gradient up- dates and dampens oscillations. Adaptive gradient methods rescale the step-size at each iteration, de- pending on local properties of the function. There are two simple heuris- tics (Toussaint, 2012): When the function value increases after a gradient step, the step-size was too large. Undo the step and decrease the step-size. When the function value decreases the step could have been larger. Try to increase the step-size. c⃝2019 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press. 230 Continuous Optimization Although the “undo” step seems to be a waste of resources, using this heuristic guarantees monotonic convergence. Example 7.2 (Solving a Linear Equation System) When we solve linear equations of the form Ax = b, in practice we solve Ax−b = 0 approximately by ﬁnding x∗ that minimizes the squared error ∥Ax − b∥2 = (Ax − b) ⊤(Ax − b) (7.9) if we use the Euclidean norm. The gradient of (7.9) with respect to x is ∇x = 2(Ax − b) ⊤A . (7.10) We can use this gradient directly in a gradient descent algorithm. How- ever, for this particular special case, it turns out that there is an analytic solution, which can be found by setting the gradient to zero. We will see more on solving squared error problems in Chapter 9. Remark. When applied to the solution of linear systems of equations Ax = b, gradient descent may converge slowly. The speed of convergence of gra- dient descent is dependent on the condition number κ = σ(A)max σ(A)min , whichcondition number is the ratio of the maximum to the minimum singular value (Section 4.5) of A. The condition number essentially measures the ratio of the most curved direction versus the least curved direction, which corresponds to our imagery that poorly conditioned problems are long, thin valleys: They are very curved in one direction, but very ﬂat in the other. Instead of di- rectly solving Ax = b, one could instead solve P −1(Ax − b) = 0, where P is called the preconditioner. The goal is to design P −1 such that P −1Apreconditioner has a better condition number, but at the same time P −1 is easy to com- pute. For further information on gradient descent, preconditioning, and convergence we refer to Boyd and Vandenberghe (2004, chapter 9). ♦ 7.1.2 Gradient Descent With Momentum As illustrated in Figure 7.3, the convergence of gradient descent may be very slow if the curvature of the optimization surface is such that there are regions that are poorly scaled. The curvature is such that the gradient descent steps hops between the walls of the valley and approaches the optimum in small steps. The proposed tweak to improve convergence is to give gradient descent some memory.Goh (2017) wrote an intuitive blog post on gradient descent with momentum. Gradient descent with momentum (Rumelhart et al., 1986) is a method that introduces an additional term to remember what happened in the previous iteration. This memory dampens oscillations and smoothes out the gradient updates. Continuing the ball analogy, the momentum term emulates the phenomenon of a heavy ball that is reluctant to change di- rections. The idea is to have a gradient update with memory to implement Draft (2019-12-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com. 7.1 Optimization Using Gradient Descent 231 a moving average. The momentum-based method remembers the update ∆xi at each iteration i and determines the next update as a linear combi- nation of the current and previous gradients xi+1 = xi − γi((∇f )(xi)) ⊤ + α∆xi (7.11) ∆xi = xi − xi−1 = α∆xi−1 − γi−1((∇f )(xi−1)) ⊤ , (7.12) where α ∈ [0, 1]. Sometimes we will only know the gradient approxi- mately. In such cases, the momentum term is useful since it averages out different noisy estimates of the gradient. One particularly useful way to obtain an approximate gradient is by using a stochastic approximation, which we discuss next. 7.1.3 Stochastic Gradient Descent Computing the gradient can be very time consuming. However, often it is possible to ﬁnd a “cheap” approximation of the gradient. Approximating the gradient is still useful as long as it points in roughly the same direction as the true gradient. stochastic gradient descentStochastic gradient descent (often shortened as SGD) is a stochastic ap- proximation of the gradient descent method for minimizing an objective function that is written as a sum of differentiable functions. The word stochastic here refers to the fact that we acknowledge that we do not know the gradient precisely, but instead only know a noisy approxima- tion to it. By constraining the probability distribution of the approximate gradients, we can still theoretically guarantee that SGD will converge. In machine learning, given n = 1, . . . , N data points, we often consider objective functions that are the sum of the losses Ln incurred by each example n. In mathematical notation, we have the form L(θ) = N∑ n=1 Ln(θ) , (7.13) where θ is the vector of parameters of interest, i.e., we want to ﬁnd θ that minimizes L. An example from regression (Chapter 9) is the negative log- likelihood, which is expressed as a sum over log-likelihoods of individual examples so that L(θ) = − N∑ n=1 log p(yn|xn, θ) , (7.14) where xn ∈ R D are the training inputs, yn are the training targets, and θ are the parameters of the regression model. Standard gradient descent, as introduced previously, is a “batch” opti- mization method, i.e., optimization is performed using the full training set c⃝2019 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press. 232 Continuous Optimization by updating the vector of parameters according to θi+1 = θi − γi(∇L(θi)) ⊤ = θi − γi N∑ n=1(∇Ln(θi)) ⊤ (7.15) for a suitable step-size parameter γi. Evaluating the sum gradient may re- quire expensive evaluations of the gradients from all individual functions Ln. When the training set is enormous and/or no simple formulas exist, evaluating the sums of gradients becomes very expensive. Consider the term ∑N n=1(∇Ln(θi)) in (7.15), we can reduce the amount of computation by taking a sum over a smaller set of Ln. In contrast to batch gradient descent, which uses all Ln for n = 1, . . . , N , we randomly choose a subset of Ln for mini-batch gradient descent. In the extreme case, we randomly select only a single Ln to estimate the gradient. The key insight about why taking a subset of data is sensible is to realize that for gradient descent to converge, we only require that the gradient is an unbiased estimate of the true gradient. In fact the term ∑N n=1(∇Ln(θi)) in (7.15) is an empirical estimate of the expected value (Section 6.4.1) of the gradient. Therefore, any other unbiased empirical estimate of the ex- pected value, for example using any subsample of the data, would sufﬁce for convergence of gradient descent. Remark. When the learning rate decreases at an appropriate rate, and sub- ject to relatively mild assumptions, stochastic gradient descent converges almost surely to local minimum (Bottou, 1998). ♦ Why should one consider using an approximate gradient? A major rea- son is practical implementation constraints, such as the size of central processing unit (CPU)/graphics processing unit (GPU) memory or limits on computational time. We can think of the size of the subset used to esti- mate the gradient in the same way that we thought of the size of a sample when estimating empirical means (Section 6.4.1). Large mini-batch sizes will provide accurate estimates of the gradient, reducing the variance in the parameter update. Furthermore, large mini-batches take advantage of highly optimized matrix operations in vectorized implementations of the cost and gradient. The reduction in variance leads to more stable conver- gence, but each gradient calculation will be more expensive. In contrast, small mini-batches are quick to estimate. If we keep the mini-batch size small, the noise in our gradient estimate will allow us to get out of some bad local optima, which we may otherwise get stuck in. In machine learning, optimization methods are used for training by min- imizing an objective function on the training data, but the overall goal is to improve generalization performance (Chapter 8). Since the goal in machine learning does not necessarily need a precise estimate of the min- imum of the objective function, approximate gradients using mini-batch approaches have been widely used. Stochastic gradient descent is very effective in large-scale machine learning problems (Bottou et al., 2018), Draft (2019-12-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com. 7.2 Constrained Optimization and Lagrange Multipliers 233 Figure 7.4 Illustration of constrained optimization. The unconstrained problem (indicated by the contour lines) has a minimum on the right side (indicated by the circle). The box constraints (−1 ⩽ x ⩽ 1 and −1 ⩽ y ⩽ 1) require that the optimal solution is within the box, resulting in an optimal value indicated by the star. −3 −2 −1 0 1 2 3 x1 −3 −2 −1 0 1 2 3x2 such as training deep neural networks on millions of images (Dean et al., 2012), topic models (Hoffman et al., 2013), reinforcement learning (Mnih et al., 2015), or training of large-scale Gaussian process models (Hensman et al., 2013; Gal et al., 2014). 7.2 Constrained Optimization and Lagrange Multipliers In the previous section, we considered the problem of solving for the min- imum of a function min x f (x) , (7.16) where f : R D → R. In this section, we have additional constraints. That is, for real-valued functions gi : RD → R for i = 1, . . . , m, we consider the constrained optimization problem (see Figure 7.4 for an illustration) min x f (x) (7.17) subject to gi(x) ⩽ 0 for all i = 1, . . . , m . It is worth pointing out that the functions f and gi could be non-convex in general, and we will consider the convex case in the next section. One obvious, but not very practical, way of converting the constrained problem (7.17) into an unconstrained one is to use an indicator function J(x) = f (x) + m∑ i=1 1(gi(x)) , (7.18) c⃝2019 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press. 234 Continuous Optimization where 1(z) is an inﬁnite step function 1(z) = { 0 if z ⩽ 0 ∞ otherwise . (7.19) This gives inﬁnite penalty if the constraint is not satisﬁed, and hence would provide the same solution. However, this inﬁnite step function is equally difﬁcult to optimize. We can overcome this difﬁculty by introduc- ing Lagrange multipliers. The idea of Lagrange multipliers is to replace theLagrange multiplier step function with a linear function. We associate to problem (7.17) the Lagrangian by introducing the La-Lagrangian grange multipliers λi ⩾ 0 corresponding to each inequality constraint re- spectively (Boyd and Vandenberghe, 2004, chapter 4) so that L(x, λ) = f (x) + m∑ i=1 λigi(x) (7.20a) = f (x) + λ⊤g(x) , (7.20b) where in the last line we have concatenated all constraints gi(x) into a vector g(x), and all the Lagrange multipliers into a vector λ ∈ Rm. We now introduce the idea of Lagrangian duality. In general, duality in optimization is the idea of converting an optimization problem in one set of variables x (called the primal variables), into another optimization problem in a different set of variables λ (called the dual variables). We introduce two different approaches to duality: In this section, we discuss Lagrangian duality; in Section 7.3.3, we discuss Legendre–Fenchel duality. Deﬁnition 7.1. The problem in (7.17) min x f (x) (7.21) subject to gi(x) ⩽ 0 for all i = 1, . . . , m is known as the primal problem, corresponding to the primal variables x.primal problem The associated Lagrangian dual problem is given byLagrangian dual problem max λ∈Rm D(λ) subject to λ ⩾ 0 , (7.22) where λ are the dual variables and D(λ) = minx∈Rd L(x, λ). Remark. In the discussion of Deﬁnition 7.1, we use two concepts that are also of independent interest (Boyd and Vandenberghe, 2004). First is the minimax inequality, which says that for any function withminimax inequality two arguments ϕ(x, y), the maximin is less than the minimax, i.e., max y min x ϕ(x, y) ⩽ min x max y ϕ(x, y) . (7.23) Draft (2019-12-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com. 7.2 Constrained Optimization and Lagrange Multipliers 235 This inequality can be proved by considering the inequality For all x, y min x ϕ(x, y) ⩽ max y ϕ(x, y) . (7.24) Note that taking the maximum over y of the left-hand side of (7.24) main- tains the inequality since the inequality is true for all y. Similarly, we can take the minimum over x of the right-hand side of (7.24) to obtain (7.23). The second concept is weak duality, which uses (7.23) to show that weak duality primal values are always greater than or equal to dual values. This is de- scribed in more detail in (7.27). ♦ Recall that the difference between J(x) in (7.18) and the Lagrangian in (7.20b) is that we have relaxed the indicator function to a linear func- tion. Therefore, when λ ⩾ 0, the Lagrangian L(x, λ) is a lower bound of J(x). Hence, the maximum of L(x, λ) with respect to λ is J(x) = max λ⩾0 L(x, λ) . (7.25) Recall that the original problem was minimizing J(x), min x∈Rd max λ⩾0 L(x, λ) . (7.26) By the minimax inequality (7.23), it follows that swapping the order of the minimum and maximum results in a smaller value, i.e., min x∈Rd max λ⩾0 L(x, λ) ⩾ max λ⩾0 min x∈Rd L(x, λ) . (7.27) This is also known as weak duality. Note that the inner part of the right- weak duality hand side is the dual objective function D(λ) and the deﬁnition follows. In contrast to the original optimization problem, which has constraints, minx∈Rd L(x, λ) is an unconstrained optimization problem for a given value of λ. If solving minx∈Rd L(x, λ) is easy, then the overall problem is easy to solve. The reason is that the outer problem (maximization over λ) is a maximum over a set of afﬁne functions, and hence is a concave function, even though f (·) and gi(·) may be nonconvex. The maximum of a concave function can be efﬁciently computed. Assuming f (·) and gi(·) are differentiable, we ﬁnd the Lagrange dual problem by differentiating the Lagrangian with respect to x, setting the differential to zero, and solving for the optimal value. We will discuss two concrete examples in Sections 7.3.1 and 7.3.2, where f (·) and gi(·) are convex. Remark (Equality Constraints). Consider (7.17) with additional equality constraints min x f (x) subject to gi(x) ⩽ 0 for all i = 1, . . . , m hj(x) = 0 for all j = 1, . . . , n . (7.28) c⃝2019 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press. 236 Continuous Optimization We can model equality constraints by replacing them with two inequality constraints. That is for each equality constraint hj(x) = 0 we equivalently replace it by two constraints hj(x) ⩽ 0 and hj(x) ⩾ 0. It turns out that the resulting Lagrange multipliers are then unconstrained. Therefore, we constrain the Lagrange multipliers corresponding to the inequality constraints in (7.28) to be non-negative, and leave the La- grange multipliers corresponding to the equality constraints unconstrained. ♦ 7.3 Convex Optimization We focus our attention of a particularly useful class of optimization prob- lems, where we can guarantee global optimality. When f (·) is a convex function, and when the constraints involving g(·) and h(·) are convex sets, this is called a convex optimization problem. In this setting, we have strongconvex optimization problem strong duality duality: The optimal solution of the dual problem is the same as the opti- mal solution of the primal problem. The distinction between convex func- tions and convex sets are often not strictly presented in machine learning literature, but one can often infer the implied meaning from context. Deﬁnition 7.2. A set C is a convex set if for any x, y ∈ C and for any scalarconvex set θ with 0 ⩽ θ ⩽ 1, we have θx + (1 − θ)y ∈ C . (7.29) Figure 7.5 Example of a convex set. Convex sets are sets such that a straight line connecting any two ele- ments of the set lie inside the set. Figures 7.5 and 7.6 illustrate convex and nonconvex sets, respectively. Figure 7.6 Example of a nonconvex set. Convex functions are functions such that a straight line between any two points of the function lie above the function. Figure 7.2 shows a non- convex function, and Figure 7.3 shows a convex function. Another convex function is shown in Figure 7.7. Deﬁnition 7.3. Let function f : RD → R be a function whose domain is a convex set. The function f is a convex function if for all x, y in the domain convex function of f , and for any scalar θ with 0 ⩽ θ ⩽ 1, we have f (θx + (1 − θ)y) ⩽ θf (x) + (1 − θ)f (y) . (7.30) Remark. A concave function is the negative of a convex function. ♦ concave function The constraints involving g(·) and h(·) in (7.28) truncate functions at a scalar value, resulting in sets. Another relation between convex functions and convex sets is to consider the set obtained by “ﬁlling in” a convex function. A convex function is a bowl-like object, and we imagine pouring water into it to ﬁll it up. This resulting ﬁlled-in set, called the epigraph ofepigraph the convex function, is a convex set. If a function f : R n → R is differentiable, we can specify convexity in Draft (2019-12-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com. 7.3 Convex Optimization 237 Figure 7.7 Example of a convex function. −3 −2 −1 0 1 2 3 x 0 10 20 30 40y y = 3x2 − 5x + 2 terms of its gradient ∇xf (x) (Section 5.2). A function f (x) is convex if and only if for any two points x, y it holds that f (y) ⩾ f (x) + ∇xf (x) ⊤(y − x) . (7.31) If we further know that a function f (x) is twice differentiable, that is, the Hessian (5.147) exists for all values in the domain of x, then the function f (x) is convex if and only if ∇2 xf (x) is positive semideﬁnite (Boyd and Vandenberghe, 2004). Example 7.3 The negative entropy f (x) = x log2 x is convex for x > 0. A visualization of the function is shown in Figure 7.8, and we can see that the function is convex. To illustrate the previous deﬁnitions of convexity, let us check the calculations for two points x = 2 and x = 4. Note that to prove convexity of f (x) we would need to check for all points x ∈ R. Recall Deﬁnition 7.3. Consider a point midway between the two points (that is θ = 0.5); then the left-hand side is f (0.5 · 2 + 0.5 · 4) = 3 log2 3 ≈ 4.75. The right-hand side is 0.5(2 log2 2) + 0.5(4 log2 4) = 1 + 4 = 5. And therefore the deﬁnition is satisﬁed. Since f (x) is differentiable, we can alternatively use (7.31). Calculating the derivative of f (x), we obtain ∇x(x log2 x) = 1 · log2 x + x · 1 x loge 2 = log2 x + 1 loge 2 . (7.32) Using the same two test points x = 2 and x = 4, the left-hand side of (7.31) is given by f (4) = 8. The right-hand side is f (x) + ∇⊤ x (y − x) = f (2) + ∇f (2) · (4 − 2) (7.33a) = 2 + (1 + 1 loge 2 ) · 2 ≈ 6.9 . (7.33b) c⃝2019 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press. 238 Continuous Optimization Figure 7.8 The negative entropy function (which is convex) and its tangent at x = 2. 0 1 2 3 4 5 x 0 5 10f(x) x log2 x tangent at x = 2 We can check that a function or set is convex from ﬁrst principles by recalling the deﬁnitions. In practice, we often rely on operations that pre- serve convexity to check that a particular function or set is convex. Al- though the details are vastly different, this is again the idea of closure that we introduced in Chapter 2 for vector spaces. Example 7.4 A nonnegative weighted sum of convex functions is convex. Observe that if f is a convex function, and α ⩾ 0 is a nonnegative scalar, then the function αf is convex. We can see this by multiplying α to both sides of the equation in Deﬁnition 7.3, and recalling that multiplying a nonnegative number does not change the inequality. If f1 and f2 are convex functions, then we have by the deﬁnition f1(θx + (1 − θ)y) ⩽ θf1(x) + (1 − θ)f1(y) (7.34) f2(θx + (1 − θ)y) ⩽ θf2(x) + (1 − θ)f2(y) . (7.35) Summing up both sides gives us f1(θx + (1 − θ)y) + f2(θx + (1 − θ)y) ⩽ θf1(x) + (1 − θ)f1(y) + θf2(x) + (1 − θ)f2(y) , (7.36) where the right-hand side can be rearranged to θ(f1(x) + f2(x)) + (1 − θ)(f1(y) + f2(y)) , (7.37) completing the proof that the sum of convex functions is convex. Combining the preceding two facts, we see that αf1(x) + βf2(x) is convex for α, β ⩾ 0. This closure property can be extended using a sim- ilar argument for nonnegative weighted sums of more than two convex functions. Draft (2019-12-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com. 7.3 Convex Optimization 239 Remark. The inequality in (7.30) is sometimes called Jensen’s inequality. Jensen’s inequality In fact, a whole class of inequalities for taking nonnegative weighted sums of convex functions are all called Jensen’s inequality. ♦ In summary, a constrained optimization problem is called a convex opti- convex optimization problemmization problem if min x f (x) subject to gi(x) ⩽ 0 for all i = 1, . . . , m hj(x) = 0 for all j = 1, . . . , n , (7.38) where all functions f (x) and gi(x) are convex functions, and all hj(x) = 0 are convex sets. In the following, we will describe two classes of convex optimization problems that are widely used and well understood. 7.3.1 Linear Programming Consider the special case when all the preceding functions are linear, i.e., min x∈Rd c⊤x (7.39) subject to Ax ⩽ b , where A ∈ R m×d and b ∈ R m. This is known as a linear program. It has d linear program Linear programs are one of the most widely used approaches in industry. variables and m linear constraints. The Lagrangian is given by L(x, λ) = c⊤x + λ⊤(Ax − b) , (7.40) where λ ∈ R m is the vector of non-negative Lagrange multipliers. Rear- ranging the terms corresponding to x yields L(x, λ) = (c + A⊤λ) ⊤x − λ⊤b . (7.41) Taking the derivative of L(x, λ) with respect to x and setting it to zero gives us c + A⊤λ = 0 . (7.42) Therefore, the dual Lagrangian is D(λ) = −λ⊤b. Recall we would like to maximize D(λ). In addition to the constraint due to the derivative of L(x, λ) being zero, we also have the fact that λ ⩾ 0, resulting in the following dual optimization problem It is convention to minimize the primal and maximize the dual. max λ∈Rm − b⊤λ (7.43) subject to c + A⊤λ = 0 λ ⩾ 0 . This is also a linear program, but with m variables. We have the choice of solving the primal (7.39) or the dual (7.43) program depending on c⃝2019 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press. 240 Continuous Optimization whether m or d is larger. Recall that d is the number of variables and m is the number of constraints in the primal linear program. Example 7.5 (Linear Program) Consider the linear program min x∈R2 − [ 5 3 ]⊤ [ x1 x2 ] subject to       2 2 2 −4 −2 1 0 −1 0 1       [ x1 x2 ] ⩽       33 8 5 −1 8       (7.44) with two variables. This program is also shown in Figure 7.9. The objective function is linear, resulting in linear contour lines. The constraint set in standard form is translated into the legend. The optimal value must lie in the shaded (feasible) region, and is indicated by the star. Figure 7.9 Illustration of a linear program. The unconstrained problem (indicated by the contour lines) has a minimum on the right side. The optimal value given the constraints are shown by the star. 0 2 4 6 8 10 12 14 16 x1 0 2 4 6 8 10x2 2x2 ≤ 33 − 2x1 4x2 ≥ 2x1 − 8 x2 ≤ 2x1 − 5 x2 ≥ 1 x2 ≤ 8 Draft (2019-12-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com. 7.3 Convex Optimization 241 7.3.2 Quadratic Programming Consider the case of a convex quadratic objective function, where the con- straints are afﬁne, i.e., min x∈Rd 1 2 x ⊤Qx + c⊤x (7.45) subject to Ax ⩽ b , where A ∈ R m×d, b ∈ R m, and c ∈ Rd. The square symmetric matrix Q ∈ R d×d is positive deﬁnite, and therefore the objective function is convex. This is known as a quadratic program. Observe that it has d variables and m linear constraints. Example 7.6 (Quadratic Program) Consider the quadratic program min x∈R2 1 2 [ x1 x2 ]⊤ [ 2 1 1 4 ] [ x1 x2 ] + [ 5 3 ]⊤ [ x1 x2 ] (7.46) subject to     1 0 −1 0 0 1 0 −1     [ x1 x2 ] ⩽     1 1 1 1     (7.47) of two variables. The program is also illustrated in Figure 7.4. The objec- tive function is quadratic with a positive semideﬁnite matrix Q, resulting in elliptical contour lines. The optimal value must lie in the shaded (feasi- ble) region, and is indicated by the star. The Lagrangian is given by L(x, λ) = 1 2 x⊤Qx + c⊤x + λ⊤(Ax − b) (7.48a) = 1 2 x⊤Qx + (c + A⊤λ) ⊤x − λ⊤b , (7.48b) where again we have rearranged the terms. Taking the derivative of L(x, λ) with respect to x and setting it to zero gives Qx + (c + A ⊤λ) = 0 . (7.49) Assuming that Q is invertible, we get x = −Q −1(c + A⊤λ) . (7.50) Substituting (7.50) into the primal Lagrangian L(x, λ), we get the dual Lagrangian D(λ) = − 1 2 (c + A⊤λ) ⊤Q−1(c + A⊤λ) − λ⊤b . (7.51) c⃝2019 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press. 242 Continuous Optimization Therefore, the dual optimization problem is given by max λ∈Rm − 1 2 (c + A⊤λ)⊤Q−1(c + A⊤λ) − λ⊤b subject to λ ⩾ 0 . (7.52) We will see an application of quadratic programming in machine learning in Chapter 12. 7.3.3 Legendre–Fenchel Transform and Convex Conjugate Let us revisit the idea of duality from Section 7.2, without considering constraints. One useful fact about a convex set is that it can be equiva- lently described by its supporting hyperplanes. A hyperplane is called a supporting hyperplane of a convex set if it intersects the convex set, andsupporting hyperplane the convex set is contained on just one side of it. Recall that we can ﬁll up a convex function to obtain the epigraph, which is a convex set. Therefore, we can also describe convex functions in terms of their supporting hyper- planes. Furthermore, observe that the supporting hyperplane just touches the convex function, and is in fact the tangent to the function at that point. And recall that the tangent of a function f (x) at a given point x0 is the evaluation of the gradient of that function at that point df (x) dx ∣ ∣ ∣x=x0. In summary, because convex sets can be equivalently described by its sup- porting hyperplanes, convex functions can be equivalently described by a function of their gradient. The Legendre transform formalizes this conceptLegendre transform .Physics students are often introduced to the Legendre transform as relating the Lagrangian and the Hamiltonian in classical mechanics. We begin with the most general deﬁnition, which unfortunately has a counter-intuitive form, and look at special cases to relate the deﬁnition to the intuition described in the preceding paragraph. The Legendre–Fenchel Legendre–Fenchel transform transform is a transformation (in the sense of a Fourier transform) from a convex differentiable function f (x) to a function that depends on the tangents s(x) = ∇xf (x). It is worth stressing that this is a transformation of the function f (·) and not the variable x or the function evaluated at x. The Legendre–Fenchel transform is also known as the convex conjugateconvex conjugate (for reasons we will see soon) and is closely related to duality (Hiriart- Urruty and Lemar´echal, 2001, chapter 5). Deﬁnition 7.4. The convex conjugate of a function f : RD → R is aconvex conjugate function f ∗ deﬁned by f ∗(s) = sup x∈RD (⟨s, x⟩ − f (x)) . (7.53) Note that the preceding convex conjugate deﬁnition does not need the function f to be convex nor differentiable. In Deﬁnition 7.4, we have used a general inner product (Section 3.2) but in the rest of this section we Draft (2019-12-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com.","libVersion":"0.3.2","langs":""}